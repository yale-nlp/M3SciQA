{"question_anchor": "Which large language model achieves a lower HVI score than OPT but a higher HVI score than Alpaca?", "answer_anchor": "GPT-4", "evidence_anchor": "locality/2310.04988/HVI_figure.png", "anchor_id": "2310.04988", "reference_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which LLM shown in the figure has a model size of 1.7T?", "answer_anchor": "GPT-4", "evidence_anchor": "locality/2310.04988/HVI_figure.png", "anchor_id": "2310.04988", "reference_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which large language model corresponds to an HVI score of 47?", "answer_anchor": "GPT-4", "evidence_anchor": "locality/2310.04988/HVI_figure.png", "anchor_id": "2310.04988", "reference_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which large language model has the second lowest HVI score among those in the figure corresponding to a purple bar?", "answer_anchor": "GPT-4", "evidence_anchor": "locality/2310.04988/HVI_figure.png", "anchor_id": "2310.04988", "reference_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model in the figure has the lowest diversity score for each of p1, p2, p3, and p4?", "answer_anchor": "T5-Large", "evidence_anchor": "locality/2310.05030/diversity_score.png", "anchor_id": "2310.05030", "reference_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model corresponds to the green dashed line?", "answer_anchor": "T5-Large", "evidence_anchor": "locality/2310.05030/diversity_score.png", "anchor_id": "2310.05030", "reference_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model has a diversity score of 2.58 for p2?", "answer_anchor": "T5-Large", "evidence_anchor": "locality/2310.05030/diversity_score.png", "anchor_id": "2310.05030", "reference_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method exhibits a score of 34.9 in the Acc-7 metric on MOSI?", "answer_anchor": "TFN", "evidence_anchor": "locality/2310.05804/comparison_table.png", "anchor_id": "2310.05804", "reference_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method corresponds to the first row of the table?", "answer_anchor": "TFN", "evidence_anchor": "locality/2310.05804/comparison_table.png", "anchor_id": "2310.05804", "reference_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method demonstrates the second lowest Acc-7 score on MOSI?", "answer_anchor": "TFN", "evidence_anchor": "locality/2310.05804/comparison_table.png", "anchor_id": "2310.05804", "reference_id": "1806.00064", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method corresponds to the second highest Acc-5 score on MOSI?", "answer_anchor": "Self-MM", "evidence_anchor": "locality/2310.05804/comparison_table.png", "anchor_id": "2310.05804", "reference_id": "2102.04830", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which optimization method exhibits the second-highest reward accuracy?", "answer_anchor": "DPO", "evidence_anchor": "locality/2310.05857/comparison_dpo.png", "anchor_id": "2310.05857", "reference_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "What optimization method exhibits an R2 score of 0.191?", "answer_anchor": "DPO", "evidence_anchor": "locality/2310.05857/comparison_dpo.png", "anchor_id": "2310.05857", "reference_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which optimization method has the lowest BLEU score among all models in the table?", "answer_anchor": "Transformer base", "evidence_anchor": "locality/2310.07096/BLEU.png", "anchor_id": "2310.07096", "reference_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which optimization method has a BLEU score of 27.3?", "answer_anchor": "Transformer base", "evidence_anchor": "locality/2310.07096/BLEU.png", "anchor_id": "2310.07096", "reference_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model corresponds to the fifth row of the table?", "answer_anchor": "UT", "evidence_anchor": "locality/2310.07096/BLEU.png", "anchor_id": "2310.07096", "reference_id": "1807.03819", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model has the second lowest MCD 1 score?", "answer_anchor": "T5-based UT", "evidence_anchor": "locality/2310.07096/CFQ_table.png", "anchor_id": "2310.07096", "reference_id": "2112.00578", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model results in the highest MCD 1 score?", "answer_anchor": "Dangle", "evidence_anchor": "locality/2310.07096/CFQ_table.png", "anchor_id": "2310.07096", "reference_id": "2110.04655", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has a 6-layer encoder and a 6-layer decoder architecture?", "answer_anchor": "FSMT", "evidence_anchor": "locality/2310.07188/overall_performance_table.png", "anchor_id": "2310.07188", "reference_id": "1907.06616", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset is used for the 'dialogue response' task?", "answer_anchor": "SODA", "evidence_anchor": "locality/2310.07188/overall_performance_table.png", "anchor_id": "2310.07188", "reference_id": "2212.10465", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model performs the best on the BBBP dataset?", "answer_anchor": "MolXPT", "evidence_anchor": "locality/2310.07276/performance_table.png", "anchor_id": "2310.07276", "reference_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model performs the second best in the ClinTox dataset?", "answer_anchor": "MolXPT", "evidence_anchor": "locality/2310.07276/performance_table.png", "anchor_id": "2310.07276", "reference_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model is shown on the penultimate line of the table?", "answer_anchor": "MolXPT", "evidence_anchor": "locality/2310.07276/performance_table.png", "anchor_id": "2310.07276", "reference_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model is shown on the first line of the table?", "answer_anchor": "BERT", "evidence_anchor": "locality/2310.08298/overall_performance.png", "anchor_id": "2310.08298", "reference_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model exhibits the highest score on the BC5CDR (Big Dict) and BC5CDR (Small Dict) datasets?", "answer_anchor": "BERT", "evidence_anchor": "locality/2310.08298/overall_performance.png", "anchor_id": "2310.08298", "reference_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which Seq2Seq/Tree model shows the highest Test Accuracy?", "answer_anchor": "Ana-CL", "evidence_anchor": "locality/2310.09619/MathQA_result_table.png", "anchor_id": "2310.09619", "reference_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which Seq2Seq/Tree model has a Test Accuracy of 79.6?", "answer_anchor": "Ana-CL", "evidence_anchor": "locality/2310.09619/MathQA_result_table.png", "anchor_id": "2310.09619", "reference_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model is on the last line of the Seq2Seq/Tree block of the table?", "answer_anchor": "Ana-CL", "evidence_anchor": "locality/2310.09619/MathQA_result_table.png", "anchor_id": "2310.09619", "reference_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model in the LLM section of the table has the highest test accuracy?", "answer_anchor": "Self-Consistency", "evidence_anchor": "locality/2310.09619/MathQA_result_table.png", "anchor_id": "2310.09619", "reference_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which LLM model has a test accuracy of 50.7?", "answer_anchor": "Self-Consistency", "evidence_anchor": "locality/2310.09619/MathQA_result_table.png", "anchor_id": "2310.09619", "reference_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which Seq2Exp model exhibits the highest test accuracy?", "answer_anchor": "Elastic", "evidence_anchor": "locality/2310.09619/MathQA_result_table.png", "anchor_id": "2310.09619", "reference_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which Seq2Exp model is marked with the Club citation symbol?", "answer_anchor": "Elastic", "evidence_anchor": "locality/2310.09619/MathQA_result_table.png", "anchor_id": "2310.09619", "reference_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model in the LLM section of the table corresponds to the highest test accuracy?", "answer_anchor": "Self-Consistency", "evidence_anchor": "locality/2310.09619/Math23k_result_table.png", "anchor_id": "2310.09619", "reference_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has a lower mean classification accuracy than VIBE but higher mean classification accuracy than UDALM on the Stance dataset?", "answer_anchor": "DPT", "evidence_anchor": "locality/2310.10191/classification_accuracy_table.png", "anchor_id": "2310.10191", "reference_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model achieves a mean classification accuracy of 0.8173 on the Stance dataset?", "answer_anchor": "DPT", "evidence_anchor": "locality/2310.10191/classification_accuracy_table.png", "anchor_id": "2310.10191", "reference_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model achieves a mean classification accuracy of 0.6712 on the Hate dataset?", "answer_anchor": "DPT", "evidence_anchor": "locality/2310.10191/classification_accuracy_table.png", "anchor_id": "2310.10191", "reference_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model shown in the figure corresponds to the green line?", "answer_anchor": "DPT", "evidence_anchor": "locality/2310.10191/accuracy_figure.png", "anchor_id": "2310.10191", "reference_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model achieves the second highest score in the Stance column?", "answer_anchor": "DPT", "evidence_anchor": "locality/2310.10191/classification_accuracy_table.png", "anchor_id": "2310.10191", "reference_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has a score lower than 0.82 but higher than 0.815 in the Stance column?", "answer_anchor": "DPT", "evidence_anchor": "locality/2310.10191/classification_accuracy_table.png", "anchor_id": "2310.10191", "reference_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model is in the second-to-last row of the table?", "answer_anchor": "DPT", "evidence_anchor": "locality/2310.10191/classification_accuracy_table.png", "anchor_id": "2310.10191", "reference_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model shown in the figure is represented by the pink line?", "answer_anchor": "LLaMA-30B", "evidence_anchor": "locality/2310.11634/average_relative_performance.png", "anchor_id": "2310.11634", "reference_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model shown in the figure performs most similarly to GPT-3.5-Turbo for the 'Plausible' and 'Foreign' scenarios?", "answer_anchor": "LLaMA-30B", "evidence_anchor": "locality/2310.11634/average_relative_performance.png", "anchor_id": "2310.11634", "reference_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model shows the best overall performance in the 'Foreign' scenario?", "answer_anchor": "LLaMA-30B", "evidence_anchor": "locality/2310.11634/average_relative_performance.png", "anchor_id": "2310.11634", "reference_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model in the figure corresponds to the grey line with a star marker?", "answer_anchor": "StarCoder", "evidence_anchor": "locality/2310.11634/average_relative_performance.png", "anchor_id": "2310.11634", "reference_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model shown in the figure is consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B?", "answer_anchor": "StarCoder", "evidence_anchor": "locality/2310.11634/average_relative_performance.png", "anchor_id": "2310.11634", "reference_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model exhibits the second best execution accuracy in few-shot prompting?", "answer_anchor": "StarCoder", "evidence_anchor": "locality/2310.11634/execution_accuracy_figure.png", "anchor_id": "2310.11634", "reference_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model exhibits the second best execution accuracy in direct prompting?", "answer_anchor": "StarCoder", "evidence_anchor": "locality/2310.11634/execution_accuracy_figure.png", "anchor_id": "2310.11634", "reference_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model achieves the lowest execution accuracy in few-shot prompting?", "answer_anchor": "LLaMA-7B", "evidence_anchor": "locality/2310.11634/execution_accuracy_figure.png", "anchor_id": "2310.11634", "reference_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model achieves the lowest execution accuracy in direct prompting?", "answer_anchor": "LLaMA-7B", "evidence_anchor": "locality/2310.11634/execution_accuracy_figure.png", "anchor_id": "2310.11634", "reference_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has the highest relative performance in few-shot prompting?", "answer_anchor": "LLaMA-30B", "evidence_anchor": "locality/2310.11634/prompt_setting_figure.png", "anchor_id": "2310.11634", "reference_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model corresponds to the brown bars in the figure?", "answer_anchor": "LLaMA-30B", "evidence_anchor": "locality/2310.11634/prompt_setting_figure.png", "anchor_id": "2310.11634", "reference_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model is represented by the lavender color in the figure?", "answer_anchor": "StarCoder", "evidence_anchor": "locality/2310.11634/prompt_setting_figure.png", "anchor_id": "2310.11634", "reference_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model achieves a score of 21.073 in 10-shot prompting?", "answer_anchor": "RoBERTa", "evidence_anchor": "locality/2310.11715/few-shot_NER_table.png", "anchor_id": "2310.11715", "reference_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which supervised method achieves the highest score in 100-shot prompting?", "answer_anchor": "RoBERTa", "evidence_anchor": "locality/2310.11715/few-shot_NER_table.png", "anchor_id": "2310.11715", "reference_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which supervised method results in the lowest score in 10-shot prompting?", "answer_anchor": "RoBERTa", "evidence_anchor": "locality/2310.11715/few-shot_NER_table.png", "anchor_id": "2310.11715", "reference_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method achieves the highest score in 10-shot prompting?", "answer_anchor": "LSFS", "evidence_anchor": "locality/2310.11715/few-shot_NER_table.png", "anchor_id": "2310.11715", "reference_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method corresponds to the penultimate row of the table?", "answer_anchor": "LSFS", "evidence_anchor": "locality/2310.11715/few-shot_NER_table.png", "anchor_id": "2310.11715", "reference_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method shows the lowest overall performance?", "answer_anchor": "SEQ2SEQ", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has a score of 3.1 in the Seen, Val, SR dataset?", "answer_anchor": "SEQ2SEQ", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method corresponds to the first row of the table?", "answer_anchor": "SEQ2SEQ", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method has the second lowest overall performance for the Seen condition?", "answer_anchor": "MOCA", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method corresopnds to a score of 25.9 in the Seen, Val, SR dataset?", "answer_anchor": "MOCA", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method exhibits the highest score in the Seen, Val, SR dataset?", "answer_anchor": "EmBERT", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method exhibits the highest score in the Seen, Val, GC dataset?", "answer_anchor": "EmBERT", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method achieves a score of 31.8 in the Seen, Test, SR dataset?", "answer_anchor": "EmBERT", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method results in a better score than MOCA but worse score than LACMA in the Seen, Val, SR dataset?", "answer_anchor": "E.T.", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method corresponds to a score of 42.0 in the Seen, Val, GC dataset?", "answer_anchor": "E.T.", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method results in a score of 14.9 in the Unseen, Test, GC dataset?", "answer_anchor": "E.T.", "evidence_anchor": "locality/2310.12344/ALFRED_table.png", "anchor_id": "2310.12344", "reference_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "In which dataset does KALMV achieve a score of 70.83 for the XL model?", "answer_anchor": "Mintaka", "evidence_anchor": "locality/2310.12836/results_table.png", "anchor_id": "2310.12836", "reference_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "In which dataset does KALMV achieve a score of 66.48 for the Large model?", "answer_anchor": "Mintaka", "evidence_anchor": "locality/2310.12836/results_table.png", "anchor_id": "2310.12836", "reference_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset corresponds to the second chart from the left?", "answer_anchor": "Mintaka", "evidence_anchor": "locality/2310.12836/ratio_figure.png", "anchor_id": "2310.12836", "reference_id": "2210.01613", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method shown in the figure corresponds to the solid red line?", "answer_anchor": "UniEval", "evidence_anchor": "locality/2310.13189/calibration_figure.png", "anchor_id": "2310.13189", "reference_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model depicted in the figure exhibits the highest fluctuation?", "answer_anchor": "UniEval", "evidence_anchor": "locality/2310.13189/calibration_figure.png", "anchor_id": "2310.13189", "reference_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has the highest performance on the En-De task in the Test2016 dataset in Previous Image-must Systems?", "answer_anchor": "VALHALLA", "evidence_anchor": "locality/2310.13361/result_table.png", "anchor_id": "2310.13361", "reference_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has the highest performance on the En-Fr task in the MSCOCO dataset?", "answer_anchor": "VALHALLA", "evidence_anchor": "locality/2310.13361/result_table.png", "anchor_id": "2310.13361", "reference_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has an overall average score of 45.68 in Previous Image-free Systems?", "answer_anchor": "VALHALLA", "evidence_anchor": "locality/2310.13361/result_table.png", "anchor_id": "2310.13361", "reference_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset has the fewest number of languages but the most number of SM tasks?", "answer_anchor": "SentiEval", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset has 1 language but 13 SM tasks?", "answer_anchor": "SentiEval", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset is located in the top left of the figure?", "answer_anchor": "SentiEval", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which Twitter dataset has the most number of languages compared to all other Twitter datasets?", "answer_anchor": "AfriSenti", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset has 1 SM task and 14 languages?", "answer_anchor": "AfriSenti", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset corresponds to the largest blue circle label?", "answer_anchor": "AfriSenti", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset is located at the bottom left of the figure?", "answer_anchor": "NaijaSenti", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which dataset is represented by the smallest blue circle?", "answer_anchor": "NaijaSenti", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which dataset includes 1 SM task and 4 languages?", "answer_anchor": "NaijaSenti", "evidence_anchor": "locality/2310.14557/comparison_figure.png", "anchor_id": "2310.14557", "reference_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset is represented by the leftmost bar in the figure?", "answer_anchor": "GSM8K", "evidence_anchor": "locality/2310.14628/comparison_figure.png", "anchor_id": "2310.14628", "reference_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which dataset exhibits the highest Method 1 accuracy?", "answer_anchor": "GSM8K", "evidence_anchor": "locality/2310.14628/comparison_figure.png", "anchor_id": "2310.14628", "reference_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset exhibits the highest accuracy for Method 2?", "answer_anchor": "GSM8K", "evidence_anchor": "locality/2310.14628/comparison_figure.png", "anchor_id": "2310.14628", "reference_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model is represented by the orange bar?", "answer_anchor": "Vanilla Transformer", "evidence_anchor": "locality/2310.15040/accuracy_figure.png", "anchor_id": "2310.15040", "reference_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model demonstrates the lowest accuracy in the SLOG-all dataset?", "answer_anchor": "Vanilla Transformer", "evidence_anchor": "locality/2310.15040/accuracy_figure.png", "anchor_id": "2310.15040", "reference_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model results in the second lowest accuracy in the COGS-all dataset?", "answer_anchor": "Vanilla Transformer", "evidence_anchor": "locality/2310.15040/accuracy_figure.png", "anchor_id": "2310.15040", "reference_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model shows 0 accuracy in the COGS-structural dataset?", "answer_anchor": "Vanilla Transformer", "evidence_anchor": "locality/2310.15040/accuracy_figure.png", "anchor_id": "2310.15040", "reference_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model is represented by the blue bar?", "answer_anchor": "T5", "evidence_anchor": "locality/2310.15040/accuracy_figure.png", "anchor_id": "2310.15040", "reference_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model has the highest accuracy in the COGS-all dataset?", "answer_anchor": "T5", "evidence_anchor": "locality/2310.15040/accuracy_figure.png", "anchor_id": "2310.15040", "reference_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model achieves a score of 3.84 in the Grounding task?", "answer_anchor": "MVQG-VL-T5", "evidence_anchor": "locality/2310.15129/human_eval_table.png", "anchor_id": "2310.15129", "reference_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model labeling 'fine-tuned' is shown in the table?", "answer_anchor": "MVQG-VL-T5", "evidence_anchor": "locality/2310.15129/human_eval_table.png", "anchor_id": "2310.15129", "reference_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model shown in the table has an overall score of less than 3.80?", "answer_anchor": "MVQG-VL-T5", "evidence_anchor": "locality/2310.15129/human_eval_table.png", "anchor_id": "2310.15129", "reference_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model is shown in the first row of the table?", "answer_anchor": "MVQG-VL-T5", "evidence_anchor": "locality/2310.15129/human_eval_table.png", "anchor_id": "2310.15129", "reference_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method has an average score of 82.8 with zero-shot prompting?", "answer_anchor": "SALAM", "evidence_anchor": "locality/2310.15746/comparison_table.png", "anchor_id": "2310.15746", "reference_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is directly above the dashed line in few-shot prompting?", "answer_anchor": "SALAM", "evidence_anchor": "locality/2310.15746/comparison_table.png", "anchor_id": "2310.15746", "reference_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method demonstrates the second highest score in the TweetEval Irony dataset for both zero-shot and few-shot prompting?", "answer_anchor": "SALAM", "evidence_anchor": "locality/2310.15746/comparison_2_table.png", "anchor_id": "2310.15746", "reference_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is represented by the blue line in the figure?", "answer_anchor": "EARL", "evidence_anchor": "locality/2310.15797/performance_comparison_figure.png", "anchor_id": "2310.15797", "reference_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method consistently achieves a higher MRR score than NodePiece?", "answer_anchor": "EARL", "evidence_anchor": "locality/2310.15797/performance_comparison_figure.png", "anchor_id": "2310.15797", "reference_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method achieves a relatively constant MRR score in the FB15k-237 dataset as entity code entropy increases?", "answer_anchor": "EARL", "evidence_anchor": "locality/2310.15797/performance_comparison_figure.png", "anchor_id": "2310.15797", "reference_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is represented by the green line in the figure?", "answer_anchor": "EARL Quantization", "evidence_anchor": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_id": "2310.15797", "reference_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method achieves the lowest J_k scores in the WN18RR dataset?", "answer_anchor": "EARL", "evidence_anchor": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_id": "2310.15797", "reference_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has a lower J_k score than Random Entity Quantization but a higher J_k score than NodePiece in the FB15k-237 dataset for all values of k between 400 and 1000?", "answer_anchor": "EARL", "evidence_anchor": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_id": "2310.15797", "reference_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model corresponds to the first row of the table?", "answer_anchor": "EQG", "evidence_anchor": "locality/2310.16446/tell_me_why_table.png", "anchor_id": "2310.16446", "reference_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model results in the highest Self-BLEU score on the TellMeWhy dataset?", "answer_anchor": "EQG", "evidence_anchor": "locality/2310.16446/tell_me_why_table.png", "anchor_id": "2310.16446", "reference_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model corresponds to the lowest BERTScore F1 score on the TellMeWhy dataset?", "answer_anchor": "EQG", "evidence_anchor": "locality/2310.16446/tell_me_why_table.png", "anchor_id": "2310.16446", "reference_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model achieves a BLEURT score of 0.4126?", "answer_anchor": "EQG", "evidence_anchor": "locality/2310.16446/tell_me_why_table.png", "anchor_id": "2310.16446", "reference_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which quantization method achieves a score of 76.3 on Deit-S with a Weight/Activation (W/A) precision of 6/6?", "answer_anchor": "PTQ4ViT", "evidence_anchor": "locality/2310.16836/comparison_table.png", "anchor_id": "2310.16836", "reference_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which quantization method achieves a score of 80.3 on Deit-B with a Weight/Activation (W/A) precision of 6/6?", "answer_anchor": "PTQ4ViT", "evidence_anchor": "locality/2310.16836/comparison_table.png", "anchor_id": "2310.16836", "reference_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which quant method achieves a lower score than APQ-ViT but still scores higher than 76.0 on Deit-S with a Weight/Activation (W/A) precision of 6/6?", "answer_anchor": "PTQ4ViT", "evidence_anchor": "locality/2310.16836/comparison_table.png", "anchor_id": "2310.16836", "reference_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which LLM model demonstrates the lowest MSE score?", "answer_anchor": "GPT-4", "evidence_anchor": "locality/2310.17428/mse_table.png", "anchor_id": "2310.17428", "reference_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which LLM model corresponds to an r score of 0.813?", "answer_anchor": "GPT-4", "evidence_anchor": "locality/2310.17428/mse_table.png", "anchor_id": "2310.17428", "reference_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model achieves the highest score on the MNLI dataset?", "answer_anchor": "BERT", "evidence_anchor": "locality/2310.18343/result_table.png", "anchor_id": "2310.18343", "reference_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model achieves the highest score on the SST-2 dataset?", "answer_anchor": "BERT", "evidence_anchor": "locality/2310.18343/result_table.png", "anchor_id": "2310.18343", "reference_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model corresponds to the first row of the table?", "answer_anchor": "BERT", "evidence_anchor": "locality/2310.18343/result_table.png", "anchor_id": "2310.18343", "reference_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which benchmark results in the highest execution accuracy for this method?", "answer_anchor": "BIRD", "evidence_anchor": "locality/2310.18538/result_table.png", "anchor_id": "2310.18538", "reference_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is in the third row of the table?", "answer_anchor": "BIRD", "evidence_anchor": "locality/2310.18538/result_table.png", "anchor_id": "2310.18538", "reference_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method achieves sentence-level precision of 60.32?", "answer_anchor": "longformer", "evidence_anchor": "locality/2310.18544/result_table.png", "anchor_id": "2310.18544", "reference_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method results in a token-level F1 score equal to 37.03?", "answer_anchor": "longformer", "evidence_anchor": "locality/2310.18544/result_table.png", "anchor_id": "2310.18544", "reference_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has an F1 score of 64.95 on PDTB-Top?", "answer_anchor": "PCP", "evidence_anchor": "locality/2311.00367/result_table.png", "anchor_id": "2311.00367", "reference_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method corresponds to a higher F1 score than that of LDSGM but a lower F1 score than 65.76 for PDTB-Top?", "answer_anchor": "PCP", "evidence_anchor": "locality/2311.00367/result_table.png", "anchor_id": "2311.00367", "reference_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model is seventh in the table?", "answer_anchor": "PCP", "evidence_anchor": "locality/2311.00367/result_table.png", "anchor_id": "2311.00367", "reference_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model corresponds to an F1 score of 65.76 on PDTB-Top?", "answer_anchor": "GOLF", "evidence_anchor": "locality/2311.00367/result_table.png", "anchor_id": "2311.00367", "reference_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model has an F1 score higher than PCP's but lower than DiscoPrompt's on PDTB-Top?", "answer_anchor": "GOLF", "evidence_anchor": "locality/2311.00367/result_table.png", "anchor_id": "2311.00367", "reference_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is listed in the table right below the PCP method?", "answer_anchor": "GOLF", "evidence_anchor": "locality/2311.00367/result_table.png", "anchor_id": "2311.00367", "reference_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method in the table corresponds to the highest ROUGE 2 score?", "answer_anchor": "PEGASUS+NTM", "evidence_anchor": "locality/2311.00588/comparison_table.png", "anchor_id": "2311.00588", "reference_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method in the table corresponds to a ROUGE L score equal to 41.39?", "answer_anchor": "PEGASUS+NTM", "evidence_anchor": "locality/2311.00588/comparison_table.png", "anchor_id": "2311.00588", "reference_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method in the table corresponds to a ROUGE-1 score equal to 44.52?", "answer_anchor": "PEGASUS+NTM", "evidence_anchor": "locality/2311.00588/comparison_table.png", "anchor_id": "2311.00588", "reference_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method exhibits the highest Accuracy@0.5 value on the RefCOCOg task?", "answer_anchor": "UniTAB", "evidence_anchor": "locality/2311.04067/result_table.png", "anchor_id": "2311.04067", "reference_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method exhibits an accuracy score of 71.0 on the VQA-v2 task?", "answer_anchor": "UniTAB", "evidence_anchor": "locality/2311.04067/result_table.png", "anchor_id": "2311.04067", "reference_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is listed in the table below the VL-BART method and above the OFA-base method?", "answer_anchor": "UniTAB", "evidence_anchor": "locality/2311.04067/result_table.png", "anchor_id": "2311.04067", "reference_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method's results are displayed in a lighter grey color in the table?", "answer_anchor": "OFA-base", "evidence_anchor": "locality/2311.04067/result_table.png", "anchor_id": "2311.04067", "reference_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method has an accuracy of 78.1% on the VQA-v2 task?", "answer_anchor": "OFA-base", "evidence_anchor": "locality/2311.04067/result_table.png", "anchor_id": "2311.04067", "reference_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method exhibits the highest accuracy on the VQA-v2 task?", "answer_anchor": "OFA-base", "evidence_anchor": "locality/2311.04067/result_table.png", "anchor_id": "2311.04067", "reference_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "For which method is the BLEU-1 score missing in the table?", "answer_anchor": "ViTCAP", "evidence_anchor": "locality/2311.08223/result_table.png", "anchor_id": "2311.08223", "reference_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a METEOR cross entropy score lower than ours but higher than X-Transformer?", "answer_anchor": "ViTCAP", "evidence_anchor": "locality/2311.08223/result_table.png", "anchor_id": "2311.08223", "reference_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method corresponds to the penultimate row of the table?", "answer_anchor": "ViTCAP", "evidence_anchor": "locality/2311.08223/result_table.png", "anchor_id": "2311.08223", "reference_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "What is the name of the score described as a \"fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits) by the model\"?", "answer_anchor": "PVI", "evidence_anchor": "locality/2311.16298/description_table.png", "anchor_id": "2311.16298", "reference_id": "2110.08420", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "What is the name of the score described as the 'influence of any example z towards another example z' by tracking their gradient dot products,' where 'we generate the self-influence scores where z = z'?", "answer_anchor": "TracIn", "evidence_anchor": "locality/2311.16298/description_table.png", "anchor_id": "2311.16298", "reference_id": "2002.08484", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method does not have results in the CSQA2.0 dev and StrategyQA test tasks shown in the table?", "answer_anchor": "ChatGPT", "evidence_anchor": "locality/2311.18397/result_table.png", "anchor_id": "2311.18397", "reference_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method demonstrates the lowest score in the CSQA2.0 dev/dev* task?", "answer_anchor": "ChatGPT", "evidence_anchor": "locality/2311.18397/result_table.png", "anchor_id": "2311.18397", "reference_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset contains 3,747,569 instances?", "answer_anchor": "OASUM", "evidence_anchor": "locality/2312.04440/dataset_table.png", "anchor_id": "2312.04440", "reference_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset has the largest number of instances?", "answer_anchor": "OASUM", "evidence_anchor": "locality/2312.04440/dataset_table.png", "anchor_id": "2312.04440", "reference_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset has the largest number of Queries|Aspects in the OABS category?", "answer_anchor": "OASUM", "evidence_anchor": "locality/2312.04440/dataset_table.png", "anchor_id": "2312.04440", "reference_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset has the largest number of Queries|Aspects in the ABS category?", "answer_anchor": "WikiAsp", "evidence_anchor": "locality/2312.04440/dataset_table.png", "anchor_id": "2312.04440", "reference_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset has the largest number of instances in the ABS category?", "answer_anchor": "WikiAsp", "evidence_anchor": "locality/2312.04440/dataset_table.png", "anchor_id": "2312.04440", "reference_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method exhibits a FLAN-T5 score of 52.4% using SGD in 24 domains?", "answer_anchor": "MSG^2", "evidence_anchor": "locality/2312.04668/comparison_table.png", "anchor_id": "2312.04668", "reference_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves a higher score than No Graph but a lower score than TOD-Flow using GPT-turbo with SGD in 24 domains?", "answer_anchor": "MSG^2", "evidence_anchor": "locality/2312.04668/comparison_table.png", "anchor_id": "2312.04668", "reference_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method in the figure corresponds to the penultimate row?", "answer_anchor": "MSG^2", "evidence_anchor": "locality/2312.04668/comparison_table.png", "anchor_id": "2312.04668", "reference_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model in the table corresponds to a 12.79% TP?", "answer_anchor": "GRIT", "evidence_anchor": "locality/2312.11523/comparison_table.png", "anchor_id": "2312.11523", "reference_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model in the table corresponds to a 84.70% WInToRe?", "answer_anchor": "GRIT", "evidence_anchor": "locality/2312.11523/comparison_table.png", "anchor_id": "2312.11523", "reference_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model achieves a higher TP score than GIT but a lower TP score than LLaVA?", "answer_anchor": "GRIT", "evidence_anchor": "locality/2312.11523/comparison_table.png", "anchor_id": "2312.11523", "reference_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "What method is demonstrated by the solid lavender line?", "answer_anchor": "GRIT", "evidence_anchor": "locality/2312.11523/figure.png", "anchor_id": "2312.11523", "reference_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method in the figure demonstrates the highest Toxicity Probability Score when the number of samples equals 1M?", "answer_anchor": "GRIT", "evidence_anchor": "locality/2312.11523/figure.png", "anchor_id": "2312.11523", "reference_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model in the table has 12M updated parameters?", "answer_anchor": "UniKGQA", "evidence_anchor": "locality/2401.00158/comparison_table.png", "anchor_id": "2401.00158", "reference_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves the highest Hits@1 score in the MQA-1H dataset?", "answer_anchor": "UniKGQA", "evidence_anchor": "locality/2401.00158/comparison_table.png", "anchor_id": "2401.00158", "reference_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has the largest number of updated parameters?", "answer_anchor": "SR+NSM+E2E", "evidence_anchor": "locality/2401.00158/comparison_table.png", "anchor_id": "2401.00158", "reference_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model exhibits a 64.1 F1 score in the WebQSP dataset?", "answer_anchor": "SR+NSM+E2E", "evidence_anchor": "locality/2401.00158/comparison_table.png", "anchor_id": "2401.00158", "reference_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model is placed below TransferNet but above UniKGQA in the table?", "answer_anchor": "SR+NSM+E2E", "evidence_anchor": "locality/2401.00158/comparison_table.png", "anchor_id": "2401.00158", "reference_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method corresponds to the orange line in the figure?", "answer_anchor": "UniKGQA", "evidence_anchor": "locality/2401.00158/comparison_figure.png", "anchor_id": "2401.00158", "reference_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method in the figure is represented by the 'x' (cross) marker?", "answer_anchor": "UniKGQA", "evidence_anchor": "locality/2401.00158/comparison_figure.png", "anchor_id": "2401.00158", "reference_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method has a lower Hits@1 score than ReasoningLM but a higher Hits@1 score than NSM across all fine-tuning samples?", "answer_anchor": "UniKGQA", "evidence_anchor": "locality/2401.00158/comparison_figure.png", "anchor_id": "2401.00158", "reference_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has a Hits@1 score ranging from 40% to 55% across the different fine-tuning samples shown in the figure?", "answer_anchor": "UniKGQA", "evidence_anchor": "locality/2401.00158/comparison_figure.png", "anchor_id": "2401.00158", "reference_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset in the table has the fewest dialogues?", "answer_anchor": "Multialpaca", "evidence_anchor": "locality/2402.04588/comparison_table.png", "anchor_id": "2402.04588", "reference_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset in the table has the fewest number of turns?", "answer_anchor": "Multialpaca", "evidence_anchor": "locality/2402.04588/comparison_table.png", "anchor_id": "2402.04588", "reference_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset in the table has the second shortest average question length?", "answer_anchor": "Multialpaca", "evidence_anchor": "locality/2402.04588/comparison_table.png", "anchor_id": "2402.04588", "reference_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset in the table has an average answer length of 83.71?", "answer_anchor": "Multialpaca", "evidence_anchor": "locality/2402.04588/comparison_table.png", "anchor_id": "2402.04588", "reference_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method results in a score of 22.4 in the GSM8K dataset?", "answer_anchor": "SpecialFT", "evidence_anchor": "locality/2310.05074/result_table.png", "anchor_id": "2310.05074", "reference_id": "2301.12726", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a GPT backbone and 7B parameters?", "answer_anchor": "DecomDistill", "evidence_anchor": "locality/2310.05074/result_table.png", "anchor_id": "2310.05074", "reference_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method shown in the table is above the 'Magister et al' row but below the 'UL2' row?", "answer_anchor": "DecomDistill", "evidence_anchor": "locality/2310.05074/result_table.png", "anchor_id": "2310.05074", "reference_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method corresponds to the leftmost bar in the figure?", "answer_anchor": "MSP", "evidence_anchor": "locality/2310.05083/comparison_figure.png", "anchor_id": "2310.05083", "reference_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method achieves the highest score in the 'w/o p_out' setting?", "answer_anchor": "MSP", "evidence_anchor": "locality/2310.05083/comparison_figure.png", "anchor_id": "2310.05083", "reference_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which benchmark is represented by the triangle marker in the figure?", "answer_anchor": "BBH", "evidence_anchor": "locality/2310.05736/result_figure.png", "anchor_id": "2310.05736", "reference_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which benchmark corresponds to the light green color in the figure?", "answer_anchor": "BBH", "evidence_anchor": "locality/2310.05736/result_figure.png", "anchor_id": "2310.05736", "reference_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which benchmark has a higher 'Generation Token Length' than ShareGPT and GSM8k?", "answer_anchor": "BBH", "evidence_anchor": "locality/2310.05736/result_figure.png", "anchor_id": "2310.05736", "reference_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which benchmark has the highest 'Generation Token Length' in the figure?", "answer_anchor": "BBH", "evidence_anchor": "locality/2310.05736/result_figure.png", "anchor_id": "2310.05736", "reference_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which benchmark achieves a 'Generation Token Length' near 400 when the 'Compression Ratio' is 1?", "answer_anchor": "BBH", "evidence_anchor": "locality/2310.05736/result_figure.png", "anchor_id": "2310.05736", "reference_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has the lowest MAE in the CH-SIMS task?", "answer_anchor": "Tensor Fusion", "evidence_anchor": "locality/2310.05804/result_table.png", "anchor_id": "2310.05804", "reference_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has the lowest score in the 'Original Persona' column?", "answer_anchor": "CSN-word", "evidence_anchor": "locality/2310.06390/result_table.png", "anchor_id": "2310.06390", "reference_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method corresponds to the third row of the table?", "answer_anchor": "CSN-word", "evidence_anchor": "locality/2310.06390/result_table.png", "anchor_id": "2310.06390", "reference_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method scores a 70.1 in the 'Revised Persona' column?", "answer_anchor": "CSN-word", "evidence_anchor": "locality/2310.06390/result_table.png", "anchor_id": "2310.06390", "reference_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model achieves the highest score in the 'T2' column?", "answer_anchor": "CEAR", "evidence_anchor": "locality/2310.06547/result_1_table.png", "anchor_id": "2310.06547", "reference_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model demonstrates the highest score in the 'T3' column?", "answer_anchor": "CEAR", "evidence_anchor": "locality/2310.06547/result_1_table.png", "anchor_id": "2310.06547", "reference_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model corresponds to the penultimate row of the table?", "answer_anchor": "CEAR", "evidence_anchor": "locality/2310.06547/result_1_table.png", "anchor_id": "2310.06547", "reference_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model scores higher than ACA but lower than RationaleCL in the 'T5' column?", "answer_anchor": "CEAR", "evidence_anchor": "locality/2310.06547/result_1_table.png", "anchor_id": "2310.06547", "reference_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model scores an 84.2 in the 'T10' column?", "answer_anchor": "CEAR", "evidence_anchor": "locality/2310.06547/result_1_table.png", "anchor_id": "2310.06547", "reference_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is shown in the first row of the table?", "answer_anchor": "PromptPG", "evidence_anchor": "locality/2310.06675/result_table.png", "anchor_id": "2310.06675", "reference_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method demonstrates the lowest EA score on the FinQA task?", "answer_anchor": "PromptPG", "evidence_anchor": "locality/2310.06675/result_table.png", "anchor_id": "2310.06675", "reference_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method achieves an F1 score with a mean of 58.86 in the TAT-QA task?", "answer_anchor": "PromptPG", "evidence_anchor": "locality/2310.06675/result_table.png", "anchor_id": "2310.06675", "reference_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves an average EA score of 67.07 in the FinQA task?", "answer_anchor": "KATE", "evidence_anchor": "locality/2310.06675/result_table.png", "anchor_id": "2310.06675", "reference_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves a higher EA score than Fixed set but a lower EA score than Diverse KATE in the FinQA task?", "answer_anchor": "KATE", "evidence_anchor": "locality/2310.06675/result_table.png", "anchor_id": "2310.06675", "reference_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is shown in the fifth row of the table?", "answer_anchor": "KATE", "evidence_anchor": "locality/2310.06675/result_table.png", "anchor_id": "2310.06675", "reference_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "What is the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category?", "answer_anchor": "ELLE", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2203.06311", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category?", "answer_anchor": "K-Adapter", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2002.01808", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category?", "answer_anchor": "CMR", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2205.02014", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage category?", "answer_anchor": "IC-Retrieval", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2210.09150", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category?", "answer_anchor": "IRCoT", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2212.10509", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the first method shown in Explicit --> Internet-enhanced category?", "answer_anchor": "Internet-Fewshot", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2203.05115", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category?", "answer_anchor": "Knowledge Solver", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2309.03118", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections category?", "answer_anchor": "MeLLo", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2305.14795", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "What is the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category?", "answer_anchor": "GRACE", "evidence_anchor": "locality/2310.07343/result_figure.png", "anchor_id": "2310.07343", "reference_id": "2211.11031", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which dataset is shown in the second row of the table?", "answer_anchor": "DuConv", "evidence_anchor": "locality/2310.07397/result_table.png", "anchor_id": "2310.07397", "reference_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which dataset has the largest number of dialogues?", "answer_anchor": "DuConv", "evidence_anchor": "locality/2310.07397/result_table.png", "anchor_id": "2310.07397", "reference_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset in the Movies domain only has a PF field?", "answer_anchor": "DuConv", "evidence_anchor": "locality/2310.07397/result_table.png", "anchor_id": "2310.07397", "reference_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has the highest score in the WQ-R task?", "answer_anchor": "DSM", "evidence_anchor": "locality/2310.08395/result_table.png", "anchor_id": "2310.08395", "reference_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is in the last row of the Full Training category?", "answer_anchor": "DSM", "evidence_anchor": "locality/2310.08395/result_table.png", "anchor_id": "2310.08395", "reference_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method achieves a score of 28.62 in the WQ-B task?", "answer_anchor": "DSM", "evidence_anchor": "locality/2310.08395/result_table.png", "anchor_id": "2310.08395", "reference_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is missing a result for the WQ-M task in the table?", "answer_anchor": "DSM", "evidence_anchor": "locality/2310.08395/result_table.png", "anchor_id": "2310.08395", "reference_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model has the highest Recall@7 score in the CamRest task?", "answer_anchor": "Q-TOD", "evidence_anchor": "locality/2310.08877/result_table.png", "anchor_id": "2310.08877", "reference_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has a Recall@7 score of 92.97 for the MWOZ task?", "answer_anchor": "Q-TOD", "evidence_anchor": "locality/2310.08877/result_table.png", "anchor_id": "2310.08877", "reference_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method shown in the table demonstrates the highest BLEU-1 score for the Test Seen task?", "answer_anchor": "SimCTG", "evidence_anchor": "locality/2310.08943/result_table.png", "anchor_id": "2310.08943", "reference_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method shown in the table achieves a PPL score of 24.466 for the Test Seen task?", "answer_anchor": "SimCTG", "evidence_anchor": "locality/2310.08943/result_table.png", "anchor_id": "2310.08943", "reference_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method corresponds to the fifth row of the table?", "answer_anchor": "SimCTG", "evidence_anchor": "locality/2310.08943/result_table.png", "anchor_id": "2310.08943", "reference_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method exhibits an accuracy of 79.44% in the CJO22 task?", "answer_anchor": "LADAN", "evidence_anchor": "locality/2310.09241/results_table.png", "anchor_id": "2310.09241", "reference_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has an accuracy of 82.82% in the CAIL2018 task?", "answer_anchor": "LADAN", "evidence_anchor": "locality/2310.09241/results_table.png", "anchor_id": "2310.09241", "reference_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is placed below the row for R-Former but above the row for NeurJudge?", "answer_anchor": "LADAN", "evidence_anchor": "locality/2310.09241/results_table.png", "anchor_id": "2310.09241", "reference_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method is represented by the lavender color?", "answer_anchor": "GeDi", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method is at the rightmost part of the figure?", "answer_anchor": "GeDi", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method has a perplexity of 60?", "answer_anchor": "GeDi", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has the highest perplexity?", "answer_anchor": "GeDi", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method in the figure corresponds to the green color?", "answer_anchor": "DExperts", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method in the figure has a perplexity of approximately 30 and an average max toxicity of 0.2?", "answer_anchor": "DExperts", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method corresponds to the brown color label in the figure?", "answer_anchor": "PPLM", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method has approximately 30 perplexity and the highest average max toxicity?", "answer_anchor": "PPLM", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has a perplexity of approximately 30 and an average max toxicity of around 0.4?", "answer_anchor": "PPLM", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method, with an average max toxicity of more than 0.3, is represented by a circle?", "answer_anchor": "PPLM", "evidence_anchor": "locality/2310.09520/comparison_figure.png", "anchor_id": "2310.09520", "reference_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method has 11.0 rounds to completion?", "answer_anchor": "MAPPO", "evidence_anchor": "locality/2310.10701/result_table.png", "anchor_id": "2310.10701", "reference_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has fewer 'rounds to completion' than GPT-4 + Belief but more 'rounds to completion' than the CBS planner?", "answer_anchor": "MAPPO", "evidence_anchor": "locality/2310.10701/result_table.png", "anchor_id": "2310.10701", "reference_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is shown in the fourth row of the table?", "answer_anchor": "MAPPO", "evidence_anchor": "locality/2310.10701/result_table.png", "anchor_id": "2310.10701", "reference_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method has an F1 score of 41.3?", "answer_anchor": "SPADE", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is in the second row of the table?", "answer_anchor": "SPADE", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a lower F1 score than Doc2Graph and a higher F1 score than GNN+MLP?", "answer_anchor": "SPADE", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has an F1 score of 53.36?", "answer_anchor": "Doc2Graph", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a lower F1 score than LayoutXLM and a higher F1 score than SPADE?", "answer_anchor": "Doc2Graph", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has an F1 score of 54.83?", "answer_anchor": "LayoutXLM", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a lower F1 score than SERA and a higher F1 score than Doc2Graph?", "answer_anchor": "LayoutXLM", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has an F1 score of 65.96?", "answer_anchor": "SERA", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a lower F1 score than BROS and a higher F1 score than LayoutXLM?", "answer_anchor": "SERA", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has an F1 score of 75?", "answer_anchor": "MSAU-PAF", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a lower F1 score than TPP and a higher F1 score than BROS?", "answer_anchor": "MSAU-PAF", "evidence_anchor": "locality/2310.11016/comparison_table.png", "anchor_id": "2310.11016", "reference_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset corresponds to the last row of the table?", "answer_anchor": "ViHOS", "evidence_anchor": "locality/2310.11166/comparison_table.png", "anchor_id": "2310.11166", "reference_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which dataset is associated with the task 'Hate Speech Spans Detection (HSSD)'?", "answer_anchor": "ViHOS", "evidence_anchor": "locality/2310.11166/comparison_table.png", "anchor_id": "2310.11166", "reference_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset has a training set size of 8,844?", "answer_anchor": "ViHOS", "evidence_anchor": "locality/2310.11166/comparison_table.png", "anchor_id": "2310.11166", "reference_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset has a test set size of 1,106?", "answer_anchor": "ViHOS", "evidence_anchor": "locality/2310.11166/comparison_table.png", "anchor_id": "2310.11166", "reference_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset has more dev set samples than UIT-VSMEC but fewer dev set samples than ViSpamReviews?", "answer_anchor": "ViHOS", "evidence_anchor": "locality/2310.11166/comparison_table.png", "anchor_id": "2310.11166", "reference_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has a macro-F1 score of 27.34?", "answer_anchor": "LegalBERT", "evidence_anchor": "locality/2310.11368/comparison_table.png", "anchor_id": "2310.11368", "reference_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model has a lower mac-F1 score than Longformer but a higher mac-F1 score than CaselawBERT?", "answer_anchor": "LegalBERT", "evidence_anchor": "locality/2310.11368/comparison_table.png", "anchor_id": "2310.11368", "reference_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model is placed fourth in the table?", "answer_anchor": "LegalBERT", "evidence_anchor": "locality/2310.11368/comparison_table.png", "anchor_id": "2310.11368", "reference_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which dataset in the table has a validation set size of 1250?", "answer_anchor": "XSumFaith", "evidence_anchor": "locality/2310.11648/comparison_table.png", "anchor_id": "2310.11648", "reference_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which dataset has fewer validation set samples than GoGenSum but more samples than FactCC?", "answer_anchor": "XSumFaith", "evidence_anchor": "locality/2310.11648/comparison_table.png", "anchor_id": "2310.11648", "reference_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which dataset is in the last row of the 'Inconsistency Detection' category?", "answer_anchor": "XSumFaith", "evidence_anchor": "locality/2310.11648/comparison_table.png", "anchor_id": "2310.11648", "reference_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method has the second-highest Avg score on the SuperGLUE task?", "answer_anchor": "HyperDecoder", "evidence_anchor": "locality/2310.11670/comparison_table.png", "anchor_id": "2310.11670", "reference_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has a CoLA score equal to 55.9 on the GLUE task?", "answer_anchor": "HyperDecoder", "evidence_anchor": "locality/2310.11670/comparison_table.png", "anchor_id": "2310.11670", "reference_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is placed directly above the PHA method in the table?", "answer_anchor": "HyperDecoder", "evidence_anchor": "locality/2310.11670/comparison_table.png", "anchor_id": "2310.11670", "reference_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method has 638K tunable parameters?", "answer_anchor": "Hyperformer", "evidence_anchor": "locality/2310.11670/comparison_table.png", "anchor_id": "2310.11670", "reference_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has the third highest Avg score on the GLUE task?", "answer_anchor": "Hyperformer", "evidence_anchor": "locality/2310.11670/comparison_table.png", "anchor_id": "2310.11670", "reference_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has a score of 73.6 in the CB dataset with 4-shot prompting?", "answer_anchor": "MPT", "evidence_anchor": "locality/2310.11670/comparison_2_table.png", "anchor_id": "2310.11670", "reference_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a score of 87.3 in the SciTail dataset with 16-shot prompting?", "answer_anchor": "MPT", "evidence_anchor": "locality/2310.11670/comparison_2_table.png", "anchor_id": "2310.11670", "reference_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a score of 71.4 in the CB dataset with 4-shot prompting?", "answer_anchor": "SPoT", "evidence_anchor": "locality/2310.11670/comparison_2_table.png", "anchor_id": "2310.11670", "reference_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has a score of 61.2 in the BoolQ dataset with 32-shot prompting?", "answer_anchor": "SPoT", "evidence_anchor": "locality/2310.11670/comparison_2_table.png", "anchor_id": "2310.11670", "reference_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model achieves an F1 score of 73.1 in the en_city category?", "answer_anchor": "Naive LongT5-Base-SS", "evidence_anchor": "locality/2310.11772/comparison_table.png", "anchor_id": "2310.11772", "reference_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model achieves a P_k score of 24.8 in the en_disease category?", "answer_anchor": "Naive LongT5-Base-SS", "evidence_anchor": "locality/2310.11772/comparison_table.png", "anchor_id": "2310.11772", "reference_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model achieves the best P_k score among the models in the first part of the table?", "answer_anchor": "Naive LongT5-Base-SS", "evidence_anchor": "locality/2310.11772/comparison_2_table.png", "anchor_id": "2310.11772", "reference_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model scores a 91.5 in the NER task?", "answer_anchor": "AMRBART", "evidence_anchor": "locality/2310.11964/comparison_table.png", "anchor_id": "2310.11964", "reference_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model scores an 81.5 in the SRL task?", "answer_anchor": "AMRBART", "evidence_anchor": "locality/2310.11964/comparison_table.png", "anchor_id": "2310.11964", "reference_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which transformer-based method achieves the highest MRR score on the FB15kET dataset?", "answer_anchor": "TET", "evidence_anchor": "locality/2310.12008/comparison_table.png", "anchor_id": "2310.12008", "reference_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method achieves an MRR score equal to 0.717 in the FB15kET dataset?", "answer_anchor": "TET", "evidence_anchor": "locality/2310.12008/comparison_table.png", "anchor_id": "2310.12008", "reference_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves an MRR score equal to 0.679 in the FB15kET dataset?", "answer_anchor": "RGCN", "evidence_anchor": "locality/2310.12008/comparison_table.png", "anchor_id": "2310.12008", "reference_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves a Hits@1 score of 0.281 in the YAGO43kET dataset?", "answer_anchor": "RGCN", "evidence_anchor": "locality/2310.12008/comparison_table.png", "anchor_id": "2310.12008", "reference_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method scores a 69.56 in the Forgotten Realms category?", "answer_anchor": "MetaBINK", "evidence_anchor": "locality/2310.12444/comparison_table.png", "anchor_id": "2310.12444", "reference_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method scores higher than 69.0 but lower than 70.0 in the Forgotten Realms category?", "answer_anchor": "MetaBINK", "evidence_anchor": "locality/2310.12444/comparison_table.png", "anchor_id": "2310.12444", "reference_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method is in the last row of the upper half of the table?", "answer_anchor": "MetaBINK", "evidence_anchor": "locality/2310.12444/comparison_table.png", "anchor_id": "2310.12444", "reference_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method achieves an accuracy of 18.4 on the GSM8K dataset?", "answer_anchor": "CoT Fine-tuned", "evidence_anchor": "locality/2310.13332/comparison_table.png", "anchor_id": "2310.13332", "reference_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves an accuracy of 63.8% in the StrategyQA dataset?", "answer_anchor": "CoT Fine-tuned", "evidence_anchor": "locality/2310.13332/comparison_table.png", "anchor_id": "2310.13332", "reference_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method has three empty entries in the table for the mathematical reasoning and commonsense reasoning tasks?", "answer_anchor": "CoT Fine-tuned", "evidence_anchor": "locality/2310.13332/comparison_table.png", "anchor_id": "2310.13332", "reference_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method in the table is listed right above One-Round Distillation and right below Specialization?", "answer_anchor": "CoT Fine-tuned", "evidence_anchor": "locality/2310.13332/comparison_table.png", "anchor_id": "2310.13332", "reference_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model is represented by a blue line in the figure?", "answer_anchor": "DialoGPT Large", "evidence_anchor": "locality/2310.13676/comparison_figure.png", "anchor_id": "2310.13676", "reference_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model is represented with a dot marker?", "answer_anchor": "DialoGPT Large", "evidence_anchor": "locality/2310.13676/comparison_figure.png", "anchor_id": "2310.13676", "reference_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which model exhibits the most negative Spearman's Correlation Coefficient?", "answer_anchor": "DialoGPT Large", "evidence_anchor": "locality/2310.13676/comparison_figure.png", "anchor_id": "2310.13676", "reference_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model in the figure has a more negative Spearman's Correlation than 0.60 when the alternative set size is set to 100 in the Mean Cosine setting?", "answer_anchor": "DialoGPT Large", "evidence_anchor": "locality/2310.13676/comparison_figure.png", "anchor_id": "2310.13676", "reference_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves the highest Precision score in the Token (I-topo) category?", "answer_anchor": "SpanBERT", "evidence_anchor": "locality/2310.14478/comparison_table.png", "anchor_id": "2310.14478", "reference_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method achieves an F1 score of 87.63 in the Token (I-topo) category?", "answer_anchor": "SpanBERT", "evidence_anchor": "locality/2310.14478/comparison_table.png", "anchor_id": "2310.14478", "reference_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model demonstrates the highest Oracle score?", "answer_anchor": "Composition", "evidence_anchor": "locality/2310.14503/comparison_table.png", "anchor_id": "2310.14503", "reference_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model achieves a 16.5 Top-1 score on the SQuAD dataset?", "answer_anchor": "Composition", "evidence_anchor": "locality/2310.14503/comparison_table.png", "anchor_id": "2310.14503", "reference_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which model has the higheset P-BLEU score in the uAD dataset?", "answer_anchor": "Composition", "evidence_anchor": "locality/2310.14503/comparison_table.png", "anchor_id": "2310.14503", "reference_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "What model is shown in the first row of the table?", "answer_anchor": "GWLAN", "evidence_anchor": "locality/2310.14523/comparison_table.png", "anchor_id": "2310.14523", "reference_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which model demonstrates the lowest zh-en score?", "answer_anchor": "GWLAN", "evidence_anchor": "locality/2310.14523/comparison_table.png", "anchor_id": "2310.14523", "reference_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which model has a de-en score of 53.87?", "answer_anchor": "GWLAN", "evidence_anchor": "locality/2310.14523/comparison_table.png", "anchor_id": "2310.14523", "reference_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is in the second row of the table?", "answer_anchor": "ConCN clusters", "evidence_anchor": "locality/2310.14793/comparison_table.png", "anchor_id": "2310.14793", "reference_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "4"}
{"question_anchor": "Which method is in the first block and has an F1 score of 50.4?", "answer_anchor": "ConCN clusters", "evidence_anchor": "locality/2310.14793/comparison_table.png", "anchor_id": "2310.14793", "reference_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method is represented by the purple line?", "answer_anchor": "MMA", "evidence_anchor": "locality/2310.14883/figure.png", "anchor_id": "2310.14883", "reference_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method is represented by the square marker?", "answer_anchor": "MMA", "evidence_anchor": "locality/2310.14883/figure.png", "anchor_id": "2310.14883", "reference_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "3"}
{"question_anchor": "Which method exhibits the lowest BLEU score in the De->En task over Average Lagging from 5 to 11?", "answer_anchor": "MMA", "evidence_anchor": "locality/2310.14883/figure.png", "anchor_id": "2310.14883", "reference_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method has an accuracy of 95.20% in automatic evaluation?", "answer_anchor": "Discup", "evidence_anchor": "locality/2310.14892/result_table.png", "anchor_id": "2310.14892", "reference_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "2"}
{"question_anchor": "Which method achieves a higher accuracy than DExpert but lower than Air-Decoding?", "answer_anchor": "Discup", "evidence_anchor": "locality/2310.14892/result_table.png", "anchor_id": "2310.14892", "reference_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "1"}
{"question_anchor": "Which method corresponds to the third row of the table?", "answer_anchor": "Discup", "evidence_anchor": "locality/2310.14892/result_table.png", "anchor_id": "2310.14892", "reference_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "4"}
