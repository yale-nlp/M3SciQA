{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE L score equal to 41.39. How does the application of normalizing flow in the neural topic model, as proposed by the paper, specifically contribute to the improvement of abstractive text summarization performance?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method in the table that demonstrates a ROUGE L score equal to 41.39?", "answer_anchor": "PEGASUS+NTM", "question_reference": "How does the application of normalizing flow in the neural topic model specifically contribute to the improvement of abstractive text summarization performance?", "explanation_reference": "The application of normalizing flow in the neural topic model allows for a more complex and expressive approximation of the document's global semantics, which in turn enhances the summarization model's ability to capture and integrate these semantics into the generated summaries. This leads to summaries that are more informative and coherent.", "evidence_reference": "In this work, we adapt the normalizing flow to the neural topic model to better grasp the global semantic patterns of the document."}
{"question": "Consider the paper that introduces the method that results in a score of 14.9 in the Unseen, Test, GC dataset. What specific method does the model proposed in the paper leverage to enhance its understanding of complex human instructions and handling of long sequences of subtasks in dynamic environments?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the score of 14.9 in Unseen, Test, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer leverage to enhance its understanding of complex human instructions and handling of long sequences of subtasks in dynamic environments?", "explanation_reference": "The question focuses on a detailed aspect of how the Episodic Transformer (E.T.) addresses the challenges of understanding complex human instructions and handling long sequences of subtasks. The answer is directly provided by the methodological approach described in the paper, where synthetic instructions are used as an intermediate representation to decouple the understanding of visual appearance from the variations in natural language instructions.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the method that exhibits a FLAN-T5 score of 52.4% using SGD in 24 domains. What is the discount factor (\\(\\lambda\\)) used in the model's recency weighting for subtask graph inference from real-world data?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shows FLAN-T5 score of 52.4% using SGE in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "What is the discount factor (\\(\\lambda\\)) used in the recency weighting for subtask graph inference from real-world data?", "explanation_reference": "The discount factor (\\(\\lambda\\)) used in the recency weighting for subtask graph inference from real-world data is mentioned as part of the method to improve graph generation by taking temporal information into account, specifically assigning higher weight if a subtask has been eligible more recently.", "evidence_reference": "we assign higher weight if a subtask has been eligible more recently: $w_{t, n} = \\max(0.1, \\lambda ^ {t_n-t}), where $0<\\lambda<1$ is the discount factor, $t_n$ is the time step when the precondition for subtask $n$ became satisfied. We used $\\alpha=0.2$ and $\\lambda=0.7$ in our experiments."}
{"question": "Consider the paper that introduces the benchmark that results in the highest execution accuracy for this method. What is the specific performance gap in execution accuracy between GPT-4 only on the BIRD testing data with and without external knowledge?", "answer": "", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Under this method, which benchmark demonstrates the highest execution accuracy?", "answer_anchor": "BIRD", "question_reference": "Based on the experimental results, what is the specific performance gap in execution accuracy between GPT-4 only on the \\textsc{Bird} testing data with and without external knowledge?", "explanation_reference": "The performance gap in execution accuracy between the best-performing LLM (GPT-4 + DIN-SQL) on the \\textsc{Bird} testing data with and without external knowledge can be inferred from the context that GPT-4 + DIN-SQL achieves a state-of-the-art result with external knowledge but does not provide a performance metric without external knowledge. Given the human performance improvement of 20.59% with external knowledge, and considering the context of LLM improvements with external knowledge, a specific performance gap can be estimated. However, the exact figure is not directly provided, making the question challenging and requiring an understanding of the impact of external knowledge on LLM performance.", "evidence_reference": "Table 2"}
{"question": "Consider the paper that introduces the dataset used for the 'dialogue response' task. What specific hyperparameter settings were used for identifying speakers in narrative using the text-davinci-002 GPT-3.5 model?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "2212.10465", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset used for Dialogue response task?", "answer_anchor": "SODA", "question_reference": "What specific hyperparameter settings were used for identifying speakers in narrative using the text-davinci-002 GPT-3.5 model?", "explanation_reference": "The question targets the detailed hyperparameter settings used in a specific step of the conversation generation process, which is a critical aspect of the methodological design. It requires an understanding of the parameters that influence the model's output during the narrative to conversation transformation.", "evidence_reference": "We leverage the \\texttt{text-davinci-002} GPT-3.5 model for identifying the speakers. We set temperature to 0, top-p to 1.0, frequency penalty to 0, presence penalty to 0, and max tokens to 16."}
{"question": "Consider the paper that introduces the method shown in the table that achieves a PPL score of 24.466 for the Test Seen task. What is the primary reason for the degeneration of neural language models as proposed in the paper?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the table gets 24.466 PPL for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the primary reason for the degeneration of neural language models as identified in the paper?", "explanation_reference": "The paper identifies the anisotropic distribution of token representations as the primary reason for model degeneration, where representations reside in a narrow subset of the entire space, leading to unnatural generated text and undesirable repetitions.", "evidence_reference": "In this work, we argue that the degeneration of neural language models stems from the anisotropic distribution of token representations, i.e., their representations reside in a narrow subset of the entire space."}
{"question": "Consider the paper that introduces the method listed in the table right below the PCP method. How does the model proposed in the paper's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact, and what does this indicate about the scoring function's effectiveness?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method in the table right below the PCP method?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF framework's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact, and what does this indicate about the scoring function's effectiveness?", "explanation_reference": "The question focuses on the comparison between the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ and its hard-label version $\\mathcal{L}_{L'}$, specifically asking about the performance impact of replacing $\\mathcal{L}_{L}$ with $\\mathcal{L}_{L'}$. The answer highlights that such a replacement leads to a significant decrease in performance, which underscores the importance of the scoring function used in $\\mathcal{L}_{L}$. This function accounts for the nuanced semantic structures within the local hierarchy, thereby contributing to the model's overall effectiveness.", "evidence_reference": "Secondly, we replace the Local Hierarchy-aware Contrastive loss $\\mathcal{L}_{L}$ (Equation (\\ref{equation: soft local})) with the hard-label version $\\mathcal{L}_{L'}$ (Equation (\\ref{equation: hard local})) and find that the performance drops notably."}
{"question": "Consider the paper that introduces the method that achieves the highest Precision score in the Token (I-topo) category. What specific performance gain does the model's Span Boundary Objective (SBO) provide over span masking alone in coreference resolution on the development set?", "answer": "", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets the highest Precision score in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "Based on the ablation studies, what specific performance gain does the Span Boundary Objective (SBO) provide over span masking alone in coreference resolution on the development set?", "explanation_reference": "The question targets the detailed outcome of applying the Span Boundary Objective (SBO) in addition to span masking, specifically focusing on its impact on coreference resolution. It requires understanding the comparative analysis provided in the ablation studies section, which discusses the incremental benefits of SBO over just span masking.", "evidence_reference": "Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone."}
{"question": "Consider the paper that introduces the method that achieves the highest score in the 'w/o p_out' setting. What is the improvement in TNR for detecting LSUN samples using DenseNet when comparing the model proposed in the paper to ODIN, given that 95% of CIFAR-100 samples are correctly detected?", "answer": "", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method that demonstrates the highest score in 'w/o p_out' setting?", "answer_anchor": "MSP", "question_reference": "What is the improvement in TNR for detecting LSUN samples using DenseNet when comparing the proposed method to ODIN, given that 95% of CIFAR-100 samples are correctly detected?", "explanation_reference": "The improvement in TNR for detecting LSUN samples using DenseNet when comparing the proposed method to ODIN, given that 95% of CIFAR-100 samples are correctly detected, is calculated based on the improvement from ODIN's performance to the proposed method's performance. The TNR for ODIN is 41.2%, and for the proposed method, it is 91.4%. Therefore, the improvement is 91.4% - 41.2% = 50.2%.", "evidence_reference": "Our method improves the TNR, i.e., the fraction of detected LSUN samples, compared to ODIN: $41.2\\\\%\\\\rightarrow 91.4\\\\%$ using DenseNet, when 95\\\\% of CIFAR-100 samples are correctly detected."}
{"question": "Consider the paper that introduces the method that has a GPT backbone and 7B parameters. What specific loss function is used for the model proposed in the paper during the training on the chain of subquestion-solution pairs for each problem?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method used with a GPT backbone and 7B parameters?", "answer_anchor": "DecomDistill", "question_reference": "In the methodology for distilling reasoning capabilities into smaller models, what specific loss function is used for the unified student model during the training on the chain of subquestion-solution pairs for each problem?", "explanation_reference": "The specific loss function used for the unified student model during training on the chain of subquestion-solution pairs for each problem is detailed in the methodology section under the description of the unified strategy. This loss function is an auto-regressive language modeling loss that computes the negative log likelihood of each token in the sequence of subquestion-solution pairs, given the previous tokens and the problem context.", "evidence_reference": "Given a step $j$ of problem $P$ (i.e., the concatenation of $q^{(j)}$ and $s^{(j)}$) consisting of a sequence of $m_j$ tokens $\\{x_j^{(1)}, \\dots, x_j^{(m_j)}\\}$, we use a typical auto-regressive language modeling loss, $\\mathcal{L}$:  \\begin{align} \\label{eq:loss_uni} & \\mathcal{L}_j(P) = & - \\sum_{k=1}^{m_j} \\log \\probP_{{uni}}\\ (x_j^{(k)}|x_j^{:(k-1)}, P) \\end{align} where $ \\probP_{{uni}}(x | c)$ is the probability assigned by $\\mathcal{M}_{uni}$ to token $x$ given context $c$, and $x^{:(y)}$ indicates the sequence $\\{x^{(1)}, \\dots, x^{(y)}\\}$."}
{"question": "Consider the paper that introduces the model that has a 6-layer encoder and a 6-layer decoder architecture. What specific method did the authors use to select high-quality, in-domain sentences from the Commoncrawl corpus for back-translation in the English to Russian translation task?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "1907.06616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model with 6-layer encoder and 6-layer decoder architecture?", "answer_anchor": "FSMT", "question_reference": "What specific method did the authors use to select high-quality, in-domain sentences from the Commoncrawl corpus for back-translation in the English to Russian translation task?", "explanation_reference": "The authors used a specific in-domain filtering method to select high-quality, in-domain sentences from the larger Commoncrawl corpus for back-translation, aiming to improve the quality of the monolingual Russian data. This method is explicitly mentioned as being described by Moore and Lewis (2010), indicating a targeted approach to enhance data quality for back-translation.", "evidence_reference": "In order to select a limited amount of high quality, in-domain sentences from the larger corpus, we adopt the method of~\\citet{moore2010intelligent} for selecting in-domain data (\\textsection\\ref{subsection:btcc})."}
{"question": "Consider the paper that introduces the dataset which has 1 SM task and 14 languages. What specific sentiment analysis sub-tasks do the authors plan to extend their dataset to in the future?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What dataset has 1 SM task and 14 languages?", "answer_anchor": "AfriSenti", "question_reference": "What specific sentiment analysis sub-tasks do the authors of AfriSenti plan to extend their dataset to in the future?", "explanation_reference": "The authors explicitly mention their future plans to extend AfriSenti to additional African languages and other sentiment analysis sub-tasks, indicating a broader scope beyond the current dataset's focus.", "evidence_reference": "In the future, we plan to extend \\textit{AfriSenti} to additional African languages and other sentiment analysis sub-tasks."}
{"question": "Consider the paper that introduces the dataset that corresponds to the largest blue circle label. What specific method was used to address the challenge of collecting tweets in languages that share geographic locations but have no curated stopword lists, and how was this method validated?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What dataset is with the largest blue circle label?", "answer_anchor": "AfriSenti", "question_reference": "What specific method was used to address the challenge of collecting tweets in languages that share geographic locations but have no curated stopword lists, and how was it validated?", "explanation_reference": "The question focuses on the detailed methodology used to collect tweets for languages without curated stopword lists, specifically asking about the approach taken to address this challenge and the validation process. The answer directly addresses this by mentioning the specific method (word co-occurrence-based approach) and the validation process (verification by native speakers), which are both detailed in the paper.", "evidence_reference": "We also used a word co-occurrence-based approach to extract stopwords using text sources from different domains. We lower-cased and removed punctuation marks and numbers, constructed a co-occurrence graph, and filtered out the words that occurred most often. Native speakers verified the generated lists before use."}
{"question": "Consider the paper that introduces the method that achieves the highest score in the 'w/o p_out' setting. How does its performance in detecting out-of-distribution samples using ResNet trained on CIFAR-10 compare when SVHN is used as OOD, in terms of True Negative Rate at 95% True Positive Rate, against the baseline and ODIN methods?", "answer": "", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method that demonstrates the highest score in 'w/o p_out' setting?", "answer_anchor": "MSP", "question_reference": "How does the proposed method's performance in detecting out-of-distribution samples using ResNet trained on CIFAR-10 compare when SVHN is used as OOD, in terms of True Negative Rate at 95% True Positive Rate, against the baseline and ODIN methods?", "explanation_reference": "The proposed method outperforms both the baseline and ODIN methods in detecting out-of-distribution samples when SVHN is used as OOD, with a True Negative Rate at 95% True Positive Rate of 96.42%, indicating its superior detection capability.", "evidence_reference": "Baseline \\citep{hendrycks2016baseline} & - & - & 32.47 & 89.88 & 85.06 & 85.40 & 93.96 \\\\ \\midrule ODIN \\citep{liang2017principled} & - & - & 86.55 & 96.65 & 91.08 & 92.54 & 98.52 \\\\ \\midrule \\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}} Mahalanobis \\\\ (ours) \\end{tabular}}& - & - & 54.51 & 93.92 & 89.13 & 91.56 & 95.95 \\\\ & - & \\checkmark & 92.26 & 98.30 & 93.72 & 96.01 & 99.28 \\\\ & \\checkmark & - & 91.45 & 98.37 & 93.55 & 96.43 & 99.35 \\\\ & \\checkmark & \\checkmark & {\\bf 96.42} & {\\bf 99.14} & {\\bf 95.75} & {\\bf 98.26} & {\\bf  99.60}"}
{"question": "Consider the paper that introduces the model that results in the highest Self-BLEU score on the TellMeWhy dataset. What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model shows the highest Self-BLEU score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach introduced in the paper, which is a combination of predicting the distribution of question types and generating summaries focused on salient events to facilitate the generation of educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. What specific aspect of the model proposed in the paper allows it to act as a 'self-ensemble' rather than necessitating multiple models for aggregation?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "What specific aspect of the self-consistency method allows it to act as a 'self-ensemble' rather than requiring multiple models for aggregation?", "explanation_reference": "The question targets the unique feature of self-consistency that distinguishes it from typical ensemble approaches, which usually involve multiple models. The self-consistency method, by generating diverse reasoning paths from a single model and selecting the most consistent answer, effectively creates a 'self-ensemble' without the need for multiple models.", "evidence_reference": "Self-consistency also differs from a typical ensemble approach where multiple models are trained and the outputs from each model are aggregated, it acts more like a ``self-ensemble'' that works on top of a \\textit{single} language model."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest accuracy for Method 2. What specific training objective combination in the verification ablation studies was found to strictly improve the performance of the model proposed in the paper?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What dataset demonstrates the highest accuracy with method 2?", "answer_anchor": "GSM8K", "question_reference": "In the verification ablation studies, which specific training objective combination was found to strictly improve the performance of verifiers?", "explanation_reference": "The paper discusses an ablation study where they compare the performance of verifiers trained with two different objectives: one with only the verification objective and another combining the verification objective with a language modeling objective. The study found that including the language modeling objective is a strict improvement over using the verification objective alone. This suggests that a better understanding of the language distribution aids the verifier in discriminating between correct and incorrect solutions.", "evidence_reference": "In \\Cref{fig:fc_verifier_loss_ablation}, we ablate the objective used when training verifiers. Although both are reasonable choices, including the language modeling objective is a strict improvement."}
{"question": "Consider the paper that introduces the method which is placed below the row for R-Former but above the row for NeurJudge. What specific methodological weakness does the model proposed in the paper address in the context of learning attention vectors for semantically close law articles, as identified in the comparison with Luo et al.'s (2017) approach?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown below the row of R-Former but above the row of NeurJudge?", "answer_anchor": "LADAN", "question_reference": "What specific methodological weakness does the LADAN model address in the context of learning attention vectors for semantically close law articles, as identified in the comparison with Luo et al.'s (2017) approach?", "explanation_reference": "The question targets a specific methodological weakness that the LADAN model aims to overcome, which is the challenge of distinguishing confusing charges when similar attention vectors are learned for semantically close law articles. This issue is directly addressed in the comparison with the approach by Luo et al. (2017), where the paper critiques the independent learning of each law article's attention vector, leading to the ineffectiveness in distinguishing confusing charges.", "evidence_reference": "Nevertheless, the weakness is that they learn each law article's attention vector independently, and this may result in that similar attention vectors are learned for semantically close law articles; hence, it is ineffective in distinguishing confusing charges."}
{"question": "Consider the paper that introduces the dataset that corresponds to the largest blue circle label. What was the highest zero-shot cross-lingual transfer performance F1 score achieved for the target language Tigrinya, and which source language was it transferred from?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What dataset is with the largest blue circle label?", "answer_anchor": "AfriSenti", "question_reference": "What was the highest zero-shot cross-lingual transfer performance F1 score achieved for the target language Tigrinya, and which source language was it transferred from?", "explanation_reference": "The highest zero-shot cross-lingual transfer performance F1 score for Tigrinya was achieved by transferring from Hausa, indicating that Hausa was the most effective source language for zero-shot learning on Tigrinya in the context of the experiments conducted.", "evidence_reference": "\\texttt{hau}\\t& \\textbf{47.1} & \\textbf{68.6} & \\textbf{57.9}"}
{"question": "Consider the paper that introduces the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category. How does the choice of $\\epsilon_\\text{init}$ and the edited layer impact GRACE, in its ability to generalize to unseen inputs while maintaining edit compression?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2211.11031", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "GRACE", "question_reference": "How does the choice of $\\epsilon_\\text{init}$ and the edited layer impact GRACE's ability to generalize to unseen inputs while maintaining edit compression?", "explanation_reference": "The impact of $\\epsilon_\\text{init}$ and the edited layer on GRACE's performance is discussed in the context of memorization versus generalization. The paper explains that editing different blocks yields varying results, with interior layers generally generalizing better than early and late layers. It also notes that larger $\\epsilon_\\text{init}$ values, while leading to better generalization, can increase interference with unrelated inputs, highlighting a trade-off between the size of the codebook and the ability to avoid memorization. This is supported by the observation that the number of keys over time flattens, indicating that $\\epsilon$ values adapt to the data distribution and control over codebook size is achieved.", "evidence_reference": "Layer and $\\epsilon_\\text{init}$ choice balance memorization and generalization...Steadily increasing Holdout shows that that \\method edits can generalize to previously-unseen holdout edits...Finally, the number of keys over time steadily flattens, indicating that the $\\epsilon$ values indeed adapt to the data distribution."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, SR dataset. How does the performance of the model proposed in the paper with OSCAR initialization and without predicting the parent object or visual region classification compare on the Seen and Unseen validation folds for the Task and GC metrics?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the best score in Seen, Val, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "How does the performance of EmBERT with OSCAR initialization and without predicting the parent object or visual region classification compare on the Seen and Unseen validation folds for the Task and GC metrics?", "explanation_reference": "The question specifically targets the performance metrics of EmBERT under a particular configuration\u2014using OSCAR initialization but without predicting the parent object ('P(O)') or utilizing the visual region classification (VRC) loss. The answer directly corresponds to the performance metrics provided in the validation fold performance table for this specific configuration.", "evidence_reference": "OSCAR & 18 & 200 & \\cblkmark & & & \\B{37.44} (\\B{28.81}) & \\B{44.62} (\\B{36.41}) & \\B{\\phantom{0}5.73} (\\B{\\phantom{0}3.09}) & \\B{15.91} (\\B{\\phantom{0}9.33})"}
{"question": "Consider the paper that introduces the method that corresponds to a score of 25.9 in the Seen, Val, SR dataset. What is the improvement percentage of the model proposed in the paper over Nguyen et al. in the Seen Goal-Condition metric?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows score of 25.9 in Seen, Val, SR dataset?", "answer_anchor": "MOCA", "question_reference": "What is the improvement percentage of MOCA over Nguyen et al. in the Seen Goal-Condition metric?", "explanation_reference": "The improvement percentage is calculated based on the performance metrics reported for the Seen Goal-Condition metric, where MOCA outperforms Nguyen et al. by 12.52%.", "evidence_reference": "We achieve an improvement of 14.42\\% and 3.20\\% in Seen and Unseen Task SR over Nguyen \\etal~\\cite{ngyuen_eval_winner} that won ALFRED challenge in ECCV 2020. MOCA outperforms them in both \\emph{Seen} and \\emph{Unseen} `Goal-Condition' metrics and gives an improvement of 12.52\\% and 3.39\\%, respectively."}
{"question": "Consider the paper that introduces the model that exhibits the most negative Spearman's Correlation Coefficient. What specific architectural feature allows it to efficiently handle the full context in a computationally efficient manner?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model in the figure has the highest Spearman's Correlation?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific architectural feature of DialoGPT allows it to efficiently handle the full context in a computationally efficient manner?", "explanation_reference": "The question assesses understanding of a core concept in the architecture of DialoGPT that enables it to efficiently process and generate conversational responses by handling the full context. The multi-layer self-attentive mechanism is a fundamental architectural feature that allows for this efficiency.", "evidence_reference": "a transformer-based architecture like GPT-2, which uses a multi-layer self-attentive mechanism to allow fully-connected cross-attention to the full context in a computationally efficient manner"}
{"question": "Consider the paper that discusses which model is represented by the orange bar. What specific linguistic function does attention head 5 in layer 5 of 6 of the Vanilla Transformer model, as indicated by the orange bar, appear to be involved in according to the authors' analysis in the attention visualizations section?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model is demonstrated by the yellow bar?", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the evidence provided in the attention visualizations section, what specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis?", "explanation_reference": "The authors present visual evidence showing that attention head 5 in layer 5 of 6 is involved in anaphora resolution, as demonstrated by the focused attention on the word 'its' and its relation to other parts of the sentence.", "evidence_reference": "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the method demonstrated by the solid lavender line. What specific activation function is employed in its parallel cross-attention mechanism for gated activation?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method demonstrated in the lavendar solid line?", "answer_anchor": "GRIT", "question_reference": "What specific activation function is employed in the parallel cross-attention mechanism of GRIT for gated activation?", "explanation_reference": "The parallel cross-attention mechanism in GRIT uses the sigmoid activation function for gated activation, as specified in the description of the parallel cross-attention design.", "evidence_reference": "c_i^g &= \\mathrm{sigmoid}(W^g[{a^{g}_{i}}; x^{\\prime}_{i}] + b^g), \\\\ c_i^r &= \\mathrm{sigmoid}(W^r[{a^{r}_{i}}; x^{\\prime}_{i}] + b^r)."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. What is the K-L divergence between the prediction results of the EQG model and the ground-truth for question type distribution learning on the test set?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model is in the first row of the table?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the prediction results of the BERT-based model and ground-truth for question type distribution learning on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how well the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the method that is in the third row of the table. What is the average time-saving percentage achieved by optimizing SQL queries using the model's two-stage optimization approach on the development set where ChatGPT accurately predicted the results?", "answer": "", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "BIRD", "question_reference": "What is the average time-saving percentage achieved by optimizing SQL queries using the two-stage optimization approach on the development set where ChatGPT accurately predicted the results?", "explanation_reference": "The average time-saving percentage of 77.75% is directly reported from the efficiency analysis section, where the paper discusses the results of optimizing SQL queries using a two-stage optimization approach on a selection of examples from the development set where ChatGPT accurately predicted the results.", "evidence_reference": "We observe that the two-stage optimization leads to an average time-saving of 77.75\\% while keeping the same results."}
{"question": "Consider the paper that introduces the method that is in the first block and has an F1 score of 50.4. What specific performance metric improvement does the model proposed in the paper, DenoiseFET, achieve when enhanced with Prior Knowledge about Labels (PKL) using BERT-large, compared to its performance without PKL on the UFET benchmark?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is in the first block while having 50.4 F1 score?", "answer_anchor": "ConCN clusters", "question_reference": "What specific performance metric improvement does the DenoiseFET model achieve when enhanced with Prior Knowledge about Labels (PKL) using BERT-large, compared to its performance without PKL on the UFET benchmark?", "explanation_reference": "The question focuses on the detailed performance improvement of the DenoiseFET model with PKL using BERT-large on the UFET benchmark, which is a specific detail that requires understanding of the experimental results presented in the paper. The answer directly addresses the improvement in F1 score, which is a critical performance metric in entity typing tasks.", "evidence_reference": "DenoiseFET & Bl & 52.6 & 47.5 & 49.8\\\\ DenoiseFET + PKL & Bl & 53.8 & 50.2 & 51.9"}
{"question": "Consider the paper that introduces the model that is on the last line of the Seq2Seq/Tree block of the table. What specific method does its solution discrimination module employ to generate negative solutions that are variants of the ground truth solution?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model on the last line of the Seq2Seq/Tree block?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module employ to generate negative solutions that are variants of the ground truth solution?", "explanation_reference": "The solution discrimination module uses a gradient-based manipulation method to identify the most vulnerable (important) token in the ground truth solution and then finds all possible alternatives for this token to generate negative solutions. This method is chosen to ensure that the negative solutions are hard for the model to distinguish from the correct solution, thereby enhancing the model's ability to correctly associate problems with their ground truth solutions.", "evidence_reference": "Our goal is to find variants of the ground truth solution as hard negative samples, which only manipulate the most vulnerable (important) token. Therefore, borrowing the idea from white-box evasion attack, we regard the token with the largest gradient as the most important and vulnerable one: \\begin{equation} \\label{grad} y_i = \\mathop{argmax\\;}_{y_i \\in Y}(\\nabla Dis([en_x(X),en_y(Y)]). \\end{equation}"}
{"question": "Consider the paper that introduces the method that achieves sentence-level precision of 60.32. What specific configuration led to the best performance in the WikiHop development set ablation study for the model proposed in the paper?", "answer": "", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method show a sentence-level precision equal to 60.32?", "answer_anchor": "longformer", "question_reference": "What specific configuration led to the best performance in the WikiHop development set ablation study for \\model?", "explanation_reference": "The best performance configuration for \\model in the WikiHop development set ablation study is indicated by the highest positive change in accuracy compared to the base \\model configuration. This configuration is detailed as having a sequence length of 4,096 and being trained for 15 epochs, which resulted in a +1.2 change in accuracy.", "evidence_reference": "\\model (seqlen: 4,096, 15 epochs) & 75.0 / +1.2"}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.8173 on the Stance dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets mean classification accuracy 0.8173 on Stance dataset?", "answer_anchor": "DPT", "question_reference": "Based on the findings, what is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data indicates the strength of the linear relationship between the vocabulary overlap across time periods and the performance of the model on this specific task. A value close to 1 suggests a strong positive correlation.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the dataset located in the top left of the figure. What is the exact match score between the labels and predictions for the model proposed in the paper, Flan-T5, on its benchmark?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset located on the top left of the figure?", "answer_anchor": "SentiEval", "question_reference": "What is the exact match score between the labels and predictions for \\texttt{Flan-T5} on the \\textsc{SentiEval} benchmark?", "explanation_reference": "The exact match score for \\texttt{Flan-T5} on the \\textsc{SentiEval} benchmark is directly reported in the results table under the section discussing the \\textsc{SentiEval} benchmark.", "evidence_reference": "\\textsc{SentiEval} & 29.07 & 38.82 & 36.64 & 47.55"}
{"question": "Consider the paper that introduces the method that has an F1 score of 64.95 on PDTB-Top. What is the primary reason for the poor performance of the PIDRP method compared to the model proposed in the paper across all four top-level senses of the PDTB?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which is the method show a 64.95 F1 score on PDTB-Top?", "answer_anchor": "PCP", "question_reference": "What is the primary reason for the poor performance of the PIDRP method compared to the PCP method across all four top-level senses of the PDTB?", "explanation_reference": "The paper suggests that the main reason for the PIDRP method's inferior performance is that predicting connectives aligns more closely with the natural language patterns encountered during the pre-training stage of the model, as opposed to directly predicting implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the method that exhibits an accuracy of 79.44% in the CJO22 task. What is the percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing the model proposed in the paper with the state-of-the-art MPBFN-WCA?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shows 79.44 accuracy in CJO22 task?", "answer_anchor": "LADAN", "question_reference": "What is the percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing LADAN with the state-of-the-art MPBFN-WCA?", "explanation_reference": "The percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing LADAN with MPBFN-WCA is directly stated in the 'Experimental Results' section, indicating the effectiveness of LADAN over the state-of-the-art method.", "evidence_reference": "Compared with the state-of-the-art MPBFN-WCA, LADAN improved the F1-scores of law article prediction, charge prediction, and term of penalty prediction on dataset CAIL-small by $2.02$\\%, $2.42$\\% and $4.20$\\% respectively, and about $3.18$\\%, $1.44$\\% and $5.79$\\% on dataset CAIL-big."}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the penultimate row. How does the model proposed in the paper ensure the prevention of cycles in the generated subtask graph to avoid causality paradoxes?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the figure in the penultimate row?", "answer_anchor": "MSG^2", "question_reference": "How does the proposed method, MSG2, ensure the prevention of cycles in the generated subtask graph to avoid causality paradoxes?", "explanation_reference": "The method ensures the prevention of cycles in the generated subtask graph by assigning layers to each subtask based on their parent-child relationships inferred from the subtask state labels. This layer-wise precondition inference ensures that edges in the subtask graph are formed from lower to higher layers, preventing cycles and thus avoiding causality paradoxes.", "evidence_reference": "To avoid this problem, we perform precondition inference in a \\emph{layer-wise} fashion similar to~\\citet{sohn-iclr20}...After each subtask is assigned to its depth, we perform precondition inference at each depth in order of increasing depth...This ensures that the edge in the subtask graph is formed from the lower depth to the higher depth, which prevents the cycle."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, GC dataset. How does the performance of the model proposed in the paper with OSCAR initialization and without predicting the parent object or visual region classification compare to its performance with these features on the seen validation fold?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the best score in Seen, Val, GC dataset?", "answer_anchor": "EmBERT", "question_reference": "How does the performance of EmBERT with OSCAR initialization and without predicting the parent object or visual region classification compare to its performance with these features on the seen validation fold?", "explanation_reference": "The performance of EmBERT on the seen validation fold is higher when it does not predict the parent object or visual region classification, as indicated by the task and GC metrics comparing the configurations with and without these features.", "evidence_reference": "OSCAR & 18 & 200 & \\cblkmark & & & \\B{37.44} (\\B{28.81}) & \\B{44.62} (\\B{36.41}) & \\B{\\phantom{0}5.73} (\\B{\\phantom{0}3.09}) & \\B{15.91} (\\B{\\phantom{0}9.33}) \\\\[1pt] OSCAR & 18 & 200 & \\cblkmark & \\cblkmark & & 36.22 (27.05) & 44.57 (35.23) & \\phantom{0}4.39 (\\phantom{0}2.21) & 13.03 (\\phantom{0}7.54)"}
{"question": "Consider the paper that introduces the method that has 11.0 rounds to completion. How does the inclusion of agent ID in the agent-specific global state influence the model's performance proposed in the paper in the SMAC domain, particularly in maps where agents may assume different roles?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having 11.0 rounds to Completion?", "answer_anchor": "MAPPO", "question_reference": "How does the inclusion of agent ID in the agent-specific global state influence MAPPO's performance in the SMAC domain, particularly in maps where agents may assume different roles?", "explanation_reference": "The inclusion of agent ID in the agent-specific global state allows for an agent-specific value function depending on an agent's type or role, which is particularly helpful when the environment contains heterogeneous agents. This is supported by the superior performance of the variant with agent ID compared to the variant without it in the 3s5z vs. 3s6z map, demonstrating the importance of agent-specific features in forming an effective global state.", "evidence_reference": "Including the agent id in the death mask, as is done in variant (1), is particularly important in maps which agents may take on different roles, as demonstrated by the superior performance of variant (1) compared to variant (4), which does not contain the agent ID in the death-mask zero-state, in the 3s5z vs. 3s6z map."}
{"question": "Consider the paper that introduces the score described as the 'influence of any example z towards another example z' by tracking their gradient dot products,' where 'we generate the self-influence scores where z = z'. What specific computational optimization is employed by the model proposed in the paper to handle the gradient computations for the fully-connected layers more efficiently?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2002.08484", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the name of the score with description 'Influence of any example z towards another example z' by tracking their gradient dot products. We generate the self-influence scores where z = z''?", "answer_anchor": "TracIn", "question_reference": "In the context of applying TrackIn to the fully connected layer of ResNet-50 trained on Imagenet, what specific computational optimization is employed to handle the gradient computations for the fully-connected layers more efficiently?", "explanation_reference": "The paper describes a specific computational optimization for handling gradient computations for fully-connected layers more efficiently by exploiting the fact that the gradient w.r.t. the weights for the layer is a rank 1 matrix. This allows for the computation of the dot (Hadamard) product of two rank 1 matrices in time and space complexity significantly lower than the naive approach, specifically \\(O((m + n) \\cdot \\sqrt{d})\\) rather than \\(O(mnd)\\), where \\(m\\) and \\(n\\) are the dimensions of the weight matrix, and \\(d\\) is the dimension for the random projections.", "evidence_reference": "Suppose we have a fully connected layer in the neural network with a weight matrix \\(W \\in \\mathbb{R}^{m \\times n}\\), where \\(m\\) is the number of units in the input to that layer, and the \\(n\\) is the number of units in the output of the layer. For the purpose of \\trackin\\ computations, it is possible to obtain a random projection of the gradient w.r.t. \\(W\\) into \\(d\\) dimensions with time and space complexity \\(O((m + n) \\cdot \\sqrt{d})\\) rather than the naive \\(O(mnd)\\) complexity that the standard random projection needs."}
{"question": "Consider the paper that introduces the model that achieves a score of 21.073 in 10-shot prompting. What specific advantage does the model's dynamic masking offer over static masking in the context of BERT pretraining according to the paper?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model demonstrates score of 21.073 in 10-shot prompting", "answer_anchor": "RoBERTa", "question_reference": "What specific advantage does dynamic masking offer over static masking in the context of BERT pretraining according to the paper?", "explanation_reference": "The paper mentions that given the results of comparing static and dynamic masking, and the additional efficiency benefits of dynamic masking, they use dynamic masking in the remainder of the experiments. This implies that beyond performance metrics, dynamic masking offers efficiency improvements which are considered advantageous in the pretraining process.", "evidence_reference": "Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments."}
{"question": "Consider the paper that analyzes the dataset shown in the second row of the table. What specific distribution statistics were analyzed in the paper to assess goal completion and knowledge exploitation?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset shown in the second row of the table?", "answer_anchor": "DuConv", "question_reference": "What specific distribution statistics were analyzed to assess goal completion and knowledge exploitation in the paper?", "explanation_reference": "The question assesses understanding of the detailed analysis conducted in the paper regarding how well the conversation goals were achieved and how the knowledge was exploited in the dialogue process. It focuses on the specific aspects of 'goal completion' and 'knowledge used', which are key to evaluating the effectiveness of the proposed models in utilizing the knowledge graph for proactive conversation.", "evidence_reference": "Analysis on goal completion and knowledge exploitation."}
{"question": "Consider the paper that introduces the supervised method that achieves the highest score in 100-shot prompting. What specific performance improvement does the model proposed in the paper offer over static masking for the SQuAD 2.0 task according to the paper's findings?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which supervised method demonstrates highest scores in 100-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does dynamic masking offer over static masking for the SQuAD 2.0 task according to the paper's findings?", "explanation_reference": "The improvement is calculated based on the reported median accuracy for the MNLI-m task using static and dynamic masking. The paper reports a median accuracy of 84.3% for static masking and 84.0% for dynamic masking on the MNLI-m task. The difference indicates a 0.4% improvement in favor of static masking, contrary to expectations.", "evidence_reference": "static & 78.3 & 84.3 & 92.5 \\\\ dynamic & 78.7 & 84.0 & 92.9 \\\\"}
{"question": "Consider the paper that introduces the method that has an F1 score of 54.83. What is the alpha value used in the data sampling process for pre-training the model, and how does it influence the balance between high- and low-resource languages?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having 54.84 F1 score?", "answer_anchor": "LayoutXLM", "question_reference": "What is the alpha value used in the data sampling process for pre-training LayoutXLM, and how does it influence the balance between high- and low-resource languages?", "explanation_reference": "The alpha value used in the data sampling process for pre-training LayoutXLM is 0.7. This value is chosen to make a reasonable compromise between performance on high- and low-resource languages, indicating that it influences the balance by adjusting the sampling probability in favor of a more equitable representation of languages regardless of their resource availability.", "evidence_reference": "Following InfoXLM~\\citep{chi2020infoxlm}, we use alpha = 0.7 for LayoutXLM to make a reasonable compromise between performance on high- and low-resource languages."}
{"question": "Consider the paper that introduces the method that demonstrates the second highest score in the TweetEval Irony dataset for both zero-shot and few-shot prompting. What specific feature of the visual embeddings allows the model, proposed by the paper, to discriminate regions from different images when multiple images are given to it?", "answer": "", "figure": "locality/2310.15746/comparison_2_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method demonstrates the second highest score in TweetEval Irony dataset in both zero-shot and few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific feature of the visual embeddings allows the model to discriminate regions from different images when multiple images are given to it?", "explanation_reference": "The feature that allows the model to discriminate regions from different images when multiple images are given to it is the use of image ids. This is explicitly mentioned as a function of the image ids encoded within the visual embeddings.", "evidence_reference": "Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in \\NLVR{}~\\cite{Suhr2019}, models take two input images)."}
{"question": "Consider the paper that introduces the method that achieves a higher accuracy than DExpert but lower than Air-Decoding. What specific methodological adjustment does the model proposed in the paper make to the control-prompt initialization process to ensure stability during training?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows a higher accuracy than DExpert but lower than Air-Decoding?", "answer_anchor": "Discup", "question_reference": "What specific methodological adjustment does DisCup make to the control-prompt initialization process to ensure stability during training?", "explanation_reference": "The adjustment made to the control-prompt initialization process in DisCup for ensuring stability during training involves re-parameterizing the control-prompts. This is achieved by introducing an external LSTM module that processes the initially randomly initialized control-prompts, making them closer to natural language and thus more stable for training.", "evidence_reference": "Empirically,  we re-parameterize the $P_k$ for stable training. An external $LSTM_{\\theta^\\prime}$ module is introduced to make the control-prompts close to the natural language."}
{"question": "Consider the paper that introduces the model that has the highest P-BLEU score in the uAD dataset. How does its corresponding \\ednascore metric balance the evaluation of faithfulness and diversity in generated summaries?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model gets the lowest P-BLEU? score in uAD dataset?", "answer_anchor": "Composition", "question_reference": "How does the \\ednascore metric balance the evaluation of faithfulness and diversity in generated summaries?", "explanation_reference": "The \\ednascore metric is designed to jointly measure faithfulness and diversity in summaries. It uses Self-Entailment to assess diversity, which is effective because both components of \\ednascore (faithfulness and diversity) yield values in a similar output space through the same trained model (Entailment), facilitating a balanced evaluation.", "evidence_reference": "The reason \\ednascore relies on Self-Entailment to measure diversity is because the faithfulness metric is also based on Entailment. This means that both components will be mapped to a score in a similar output space (i.e., they both yield values between 0 and 1 obtained through the same trained model), making it more likely to be properly balanced when mixed."}
{"question": "Consider the paper that introduces the LLM model that corresponds to an r score of 0.813. What specific methodological difference in the evaluation setup for the model's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "answer": "", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the LLM model that demonstrates the r score equal to 0.813?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "explanation_reference": "This methodological difference is significant because sampling at temperature 0 can lead to more deterministic outcomes based on the generated explanation, potentially affecting the model's performance on these exams in a way that differs from how choices were determined in other exams.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.717 in the FB15kET dataset. What specific aspect of the model's design, proposed in the paper, allows it to semantically strengthen the representation of an entity through class membership of types?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets MRR score equal to 0.717 in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What specific aspect of the TET's design allows it to semantically strengthen the representation of an entity through class membership of types?", "explanation_reference": "The question focuses on a detailed aspect of the Transformer-based Entity Typing (TET) approach, specifically how it enhances the semantic representation of an entity. The answer directly addresses this by pointing out that TET utilizes information regarding the class membership of types, which is a methodological detail that semantically strengthens the representation of an entity. This detail is crucial for understanding how TET improves upon existing methods in knowledge graph entity typing by leveraging class membership information to enrich entity representations.", "evidence_reference": "Furthermore, TET uses information about class membership of types to semantically strengthen the representation of an entity."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model is in the first row of the table?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach of combining question type prediction with event-centric summarization, which is the core methodological innovation of the study for generating educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than Doc2Graph and a higher F1 score than GNN+MLP. What is the observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having lower F1 score than Doc2Graph and higher F1 score than GNN+MLP?", "answer_anchor": "SPADE", "question_reference": "what is the observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle?", "explanation_reference": "The Tail Collision Avoidance algorithm is integrated to handle the property where individual text nodes have a single incoming edge for each relation except in special documents like tables. The observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle is +1.0%.", "evidence_reference": "To push the performance further, we notice that individual text nodes have a single incoming edge for each relation except in special documents like table (Fig. \\ref{fig_paradigm}). Using this property, we integrate Tail Collision Avoidance algorithm (\\caabb) that iteratively trims the tail-sharing-edges and generate new edges until the process becomes self-consistent (Section \\ref{sec:graph_gen}). $F_1$ increases by +1.0\\% and +0.8\\% with and without the oracle upon the integration (2nd row, \\cordabb)."}
{"question": "Consider the paper that introduces the method that corresponds to the orange line in the figure. What is the core component of UniKGQA that enables the propagation of matching information along the directed edges on KGs?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method in the figure is demonstrated by the orange line?", "answer_anchor": "UniKGQA", "question_reference": "What is the core component of UniKGQA that enables the propagation of matching information along the directed edges on KGs?", "explanation_reference": "The core component that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs) in UniKGQA is the matching information propagation module. This module is crucial for the model's ability to effectively navigate and reason over the KG by leveraging the semantic relationships and structure within the graph.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the Seq2Exp model marked with the Club citation symbol. What specific advantage does the model proposed in the paper demonstrate over FinQANet in terms of handling operands in the extended FinQA dataset?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which Seq2Exp model is with Club citation mark?", "answer_anchor": "Elastic", "question_reference": "What specific advantage does ELASTIC demonstrate over FinQANet in terms of handling operands in the extended FinQA dataset?", "explanation_reference": "The advantage is highlighted by ELASTIC's ability to solve questions from the extended FinQA dataset that require generating operators with more than two operands, showcasing its adaptability and flexibility in handling diverse numerical reasoning tasks.", "evidence_reference": "These questions are proposed based on the original passages in the FinQA dataset. In addition, they are about superlative questions, which require to be solved by using superlative operators (i.e., \\(\\textit{smallest}\\) and \\(\\textit{biggest}\\)). As a result, unlike questions from the original FinQA dataset, the numbers of operands used to solve these extended questions are not limited to two."}
{"question": "Consider the paper that introduces the method that is placed directly above the PHA method in the table. What does the clustering of empty string answers in the PCA visualisation for NewsQA instances suggest about the model's ability to handle specific types of outputs?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shown in the table is right above PHA method?", "answer_anchor": "HyperDecoder", "question_reference": "Based on the analysis of hypernetwork embeddings, what does the clustering of empty string answers in the PCA visualisation for NewsQA instances suggest about the model's ability to handle specific types of outputs?", "explanation_reference": "The clustering of empty string answers in the PCA visualisation for NewsQA instances suggests that the hypernetwork has learned to map from embedding space to specific output types, including the differentiation between non-empty and empty string answers. This indicates the model's effective control over the decoder to generate desired outputs based on the input, showcasing its flexibility and adaptability in handling various output types.", "evidence_reference": "A visualisation of the hypernetwork embeddings generated for NewsQA in Figure \\ref{fig:newsqa_embed} further shows that empty string answers are generally clustered together."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. How does its performance on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model on the first row of the table?", "answer_anchor": "BERT", "question_reference": "How does the performance of BERT on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "explanation_reference": "The performance comparison between fine-tuning and feature-based approaches for the NER task, under various masking strategies during pre-training, shows that the fine-tuning approach consistently outperforms the feature-based approach. This is evident from the Dev set results for NER, where the fine-tuning approach yields higher F1 scores compared to the feature-based approach across all masking strategies.", "evidence_reference": "In the table presented in the Ablation for Different Masking Procedures section, the Dev set results for NER under different masking strategies show that the fine-tuning approach (95.4, 94.9, 95.2, 95.2 for different strategies) consistently achieves higher F1 scores than the feature-based approach (94.9, 94.0, 94.6, 94.7 for the same strategies), indicating better performance."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest accuracy for Method 2. What specific architectural feature allows the model proposed in the paper's verifier models to simultaneously perform language modeling and correctness prediction tasks?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What dataset demonstrates the highest accuracy with method 2?", "answer_anchor": "GSM8K", "question_reference": "What specific architectural feature allows the verifier models to simultaneously perform language modeling and correctness prediction tasks?", "explanation_reference": "The specific architectural feature that enables verifier models to undertake both language modeling and correctness prediction tasks is the implementation of a small scalar head. This scalar head operates on the logits outputted by the language model\u2019s final unembedding layer, allowing for predictions on a per-token basis. This design choice facilitates the verifier's dual functionality by shifting and scaling the logit corresponding to a special token in the vocabulary reserved for the verifier\u2019s predictions, while other tokens continue to represent the language modeling objective.", "evidence_reference": "We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model\u2019s final unembedding layer. Specifically, the bias and gain shift and scale the logit corresponding to a special token in the vocabulary."}
{"question": "Consider the paper that discusses the Twitter dataset that has the most number of languages compared to all other Twitter datasets. What specific challenge did annotators face when annotating tweets in Mozambican Portuguese and Xitsonga in the model proposed in the paper, and how did it affect the final dataset?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset (Twitter) has the most number of languages compared to all Twitter datasets?", "answer_anchor": "AfriSenti", "question_reference": "What specific challenge did annotators face when annotating tweets in Mozambican Portuguese and Xitsonga, and how did it affect the final dataset?", "explanation_reference": "The challenge of code-mixing and sarcasm in tweets made it difficult for annotators to determine the intended meaning, leading to the exclusion of many tweets from the final dataset due to disagreements among annotators on the presence of sarcasm.", "evidence_reference": "One of the significant challenges for the Mozambican Portuguese and Xitsonga data annotators was the presence of code-mixed and sarcastic tweets. Code-mixing in tweets made it challenging for the annotators to determine the intended meaning of the tweet as it involved multiple languages spoken in Mozambique that some annotators were unfamiliar with. Similarly, the presence of two variants of Xitsonga spoken in Mozambique (Changana and Ronga) added to the complexity of the annotation task. Additionally. we excluded many tweets from the final dataset as sarcasm present in tweets was another source of disagreement among the annotators."}
{"question": "Consider the paper that introduces the method that has an F1 score of 75. How does the inclusion of Coordinate Convolution and Corner Pooling individually and in combination affect the F1 scores for Entity Labeling and Entity Linking tasks in the model proposed in the paper?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having 75 F1 score?", "answer_anchor": "MSAU-PAF", "question_reference": "How does the inclusion of Coordinate Convolution and Corner Pooling individually and in combination affect the F1 scores for Entity Labeling and Entity Linking tasks in the MSAU-PAF model?", "explanation_reference": "The ablation study section provides a detailed comparison of the model's performance with and without the inclusion of Coordinate Convolution and Corner Pooling. It quantitatively demonstrates the individual and combined impact of these components on the model's effectiveness in both Entity Labeling and Entity Linking tasks.", "evidence_reference": "Ablation study for CoordConv...experienced a total gain of 0.02 in F1 score in both Entity Labeling and Entity Linking...Ablation study for Corner Pooling...an increase of 0.01 in F1 score in Entity Labeling task and 0.01 in Entity Linking...combining CoordConv and Corner Pooling can even increase the model performance further:...improves the F1 score by 0.03 by the total in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the model which has a lower mean classification accuracy than VIBE but higher mean classification accuracy than UDALM on the Stance dataset. What mathematical modification is applied in its methodology for quantifying temporal degradation to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model has lower accuracy than VIBE but higher accuracy than UDALM?", "answer_anchor": "DPT", "question_reference": "In the methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "explanation_reference": "The modification ensures that the calculated value reflects performance deterioration in a consistent manner, increasing as performance worsens, irrespective of whether the misalignment is due to training data being from the past or future relative to the evaluation data.", "evidence_reference": "Let $S_{t' \\shortto t}$ indicate the performance a model trained on timestamp $t'$ data and evaluated on the timestamp $t$. Let $$ {D} (t' \\shortto{} t) = -\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t), $$  In other words, ${D} (t' \\shortto{} t)$ is a modified difference in performance between a aligned and misaligned models."}
{"question": "Consider the paper that introduces the dataset that includes 1 SM task and 4 languages. What specific methodological approach did the authors of the paper use to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria, particularly in this context?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset with 1 SM task and 4 languages?", "answer_anchor": "NaijaSenti", "question_reference": "What specific methodological approach did the authors use to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "explanation_reference": "The authors addressed the issue of stopwords overlap causing tweets to be collected in an unintended language by collecting tweets based on the geographical locations where the target language is predominantly spoken. This methodological approach leverages the geographical parameters (location, longitude, latitude, and radius) to specify a circular geographic area for tweet collection, thus improving the accuracy of language-specific data collection.", "evidence_reference": "Stopwords overlap across indigenous languages in a multilingual society such as Nigeria. This results in tweets being collected in a language that differs from the query language. To mitigate this, we collected tweets based on locations where a language is predominantly spoken, using the location, longitude, latitude and radius parameters (25 miles) to specify a circular geographic area."}
{"question": "Consider the paper that introduces the model that results in the highest MCD 1 score. How does its approach, specifically the \\textsc{Dangle} model's methodology, improve the translation of novel compound phrases by handling the entanglement problem in machine translation?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2110.04655", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model dmonstrates the highest MCD 1 score?", "answer_anchor": "Dangle", "question_reference": "How does the proposed \\textsc{Dangle} model's approach to handling the entanglement problem in machine translation specifically improve the translation of novel compound phrases?", "explanation_reference": "The \\textsc{Dangle} model's adaptive encoding mechanism allows it to effectively handle novel compound phrases by breaking down the complex representation task into smaller, more manageable sub-tasks associated with familiar phrases. This approach enables the model to generate accurate translations for compounds that are new or rare in the training data by leveraging its understanding of the component phrases.", "evidence_reference": "In the CG test set, five new utterances are constructed by embedding the novel compound 'behind the small doctor on the floor' into five sentence templates. In the training set, the phrases \u201cbehind the [ADJ] [NOUN]\u201d and \u201cthe [ADJ] [NOUN] on the floor\u201d appear frequently, but the phrase \u201cbehind the [ADJ] [NOUN] the [ADJ] [NOUN]\u201d is very rare. This poses a serious challenge for the baseline encoder-decoder model, which mistakenly translates the compound phrase into \\mbox{\\begin{CJK*}{UTF8}{gbsn} \u5730\u677f \u540e\u9762 \u7684 \u5c0f \u533b\u751f\\end{CJK*}} (the small doctor behind the floor), or \\mbox{\\begin{CJK*}{UTF8}{gbsn} \u5730\u677f \u4e0a \u7684 \u5c0f \u533b\u751f\\end{CJK*}} (the small doctor on the floor), or altogether ignores the translation of some content words like \\mbox{\\begin{CJK*}{UTF8}{gbsn} \u5730\u677f \u540e\u9762\\end{CJK*}} (behind the floor). It seems the baseline model cannot simultaneously represent the relation between \u201cbehind\u201d and \u201cthe small doctor\u201d and the relation between \u201cthe small doctor\u201d and \u201cthe floor\u201d, even though the two are conditionally independent. In contrast, \\textsc{Dangle} generates the correct translation \\mbox{\\begin{CJK*}{UTF8}{gbsn} \u5730\u677f \u4e0a \u7684 \u5c0f\u533b\u751f \u540e \u9762\\end{CJK*}} in all five contexts."}
{"question": "Consider the paper that introduces the model that has an F1 score higher than PCP's but lower than DiscoPrompt's on PDTB-Top. How does its utilization of the Multi-Head Interactive Attention (MHIA) module specifically contribute to the model's performance in implicit discourse relation recognition?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method with an F1 score higher than PCP but lower than DiscoPrompt?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF model's utilization of the Multi-Head Interactive Attention (MHIA) module specifically contribute to its performance in implicit discourse relation recognition?", "explanation_reference": "The ablation study demonstrates that each component of the GOLF model, including the MHIA module, plays a crucial role in achieving the model's state-of-the-art performance. The specific impact of removing the MHIA module is quantified in terms of reduced accuracy and F1 scores, indicating its importance in capturing the interaction between discourse arguments effectively.", "evidence_reference": "Table \\ref{tab: ablation} indicates that eliminating any of the four modules would hurt the performance across all three levels and reduce the consistency among multi-level label predictions. At the same time, the Local Hierarchy-aware Contrastive loss contributes mostly."}
{"question": "Consider the paper that introduces the method that has the highest perplexity. How does the model's, proposed by the paper, zero-shot capability to generalize to new control codes potentially emerge from its training on generative classifiers?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having the highest perplexity?", "answer_anchor": "GeDi", "question_reference": "How does GeDi's zero-shot capability to generalize to new control codes potentially emerge from its training on generative classifiers?", "explanation_reference": "GeDi's ability to generalize to new control codes in a zero-shot manner is likely because generative classifiers can classify unseen topics zero-shot from learned word embeddings. This foundational aspect of generative classifiers enables GeDi to guide generation towards a wide array of topics beyond its initial training scope.", "evidence_reference": "This ability likely emerges because generative classifiers can classify unseen topics zero-shot from learned word embeddings \\citep{yogatama2017generative}, and GeDi uses generative classifiers to guide generation."}
{"question": "Consider the paper that introduces the method that has the second-highest Avg score on the SuperGLUE task. Based on the analysis of hypernetwork embeddings, what does the clustering of empty string answers in the PCA visualisation for NewsQA instances suggest about the model's ability to handle specific types of outputs?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the second highest Avg score on SuperGLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "Based on the analysis of hypernetwork embeddings, what does the clustering of empty string answers in the PCA visualisation for NewsQA instances suggest about the model's ability to handle specific types of outputs?", "explanation_reference": "The clustering of empty string answers in the PCA visualisation for NewsQA instances suggests that the hypernetwork has learned to map from embedding space to specific output types, including the differentiation between non-empty and empty string answers. This indicates the model's effective control over the decoder to generate desired outputs based on the input, showcasing its flexibility and adaptability in handling various output types.", "evidence_reference": "A visualisation of the hypernetwork embeddings generated for NewsQA in Figure \\ref{fig:newsqa_embed} further shows that empty string answers are generally clustered together."}
{"question": "Consider the paper that introduces the method that shows the lowest overall performance. What is the average number of action steps in the expert demonstrations within the ALFRED dataset for the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the lowest over performance?", "answer_anchor": "SEQ2SEQ", "question_reference": "What is the average number of action steps in the expert demonstrations within the ALFRED dataset?", "explanation_reference": "The average number of action steps in the expert demonstrations is directly stated as part of the dataset's characteristics.", "evidence_reference": "For 2,685 combinations of task parameters, we generate three expert demonstrations per parameter set, for a total of 8,055 unique demonstrations with an average of 50 action steps."}
{"question": "Consider the paper that introduces the method that has a perplexity of approximately 30 and an average max toxicity of around 0.4. What specific strategy does the model proposed in the paper employ to ensure the appearance of guide words in generated text without necessitating a pre-defined ordering of constraints?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method with around 30 perplexity and around 0.4 average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What specific strategy does the paper propose for ensuring the appearance of guide words in generated text without requiring pre-defined ordering of constraints?", "explanation_reference": "The paper proposes two strategies for controlling generation towards a list of guide words, one of which is 'Guide Closest'. This strategy does not require the guide words to be ordered and works by shifting the score function by the highest cosine similarity across all words in the set of guide words that have not appeared before the current step. This approach allows for the flexible inclusion of guide words in the generated text, aligning with the paper's goal of imposing hard constraints on language generation without the need for pre-defined ordering.", "evidence_reference": "Given a list of guide words $W$, we propose two approaches for both the case where we need words $w_n$ to appear in a fixed order as well as when any order suffices. [...] At any given decoding step, we shift the score function by the highest cosine similarity across all words $w\\in W_{t}$. [...] Explicitly, we score $y_{t}$ as \\begin{align} \\score'(y_{t}, W_{t}\\mid \\yy_{<t}) = & \\,\\score(y_{t} \\mid \\yy_{<t}) \\ + \\\\ \\lambda \\cdot \\max &\\Big(0, \\underset{{w\\in W_t}}{\\max}\\, \\semdist(\\gamma(y_t), \\gamma(w)\\Big) \\nonumber \\end{align}"}
{"question": "Consider the paper that introduces the method shown in the first row of the table. What specific aspect of the policy gradient strategy, as proposed by the paper, contributes to the reduction of prediction variance in the selection of in-context examples for few-shot GPT-3?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is shown in the first row of the table?", "answer_anchor": "PromptPG", "question_reference": "What specific aspect of the policy gradient strategy contributes to the reduction of prediction variance in the selection of in-context examples for few-shot GPT-3?", "explanation_reference": "The policy gradient strategy, by learning to select in-context examples efficiently and stably without human-designed heuristics, directly contributes to reducing the prediction variance compared to random selection. This approach allows for a more systematic and adaptive selection process, which is less prone to the instability associated with random or heuristic-based selections.", "evidence_reference": "Inspired by reinforcement learning's ability to search for an optimal action policy, we propose applying the policy gradient strategy to learn to select in-context examples more efficiently and stably without designing human-designed heuristics."}
{"question": "Consider the paper that introduces the method shown in the fourth row of the table. What specific form of global state input does the model proposed in the paper utilize in the SMAC domain to address the lack of critical local information?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method showing in the fourth row of the table?", "answer_anchor": "MAPPO", "question_reference": "What specific form of global state input to the value function does MAPPO utilize in the SMAC domain to address the lack of critical local information?", "explanation_reference": "The paper specifies that to address the lack of critical local information in the environment-provided global state, MAPPO utilizes an Agent-Specific Global State (AS) for each agent in the SMAC domain. This AS state is formed by concatenating the environment-provided global state with the local observation for each agent, providing the value function with a more comprehensive description of the environment state.", "evidence_reference": "To address the weaknesses of the \\emph{CL} and \\emph{EP} states, we allow the value function to leverage both global and local information by forming an \\textbf{Agent-Specific Global State (AS)} which creates a global state for agent $i$ by concatenating the \\emph{EP} state and $o_i$, the local observation for agent $i$."}
{"question": "Consider the paper that introduces the dataset which has a training set size of 8,844. What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included in the dataset?\n\nA) 0.5%\nB) 1.2%\nC) 2.8%\nD) 3.4%", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset having training set size 8,844?", "answer_anchor": "ViHOS", "question_reference": "What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included in the dataset?", "explanation_reference": "The improvement in F1-score for the PhoBERT_Large model when additional clean comments are included is directly stated in the Experiments and Results section, indicating the performance enhancement due to the inclusion of additional clean comments.", "evidence_reference": "PhoBERT$_{Large}$ considerably outperforms other models in the dataset without additional clean data, achieving 0.6867 in F1-score. In addition, the best model trained on Full data is XLM-R$_{Large}$, which has an F1-score of 0.7770. We find that XLM-R$_{Large}$ increased by 0.1014 and PhoBERT$_{Large}$ increased by 0.0849."}
{"question": "Consider the paper that introduces the model that shows the best overall performance in the 'Foreign' scenario. What specific condition must hold for the kernel estimator in the WR algorithm to ensure the convergence of the cdf $F^*(x)$ of $X^*$ to the target $F_0(x)$ as $n$ approaches infinity?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shown the best overall performance?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition must hold for the kernel estimator in the WR algorithm to ensure the convergence of the cdf $F^*(x)$ of $X^*$ to the target $F_0(x)$ as $n$ approaches infinity?", "explanation_reference": "The condition specified ensures that the kernel estimator used in the WR algorithm has the appropriate convergence properties. It combines requirements on the bandwidth $h_n$, the sample size $n$, and the smoothness of the density function $f$, which must be $k$ times derivable. This condition is necessary to guarantee that the cumulative distribution function (cdf) of the resampled $X^*$ converges to the target distribution $F_0(x)$ as the sample size $n$ grows to infinity.", "evidence_reference": "We introduce a classical assumption \\begin{description} \\item {\\bf (C)} : $h_n^k + \\frac{\\log(n)}{nh_n} =O( e_n^2)$ holds and  $f \\in {\\cal C}^k$ ($k$ times derivable) for some $k\\in \\N^*$. \\end{description}"}
{"question": "Consider the paper that introduces the model that scores an 81.5 in the SRL task. How does its performance change when further fine-tuning on silver data after already being used for pre-training?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets score of 81.5 in SRL task?", "answer_anchor": "AMRBART", "question_reference": "How does the performance change when further fine-tuning the model on silver data after it has already been used for pre-training?", "explanation_reference": "The performance decreases when further fine-tuning the model on silver data after it has already been used for pre-training, indicating that this additional fine-tuning step does not bring improvement and might even harm the model's performance due to the noise in the silver data.", "evidence_reference": "As discussed in Section~\\ref{sec:main-res}, we find that graph pre-training is a better way to make use of silver data compared with fine-tuning. We further investigate whether fine-tuning our model on silver data can still bring improvement. As shown in Table~\\ref{tab:silverdata}, our models achieve the best performance on all tasks and datasets, indicating that further fine-tuning our models on silver data decreases the performance."}
{"question": "Consider the paper that introduces the model that has the highest relative performance in few-shot prompting. How does the temperature rescaling phenomenon observed in RLHF differ when responding to creative versus factual prompts?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model has the highest accuracy in few-shot prompting?", "answer_anchor": "LLaMA-30B", "question_reference": "How does the temperature rescaling phenomenon observed in RLHF differ when responding to creative versus factual prompts?", "explanation_reference": "The temperature rescaling phenomenon observed in RLHF shows that for creative prompts, an increase in temperature continues to generate diversity across various RLHF iterations, as indicated by the Self-BLEU slope pattern comparable to that of the SFT model. However, for factual prompts, despite the rising temperature, the model learns to consistently provide the same response, as shown by the diminishing Self-BLEU slope over time. This indicates that RLHF dynamically adjusts the temperature based on the type of prompt, maintaining diversity for creative prompts while reducing it for factual ones.", "evidence_reference": "For instance, when it comes to prompts associated with creativity, such as ``Write a poem,'' an increase in temperature continues to generate diversity across our various RLHF iterations. This can be observed in the Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model. On the other hand, for prompts based on factual information, such as ``What is the capital of ?'' the Self-BLEU slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to consistently provide the same response to factual prompts."}
{"question": "Consider the paper that introduces the model that corresponds to the fifth row of the table. What is the role of the 'remainders' variable in the implementation of the Universal Transformer, with dynamic halting?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1807.03819", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model in the fifth row of the table?", "answer_anchor": "UT", "question_reference": "In the implementation of the Universal Transformer with dynamic halting, what is the role of the 'remainders' variable?", "explanation_reference": "The 'remainders' variable is used to compute the amount of computation (or 'time') left for each input symbol that has not yet halted. When a symbol halts, its remainder (the portion of computation time not used) is calculated and added to the halting probability, ensuring that the total computation time sums to 1 for each symbol. This mechanism allows the model to dynamically allocate different amounts of computation to different parts of the input.", "evidence_reference": "remainders += new_halted * (1 - halting_probability)"}
{"question": "Consider the paper that introduces the benchmark that has the highest 'Generation Token Length' in the figure. What specific criteria were used to exclude tasks from its subset, known as BIG-Bench Hard (BBH), due to their reliance on specialized knowledge or being outside the scope of the work?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which benchmark has the highest 'Generation Token Length' in the figure?", "answer_anchor": "BBH", "question_reference": "What specific criteria were used to exclude tasks from the BIG-Bench Hard (BBH) subset due to their reliance on specialized knowledge or being outside the scope of the work?", "explanation_reference": "The criteria for excluding tasks from the BBH subset due to their reliance on specialized knowledge or being outside the scope of the work were explicitly listed in the appendix under the heading 'Criteria: Task is outside the scope of this work.'", "evidence_reference": "Criteria: Task is outside the scope of this work: not solvable by authors within 60 minutes, requires specialized knowledge, or not even worth attempting with chain-of-thought."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category. How does the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category ensure the functionality preservation during the model expansion process?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.06311", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category?", "answer_anchor": "ELLE", "question_reference": "How does ELLE ensure the functionality preservation during the model expansion process?", "explanation_reference": "The paper describes a two-step process to ensure functionality preservation during model expansion. First, for width expansion, ELLE uses function preserving initialization (FPI) which expands the matrices of all modules of a Transformer layer to larger sizes through parameter replication and introduces random noises to break the symmetry after replication. For depth expansion, ELLE introduces a novel layer insertion method that allows for flexible expansion by copying and inserting layers without violating the processing order of the original layer sequence. Following these expansions, a function recovering warmup (FRW) is performed to recover any lost functionality and prepare the model for further training.", "evidence_reference": "For width expansion, we borrow the function preserving initialization (FPI) from \\citet{chen2021bert2bert}. For a brief introduction, FPI expands the matrices of all modules of a Transformer layer to arbitrary larger sizes and constructs an enlarged PLM $\\mathcal{M}_{i-1}^\\text{W}$. $\\mathcal{M}_{i-1}^\\text{W}$ is initialized using the corresponding matrices of the original $\\mathcal{M}_{i-1}$ through parameter replication. For depth expansion, previous works generally resort to stacking all the original PLM layers into $2\\times$ layers through parameter replication. Such initialization is demonstrated to improve training efficiency. However, the above \\textit{layer stacking} method restricts the number of layers of the enlarged PLM $\\mathcal{M}_{i-1}^\\text{D}$ to be integer multiples of that of the original PLM $\\mathcal{M}_{i-1}$, which is not flexible for practical uses. To improve the expansion flexibility so that $\\mathcal{M}_{i-1}$ could be expanded with arbitrary number of layers, we propose a novel \\textit{layer insertion} method to construct a new PLM $\\mathcal{M}_{i-1}^\\text{D}$ with $L+L'$ layers, where $1 \\le L' \\le L$. Since the above model expansion cannot ensure exact function preservation and inevitably results in functionality loss and performance drops, we pre-train the initialized PLM $\\mathcal{M}_{i-1}^{\\text{WD}}$ on the previous corpora $\\overline{\\mathcal{D}}_{i-1}^{sub}$ conserved in the memory to recover the language abilities lost during model expansion, which is dubbed as function recovering warmup (FRW)."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 25.9 in the Seen, Val, SR dataset. How does the model's approach, proposed in the paper, to handling object interactions differ from the method proposed by Shridhar et al. in terms of object class reasoning?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows score of 25.9 in Seen, Val, SR dataset?", "answer_anchor": "MOCA", "question_reference": "How does the MOCA model's approach to handling object interactions differ from the method proposed by Shridhar et al. in terms of object class reasoning?", "explanation_reference": "MOCA's approach to object interactions involves reasoning about object classes, enabling it to interact with the correct object by leveraging object-centric localisation (OCL). This contrasts with Shridhar et al.'s method, which generates class-agnostic interaction masks, lacking the ability to reason about object categories.", "evidence_reference": "Object-Centric Localisation (OCL) allows our method to reason about object classes (Sec.~\\ref{section:target_class}) which ensures interaction with the correct object. This is in contrast with~\\cite{shridhar2020alfred} that upsamples a linear embedding via a deconvolution network and predicts class-agnostic masks, thereby not preserving any information about object category."}
{"question": "Consider the paper that introduces the model that has the highest relative performance in few-shot prompting. What specific method does the paper propose to address the limitations of large language models (LLMs) in maintaining multi-turn consistency in dialogues?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model has the highest accuracy in few-shot prompting?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific method does the paper propose to address the limitations of LLMs in maintaining multi-turn consistency in dialogues?", "explanation_reference": "The paper introduces Ghost Attention (GAtt) as a method to address the limitations of Large Language Models (LLMs) in maintaining multi-turn consistency in dialogues. GAtt is described as a simple method inspired by Context Distillation that hacks the fine-tuning data to help the attention focus in a multi-stage process, enabling dialogue control over multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right)."}
{"question": "Consider the paper that introduces the method that exhibits a score of 34.9 in the Acc-7 metric on MOSI. What specific regularization techniques were applied to the $\\mathcal{U}_v$, $\\mathcal{U}_a$, and $\\mathcal{U}_s$ subnetworks in the model proposed in the paper, and what were their parameter values?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method that shows 34.7 score in Acc-7 metric on MOSI?", "answer_anchor": "TFN", "question_reference": "What specific regularization techniques were applied to the $\\mathcal{U}_v$, $\\mathcal{U}_a$, and $\\mathcal{U}_s$ subnetworks in the Tensor Fusion Network model, and what were their parameter values?", "explanation_reference": "The regularization techniques applied to the subnetworks are explicitly mentioned in the methodology section, specifying both the dropout rate and the L2 norm coefficient used for regularization.", "evidence_reference": "The TFN model is trained using the Adam optimizer with the learning rate $5\\mathrm{e}\u22124$. $\\mathcal{U}_v$ and $\\mathcal{U}_a$, $\\mathcal{U}_s$ subnetworks are regularized using dropout on all hidden layers with $p=0.15$ and L2 norm coefficient $0.01$."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, SR dataset. What specific architectural modification allows its Segment-Level Recurrent Action Decoder to perform cross-attention over encoder states, diverging from the original TransformerXL design?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the best score in Seen, Val, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "What specific architectural modification allows the Segment-Level Recurrent Action Decoder in EmBERT to perform cross-attention over encoder states, diverging from the original TransformerXL design?", "explanation_reference": "The question targets a detailed aspect of the Segment-Level Recurrent Action Decoder's design, specifically how it adapts the TransformerXL architecture to enable cross-attention between decoder and encoder states. This is a nuanced detail that requires understanding both the original TransformerXL limitations and the specific modifications made by EmBERT to overcome these limitations for its task.", "evidence_reference": "Therefore, we introduce two novel elements to its architecture: 1) \\textit{encoder hidden states cache}; 2) \\textit{cross-attention over encoder states}. First, our extended context is composed of both agent state representations and hidden states from the previous segment $\\mathbf{s}_i$. In addition, to perform cross-attention between decoder and encoder hidden states, we modify the TransformerXL self-attention mechanism following common practice in designing transformer decoders~\\cite{vaswani2017attention}."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 84.70% WInToRe. What specific computational advantage does it have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shows 84.70% WInToRe?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "explanation_reference": "GRIT's computational advantage in feature extraction time over VinVL and ${\\cal M}^2$ Transformer is quantitatively significant, reducing the time to 31 ms compared to 304 ms and 736 ms, respectively. This efficiency is achieved by utilizing a Deformable DETR-based detector that eliminates the need for computationally expensive regional operations such as NMS and RoI Align, which are used in the other methods.", "evidence_reference": "Table \\ref{tab:extraction} shows the comparison on feature extraction. VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms. ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms. \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms."}
{"question": "Consider the paper that introduces the model shown in the first row of the table. What is the primary reason for using hard constraints over soft constraints in the human input autocompletion process according to the authors?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "GWLAN", "question_reference": "What is the primary reason for using hard constraints over soft constraints in the human input autocompletion process according to the authors?", "explanation_reference": "The authors decided to use human inputs as hard constraints in their experiments because this method was found to be efficient and simple. Despite the comparable performance of soft and hard constraint methods in preliminary experiments, the efficiency and simplicity of using hard constraints made it the preferred choice.", "evidence_reference": "Therefore, we propose to use the human inputs as hard constraints in our later experiments, because of the method's efficiency and simplicity."}
{"question": "Consider the paper that introduces the dataset shown in the second row of the table. What is the percentage of dialogues in the norm generation model that achieved a goal completion score of 2?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset shown in the second row of the table?", "answer_anchor": "DuConv", "question_reference": "What is the percentage of dialogues in the norm generation model that achieved a goal completion score of 2?", "explanation_reference": "The percentage of dialogues that achieved a goal completion score of 2 in the norm generation model is directly reported in the analysis of goal completion and knowledge exploitation.", "evidence_reference": "goal completion & 2 & 43%"}
{"question": "Consider the paper that introduces the large language model that achieves a lower HVI score than OPT but a higher HVI score than Alpaca. What is the specific improvement in percentage points of the model proposed in the paper over GPT-3.5 in internal adversarially-designed factuality evaluations?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the large language model that demonstrates lower HVI score than OPT but higher HVI score than Alpaca?", "answer_anchor": "GPT-4", "question_reference": "What is the specific improvement in percentage points of GPT-4 over GPT-3.5 in internal adversarially-designed factuality evaluations?", "explanation_reference": "The improvement is directly stated as a comparison between GPT-4 and GPT-3.5, highlighting the progress made in reducing hallucinations and improving factuality.", "evidence_reference": "GPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have themselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our latest GPT-3.5 on our internal, adversarially-designed factuality evaluations."}
{"question": "Consider the paper that introduces the method that has an F1 score of 64.95 on PDTB-Top. What is the main reason this method, specifically the PCP method, performs worse than the model proposed in the paper on all four top-level senses of the PDTB, especially on the Temporal sense?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which is the method show a 64.95 F1 score on PDTB-Top?", "answer_anchor": "PCP", "question_reference": "What is the main reason the PIDRP method performs worse than the PCP method on all four top-level senses of the PDTB, especially on the Temporal sense?", "explanation_reference": "The paper suggests that the PIDRP method's poorer performance compared to the PCP method across all top-level senses, particularly in the Temporal sense, is due to the fact that connective prediction aligns more closely with the natural language patterns utilized during the model's pre-training stage, as opposed to direct implicit discourse relation prediction.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the model that has the second lowest MCD 1 score. How is the update vector \\(a_{ij}\\) for a single edge state \\(x_{ij}\\) computed during the triangular attention process in the model proposed in the paper?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2112.00578", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model dmonstrates the lowest MCD 1 score?", "answer_anchor": "T5-based UT", "question_reference": "In the Edge Transformer model, how is the update vector \\(a_{ij}\\) for a single edge state \\(x_{ij}\\) computed during the triangular attention process?", "explanation_reference": "The answer directly describes the computation process for the update vector \\(a_{ij}\\) during the triangular attention mechanism in the Edge Transformer model, as detailed in the paper. This process involves summing the product of attention weights \\(\\alpha_{ilj}\\) and value vectors \\(v_{ilj}\\) for each triangle formed by edges in the input graph, and then applying a weight matrix \\(W^o\\) to the result.", "evidence_reference": "For a single edge state \\(x_{ij}\\) a single-head triangular attention update outputs a vector \\(a_{ij}\\) that is computed as follows: \\(a_{ij} = W^o \\sum\\limits_{l \\in \\left[1, n\\right]} \\alpha_{ilj} v_{ilj}\\)."}
{"question": "Consider the paper that introduces the model that achieves the second highest score in the Stance column. What mathematical modification is applied in its methodology for quantifying temporal degradation (TD) to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shows consistently low accuracy than VIBE model?", "answer_anchor": "DPT", "question_reference": "In the methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "explanation_reference": "The modification ensures that the calculated value reflects performance deterioration in a consistent manner, increasing as performance worsens, irrespective of whether the misalignment is due to training data being from the past or future relative to the evaluation data.", "evidence_reference": "Let $S_{t' \\shortto t}$ indicate the performance a model trained on timestamp $t'$ data and evaluated on the timestamp $t$. Let $$ {D} (t' \\shortto{} t) = -\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t), $$  In other words, ${D} (t' \\shortto{} t)$ is a modified difference in performance between a aligned and misaligned models."}
{"question": "Consider the paper that introduces the method which has a lower F1 score than LayoutXLM and a higher F1 score than SPADE. What specific combination of features and hyperparameters resulted in the highest Key-Value F1 score in the ablation studies of the model proposed in the paper on FUNSD, according to Table 1?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having lower F1 score than LayoutXLM and high F1 score than SPADE?", "answer_anchor": "Doc2Graph", "question_reference": "What specific combination of features and hyperparameters resulted in the highest Key-Value F1 score in the ablation studies of the Doc2Graph model on FUNSD, according to Table 1?", "explanation_reference": "The highest Key-Value F1 score is directly reported in the ablation studies table (Table 1) under the combination of features and hyperparameters that includes Geometrical, Textual, and Visual features with the Edge Predictor (EP) Inner dimension set to 300 and the Input Projector Fully Connected (IP FC) layers output dimension also set to 300. This combination resulted in a Key-Value F1 score of 0.5895.", "evidence_reference": "Geometrical and textual features make the largest contribution, while visual features bring almost three points more to the Key-Value F1 score by an important increase in terms of network parameters (2.3 times more). Textual and geometrical features remain crucial for the task at hand, and their combination increase by a large amount both of their scores when used in isolation... The hyperparameters shown in the table refer to the edge predictor (EP) inner layer input dimension and the input projector fully connected (IP FC) layers (per each modality) output dimension, respectively... These changes bring an improvement of 13 points on the key-value F1 scores, between the third and fourth line of the table where we keep the features fixed... \\cmark        & \\cmark             & \\cmark          & 300                 & 300                    & \\textbf{0.9964}                    & \\textbf{0.5895}               & \\textbf{0.7903}                & 2.68"}
{"question": "Consider the paper that introduces the model that has a lower mean classification accuracy than VIBE but higher mean classification accuracy than UDALM on the Stance dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model has lower accuracy than VIBE but higher accuracy than UDALM?", "answer_anchor": "DPT", "question_reference": "Based on the findings, what is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data indicates the strength of the linear relationship between the vocabulary overlap across time periods and the performance of the model on this specific task. A value close to 1 suggests a strong positive correlation.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the model that achieves a higher TP score than GIT but a lower TP score than LLaVA. What specific activation function is employed in its parallel cross-attention mechanism for gated activation?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which models shows higher TP score than GIT but lower TP score than LLaVA?", "answer_anchor": "GRIT", "question_reference": "What specific activation function is employed in the parallel cross-attention mechanism of GRIT for gated activation?", "explanation_reference": "The parallel cross-attention mechanism in GRIT uses the sigmoid activation function for gated activation, as specified in the description of the parallel cross-attention design.", "evidence_reference": "c_i^g &= \\mathrm{sigmoid}(W^g[{a^{g}_{i}}; x^{\\prime}_{i}] + b^g), \\\\ c_i^r &= \\mathrm{sigmoid}(W^r[{a^{r}_{i}}; x^{\\prime}_{i}] + b^r)."}
{"question": "Consider the paper that introduces the dataset in the last row of the 'Inconsistency Detection' category. Based on the human evaluation results, which model demonstrated the highest percentage of factual hallucinations among its extrinsically hallucinated summaries?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset in the last row of `Inconsistency Detection` category?", "answer_anchor": "XSumFaith", "question_reference": "Based on the human evaluation results, which model demonstrated the highest percentage of factual hallucinations among its extrinsically hallucinated summaries?", "explanation_reference": "The \\bencdec model showed the highest percentage of factual hallucinations among its extrinsically hallucinated summaries, indicating that while it did produce hallucinations, a significant portion of these were still factually correct.", "evidence_reference": "The numbers in ``Hallucinated'' columns show the percentage of summaries out of 500 where at least one word was annotated by all three annotators as an intrinsic (I) or extrinsic (E) hallucination. When a summary is not marked with any hallucination, it is ``faithful'' (1- I$\\cup$E). The ``factual'' columns within the ``Hallucinated'' column show for each type (I, E and I$\\cup$E), the percentage of summaries out of 500 annotated by all three annotators as factual. The final ``Factual'' column shows the total percentage of factual summaries (Faithful + I$\\cup$E$_{\\mbox{factual}}$). The highest numbers for faithful and factual, and the lowest numbers for hallucinations are boldfaced."}
{"question": "Consider the paper that introduces the method that achieves a higher EA score than Fixed set but a lower EA score than Diverse KATE in the FinQA task. What is the impact of the order of in-context examples on the model's results for the NQ dataset using the KATE$_{\\text{nli+sts-b}}$ method?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets higher EA score than Fixed set but lower EA score than Diverse KATE in FinQA task?", "answer_anchor": "KATE", "question_reference": "What is the impact of the order of in-context examples on KATE's results for the NQ dataset using KATE$_{\\text{nli+sts-b}}$ method?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results reveals that, for the NQ dataset, arranging the examples in reverse order (where the example closer to the test prompt in the embedding space is placed closer to the test prompt in the input sequence) yields the best performance. This suggests that the proximity of semantically similar sentences to the test example may help GPT-3 leverage the relevant information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the method with an average max toxicity of more than 0.3, represented by a circle. How does the PPLM method ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is method with average max toxicity more than 0.3 but with circle label?", "answer_anchor": "PPLM", "question_reference": "How does the proposed K2T method ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "explanation_reference": "The method described as K2T modifies the score function at each decoding step by adding a shift towards the semantic space of a given guide word. This approach encourages the explicit appearance of the guide word and appropriate context for it, without requiring additional models or fine-tuning, thus maintaining the fluency and diversity of the generated text.", "evidence_reference": "In this work, we propose \\emph{Keyword2Text} (K2T), a new and simple plug-and-play method for exerting hard control during text generation. By modifying the score function, we can incorporate a semantic shift at decoding time, without additional models or fine-tuning."}
{"question": "Consider the paper that introduces the model which performs the second best in the ClinTox dataset. What specific tokenization method is used for SMILES sequences in the MolXPT pre-training corpus, and how does this method differ from the tokenization of text?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model perform the second best in the ClinTox dataset?", "answer_anchor": "MolXPT", "question_reference": "What specific tokenization method is used for SMILES sequences in the MolXPT pre-training corpus, and how does it differ from the tokenization of text?", "explanation_reference": "The answer directly addresses the question by specifying the distinct tokenization methods used for SMILES sequences and text in the MolXPT pre-training corpus. SMILES sequences are tokenized using a method based on a regular expression, which is different from the byte-pair encoding method used for text, highlighting the tailored approach to handle the unique structure of SMILES compared to natural language text.", "evidence_reference": "Text and SMILES are tokenized separately. For text, we use byte-pair encoding (BPE) to split the words into subwords. For SMILES sequences (including those in wrapped sequences), we tokenize them with the regular expression from Schwaller et al., 2018."}
{"question": "Consider the paper that introduces the method that has an accuracy of 95.20% in automatic evaluation. What is the temperature parameter value (\\(\\alpha\\)) used for positive sentiment control in the model proposed by the paper?\n\nA) 0.5\nB) 0.8\nC) 1.0\nD) 1.5", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows 95.20 accuracy in automatic evaluation?", "answer_anchor": "Discup", "question_reference": "What is the temperature parameter value (\\(\\alpha\\)) used for positive sentiment control in DisCup?", "explanation_reference": "The temperature parameter (\\(\\alpha\\)) controls the sharpness of the probability distribution in DisCup's unlikelihood training. For positive sentiment control, a specific value of \\(\\alpha\\) is chosen to optimize performance.", "evidence_reference": "For our approach, we search the temperature \\(\\alpha\\) over the value \\(\\{0.1, 0.01, 0.005, 0.001\\}\\), and finally chose \\(\\alpha = 0.005\\) for positive sentiment control,  \\(\\alpha = 0.01\\) for negative sentiment and detoxication."}
{"question": "Consider the paper that introduces the method that results in a better score than MOCA but worse score than LACMA in the Seen, Val, SR dataset. What is the primary purpose of leveraging synthetic instructions in the training of the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the score better than MOCA but worse than LACMA in Seen, Val, SR dataset?", "answer_anchor": "E.T.", "question_reference": "What is the primary purpose of leveraging synthetic instructions in the training of the Episodic Transformer?", "explanation_reference": "The question assesses the understanding of a specific strategy employed in the training process of the Episodic Transformer, which is the use of synthetic instructions. This strategy is aimed at addressing a particular challenge in the training phase, which is to separate the process of understanding the visual aspects of an environment from the complexities introduced by the variations in natural language instructions. This is a detail that directly relates to the core concept of how the Episodic Transformer is trained to handle the dual challenges of visual and language understanding in navigation tasks.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the model that shows 0 accuracy in the COGS-structural dataset. What specific mechanism does it employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model shows 0 accuracy in COGS-structural dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific mechanism does the Transformer employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "explanation_reference": "The mechanism described directly addresses the need to prevent future information from influencing the prediction of the current position in the sequence, which is crucial for maintaining the auto-regressive nature of the model. This is achieved by modifying the self-attention sub-layer in the decoder to mask out future positions.", "evidence_reference": "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, GC dataset. What was the model's Task success rate in the Seen validation fold when initializing the multi-modal encoder with BERT instead of OSCAR, without predicting navigation target $O$, target parent object $P(O)$, and without the visual region classification (VRC) loss?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the best score in Seen, Val, GC dataset?", "answer_anchor": "EmBERT", "question_reference": "In the ablation studies, what was the Task success rate in the Seen validation fold when initializing the multi-modal encoder with BERT instead of OSCAR, without predicting navigation target $O$, target parent object $P(O)$, and without the visual region classification (VRC) loss?", "explanation_reference": "The question focuses on a specific detail from the ablation studies regarding the performance impact of initializing the multi-modal encoder with BERT instead of OSCAR, and the absence of certain predictions and losses. The answer directly corresponds to the Task success rate in the Seen validation fold under these conditions.", "evidence_reference": "BERT & 18 & 200 & \\cblkmark & \\cblkmark & &  26.46 (19.41)   &  35.70 (27.04)    & \\phantom{0}3.53 (\\phantom{0}1.77)   & 13.02 (\\phantom{0}7.57)"}
{"question": "Consider the paper that introduces the benchmark that has a higher 'Generation Token Length' than ShareGPT and GSM8k. What is the average human-rater performance for the 'Temporal Sequences' task in the suite it belongs to?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which benchmark has the higher 'Generation Token Length' than ShareGPT and GSM8k?", "answer_anchor": "BBH", "question_reference": "What is the average human-rater performance for the 'Temporal Sequences' task in the BIG-Bench Hard (BBH) suite?", "explanation_reference": "The average human-rater performance for the 'Temporal Sequences' task is directly provided in the detailed results table for the BBH tasks.", "evidence_reference": "Temporal Sequences & 25.0 & 52.2 & 90.8 & 100 & 33.6 & 67.2 & 77.6 & 96.8 & 39.6 & 78.8"}
{"question": "Consider the paper that introduces the model in the table that has 12M updated parameters. What is the unique aspect of the model architecture of UniKGQA that differentiates it from previous works in multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model in the table has 12M updated parameters?", "answer_anchor": "UniKGQA", "question_reference": "What is the unique aspect of the UniKGQA's model architecture that differentiates it from previous works in multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the unique aspect of UniKGQA's model architecture by highlighting the combination of a semantic matching module and a matching information propagation module, which is a distinctive approach compared to previous works in the field of multi-hop KGQA tasks. This combination allows for effective semantic matching between questions and relations and the propagation of this matching information along the directed edges on KGs, which is crucial for unifying retrieval and reasoning in the context of multi-hop KGQA.", "evidence_reference": "For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method that has a Hits@1 score ranging from 40% to 55% across the different fine-tuning samples shown in the figure. What is the core component of the model proposed in the paper that enables the propagation of matching information along the directed edges on KGs?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has Hit@1 score range from 40% to 55% as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What is the core component of UniKGQA that enables the propagation of matching information along the directed edges on KGs?", "explanation_reference": "The core component that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs) in UniKGQA is the matching information propagation module. This module is crucial for the model's ability to effectively navigate and reason over the KG by leveraging the semantic relationships and structure within the graph.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the model that achieves a BLEURT score of 0.4126. What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model gets 0.4126 in BLEURT score?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach introduced in the paper, which is a combination of predicting the distribution of question types and generating summaries focused on salient events to facilitate the generation of educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in direct prompting. What specific method does it employ to ensure the attention mechanism maintains dialogue control over multiple turns?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shows the lowest execuation accuracy in direct prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "What specific method does GAtt employ to ensure the attention mechanism focuses on maintaining dialogue control over multiple turns?", "explanation_reference": "GAtt, or Ghost Attention, is designed to improve dialogue control over multiple turns by manipulating the fine-tuning data in a way that enhances the model's focus on relevant instructions throughout a conversation. This method is a strategic intervention in the training process to address the challenge of models forgetting initial instructions after several dialogue turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process."}
{"question": "Consider the paper that introduces the model represented by the orange bar. What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 of the Vanilla Transformer exhibit, as shown in the attention visualizations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model is demonstrated by the yellow bar?", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "explanation_reference": "The answer is directly supported by the descriptions provided in the Attention Visualizations section, where it is mentioned that many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult', and that two attention heads, also in layer 5 of 6, are apparently involved in anaphora resolution. These behaviors indicate that the attention heads have learned to perform tasks related to the structural aspects of sentences.", "evidence_reference": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb `making', completing the phrase `making...more difficult'.  Attentions here shown only for the word `making'. Different colors represent different heads. Best viewed in color. Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that shows the highest Test Accuracy. What specific method does the model's solution discrimination module employ to enhance the association between a math word problem and its correct solution?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which Seq2Seq model shows the higest Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module employ to enhance the association between a math word problem and its correct solution?", "explanation_reference": "The solution discrimination module enhances the association between a math word problem and its correct solution by employing a method that focuses on gradient-guided token selection. This method identifies the most vulnerable (important) token in the solution equation based on the largest gradient, aiming to manipulate it for generating hard negative samples. This approach is designed to make the solver more accurately associate problems with their correct solutions by distinguishing them from similar but incorrect solutions.", "evidence_reference": "For generating negative solutions, any manipulation of the ground truth solution can lead to a negative one, as implemented in  \\cite{ijcai2021}. However, the  random modification neglects   the importance of tokens   in the solution equation. Although all negative solutions ultimately lead to a wrong answer, the roles they play in minimizing loss functions and serving as contrastive examples to a positive true solution are at different levels of importance.  Our goal is to find  variants of the ground truth solution as hard negative samples, which only manipulate the most vulnerable (important) token.  In fact, this target is  similar to evasion attack \\cite{carlini2017towards} on texts, i.e., maximum effect and minimum manipulation. Therefore, borrowing the idea from white-box evasion attack, we regard the token with the largest gradient as the most important and vulnerable one: \\begin{equation} \\label{grad} y_i = \\mathop{argmax\\;}_{y_i \\in Y}(\\nabla Dis([en_x(X),en_y(Y)]). \\end{equation}"}
{"question": "Consider the paper that introduces the model that achieves a score of 21.073 in 10-shot prompting. What specific performance improvement does the model proposed in the paper achieve on the RACE dataset compared to XLNet, and what does this imply about its ability to handle tasks requiring reasoning over longer contexts?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model demonstrates score of 21.073 in 10-shot prompting", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does RoBERTa achieve on the RACE dataset compared to XLNet, and what does this imply about its ability to handle tasks requiring reasoning over longer contexts?", "explanation_reference": "The question focuses on the detailed comparison of performance improvements of RoBERTa over XLNet on the RACE dataset, which is known for its requirement of reasoning over longer texts. The answer directly reflects RoBERTa's enhanced ability to manage such tasks, as evidenced by its higher accuracy scores.", "evidence_reference": "In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. \\ourmodel{} & \\bf{83.2} &  \\bf{86.5} & \\bf{81.3}\\\\ \\xlnetlarge{} & 81.7 & 85.4 & 80.2 \\\\"}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE-1 score equal to 44.52. What specific preprocessing step is applied to the Bag-of-Words (BoW) representations before training the model proposed in the paper?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method in the table that demonstrates a ROUGE 1 score equal to 44.52?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific preprocessing step is applied to the Bag-of-Words (BoW) representations before training the flow-based neural topic model?", "explanation_reference": "The specific preprocessing step applied to the BoW representations before training the flow-based neural topic model is the removal of stopwords. This step is crucial for reducing noise and focusing on the meaningful content of the documents.", "evidence_reference": "Following \\cite{wang2020friendly}, we preprocess to remove stopwords in the BoW representations."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category, which is K-Adapter. What specific improvement does K-Adapter achieve over RoBERTa in terms of Exact Match (EM) score on the SearchQA dataset?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2002.01808", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category?", "answer_anchor": "K-Adapter", "question_reference": "What specific improvement does \\textsc{K-Adapter} (F+L) achieve over RoBERTa in terms of Exact Match (EM) score on the SearchQA dataset?", "explanation_reference": "The improvement is calculated based on the EM scores provided for \\textsc{K-Adapter} (F+L) and RoBERTa on the SearchQA dataset. The EM score for \\textsc{K-Adapter} (F+L) is 61.96, and for RoBERTa, it is 59.01. The difference between these scores represents the specific improvement achieved by \\textsc{K-Adapter} (F+L) over RoBERTa. 61.69%-59.01%=2.68%", "evidence_reference": "RoBERTa                                  & 59.01          & 65.62          & 40.83          & 48.84          & 80.59             \\\\ \\textsc{K-Adapter} (F+L)                               & \\textbf{61.96} & \\textbf{67.31} & \\textbf{46.32}          & \\textbf{53.00}          & \\textbf{81.83}"}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. Why does the model's performance worsen when using beam search to decode each reasoning path instead of sampling?", "answer": "", "figure": "locality/2310.09619/Math23k_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "In the context of the self-consistency method, why does using beam search to decode each reasoning path result in worse performance compared to sampling?", "explanation_reference": "The paper explains that the key to better performance in self-consistency is the diversity of the reasoning paths. Beam search, by its nature, tends to produce less diverse outputs compared to sampling methods, which is why its performance is worse when used within the self-consistency framework.", "evidence_reference": "Note self-consistency can also adopt beam search to decode each reasoning path (results are shown as ``Self-consistency using beam search''), but its performance is worse compared to self-consistency with sampling. The reason is that beam search yields a lower diversity in the outputs."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. What specific aspect of the model proposed in the paper allows it to act as a 'self-ensemble' rather than requiring multiple models for aggregation?", "answer": "", "figure": "locality/2310.09619/Math23k_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "What specific aspect of the self-consistency method allows it to act as a 'self-ensemble' rather than requiring multiple models for aggregation?", "explanation_reference": "The question targets the unique feature of self-consistency that distinguishes it from typical ensemble approaches, which usually involve multiple models. The self-consistency method, by generating diverse reasoning paths from a single model and selecting the most consistent answer, effectively creates a 'self-ensemble' without the need for multiple models.", "evidence_reference": "Self-consistency also differs from a typical ensemble approach where multiple models are trained and the outputs from each model are aggregated, it acts more like a ``self-ensemble'' that works on top of a \\textit{single} language model."}
{"question": "Consider the paper that introduces the method that is represented by the square marker. How does the model proposed in the paper ensure that all attention heads do not deviate significantly in their reading speeds during training?", "answer": "", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is demonstrated by square marker?", "answer_anchor": "MMA", "question_reference": "How does the MMA model ensure that all attention heads do not deviate significantly in their reading speeds during training?", "explanation_reference": "The head divergence loss, L_{var}, is designed to minimize the variance in expected delays across all attention heads, thereby ensuring that no single head reads significantly faster or slower than the others. This regularization term effectively controls the divergence of the heads, making sure they operate at similar speeds.", "evidence_reference": "The final objective function is presented in \\autoref{eq:objective}:  \\begin{equation} L(\\theta) = -\\log(\\vy \\mid \\vx; \\theta) + \\lambda_{avg} L_{avg} + \\lambda_{var} L_{var} \\label{eq:objective} \\end{equation}  where $\\lambda_{avg}$, $\\lambda_{var}$ are hyperparameters that control both losses. Intuitively, while $\\lambda_{avg}$ controls the overall speed, $\\lambda_{var}$ controls the divergence of the heads."}
{"question": "Consider the paper that introduces the model that corresponds to the fifth row of the table. How does its dynamic halting mechanism influence performance on machine translation tasks compared to smaller, structured algorithmic and linguistic inference tasks?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1807.03819", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model in the fifth row of the table?", "answer_anchor": "UT", "question_reference": "How does the Universal Transformer's dynamic halting mechanism influence its performance on machine translation tasks compared to smaller, structured algorithmic and linguistic inference tasks?", "explanation_reference": "The dynamic halting mechanism, while improving accuracy on several smaller, structured algorithmic and linguistic inference tasks, has a marginal negative impact on machine translation tasks.", "evidence_reference": "We also add a dynamic per-position halting mechanism, allowing the model to choose the required number of refinement steps for each symbol dynamically, and show for the first time that such a conditional computation mechanism can in fact improve accuracy on several smaller, structured algorithmic and linguistic inference tasks (although it marginally degraded results on MT)."}
{"question": "Consider the paper that introduces the method represented by the green line in the figure. How is the initial representation for non-reserved entities generated before being input into the GNN in the model's entity-agnostic encoding process proposed in the paper?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method represented in the green line from the figure?", "answer_anchor": "EARL Quantization", "question_reference": "In the EARL model's entity-agnostic encoding process, how is the initial representation for non-reserved entities generated before being input into the GNN?", "explanation_reference": "The initial representation for non-reserved entities in the EARL model is generated by first encoding the ConRel and $k$NResEnt information separately and then combining these two encoded pieces of information. This combination is achieved through a concatenation followed by a transformation using a 2-layer MLP, as described in the Entity-Agnostic Encoding section.", "evidence_reference": "In our GNN framework, similar to previous works \\cite{CompGCN, MaKEr}, we use a linear transformation on the concatenation of entity and relation representations to aggregate the neighbor information. Specifically, the message aggregation for the entity $e$ is: \\begin{equation} \\begin{aligned} \\mathbf{m}_{e}^{l} = \\sum_{(r, t) \\in \\mathcal{O}(e)} \\mathbf{W}_{\\text{out}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_t] + \\sum_{(r,h) \\in \\mathcal{I}(e)} \\mathbf{W}_{\\text{in}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_h], \\label{eq:gnn-agg} \\end{aligned} \\end{equation} where $\\mathcal{O}(e)$ denotes the out-going relation-entity pair set of $e$ and $\\mathcal{I}(e)$ denotes the in-going relation-entity pair set. $\\mathbf{W}_{\\text{out}}^l$ and $\\mathbf{W}_{\\text{in}}^l$ are transformation matrices for out-going and in-going pairs. $l \\in [0, \\dots, L]$ denotes the layer of GNN and $L$ is the total number of GNN layers. The input entity representations are calculated in Equation (\\ref{eq:info-combine}), and the input relation representations (e.g., $\\mathbf{h}_{r}^{0}$) are looked up in a trainable relation embedding matrix $\\mathbf{R} \\in \\mathbb{R}^{|\\mathcal{R}|\\times d}$.  The entity representation of $e$ in the GNN is updated as follows: \\begin{equation} \\mathbf{h}_{e}^{l+1} = \\sigma \\left( \\frac{1}{c}\\mathbf{m}_{e}^{l} + \\mathbf{W}_{\\text{self}}^{l} \\mathbf{h}_{e}^{l} \\right), \\label{eq:gnn-update} \\end{equation} where $c=|\\mathcal{I}(e)+\\mathcal{O}(e)|$ is a normalization constant. $\\mathbf{W}_{\\rm self}^{l}$ is a matrix for self representation update, and $\\sigma$ is an activation function. Furthermore, relation representations will also be updated in each layer: $\\mathbf{h}_{r}^{l+1} = \\sigma \\left( \\mathbf{W}_{\\text{rel}}^{l} \\mathbf{h}_{r}^{l} \\right)$. We use the output representations in the $L$-th layer for entities and relations as their embeddings to calculate scores next."}
{"question": "Consider the paper that focuses on the dataset represented by the leftmost bar in the figure. In the verification ablation studies, which specific training objective combination was found to strictly improve the performance of the model proposed in the paper?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset represented on the leftmost of the figure?", "answer_anchor": "GSM8K", "question_reference": "In the verification ablation studies, which specific training objective combination was found to strictly improve the performance of verifiers?", "explanation_reference": "The paper discusses an ablation study where they compare the performance of verifiers trained with two different objectives: one with only the verification objective and another combining the verification objective with a language modeling objective. The study found that including the language modeling objective is a strict improvement over using the verification objective alone. This suggests that a better understanding of the language distribution aids the verifier in discriminating between correct and incorrect solutions.", "evidence_reference": "In \\Cref{fig:fc_verifier_loss_ablation}, we ablate the objective used when training verifiers. Although both are reasonable choices, including the language modeling objective is a strict improvement."}
{"question": "Consider the paper that introduces the optimization method that has a BLEU score of 27.3. What specific mechanism does the model proposed in the paper employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What optimization method shows BLEU score of 27.3?", "answer_anchor": "Transformer base", "question_reference": "What specific mechanism does the Transformer employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "explanation_reference": "The mechanism described directly addresses the need to prevent future information from influencing the prediction of the current position in the sequence, which is crucial for maintaining the auto-regressive nature of the model. This is achieved by modifying the self-attention sub-layer in the decoder to mask out future positions.", "evidence_reference": "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than SERA and a higher F1 score than Doc2Graph. What is the alpha value used in the data sampling process for pre-training the model proposed in the paper, and how does it influence the balance between high- and low-resource languages?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having lower F1 score than SERA and higher F1 score than Doc2Graph?", "answer_anchor": "LayoutXLM", "question_reference": "What is the alpha value used in the data sampling process for pre-training LayoutXLM, and how does it influence the balance between high- and low-resource languages?", "explanation_reference": "The alpha value used in the data sampling process for pre-training LayoutXLM is 0.7. This value is chosen to make a reasonable compromise between performance on high- and low-resource languages, indicating that it influences the balance by adjusting the sampling probability in favor of a more equitable representation of languages regardless of their resource availability.", "evidence_reference": "Following InfoXLM~\\citep{chi2020infoxlm}, we use alpha = 0.7 for LayoutXLM to make a reasonable compromise between performance on high- and low-resource languages."}
{"question": "Consider the paper that introduces the method represented by the blue line in the figure. What is the relative increase in MRR achieved by the model proposed in the paper compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method represented in the blue line from the figure?", "answer_anchor": "EARL", "question_reference": "What is the relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "explanation_reference": "The relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset is mentioned in the section summarizing the main results, where it states that EARL uses only 62% parameters and obtains a relative increase of 4.7% on MRR in comparison with RotatE.", "evidence_reference": "Specifically, on FB15k-237, \\model~uses only 62\\% parameters and obtains a relative increase of 4.7\\% on MRR in comparison with RotatE."}
{"question": "Consider the paper that introduces the method that is missing a result for the WQ-M task in the table. How does the model proposed in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method does not show a result for WQ M task?", "answer_anchor": "DSM", "question_reference": "How does the proposed approach in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "explanation_reference": "The paper discusses leveraging external natural questions to diversify question generation, indicating that this method can generate questions with different expressions by covering a broader range of semantic patterns and expressions. This approach implicitly addresses both word-level and structure-level diversity by introducing varied semantic patterns from external sources.", "evidence_reference": "In our approach, we introduce external natural questions to diversify question generation, which can generate questions with different expressions, since these natural questions cover a wider range of semantic patterns and expressions."}
{"question": "Consider the paper that introduces the optimization method which exhibits the second-highest reward accuracy. How does the reparameterization of the reward function in terms of its corresponding optimal policy and the reference policy in the model proposed by the paper address the under-specification issue inherent in the Plackett-Luce family of models?", "answer": "", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What optimization method demonstrates the second highest reward accuracy?", "answer_anchor": "DPO", "question_reference": "In the context of Direct Preference Optimization (DPO), how does the reparameterization of the reward function in terms of its corresponding optimal policy and the reference policy address the under-specification issue inherent in the Plackett-Luce family of models?", "explanation_reference": "The reparameterization of the reward function in terms of its corresponding optimal policy and the reference policy directly addresses the under-specification issue by canceling out the partition function in the preference model equation. This simplification allows the human preference probability to be expressed solely in terms of the optimal policy and reference policy, thus providing a clear and direct method to optimize the language model policy based on human preferences without the need for an explicit reward model.", "evidence_reference": "Substituting the reparameterization in Eq.~\\ref{eq:main_eq} for $r^*(x,y)$ into the preference model Eq.~\\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\\pi^*$ and reference policy $\\piref$."}
{"question": "Consider the paper that introduces the model that is in the second-to-last row of the table. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of AI venue classification?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model does not show a decrease in accuracy from the figure?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of AI venue classification?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the AI venue classification task. A value of 0.9303959931770183 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also tends to increase.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method which has a lower F1 score than Doc2Graph and a higher F1 score than GNN+MLP. What specific algorithm does the model proposed in the paper apply to prevent loops and token redundancy in parses, and how does it function?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having lower F1 score than Doc2Graph and higher F1 score than GNN+MLP?", "answer_anchor": "SPADE", "question_reference": "What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically mentioned as a method applied by SPADE to prevent loops and token redundancy in parses. It functions by iteratively trimming tail-sharing-edges and generating new edges until the process becomes self-consistent, with a maximum iteration limit set to 20.", "evidence_reference": "Based on this property, we apply the following simple yet powerful tail collision avoidance algorithm: (1) at each tail node having multiple incoming edges, all edges are trimmed except the one with the highest linking probability; (2) at each head node of the trimmed edges, the new tail node is found by drawing the next probable edge whose probability is larger than $p_{th}$ and belongs to the top three; (3) go back to Step 1 and repeat the routine until the process becomes self-consistent or the max iteration limit is reached (set to 20 in this paper). The algorithm prevents loops and token redundancy in parses."}
{"question": "Consider the paper that introduces the method that is placed directly above the PHA method in the table. What is the statistical significance level (p-value) used to determine the performance difference of the model proposed in the paper in the GLUE benchmark?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shown in the table is right above PHA method?", "answer_anchor": "HyperDecoder", "question_reference": "What is the statistical significance level (p-value) used to determine the performance difference of the Hyperdecoder model in the GLUE benchmark?", "explanation_reference": "The statistical significance level used to determine the performance difference of the Hyperdecoder model in the GLUE benchmark is indicated by the symbol '*' next to the performance values, which is defined as p < 0.05 in the paper.", "evidence_reference": "* indicates value is statistically significant ($p < 0.05$)."}
{"question": "Consider the paper that introduces the model that achieves an F1 score of 73.1 in the en_city category. How does its structured summarization approach ensure the generation of valid segment boundary positions within the task semantics?", "answer": "", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model gets 73.1 F1 score in en_city category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "How does the structured summarization approach ensure the generation of valid segment boundary positions within the task semantics?", "explanation_reference": "The paper addresses the potential issue of generating invalid segment boundary positions in the output sequence of the structured summarization models. It does so by calculating the fraction of erroneous outputs, such as non-integer-convertible components or out-of-bound segment boundary positions, and demonstrating that such errors are exceedingly rare. This indicates that the transformer decoders used in the structured summarization models are capable of accurately generating tokens that represent integer values within the semantic bounds of the task, ensuring the validity of the generated segment boundary positions.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics."}
{"question": "Consider the paper that introduces the method which is directly above the dashed line in few-shot prompting. How does the model proposed in the paper's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "answer": "", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method is shown right above the dashed line in few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "explanation_reference": "Traditional methods for the RefCOCOg task typically involve classification over a set of visual regions, requiring task-specific architectures and objectives. In contrast, the unified framework proposed in the paper treats RefCOCOg as a text generation task, leveraging the same language modeling architecture and objective used for other vision-and-language tasks. This approach allows for more flexible architecture design and eliminates the need for task-specific modifications.", "evidence_reference": "While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously~\\cite{Yu2018,Chen2020} formulated classification task over a set of visual regions, allowing more flexible architecture design."}
{"question": "Consider the paper that introduces the method that has the second lowest overall performance for the Seen condition. How does the model's, proposed in the paper, factorized approach compare to Shridhar et al.'s single-branch model in terms of task success rates across the 7 high-level categories in the ALFRED benchmark?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the second lowest over performance for Seen condition?", "answer_anchor": "MOCA", "question_reference": "How does the performance of MOCA's factorized approach compare to Shridhar et al.'s single-branch model in terms of task success rates across the 7 high-level categories in the ALFRED benchmark?", "explanation_reference": "The question focuses on comparing the performance of MOCA's factorized approach to the single-branch model by Shridhar et al. across different task types in the ALFRED benchmark. The answer is supported by the data presented in the paper, which shows MOCA achieving higher task success rates in both seen and unseen environments across all categories, indicating the effectiveness of MOCA's factorized approach over the single-branch model.", "evidence_reference": "Tasks in ALFRED~[33] are divided into 7 high-level categories. Table~\\ref{tab:abla_task} shows the performance of our factorized agent on each task type. On short-horizon tasks such as \\textbf{Pick \\& Place} and \\textbf{Examine}, Shridhar \\etal~[33] which is a single-branch model succeeds in some trajectories in seen environments, but has near zero unseen success rates. However, our agent outperforms them in both seen and unseen scenes by large margins."}
{"question": "Consider the paper that introduces the dataset in the last row of the 'Inconsistency Detection' category. What is the Spearman's correlation coefficient for the model's textual entailment scores with factual human scores?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset in the last row of `Inconsistency Detection` category?", "answer_anchor": "XSumFaith", "question_reference": "What is the Spearman's correlation coefficient for the textual entailment scores with factual human scores?", "explanation_reference": "The Spearman's correlation coefficient for textual entailment scores with factual human scores directly measures the strength and direction of association between these two variables. The value of 0.264 indicates a weak positive correlation, suggesting that as textual entailment scores increase, factual human scores tend to increase as well, albeit weakly.", "evidence_reference": "Spearman's correlation coefficient ($|r_s|$) of different metrics with faithful and factual annotations. [...] Entailment & \\textbf{0.431} & \\textbf{0.264}"}
{"question": "Consider the paper that introduces which model is shown on the penultimate line of the table. What is the peak learning rate used during the pre-training of the MolXPT?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shown on the penult line?", "answer_anchor": "MolXPT", "question_reference": "What is the peak learning rate used during the pre-training of MolXPT?", "explanation_reference": "The peak learning rate is a specific hyperparameter value used during the pre-training phase of the MolXPT model. It is mentioned directly in the section detailing the pre-training hyper-parameters.", "evidence_reference": "The peak learning rate is $0.0005$ and the warm-up steps are 20000."}
{"question": "Consider the paper that introduces the method in the table that corresponds to the highest ROUGE 2 score. How does the application of normalizing flow in the model proposed in the paper specifically contribute to the improvement of abstractive text summarization performance?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method in the table that demonstrates the highest ROUGE 2 score?", "answer_anchor": "PEGASUS+NTM", "question_reference": "How does the application of normalizing flow in the neural topic model specifically contribute to the improvement of abstractive text summarization performance?", "explanation_reference": "The application of normalizing flow in the neural topic model allows for a more complex and expressive approximation of the document's global semantics, which in turn enhances the summarization model's ability to capture and integrate these semantics into the generated summaries. This leads to summaries that are more informative and coherent.", "evidence_reference": "In this work, we adapt the normalizing flow to the neural topic model to better grasp the global semantic patterns of the document."}
{"question": "Consider the paper that introduces the method that has an accuracy of 78.1% on the VQA-v2 task. How does the model's performance on the visual question answering (VQA) task compare when using images of resolution 480x480 versus 640x640 during finetuning?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method shows 78.1 Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "How does the OFA model's performance on the visual question answering (VQA) task compare when using images of resolution 480x480 versus 640x640 during finetuning?", "explanation_reference": "The paper mentions that when finetuning OFA models, particularly the $\\text{OFA}\\rm_{Large}$ and $\\text{OFA}\\rm_{Huge}$, the image resolution is increased from 480x480 to 640x640. This detail implies that using a higher resolution during finetuning is expected to improve the model's performance on tasks like VQA, as higher resolution images provide more detailed visual information for the model to analyze.", "evidence_reference": "When finetuning $\\text{OFA}\\rm_{Large}$ and $\\text{OFA}\\rm_{Huge}$, we increase the image resolution from $480$ to $640$."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, SR dataset. What was the Task success rate in the Seen validation fold when initializing the multi-modal encoder with BERT instead of OSCAR, without predicting navigation target $O$, target parent object $P(O)$, and without the visual region classification (VRC) loss in the ablation studies?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the best score in Seen, Val, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "In the ablation studies, what was the Task success rate in the Seen validation fold when initializing the multi-modal encoder with BERT instead of OSCAR, without predicting navigation target $O$, target parent object $P(O)$, and without the visual region classification (VRC) loss?", "explanation_reference": "The question focuses on a specific detail from the ablation studies regarding the performance impact of initializing the multi-modal encoder with BERT instead of OSCAR, and the absence of certain predictions and losses. The answer directly corresponds to the Task success rate in the Seen validation fold under these conditions.", "evidence_reference": "BERT & 18 & 200 & \\cblkmark & \\cblkmark & &  26.46 (19.41)   &  35.70 (27.04)    & \\phantom{0}3.53 (\\phantom{0}1.77)   & 13.02 (\\phantom{0}7.57)"}
{"question": "Consider the paper that introduces the model that has an F1 score higher than PCP's but lower than DiscoPrompt's on PDTB-Top. What is the Dice similarity score between Instances (1) and (3) in Figure 1, and how is it calculated?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method with an F1 score higher than PCP but lower than DiscoPrompt?", "answer_anchor": "GOLF", "question_reference": "What is the Dice similarity score between Instances (1) and (3) in Figure 1, and how is it calculated?", "explanation_reference": "The Dice similarity score between Instances (1) and (3) is calculated based on their sense label sequences across three hierarchical levels (Top, Second, and Connective), considering six sub-paths in the hierarchies. The calculation involves the Dice coefficient for each sub-path among the hierarchical levels and takes the average as the similarity score. The paper provides a detailed example calculation leading to a score of approximately 0.7.", "evidence_reference": "Taking Instances (1) and (3) in Figure \\ref{fig: example1} as examples, their label sequences are \\emph{Top: Comparison, Sec: Contrast, Conn: but} and \\emph{Top: Comparison, Sec: Contrast, Conn: however}, respectively. Then the similarity score would be $\\frac{1}{6}(\\frac{2\\times 1}{1+1} + \\frac{2\\times 1}{1+1} + \\frac{2\\times 0}{1+1} + \\frac{2\\times 2}{2+2} + \\frac{2\\times 1}{2+2} + \\frac{2\\times 2}{3+3})\\approx0.7$."}
{"question": "Consider the paper that introduces the optimization method that exhibits the second-highest reward accuracy. What specific loss function modification is suggested in the context of the model proposed by the paper to mitigate the issue of model degeneration observed with a naive probability ratio objective?", "answer": "", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What optimization method demonstrates the second highest reward accuracy?", "answer_anchor": "DPO", "question_reference": "In the context of the DPO algorithm's implementation, what specific loss function modification is suggested to mitigate the issue of model degeneration observed with a naive probability ratio objective?", "explanation_reference": "The paper suggests incorporating a dynamic, per-example importance weight into the DPO algorithm's loss function to prevent model degeneration, a problem observed with a naive probability ratio objective. This modification is crucial for maintaining the quality and diversity of the language model's outputs.", "evidence_reference": "Intuitively, the gradient of the loss function $\\mathcal{L}_\\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint."}
{"question": "Consider the paper that introduces the method that demonstrates the second lowest Acc-7 score on MOSI. What is the theoretical computational complexity reduction achieved by the proposed model in the paper compared to the Tensor Fusion Networks (TFN) model?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1806.00064", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method demonstrates the lowest Acc-7 score on MOSI?", "answer_anchor": "TFN", "question_reference": "What is the theoretical computational complexity reduction achieved by the proposed Low-rank Multimodal Fusion method compared to the Tensor Fusion Networks (TFN) model?", "explanation_reference": "The question focuses on the specific detail of computational complexity reduction achieved by the proposed method over the TFN model, which is a critical aspect of the paper's contribution to efficiency in multimodal fusion. The answer succinctly captures the essence of the theoretical improvement in computational complexity, moving from an exponential to a linear relationship with respect to the number of modalities.", "evidence_reference": "Theoretically, the model complexity of our fusion method is $O(d_y \\times r \\times \\sum_{m=1}^M d_m)$ compared to $O(d_y \\prod_{m=1}^M d_m)$ of TFN from Section \\ref{par:stupid_tensor}."}
{"question": "Consider the paper that introduces the method that has a score of 3.1 in the Seen, Val, SR dataset. What specific feature of the ALFRED dataset's expert demonstrations differentiates it from using a simple object class prediction approach for interactions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows score of 3.1 in Seen, Val, SR dataset?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific feature of the ALFRED dataset's expert demonstrations differentiates it from using a simple object class prediction approach for interactions?", "explanation_reference": "The ALFRED dataset's expert demonstrations involve specifying a pixelwise interaction mask of the target object for interactions, which is more realistic and detailed compared to a simple object class prediction approach where localization is treated as a solved problem. This feature allows for precise interaction with objects in the environment, highlighting the dataset's complexity and its focus on realistic simulation for training models.", "evidence_reference": "Motivated by work in robotics on segmentation-based grasping~\\cite{mousavian2019graspnet}, agents in \\dataset{} interact with objects visually, specifying a pixelwise interaction mask of the target object."}
{"question": "Consider the paper that introduces the method that achieves the highest Precision score in the Token (I-topo) category. Which specific performance gain does the model proposed in the paper, SpanBERT, attain on the TACRED relation extraction benchmark compared to the BERT baseline reimplementation?", "answer": "", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets the highest Precision score in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific performance gain does SpanBERT achieve on the TACRED relation extraction benchmark compared to the BERT baseline reimplementation?", "explanation_reference": "The question focuses on the detailed performance comparison between SpanBERT and the reimplementation of BERT on the TACRED relation extraction benchmark. The answer directly reflects the improvement SpanBERT provides over the BERT baseline reimplementation in terms of F1 score, which is a measure of a test's accuracy.", "evidence_reference": "Table~\\ref{tab:tacred-results} shows the performance on TACRED. \\ourmodel\\ exceeds our reimplementation of BERT by 3.3\\% F1 and achieves close to the current state of the art ~\\cite{Soares2019matching} --- Our model performs better than their BERT$_\\text{EM}$ but is 0.7 point behind BERT$_\\text{EM}$ + MTB which used entity-linked text for additional pre-training."}
{"question": "Consider the paper that introduces the model that has the largest number of updated parameters. What specific strategy does the model proposed in the paper employ to update the question embedding during the path expansion process?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has the most number of updated parameters?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific strategy does the subgraph retriever (\\model) employ to update the question embedding during the path expansion process?", "explanation_reference": "The strategy for updating the question embedding during the path expansion process involves concatenating the original question with the historical expanded relations as the input of RoBERTa, which is a specific detail of how the model updates the question representation to reflect the context of the path being expanded.", "evidence_reference": "we update the embedding of the question by simply concatenating the original question with the historical expanded relations in $p^{(t)}$ as the input of RoBERTa, \\emph{i.e.}, \\beq{ \\label{eq:updatequestion} f(q^{(t)}) = \\mbox{RoBERTa}([q;r_{1};\\cdots;r_{t}]), }"}
{"question": "Consider the paper that introduces the method that is placed directly above the PHA method in the table. What was the average GLUE benchmark performance of the model proposed in the paper when using regular adapters in both the encoder and decoder?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shown in the table is right above PHA method?", "answer_anchor": "HyperDecoder", "question_reference": "In the ablation study comparing different adapter configurations, what was the average GLUE benchmark performance when using regular adapters in both the encoder and decoder?", "explanation_reference": "The question specifically targets the results from the ablation study that varied adapter configurations across the encoder and decoder. The average GLUE benchmark performance for the configuration using regular adapters in both the encoder and decoder (referred to as 'Manual-Manual' in the detailed ablation results) directly answers the question.", "evidence_reference": "Manual-Manual & 2.9% & 58.5 & 95.3 & 91.7 / 91.4 & 89.7 / 92.5 & 91.2 / 88.3 & 89.8 & 94.0 & 81.2 & 86.4"}
{"question": "Consider the paper that introduces the method that achieves a score of 31.8 in the Seen, Test, SR dataset. What was the Task success rate in the Seen validation fold when initializing the multi-modal encoder with BERT instead of OSCAR, without predicting navigation target $O$, target parent object $P(O)$, and without the visual region classification (VRC) loss?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the score of 31.8 in Seen, Test, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "In the ablation studies, what was the Task success rate in the Seen validation fold when initializing the multi-modal encoder with BERT instead of OSCAR, without predicting navigation target $O$, target parent object $P(O)$, and without the visual region classification (VRC) loss?", "explanation_reference": "The question focuses on a specific detail from the ablation studies regarding the performance impact of initializing the multi-modal encoder with BERT instead of OSCAR, and the absence of certain predictions and losses. The answer directly corresponds to the Task success rate in the Seen validation fold under these conditions.", "evidence_reference": "BERT & 18 & 200 & \\cblkmark & \\cblkmark & &  26.46 (19.41)   &  35.70 (27.04)    & \\phantom{0}3.53 (\\phantom{0}1.77)   & 13.02 (\\phantom{0}7.57)"}
{"question": "Consider the paper that introduces the model that has a score lower than 0.82 but higher than 0.815 in the Stance column. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of AI venue classification?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has accuracy consistently lower than 0.82 across all volumne of adaptive data?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of AI venue classification?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the AI venue classification task. A value of 0.9303959931770183 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also tends to increase.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the dataset which has 1 language but 13 SM tasks. What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly in the context of Chinese sentiment analysis, particularly within that dataset?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset has 1 language but 13 SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly in the context of Chinese sentiment analysis?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity in sentiment analysis, specifically mentioning the Chinese phrase '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said') as an example. This phrase can be used ironically and may not necessarily indicate agreement, illustrating the subtlety and cultural specificity of sentiment expression. The correct interpretation of this phrase requires familiarity with social media contexts, highlighting the challenge for models to understand such nuanced language use.", "evidence_reference": "For example, on Chinese social media, a comment '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically. However, this linguistic phenomenon may require familiarity with social media to interpret correctly."}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in direct prompting. What specific method does the paper propose to address its limitations in maintaining multi-turn consistency in dialogues?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shows the lowest execuation accuracy in direct prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "What specific method does the paper propose to address the limitations of LLMs in maintaining multi-turn consistency in dialogues?", "explanation_reference": "The paper introduces Ghost Attention (GAtt) as a method to address the limitations of Large Language Models (LLMs) in maintaining multi-turn consistency in dialogues. GAtt is described as a simple method inspired by Context Distillation that hacks the fine-tuning data to help the attention focus in a multi-stage process, enabling dialogue control over multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right)."}
{"question": "Consider the paper that introduces the method which has the highest perplexity. What specific computational advantage does this method have over a unidirectional classifier in terms of the number of forward passes required for computing classification probabilities for next tokens?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having the highest perplexity?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's method of computing classification probabilities for next tokens have over a unidirectional classifier in terms of the number of forward passes required?", "explanation_reference": "The computational advantage is highlighted by the fact that GeDi can compute the classification probabilities for every possible next token using significantly fewer computations compared to a unidirectional classifier, which would require a forward pass for each token in the vocabulary. This efficiency is achieved through GeDi's method of applying Bayes rule for partial sequences during generation, which allows for the computation to be done in two parallel forward passes for the desired and anti-control codes, significantly reducing the number of necessary computations.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the dataset in the table that has a validation set size of 1250. What is the model proposed in the paper's Spearman's correlation coefficient for the textual entailment scores with factual human scores?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset having 1250 val set from the table?", "answer_anchor": "XSumFaith", "question_reference": "What is the Spearman's correlation coefficient for the textual entailment scores with factual human scores?", "explanation_reference": "The Spearman's correlation coefficient for textual entailment scores with factual human scores directly measures the strength and direction of association between these two variables. The value of 0.264 indicates a weak positive correlation, suggesting that as textual entailment scores increase, factual human scores tend to increase as well, albeit weakly.", "evidence_reference": "Spearman's correlation coefficient ($|r_s|$) of different metrics with faithful and factual annotations. [...] Entailment & \\textbf{0.431} & \\textbf{0.264}"}
{"question": "Consider the paper that introduces the method that has an F1 score of 65.96. What specific layout features are used in the GCN encoder for updating the representation of the entity and edge in the model proposed by the paper?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having 65.96 F1 score?", "answer_anchor": "SERA", "question_reference": "What specific layout features are used in the GCN encoder for updating the representation of the entity and edge in the proposed model?", "explanation_reference": "The specific layout features used in the GCN encoder for updating the representation of the entity and edge are the horizontal and vertical distances between the two entity boxes, represented as [x_{i,j}, y_{i,j}].", "evidence_reference": "The edge embedding consists of 2 layout features, as the following equation shows: \\begin{equation} \\mathbf{r}_{i, j} = [x_{i,j}, y_{i,j}] \\f\\f\\f \\end{equation} where $x_{ij}$ and $y_{ij}$ are horizontal and vertical distance between the two entity boxes respectively: \\begin{equation} \\begin{aligned} \\label{ra} \\scriptsize &x_{i, j} = min(| x_i^1 - x_j^2 |, | x_j^1 - x_i^2 |) \\\\ &y_{i, j} = min(| y_i^1 - y_j^2 |, | y_j^1 - y_i^2 |) \\end{aligned} \\end{equation}"}
{"question": "Consider the paper that introduces the method that has the second lowest overall performance for the Seen condition. What is the improvement percentage of the model proposed in the paper over Nguyen et al. in the Seen Goal-Condition metric?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the second lowest over performance for Seen condition?", "answer_anchor": "MOCA", "question_reference": "What is the improvement percentage of MOCA over Nguyen et al. in the Seen Goal-Condition metric?", "explanation_reference": "The improvement percentage is calculated based on the performance metrics reported for the Seen Goal-Condition metric, where MOCA outperforms Nguyen et al. by 12.52%.", "evidence_reference": "We achieve an improvement of 14.42\\% and 3.20\\% in Seen and Unseen Task SR over Nguyen \\etal~\\cite{ngyuen_eval_winner} that won ALFRED challenge in ECCV 2020. MOCA outperforms them in both \\emph{Seen} and \\emph{Unseen} `Goal-Condition' metrics and gives an improvement of 12.52\\% and 3.39\\%, respectively."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 76.3 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What is the specific range of values for R1 when applying twin uniform quantization to post-softmax activations in the model proposed by the paper?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the quant method show 76.3 score on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific range of values for R1 when applying twin uniform quantization to post-softmax activations?", "explanation_reference": "The range for R1 when applying twin uniform quantization to post-softmax activations is defined to well quantify the values using a small scaling factor, Delta_{text{R1}}^{s}, ensuring that R1 covers values very close to zero which are predominant in post-softmax distributions. This range is specifically designed to address the unbalanced distribution of post-softmax values, where most values are close to zero, and a few large values are crucial for the attention mechanism.", "evidence_reference": "For values after softmax, the values in R1 = $[0,2^{k-1}\\Delta_{\\text{R1}}^{s})$ can be well quantified by using a small $\\Delta_{\\text{R1}}^{s}$. To avoid the effect of calibration dataset, we keeps $\\Delta_{\\text{R2}}^{s}$ fixed to $1/2^{k-1}$. Therefore, R2 = $[0,1]$ can cover the whole range, and large values can be well quantified in R2."}
{"question": "Consider the paper that introduces the method that has the second-highest Avg score on the SuperGLUE task. What specific advantage does the model proposed in the paper offer for handling long-context out-of-domain datasets in MRQA compared to a simple linear classifier?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the second highest Avg score on SuperGLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "What specific advantage does the Hyperdecoder approach offer for handling long-context out-of-domain datasets in MRQA compared to a simple linear classifier?", "explanation_reference": "The Hyperdecoder's ability to control the decoder to output specific labels when needed, and its flexibility to generate arbitrary text output for long-context out-of-domain datasets, distinguishes it from a simple linear classifier. This flexibility is crucial for multi-tasking and handling long-context documents where the model must switch between generating short set labels and arbitrary longer text.", "evidence_reference": "This is especially important for multi-tasking and long-context documents where the model must swap between generating short set labels and arbitrary longer text."}
{"question": "Consider the paper that introduces the benchmark that corresponds to the light green color in the figure. What specific improvement does CoT prompting provide over answer-only prompting for the model proposed in the paper on the 'Tracking Shuffled Objects' task?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which benchmark is represented using the light green color from the figure?", "answer_anchor": "BBH", "question_reference": "What specific improvement does CoT prompting provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "explanation_reference": "The improvement is calculated by comparing the performance increase from answer-only prompting to CoT prompting for the Codex model on the 'Tracking Shuffled Objects' task. This is derived from the performance metrics provided, showing a significant gain when using CoT prompting.", "evidence_reference": ""}
{"question": "Consider the paper that introduces the method that has an accuracy of 82.82% in the CAIL2018 task. What specific methodological limitation does the reliance on the model's graph construction layer's threshold setting introduce in its ability to distinguish confusing law articles?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shows 82.82 accuracy in CAIL2018 task?", "answer_anchor": "LADAN", "question_reference": "What specific methodological limitation does the reliance on the graph construction layer's threshold setting introduce in the LADAN model's ability to distinguish confusing law articles?", "explanation_reference": "The reliance on the graph construction layer's threshold setting for dividing law articles into communities introduces a methodological limitation in terms of accuracy. If the threshold is not optimally set, it can lead to inaccurate community detection, which in turn affects the model's ability to effectively distinguish between confusing law articles. This is because the graph distillation operator's performance is contingent on the accuracy of the law article communities obtained by the graph construction layer.", "evidence_reference": "GCL is more critical than GDO because GDO has a limited performance when the law article communities obtained by GCL are not accurate. When removing both GCL and GDO, the accuracy of LADAN decreases to that of HARNN+MTL, which powerfully demonstrates the effectiveness of our method exploiting differences among similar law articles."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. How does the performance of this method compare to OneR QA and NoR QA across different model sizes, specifically for the smallest model size?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2212.10509", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "IRCoT", "question_reference": "How does the performance of \\iconsys QA compare to OneR QA and NoR QA across different model sizes, specifically for the smallest model size?", "explanation_reference": "The question focuses on the comparison of \\iconsys QA's performance with OneR QA and NoR QA across different model sizes, highlighting the exception for the smallest model size. The answer directly addresses this by stating that \\iconsys QA outperforms OneR QA for all model sizes except for the smallest, 0.3B, which is a specific detail extracted from the results section discussing model scale impacts on QA performance.", "evidence_reference": "For all sizes except the smallest (0.2B), we see \\iconsys QA is better than OneR QA. Moreover, \\iconsys with a 3B model even outperforms OneR and NoR with a 58X larger 175B GPT3 model in all datasets."}
{"question": "Consider the paper that introduces the method that has a score of 61.2 in the BoolQ dataset with 32-shot prompting. Why was the LM adapted version of T5 chosen for prompt tuning in the experimental setup of the model proposed in the paper, despite the original version showing better performance in preliminary experiments for model tuning approaches?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having score of 61.2 in BoolQ dataset with 32-shot prompting?", "answer_anchor": "SPoT", "question_reference": "In the experimental setup for SPoT, why was the LM adapted version of T5 chosen over the original version for prompt tuning, despite the original version showing better performance in preliminary experiments for model tuning approaches?", "explanation_reference": "The LM adapted version of T5 was chosen for prompt tuning because it was found to be easier to optimize for PromptTuning, despite the original version of T5 1.1 showing better performance in preliminary experiments for model tuning approaches.", "evidence_reference": "Our frozen models are built on top of the pre-trained T5 checkpoints of all sizes... In our experiments with SPoT, we leverage the LM adapted version of T5, which was found to be easier to optimize for PromptTuning. In preliminary experiments, we found that using the original version of T5 1.1 (which was pre-trained exclusively on span corruption) for model tuning approaches results in better performance than using the LM adapted version."}
{"question": "Consider the paper that introduces the method in the table that is listed right above One-Round Distillation and right below Specialization. What is the accuracy of the model proposed in the paper, specifically T5 XXL finetuned on 20% of the GSM8K dataset data when using an external calculator?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method in the table is listed right above One-Round Distillation and right below Specializing?", "answer_anchor": "CoT Fine-tuned", "question_reference": "Based on the ablation study findings, what is the accuracy of T5 XXL finetuned on 20% of the GSM8K dataset data when using an external calculator?", "explanation_reference": "The ablation study on dataset size reveals that finetuning T5 XXL on only 20% of the GSM8K dataset data results in an accuracy of 20.47% when an external calculator is used. This detail directly answers the question by providing the specific performance metric for a subset of the data under specified conditions.", "evidence_reference": "20\\% (1067 examples) & 11.22 & 20.47"}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that shows the highest Test Accuracy. What specific methodological limitation does the paper acknowledge regarding the model's ability to incorporate commonsense knowledge, and how does it suggest addressing this limitation in future work?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which Seq2Seq model shows the higest Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific methodological limitation does the paper acknowledge regarding the solver's ability to incorporate commonsense knowledge, and how does it suggest addressing this limitation in future work?", "explanation_reference": "The paper acknowledges a limitation in the solver's ability to incorporate commonsense knowledge, which is essential for solving real-world MWP scenarios that require understanding of concepts like '1km = 1000m' or 'one day = 24 hours'. The suggested approach to address this limitation in future work is by injecting commonsense knowledge into MWP solvers, indicating a direction for enhancing the solver's capability to handle problems requiring such knowledge.", "evidence_reference": "As mentioned in \\cite{lin2020numersense,DBLP:journals/corr/abs-2107-13435}, MWP solving in the real-word scenario requires many commonsense knowledge, e.g., 1km = 1000m and one day = 24 hours. When these commonsense constants are not explicitly given in the problem description, our MWP solver has no chance to solve problems that require them. A future direction could be injecting commonsense knowledge into MWP solvers."}
{"question": "Consider the paper that introduces the method that has fewer 'rounds to completion' than GPT-4 + Belief but more 'rounds to completion' than the CBS planner. What specific form of global state input does the model proposed in the paper utilize in the SMAC domain to address the lack of critical local information?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having lower `rounds to completion` than GPT-4 + Belief but higher `rounds to completion` than CBS planner?", "answer_anchor": "MAPPO", "question_reference": "What specific form of global state input to the value function does MAPPO utilize in the SMAC domain to address the lack of critical local information?", "explanation_reference": "The paper specifies that to address the lack of critical local information in the environment-provided global state, MAPPO utilizes an Agent-Specific Global State (AS) for each agent in the SMAC domain. This AS state is formed by concatenating the environment-provided global state with the local observation for each agent, providing the value function with a more comprehensive description of the environment state.", "evidence_reference": "To address the weaknesses of the \\emph{CL} and \\emph{EP} states, we allow the value function to leverage both global and local information by forming an \\textbf{Agent-Specific Global State (AS)} which creates a global state for agent $i$ by concatenating the \\emph{EP} state and $o_i$, the local observation for agent $i$."}
{"question": "Consider the paper that introduces the model shown in the figure that performs most similarly to GPT-3.5-Turbo for the 'Plausible' and 'Foreign' scenarios. What specific condition must hold for the kernel estimator in the WR algorithm to ensure the convergence of the cdf $F^*(x)$ of $X^*$ to the target $F_0(x)$ as $n$ approaches infinity?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shown in the figure has a similar performance to GPT-3.5-Turbo?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition must hold for the kernel estimator in the WR algorithm to ensure the convergence of the cdf $F^*(x)$ of $X^*$ to the target $F_0(x)$ as $n$ approaches infinity?", "explanation_reference": "The condition specified ensures that the kernel estimator used in the WR algorithm has the appropriate convergence properties. It combines requirements on the bandwidth $h_n$, the sample size $n$, and the smoothness of the density function $f$, which must be $k$ times derivable. This condition is necessary to guarantee that the cumulative distribution function (cdf) of the resampled $X^*$ converges to the target distribution $F_0(x)$ as the sample size $n$ grows to infinity.", "evidence_reference": "We introduce a classical assumption \\begin{description} \\item {\\bf (C)} : $h_n^k + \\frac{\\log(n)}{nh_n} =O( e_n^2)$ holds and  $f \\in {\\cal C}^k$ ($k$ times derivable) for some $k\\in \\N^*$. \\end{description}"}
{"question": "Consider the paper that introduces the model represented with a dot marker. Which variant of DialoGPT was statistically indistinguishable from human responses in terms of relevance according to the human evaluation results?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model represneted with a dot marker?", "answer_anchor": "DialoGPT Large", "question_reference": "Based on the human evaluation results, which variant of DialoGPT was statistically indistinguishable from human responses in terms of relevance?", "explanation_reference": "The statistical significance testing for human evaluation showed that the differences between the 345M model and human responses in terms of relevance were not statistically significant, indicating that the 345M variant of DialoGPT was indistinguishable from human responses in this aspect.", "evidence_reference": "The differences between 345M model (2) and human response (1) are not statistically significant."}
{"question": "Consider the paper that introduces the method that has the lowest score in the 'Original Persona' column. What is the primary reason for the performance drop when the hyperparameter gamma is set to 1 in the model proposed by the paper?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the lowest score on 'Original Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the primary reason for the performance drop when the hyperparameter gamma is set to 1 in the CSN model?", "explanation_reference": "Setting gamma to 1 results in no document content being selected for response matching, effectively degenerating the model to non-document-grounded response selection, which significantly reduces its performance.", "evidence_reference": "On the other hand, when  $\\gamma=1$, \\ie, no document content is selected, it degenerates to non document-grounded response selection and the performance also drops sharply."}
{"question": "Consider the paper that introduces the method that has a score of 61.2 in the BoolQ dataset with 32-shot prompting. What specific metric is used by the model proposed in the paper to compute the similarity between the average pooled representations of the prompt tokens when defining task similarity through prompts?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having score of 61.2 in BoolQ dataset with 32-shot prompting?", "answer_anchor": "SPoT", "question_reference": "What specific metric is used to compute the similarity between the average pooled representations of the prompt tokens when defining task similarity through prompts?", "explanation_reference": "The metric used to compute the similarity between the average pooled representations of the prompt tokens when defining task similarity through prompts is explicitly mentioned as cosine similarity.", "evidence_reference": "We compute the cosine similarity between the average pooled representations of the prompt tokens: \\[sim(t^1, t^2) = cos(\\dfrac{1}{\\bmmc{L}}\\sum_{i} \\bm{e}_i^{1}, \\dfrac{1}{\\bmmc{L}}\\sum_{j} \\bm{e}_j^{2}),\\] where $\\bm{e}_i^{1}, \\bm{e}_j^{2}$ denote the respective prompt tokens of $\\bm{e}^{1}, \\bm{e}^{2}$, and $cos$ denotes the cosine similarity."}
{"question": "Consider the paper that introduces the method that exhibits a FLAN-T5 score of 52.4% using SGD in 24 domains. What is the threshold value used for transition purity in the ProceL dataset by the model proposed in the paper to avoid forming a cycle in the resulting subtask graph?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shows FLAN-T5 score of 52.4% using SGE in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "What is the threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph?", "explanation_reference": "The threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph is mentioned as part of the layer-wise precondition inference strategy to ensure that the subtask graph does not contain any cycles. This threshold is used to determine the ancestor-descendant relationships between subtasks based on their sequence of occurrence in the videos.", "evidence_reference": "We used \\(\\delta=0.96\\) for ProceL and \\(\\delta=0.55\\) for CrossTask in the experiment."}
{"question": "Consider the paper that introduces the method that has a score of 3.1 in the Seen, Val, SR dataset. What is the average number of action steps in the expert demonstrations within the ALFRED dataset for the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows score of 3.1 in Seen, Val, SR dataset?", "answer_anchor": "SEQ2SEQ", "question_reference": "What is the average number of action steps in the expert demonstrations within the ALFRED dataset?", "explanation_reference": "The average number of action steps in the expert demonstrations is directly stated as part of the dataset's characteristics.", "evidence_reference": "For 2,685 combinations of task parameters, we generate three expert demonstrations per parameter set, for a total of 8,055 unique demonstrations with an average of 50 action steps."}
{"question": "Consider the paper that introduces the model shown in the table that has an overall score of less than 3.80. What specific improvement in percentage points did the model proposed in the paper achieve over its discriminative counterparts on the out-of-domain subset for the VQA task?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shown in the table with overall score less than 3.80?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific improvement in percentage points did the generative models achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "explanation_reference": "The improvement is highlighted in the comparison between generative and discriminative models for the VQA task, specifically on the out-of-domain subset. The generative models (\\ourst{} and \\oursb{}) improved upon the discriminative baselines by 6 and 6.2 percentage points respectively, demonstrating the effectiveness of using generative modeling for questions with answers not included in the top-K answer candidates.", "evidence_reference": "This improvement is more significant on the out-of-domain subset, where the generative \\ourst{} and \\oursb{} achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling."}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What is the implication of the model's performance, specifically this method, on the CMUDoG dataset for future research directions in document-grounded conversation systems?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "CSN-word", "question_reference": "What is the implication of the CSN model's performance on the CMUDoG dataset for future research directions in document-grounded conversation systems?", "explanation_reference": "The performance of the CSN model on the CMUDoG dataset, which showed that both sentence-level and word-level content selection strategies work equally well, implies that future research could benefit from exploring document content selection at the topic level. This is suggested as a potential future work direction in the conclusion, indicating that the findings from the CMUDoG dataset performance could lead to the exploration of more granular or higher-level content selection mechanisms, such as topic-level selection, to further improve the effectiveness of document-grounded conversation systems.", "evidence_reference": "As a future work, it would be interesting to study if the selection can be done at topic level, in addition to sentence and word levels."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest number of turns. How does the performance of the model proposed in the paper, specifically \\textsc{PolyLM}-\\mySFTDatasetName-13B, on the TyDiQA-GoldP benchmark in Arabic compare to its performance in Korean?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the least number of turns from the table?", "answer_anchor": "Multialpaca", "question_reference": "How does the performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark in Arabic compare to its performance in Korean?", "explanation_reference": "The performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark is higher in Arabic (50.7 F1 score) compared to Korean (30.1 F1 score), indicating better comprehension and response generation capabilities in Arabic.", "evidence_reference": "BLOOMZ-MT-7.1B   & 22.4          & 36.6 & 26.9 & 5.8  & 9.1  & 20.2    & 26.7 & 2.4  & 14.4 & 26.5 & 17.5    \\nLLaMA-Alpaca-13B & \\textbf{59.2} & 20.8 & 48.6 & 19.3 & 37.7 & 37.1    & 11.0 & 50.6 & 20.7 & 5.7  & 22.0    \\n\\textsc{Poly}LM-\\mySFTDatasetName-13B & 58.7 & \\textbf{50.7} & \\textbf{52.1} & \\textbf{30.1} & \\textbf{40.3} & \\textbf{46.4} & 2.5 & 8.5 & 4.6 & 1.9 & 4.4"}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What specific methodological adjustment does the model proposed by the paper, DisCup, make to the control-prompt initialization process to ensure stability during training?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "Discup", "question_reference": "What specific methodological adjustment does DisCup make to the control-prompt initialization process to ensure stability during training?", "explanation_reference": "The adjustment made to the control-prompt initialization process in DisCup for ensuring stability during training involves re-parameterizing the control-prompts. This is achieved by introducing an external LSTM module that processes the initially randomly initialized control-prompts, making them closer to natural language and thus more stable for training.", "evidence_reference": "Empirically,  we re-parameterize the $P_k$ for stable training. An external $LSTM_{\\theta^\\prime}$ module is introduced to make the control-prompts close to the natural language."}
{"question": "Consider the paper that introduces the model which is placed fourth in the table. What specific aspect of its performance on the Contracts dataset compared to larger models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shown in the fourth row of the table?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's performance on the \\contractsdata dataset compared to larger models?", "explanation_reference": "The question targets a detailed aspect of the \\legalbertsmall model's performance, specifically its efficiency and effectiveness on the \\contractsdata dataset compared to larger models. This is a critical analysis question because it asks for a comparison based on the model's size and performance, highlighting the balance between computational resources and task effectiveness.", "evidence_reference": "Most importantly, (iv) we release \\legalbert, a family of \\bert models for the legal domain, intended to assist legal \\nlp research, computational law, and legal technology applications. This family includes \\legalbertsmall, a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-De task in the Test2016 dataset in Previous Image-must Systems. What specific loss function is introduced in the paper to reduce the mismatch between training and inference in its framework?", "answer": "", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model demonstrates the highest performance in En-De task in Test2016 dataset in Previous Image-must Systems?", "answer_anchor": "VALHALLA", "question_reference": "What specific loss function is introduced to reduce the mismatch between training and inference in the VALHALLA framework?", "explanation_reference": "The consistency loss is introduced to reduce the mismatch between training and inference by encouraging consistency between predictions using either ground-truth or hallucinated visual representations.", "evidence_reference": "To reduce this mismatch, we define a \\emph{consistency loss}...where $y_i^M = p(y_i | x, z, y_{<i}; \\mathbf{f}_{\\mathbf{T}})$ and $y_i^H = p(y_i | x, \\hat{z}, y_{<i}; \\mathbf{f}_{\\mathbf{T}})$ are the next word distributions from ground-truth visual tokens and hallucinated features respectively, and $\\text{KL}[y^M_i \\Vert y^H_i]$ is the Kullback-Leibler divergence between the two conditional distributions."}
{"question": "Consider the paper that introduces the method that is in the first block and has an F1 score of 50.4. What is the recall value for the model proposed in the paper using BERT-large (Bl) as the language model encoder in the UFET task before applying the Prior Knowledge about Labels (PKL) strategy?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is in the first block while having 50.4 F1 score?", "answer_anchor": "ConCN clusters", "question_reference": "What is the recall value for the DenoiseFET model using BERT-large (Bl) as the language model encoder in the UFET task before applying the Prior Knowledge about Labels (PKL) strategy?", "explanation_reference": "The recall value for the DenoiseFET model using BERT-large (Bl) as the language model encoder in the UFET task before applying the PKL strategy is directly reported in the experimental results section, indicating the model's ability to correctly identify relevant labels before the enhancement with PKL.", "evidence_reference": "DenoiseFET & Bl & 52.6 & 47.5 & 49.8"}
{"question": "Consider the paper that introduces the method that corresponds to the brown color label in the figure. What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the controlled generation experiments for the model proposed in the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method demonstrated in the figure with brown color label?", "answer_anchor": "PPLM", "question_reference": "What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the controlled generation experiments?", "explanation_reference": "The selection of \\(\\lambda_0=5\\) is based on empirical results showing that this value resulted in the best combination of average perplexity and repetition score, indicating a balance between fluency and diversity in the generated text.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the method that achieves a higher score than No Graph but a lower score than TOD-Flow using GPT-turbo with SGD in 24 domains. How does the model proposed in the paper ensure the prevention of cycles in the generated subtask graph to avoid causality paradoxes?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method has higher score than No Graph but lower score than TOD-Flow using GPT-turbo with SGD in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "How does the proposed method, MSG2, ensure the prevention of cycles in the generated subtask graph to avoid causality paradoxes?", "explanation_reference": "The method ensures the prevention of cycles in the generated subtask graph by assigning layers to each subtask based on their parent-child relationships inferred from the subtask state labels. This layer-wise precondition inference ensures that edges in the subtask graph are formed from lower to higher layers, preventing cycles and thus avoiding causality paradoxes.", "evidence_reference": "To avoid this problem, we perform precondition inference in a \\emph{layer-wise} fashion similar to~\\citet{sohn-iclr20}...After each subtask is assigned to its depth, we perform precondition inference at each depth in order of increasing depth...This ensures that the edge in the subtask graph is formed from the lower depth to the higher depth, which prevents the cycle."}
{"question": "Consider the paper that introduces the dataset which has the largest number of instances in the ABS category. What specific aspect of the 'Software' domain in the model proposed in the paper exhibits a notable discrepancy in precision during aspect discovery, and how is this discrepancy quantitatively demonstrated?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset with the most number of instances in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What specific aspect of the 'Software' domain exhibits a notable discrepancy in precision during aspect discovery, and how is this discrepancy quantitatively demonstrated?", "explanation_reference": "The question focuses on a detailed part of the paper that discusses the aspect discovery performance within the 'Software' domain. It asks for a specific quantitative measure (precision) related to aspect discovery, which is directly answered by the paper's findings on the precision rate achieved under a certain threshold setting.", "evidence_reference": "We observed a general trend of low precision for aspect discovery. We hypothesize that this is due to limited target aspects for each article; correctly extracted aspects affect negatively to precision if they do not exist in the target article. To quantify this, 10 random articles are selected from the validation set in Software domain. For each article, we extract 10 sentences labeled with the highest confidence for each of the 10 aspects, resulting in 1,000 sentences in total. Each sentence is annotated with binary labels indicating whether it is correctly associated with the aspect or not. With the threshold $\\lambda$ set to 0.9, we achieved the precision of 45.1, which shows that the aspect discovery has the ability to extract aspects, but not as good at extracting \\textit{relevant} aspects for the article."}
{"question": "Consider the paper that introduces the method that has an F1 score of 64.95 on PDTB-Top. What is the primary reason the model proposed in the paper performs worse than the PCP method across all four top-level senses of the PDTB, especially on the Temporal sense?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which is the method show a 64.95 F1 score on PDTB-Top?", "answer_anchor": "PCP", "question_reference": "What is the primary reason the PIDRP method performs worse than the PCP method across all four top-level senses of the PDTB, especially on the Temporal sense?", "explanation_reference": "The primary reason for the PIDRP method's inferior performance compared to the PCP method is attributed to the nature of connective prediction being more aligned with the natural language patterns that the model was exposed to during its pre-training phase, as opposed to the direct prediction of implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the model that achieves the best P_k score among the models in the first part of the table. How does its structured summarization approach ensure the generation of valid sentence boundary positions within the task semantics?", "answer": "", "figure": "locality/2310.11772/comparison_2_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets the best P_k score on the upper part of the table??", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "How does the structured summarization approach ensure the generation of valid sentence boundary positions within the task semantics?", "explanation_reference": "The paper addresses the potential issue of generating invalid sentence boundary positions in the output sequence of the structured summarization models. It does so by calculating the fraction of erroneous segment boundary positions in the output sequences when tested on different datasets. The results demonstrate that transformer decoders are capable of accurately generating tokens that represent integer values within the semantic bounds of the task, thereby ensuring the generation of valid sentence boundary positions.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics."}
{"question": "Consider the paper that introduces the method that exhibits the highest accuracy on the VQA-v2 task. What specific improvement in CIDEr score does the model's, proposed by the paper, text infilling pretraining task contribute to image captioning?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method shows the highest Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "What specific improvement in CIDEr score does text infilling pretraining task contribute to image captioning?", "explanation_reference": "The improvement in CIDEr score due to the text infilling pretraining task for image captioning is directly stated in the Ablation on Multitask Pretraining section, indicating the specific impact of this task on enhancing the model's performance in generating image captions.", "evidence_reference": "Text infilling brings improvement on image caption ($+0.8$ CIDEr)"}
{"question": "Consider the paper that introduces the method that has a lower F1 score than TPP and a higher F1 score than BROS. How does the inclusion of Coordinate Convolution and Corner Pooling individually and in combination affect the model's, proposed by the paper, F1 scores for Entity Labeling and Entity Linking tasks?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having lower F1 score than TPP and higher F1 score than BROS?", "answer_anchor": "MSAU-PAF", "question_reference": "How does the inclusion of Coordinate Convolution and Corner Pooling individually and in combination affect the F1 scores for Entity Labeling and Entity Linking tasks in the MSAU-PAF model?", "explanation_reference": "The ablation study section provides a detailed comparison of the model's performance with and without the inclusion of Coordinate Convolution and Corner Pooling. It quantitatively demonstrates the individual and combined impact of these components on the model's effectiveness in both Entity Labeling and Entity Linking tasks.", "evidence_reference": "Ablation study for CoordConv...experienced a total gain of 0.02 in F1 score in both Entity Labeling and Entity Linking...Ablation study for Corner Pooling...an increase of 0.01 in F1 score in Entity Labeling task and 0.01 in Entity Linking...combining CoordConv and Corner Pooling can even increase the model performance further:...improves the F1 score by 0.03 by the total in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in few-shot prompting. What specific method does it employ to ensure the attention mechanism focuses on maintaining dialogue control over multiple turns?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shows the lowest execuation accuracy in few-shot prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "What specific method does GAtt employ to ensure the attention mechanism focuses on maintaining dialogue control over multiple turns?", "explanation_reference": "GAtt, or Ghost Attention, is designed to improve dialogue control over multiple turns by manipulating the fine-tuning data in a way that enhances the model's focus on relevant instructions throughout a conversation. This method is a strategic intervention in the training process to address the challenge of models forgetting initial instructions after several dialogue turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, SR dataset. How does the performance of the model proposed in the paper with OSCAR initialization and without predicting the parent object or visual region classification compare to its performance with these features on the seen validation fold?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the best score in Seen, Val, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "How does the performance of EmBERT with OSCAR initialization and without predicting the parent object or visual region classification compare to its performance with these features on the seen validation fold?", "explanation_reference": "The performance of EmBERT on the seen validation fold is higher when it does not predict the parent object or visual region classification, as indicated by the task and GC metrics comparing the configurations with and without these features.", "evidence_reference": "OSCAR & 18 & 200 & \\cblkmark & & & \\B{37.44} (\\B{28.81}) & \\B{44.62} (\\B{36.41}) & \\B{\\phantom{0}5.73} (\\B{\\phantom{0}3.09}) & \\B{15.91} (\\B{\\phantom{0}9.33}) \\\\[1pt] OSCAR & 18 & 200 & \\cblkmark & \\cblkmark & & 36.22 (27.05) & 44.57 (35.23) & \\phantom{0}4.39 (\\phantom{0}2.21) & 13.03 (\\phantom{0}7.54)"}
{"question": "Consider the paper that introduces the method that achieves an F1 score with a mean of 58.86 in the TAT-QA task. What specific mathematical reasoning capability does the TabMWP dataset aim to assess in machines, as highlighted by the challenges presented in its design?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method demonstrates F1 score with mean 58.86 in TAT-QA task?", "answer_anchor": "PromptPG", "question_reference": "What specific mathematical reasoning capability does the TabMWP dataset aim to assess in machines, as highlighted by the challenges presented in its design?", "explanation_reference": "The TabMWP dataset is designed to assess machines' ability to perform multi-hop mathematical reasoning over heterogeneous information, which involves looking up table cells given textual clues and conducting multi-step operations to predict the final answer. This capability is highlighted as a core challenge presented by the dataset's design, requiring systems to align and reason over both textual and tabular data.", "evidence_reference": "To solve problems in \\data, a system requires multi-hop mathematical reasoning over heterogeneous information by looking up table cells given textual clues and conducting multi-step operations to predict the final answer."}
{"question": "Consider the paper that introduces the large language model which has the second lowest HVI score among those in the figure corresponding to a purple bar. What is the specific improvement in percentage points of the model proposed in the paper over GPT-3.5 in internal adversarially-designed factuality evaluations?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the large language model that demonstrates the second lowest HVI score shown in purple bar?", "answer_anchor": "GPT-4", "question_reference": "What is the specific improvement in percentage points of GPT-4 over GPT-3.5 in internal adversarially-designed factuality evaluations?", "explanation_reference": "The improvement is directly stated as a comparison between GPT-4 and GPT-3.5, highlighting the progress made in reducing hallucinations and improving factuality.", "evidence_reference": "GPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have themselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our latest GPT-3.5 on our internal, adversarially-designed factuality evaluations."}
{"question": "Consider the paper that introduces the model labeling 'fine-tuned' shown in the table. What specific adaptation in the text embeddings allows the MVQG-VL-T5 model to build correspondence among query text, label text, and objects in grounding tasks?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model being fine-tuned shown in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific adaptation in the text embeddings allows the model to build correspondence among query text, label text, and objects in grounding tasks?", "explanation_reference": "The specific adaptation that allows the model to build correspondence among query text, label text, and objects in grounding tasks is the use of embedding sharing. This is achieved by reusing the text embeddings of visual sentinel tokens as region id embeddings, which enables the model to establish a connection between the text and visual elements, particularly useful in grounding tasks.", "evidence_reference": "In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens \\{\\texttt{<vis\\_1>}, $\\dots$, \\texttt{<vis\\_n>}\\}, which corresponds to image regions. As illustrated in Fig.~\\ref{fig:architecture}, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec.~\\ref{sec:visual_embeddings}. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec.~\\ref{sec:pretraining}, referring expression comprehension in Sec.~\\ref{sec:refcoco})."}
{"question": "Consider the paper that introduces the method represented by the green line in the figure. What is the primary reason for the model's performance variance in its ablation studies across different datasets as proposed in the paper?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method represented in the green line from the figure?", "answer_anchor": "EARL Quantization", "question_reference": "What is the primary reason for the performance variance in EARL's ablation studies across different datasets?", "explanation_reference": "The variance in performance across different datasets in the ablation studies of EARL is attributed to the data statistics of these datasets, particularly the number of relations they contain. Datasets with more relations provide sufficient distinguishable information for entity embeddings, which influences the impact of removing certain components (e.g., Reserved Entity, ConRel, $k$NResEnt) on the model's performance.", "evidence_reference": "FB15k-237 and CoDEx-L have more relations than WN18RR and YAGO3-10, and diverse relations provide enough distinguishable information for entity embeddings. Thus, even in the 'w/o Reserved Entity' and 'w/o $k$NResEnt', performance is not affected dramatically since ConRel information still exists."}
{"question": "Consider the paper that introduces the method that has the second-highest Avg score on the SuperGLUE task. What is the statistical significance level (p-value) used to determine the performance difference of the model proposed in the paper in the GLUE benchmark?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the second highest Avg score on SuperGLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "What is the statistical significance level (p-value) used to determine the performance difference of the Hyperdecoder model in the GLUE benchmark?", "explanation_reference": "The statistical significance level used to determine the performance difference of the Hyperdecoder model in the GLUE benchmark is indicated by the symbol '*' next to the performance values, which is defined as p < 0.05 in the paper.", "evidence_reference": "* indicates value is statistically significant ($p < 0.05$)."}
{"question": "Consider the paper that introduces the model represented by the lavender color in the figure. Which specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools, according to the model?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model is demonstrated in the lavender color?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "explanation_reference": "The preprocessing step mentioned specifically applies a lowercase filter and Lucene's ASCIIFoldingFilter to the code, followed by tokenization using a 3-gram tokenizer, before indexing it using Elasticsearch. This step is crucial for preparing the code for efficient and effective search functionality.", "evidence_reference": "The code itself is preprocessed using a lowercase filter and Lucene's \\texttt{ASCIIFoldingFilter}, tokenized using a 3-gram tokenizer, and indexed using the default Lucene implementation of BM25 as a similarity function."}
{"question": "Consider the paper that introduces the model which achieves the highest score on the SST-2 dataset. What is the Dev F1 score for the feature-based approach using BERTbase with embeddings only, on the CoNLL-2003 NER task?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model that demonstrates the highest score on SST-2 dataset?", "answer_anchor": "BERT", "question_reference": "What is the Dev F1 score for the feature-based approach using BERTbase with embeddings only on the CoNLL-2003 NER task?", "explanation_reference": "The Dev F1 score directly measures the performance of the feature-based approach using BERTbase with only embeddings on the CoNLL-2003 Named Entity Recognition task, indicating how well the model performed without fine-tuning or additional context.", "evidence_reference": "Feature-based approach (\\bertbase) &  &  \\\\ \\;\\;\\;Embeddings & 91.0 &- \\\\"}
{"question": "Consider the paper that introduces the method that has fewer 'rounds to completion' than GPT-4 + Belief but more 'rounds to completion' than the CBS planner. What specific implementation detail is suggested in the paper to improve the stability of policy and value learning in the model proposed in the paper when dealing with the non-stationarity of MARL environments?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having lower `rounds to completion` than GPT-4 + Belief but higher `rounds to completion` than CBS planner?", "answer_anchor": "MAPPO", "question_reference": "What specific implementation detail is suggested to improve the stability of policy and value learning in MAPPO when dealing with the non-stationarity of MARL environments?", "explanation_reference": "The suggestion to use fewer epochs per update in MAPPO is aimed at improving the stability of policy and value learning by limiting the non-stationarity inherent in MARL environments. This approach is hypothesized to mitigate the effects of rapidly changing policies among agents, which can destabilize learning.", "evidence_reference": "However, we find that in multi-agent domains, MAPPO's performance degrades when samples are re-used too often. Thus, we use 15 epochs for easy tasks, and 10 or 5 epochs for difficult tasks. We hypothesize that this pattern could be a consequence of non-stationarity in MARL: using fewer epochs per update limits the change in the agents' policies, which could improve the stability of policy and value learning."}
{"question": "Consider the paper that introduces the method that has approximately 30 perplexity and the highest average max toxicity. How does the model proposed in the paper ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is method with around 30 perplexity and the highest average max toxicity?", "answer_anchor": "PPLM", "question_reference": "How does the proposed K2T method ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "explanation_reference": "The method described as K2T modifies the score function at each decoding step by adding a shift towards the semantic space of a given guide word. This approach encourages the explicit appearance of the guide word and appropriate context for it, without requiring additional models or fine-tuning, thus maintaining the fluency and diversity of the generated text.", "evidence_reference": "In this work, we propose \\emph{Keyword2Text} (K2T), a new and simple plug-and-play method for exerting hard control during text generation. By modifying the score function, we can incorporate a semantic shift at decoding time, without additional models or fine-tuning."}
{"question": "Consider the paper that introduces the method that has the third highest Avg score on the GLUE task. What is the dimension of the task embedding (I_\\tau) used in the experiments proposed by the paper?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the second highest Avg score on GLUE task?", "answer_anchor": "Hyperformer", "question_reference": "What is the dimension of the task embedding (I_\\tau)used in the experiments?", "explanation_reference": "The dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) is explicitly stated as part of the experimental setup, indicating the specific size used for encoding task features.", "evidence_reference": "We set the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) to \\(t'=512\\), and the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) to \\(t=64\\)."}
{"question": "Consider the paper that introduces the model that has a 6-layer encoder and a 6-layer decoder architecture. What specific method was used to select high-quality, in-domain sentences from the Commoncrawl corpus for back-translation into Russian, and what was the selection criterion based on?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "1907.06616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model with 6-layer encoder and 6-layer decoder architecture?", "answer_anchor": "FSMT", "question_reference": "What specific method was used to select high-quality, in-domain sentences from the Commoncrawl corpus for back-translation into Russian, and what was the selection criterion based on?", "explanation_reference": "The question focuses on the methodological detail of how the paper selected high-quality, in-domain sentences from the Commoncrawl corpus for back-translation into Russian. The answer is directly provided by describing the specific filtering method and the selection criterion used, which is a detail that reflects the paper's approach to enhancing the quality of back-translated data for training.", "evidence_reference": "In order to select a limited amount of high quality, in-domain sentences from the larger corpus, we adopt the method of~\\citet{moore2010intelligent} for selecting in-domain data (\\textsection\\ref{subsection:btcc}). We select a cutoff of $0.01$, and use all sentences that score higher than this value for back-translation, or about 5\\% of the entire dataset."}
{"question": "Consider the paper that introduces the model shown in the figure represented by the pink line. What specific condition under the DA-WR algorithm ensures the convergence of the cumulative distribution function resulting from the data augmentation process to the target distribution?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shown in the figure is represented by the pink line?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition under the DA-WR algorithm ensures the convergence of the cumulative distribution function resulting from the data augmentation process to the target distribution?", "explanation_reference": "The specific conditions (C)-(C'') ensure the convergence of the cumulative distribution function resulting from the DA-WR algorithm to the target distribution by addressing the maximum difference in indicators and weights between the initial and augmented datasets, ensuring these differences diminish as the sample size increases.", "evidence_reference": "Assume that {\\bf (C)-(C'')} hold and that the support of  $F$ contains the support of  $F_0$. Then for all $x \\in {\\cal X}$, the cdf resulting from the DA-WR algorithm converges in probabilities to $F_0(x)$ as $n \\to +\\infty$."}
{"question": "Consider the paper that introduces the model that corresponds to the lowest BERTScore F1 score on the TellMeWhy dataset. What is the K-L divergence between the predicted and ground-truth question type distributions on the test set?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model shows the lowest BERTScore F1 score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the predicted and ground-truth question type distributions on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how closely the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the large language model that corresponds to an HVI score of 47. What specific performance improvement does the model proposed in the paper exhibit over its predecessor in the context of the Uniform Bar Exam?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the large language model that demonstrates 47 HVI scores?", "answer_anchor": "GPT-4", "question_reference": "What specific performance improvement does GPT-4 exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "explanation_reference": "The question assesses understanding of GPT-4's significant improvement in performance on a professional benchmark, the Uniform Bar Exam, compared to its predecessor. This detail highlights GPT-4's advanced capabilities in understanding and generating natural language in complex scenarios.", "evidence_reference": "For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers. This contrasts with GPT-3.5, which scores in the bottom 10%."}
{"question": "Consider the paper that introduces the method that achieves an F1 score of 87.63 in the Token (I-topo) category. What specific advantage does the model's pre-training procedure offer over BERT's original method in terms of the span masking scheme?", "answer": "", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets F1 score 87.63 in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific advantage does SpanBERT's pre-training procedure offer over BERT's original method in terms of the span masking scheme?", "explanation_reference": "SpanBERT's pre-training procedure introduces a novel span masking scheme and a span boundary objective (SBO) that enables the model to predict the content of entire masked spans using only the representations of the boundary tokens. This approach differs significantly from BERT's method of masking individual tokens and predicting them independently, thus potentially capturing better the relationships between tokens within a span.", "evidence_reference": "Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it."}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What is the average number of action steps in the expert demonstrations within the ALFRED dataset for the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "SEQ2SEQ", "question_reference": "What is the average number of action steps in the expert demonstrations within the ALFRED dataset?", "explanation_reference": "The average number of action steps in the expert demonstrations is directly stated as part of the dataset's characteristics.", "evidence_reference": "For 2,685 combinations of task parameters, we generate three expert demonstrations per parameter set, for a total of 8,055 unique demonstrations with an average of 50 action steps."}
{"question": "Consider the paper that introduces the model that has the highest Recall@7 score in the CamRest task. What specific performance improvement does the model, specifically Q-TOD, achieve on the Entity F1 metric for the SMD dataset when comparing the system with a fine-tuned retriever against the off-the-shelf retriever?", "answer": "", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has the highest Recall@7 score in CamRest task?", "answer_anchor": "Q-TOD", "question_reference": "What specific performance improvement does Q-TOD achieve on the Entity F1 metric for the SMD dataset when comparing the system with a fine-tuned retriever against the off-the-shelf retriever?", "explanation_reference": "The question focuses on the detailed performance comparison between Q-TOD systems employing an off-the-shelf retriever versus a fine-tuned retriever specifically for the SMD dataset on the Entity F1 metric. The answer directly reflects the incremental benefit of fine-tuning the knowledge retriever for precision in knowledge retrieval, as indicated by the slight improvement in the Entity F1 score.", "evidence_reference": "~~Q-TOD (T5-Large) & ~~~~~71.11~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~71.17 (+0.06)~~"}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What specific challenge does the ALFRED benchmark introduce to the community that is highlighted as a significant gap in the model proposed by the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific challenge does the ALFRED benchmark introduce to the community that is highlighted as a significant gap in existing models?", "explanation_reference": "The paper emphasizes that the long horizon of tasks in the ALFRED benchmark poses a significant challenge for existing models, which is a gap that needs to be addressed for improving performance on complex vision-and-language planning tasks.", "evidence_reference": "While this model is relatively competent at accomplishing some sub-goals (\\eg operating microwaves is similar across \\textbf{Heat \\& Place} tasks), the overall task success rates are poor. The long horizon of \\dataset{} tasks poses a significant challenge  with sub-problems including visual semantic navigation, object detection, referring expression grounding, and action grounding."}
{"question": "Consider the paper that introduces the model depicted in the figure which exhibits the highest fluctuation. What specific method from continual learning is employed in the model proposed by the paper to address the negative transfer problem observed in several dimensions during its training process?", "answer": "", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the figure demonstrates the highest fluctuation?", "answer_anchor": "UniEval", "question_reference": "What specific method from continual learning is employed in UniEval to address the negative transfer problem observed in several dimensions during its training process?", "explanation_reference": "The paper mentions employing a simple and effective method from continual learning to tackle the negative transfer problem observed in several dimensions, specifically stating that whenever a new dimension is introduced, a small portion of data from all previous dimensions is added to replay. This method is known as the 'replay method' in continual learning, which helps in retaining the performance on previously learned tasks while learning new ones.", "evidence_reference": "To tackle this issue, we employ a simple and effective method from continual learning~\\cite{parisi2019continual}: whenever a new dimension is introduced, we add small portion of data from all previous dimensions to replay."}
{"question": "Consider the paper that introduces the supervised method that results in the lowest score in 10-shot prompting. What specific performance improvement does the model proposed in the paper achieve on the RACE dataset compared to XLNet, and what does this imply about its ability to handle tasks requiring reasoning over longer contexts?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which supervised method demonstrates lowest scores in 10-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does RoBERTa achieve on the RACE dataset compared to XLNet, and what does this imply about its ability to handle tasks requiring reasoning over longer contexts?", "explanation_reference": "The question focuses on the detailed comparison of performance improvements of RoBERTa over XLNet on the RACE dataset, which is known for its requirement of reasoning over longer texts. The answer directly reflects RoBERTa's enhanced ability to manage such tasks, as evidenced by its higher accuracy scores.", "evidence_reference": "In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. \\ourmodel{} & \\bf{83.2} &  \\bf{86.5} & \\bf{81.3}\\\\ \\xlnetlarge{} & 81.7 & 85.4 & 80.2 \\\\"}
{"question": "Consider the paper that introduces the model that has an overall average score of 45.68 in Previous Image-free Systems. How does its performance with hallucinated visual tokens compare to using ground-truth visual representations on the Multi30K dataset for the EN$\\rightarrow$DE task using the Transformer-Tiny model?", "answer": "", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model demonstrates the overall average score of 45.68 in previous image-free systems?", "answer_anchor": "VALHALLA", "question_reference": "How does the performance of the model proposed in the paper with hallucinated visual tokens compare to using ground-truth visual representations on the Multi30K dataset for the EN$\\rightarrow$DE task using the Transformer-Tiny model?", "explanation_reference": "The performance of VALHALLA with hallucinated visual tokens is very similar to when using ground-truth visual representations, demonstrating the model's strong ability to generate visual representations that are semantically consistent with the ground-truth.", "evidence_reference": "Moreover, \\ours has very similar performance with either hallucinated (\\texttt{V}) or ground-truth representation (\\texttt{VM}), showing strong ability to generate visual representations that are semantically consistent with the ground-truth."}
{"question": "Consider the paper that introduces the model that scores an 81.5 in the SRL task. What specific dynamic masking rate formula is adopted for the tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} during its pre-training?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets score of 81.5 in SRL task?", "answer_anchor": "AMRBART", "question_reference": "What specific dynamic masking rate formula is adopted for the tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} during pre-training?", "explanation_reference": "The formula for the dynamic masking rate is directly provided in the section discussing the unified pre-training framework, indicating how the masking probability increases over time during pre-training for specific tasks.", "evidence_reference": "Formally, at step $t$, we calculate the masking probability $p$ as:  \\begin{equation} \\label{eq:maskrate} p = 0.1 + 0.75 * t/T, \\end{equation} where $0.1$ is the initial masking rate, $T$ denotes the total training step. $p$ increases as $t$ grows, as $t$ approaches to $T$, the pre-training tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} are closer to fine-tuning tasks."}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the green color. What specific method does the model proposed in the paper use to ensure that only tokens with high probability are selected during text generation, according to the logical coherence and evidential backing provided?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method in the figure represented by teh green color?", "answer_anchor": "DExperts", "question_reference": "What specific method does the DExperts model use to ensure that only tokens with high probability are selected during text generation, according to the logical coherence and evidential backing provided in the paper?", "explanation_reference": "The DExperts model uses a 'product of experts' approach to ensure that during text generation, tokens only receive high probability if they are considered likely by the experts and unlikely by the anti-experts. This method is critical for achieving controlled text generation by filtering out undesirable attributes while promoting desired ones.", "evidence_reference": "Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts."}
{"question": "Consider the paper that introduces the method that achieves the highest Precision score in the Token (I-topo) category. What specific advantage does the model's pre-training procedure offer over BERT's original method in terms of the span masking scheme?", "answer": "", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets the highest Precision score in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific advantage does SpanBERT's pre-training procedure offer over BERT's original method in terms of the span masking scheme?", "explanation_reference": "SpanBERT's pre-training procedure introduces a novel span masking scheme and a span boundary objective (SBO) that enables the model to predict the content of entire masked spans using only the representations of the boundary tokens. This approach differs significantly from BERT's method of masking individual tokens and predicting them independently, thus potentially capturing better the relationships between tokens within a span.", "evidence_reference": "Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it."}
{"question": "Consider the paper that introduces the method which is represented by the lavender color. What specific computational advantage does GeDi have over a unidirectional classifier in terms of the number of forward passes required for computing classification probabilities for next tokens?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method represented by the lavender color?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's method of computing classification probabilities for next tokens have over a unidirectional classifier in terms of the number of forward passes required?", "explanation_reference": "The computational advantage is highlighted by the fact that GeDi can compute the classification probabilities for every possible next token using significantly fewer computations compared to a unidirectional classifier, which would require a forward pass for each token in the vocabulary. This efficiency is achieved through GeDi's method of applying Bayes rule for partial sequences during generation, which allows for the computation to be done in two parallel forward passes for the desired and anti-control codes, significantly reducing the number of necessary computations.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the model that demonstrates the highest score in the 'T3' column. What hyperparameter values were used for the FewRel dataset in the experiments?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model demonstrates the highest score in 'T3' column?", "answer_anchor": "CEAR", "question_reference": "What hyperparameter values were used for the FewRel dataset in the experiments?", "explanation_reference": "The answer directly lists the specific hyperparameter values used for the FewRel dataset as mentioned in the Implementation Details section of the paper, providing precise and concise information.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 63.8% in the StrategyQA dataset. What specific modification to the few-shot prompts used in the model's generation of Chain of Thought (CoT) is highlighted as key for improving the quality of generated data?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets 63.8 accuracy in StrategyQA dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as key for improving the quality of generated data?", "explanation_reference": "The specific modification mentioned is crucial because it allows the large language models (LLMs) to correct small mistakes in the chain of thought (CoT), thereby improving the quality of the generated data for finetuning smaller models.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the model that exhibits a 64.1 F1 score in the WebQSP dataset. What specific effect does the removal of the path ending strategy have on its Hits@1 retrieval performance on the same dataset according to the paper?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows 64.1 F1 score in WebQSP dataset?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific effect does the removal of the path ending strategy have on the Hits@1 retrieval performance on the WebQSP dataset according to the paper?", "explanation_reference": "The removal of the path ending strategy (PE) results in a significant decrease in the Hits@1 retrieval performance on the WebQSP dataset, indicating the critical role of this strategy in the model's ability to accurately retrieve relevant subgraphs.", "evidence_reference": "Table~\\ref{tb:retrieverperformance} indicates that based on \\model, Hits@1 drops 4.3-15.0\\% when removing QU (\\smodel w/o QU) and Hits@1 drops 2.1-18.5\\% when changing PE to the fixed path length $T$ (\\smodel w/o PE), where the optimal $T$ is set to 3 on both WebQSP and CWQ."}
{"question": "Consider the paper that introduces the model that achieves a higher TP score than GIT but a lower TP score than LLaVA. What specific computational advantage does it have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which models shows higher TP score than GIT but lower TP score than LLaVA?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "explanation_reference": "GRIT's computational advantage in feature extraction time over VinVL and ${\\cal M}^2$ Transformer is quantitatively significant, reducing the time to 31 ms compared to 304 ms and 736 ms, respectively. This efficiency is achieved by utilizing a Deformable DETR-based detector that eliminates the need for computationally expensive regional operations such as NMS and RoI Align, which are used in the other methods.", "evidence_reference": "Table \\ref{tab:extraction} shows the comparison on feature extraction. VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms. ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms. \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms."}
{"question": "Consider the paper that introduces the method in the table that corresponds to the highest ROUGE 2 score. What specific activation function is used in the non-linear transformation within the BoW Encoder of the neural topic model proposed by the paper?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method in the table that demonstrates the highest ROUGE 2 score?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific activation function is used in the non-linear transformation within the BoW Encoder of the neural topic model?", "explanation_reference": "The paper specifies that the non-linear transformation within the BoW Encoder uses a tanh activation function. This detail is crucial for understanding the specific architecture and functionality of the neural topic model described.", "evidence_reference": "the input $\\textbf{x}_{bow}$ is first encoded into a latent variable $\\mathbf{z}$ by a topic encoder. Each input is passed to obtain the prior mean $\\mu$ and prior standard deviation $\\sigma$ \\vspace{-2mm} \\begin{equation} \\pi = f_{MLP}(\\textbf{x}_{bow}), \\mu = f_1(\\pi), \\log \\sigma = f_2(\\pi) \\end{equation} where $f_{MLP}$ is a non-linear transformation with a $\\tanh$ activation function; $f_1$ and $f_2$ are two linear transformations with bias."}
{"question": "Consider the paper that introduces the method that has fewer 'rounds to completion' than GPT-4 + Belief but more 'rounds to completion' than the CBS planner. What is the impact of using a death mask with agent-specific global state (AS) on the model's performance in the SMAC domain?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having lower `rounds to completion` than GPT-4 + Belief but higher `rounds to completion` than CBS planner?", "answer_anchor": "MAPPO", "question_reference": "What is the impact of using a death mask with agent-specific global state (AS) on MAPPO's performance in the SMAC domain?", "explanation_reference": "The death mask effectively handles the drastic distribution shift in the critic input when an agent dies, by replacing the state for a dead agent with a zero state containing the agent's ID. This approach leads to superior performance compared to other methods of handling agent deaths, as it allows the critic to more accurately fit the average post-death reward for agent $a$ to the input $\\boldsymbol{0_a}$.", "evidence_reference": "Fig.~\\ref{fig:app-Ablation-death-new} demonstrates the effect of death mask on MAPPO(AS)'s performance in the SMAC domain, showing significant improvement."}
{"question": "Consider the paper that introduces the method that corresponds to the brown color label in the figure. What is the success rate (SR) percentage for the model's Guide Context strategy with a lambda value of 20?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is method demonstrated in the figure with brown color label?", "answer_anchor": "PPLM", "question_reference": "What is the success rate (SR) percentage for the Guide Context strategy with a lambda value of 20?", "explanation_reference": "The success rate (SR) for the Guide Context strategy with a lambda value of 20 is directly reported in the paper's results, indicating the effectiveness of this strategy at ensuring the appearance of guide words in the generated text.", "evidence_reference": "Section name: Hyperparameter Analysis_10\\nparagraphs: \\emph{C.} $\\lambda=20$ & \\textbf{95.1} \\rpm 2.3 & 99.3 \\rpm 20.1 & 13.4 \\rpm 2.1"}
{"question": "Consider the paper that introduces the model that achieves the highest score in the 'T2' column. What is the observed accuracy drop percentage for the model proposed in the paper on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates the highest score in 'T2' column?", "answer_anchor": "CEAR", "question_reference": "What is the observed accuracy drop percentage for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "explanation_reference": "The accuracy drop for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00) is directly reported in the empirical study results table.", "evidence_reference": "CRL & [0.85, 1.00) & 71.1 & 9.7 & 64.8 & 11.4"}
{"question": "Consider the paper that introduces the model labeling 'fine-tuned' shown in the table. How does the unified framework's approach to handling the RefCOCOg task diverge in performance between this model and the VL-BART model, and what is hypothesized as the reason for this divergence?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model being fine-tuned shown in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "explanation_reference": "The paper hypothesizes that the divergence in performance on the RefCOCOg task between VL-T5 and VL-BART is due to the different methods of positional encoding used by T5 and BART. Specifically, BART uses learned absolute positional embeddings, which might lead to the model memorizing the positions of training objects, resulting in high training accuracy but low validation accuracy. This hypothesis is supported by the observation of VL-BART's performance drop in the RefCOCOg task compared to VL-T5.", "evidence_reference": "We also observe that our experiments with \\oursb{} on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in self-attention layers instead. We hypothesize that \\oursb{} found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy)."}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 63.8% in the StrategyQA dataset. What key modification did the authors make to the few-shot prompts to improve the quality of chain of thought reasoning generated by the model proposed in the paper?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets 63.8 accuracy in StrategyQA dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What key modification to the few-shot prompts did the authors make to improve the quality of chain of thought reasoning generated by the teacher model?", "explanation_reference": "The key modification mentioned in the paper aimed to improve the quality of the generated chain of thought by guiding the teacher model with the target answer, which helps in correcting small mistakes in the CoT reasoning.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the model that demonstrates the lowest zh-en score. What specific advantage does it demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model demonstrates the lowest zh-en score?", "answer_anchor": "GWLAN", "question_reference": "What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "explanation_reference": "The advantage is highlighted by the fact that even though there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment. This suggests that the joint model's ability to handle multiple related tasks with a single model architecture simplifies the deployment process compared to having separate models for each context type.", "evidence_reference": "Compared with \\textsc{WPM-Sep}, \\textsc{WPM-Joint} shows two advantages. On one hand, even there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment."}
{"question": "Consider the paper that introduces the model that achieves a score of 21.073 in 10-shot prompting. What specific performance improvement does the model proposed in the paper offer over static masking for the SQuAD 2.0 task according to the paper's findings?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates score of 21.073 in 10-shot prompting", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does dynamic masking offer over static masking for the SQuAD 2.0 task according to the paper's findings?", "explanation_reference": "The improvement is calculated based on the reported median accuracy for the MNLI-m task using static and dynamic masking. The paper reports a median accuracy of 84.3% for static masking and 84.0% for dynamic masking on the MNLI-m task. The difference indicates a 0.4% improvement in favor of static masking, contrary to expectations.", "evidence_reference": "static & 78.3 & 84.3 & 92.5 \\\\ dynamic & 78.7 & 84.0 & 92.9 \\\\"}
{"question": "Consider the paper that introduces the method in the figure that has a perplexity of approximately 30 and an average max toxicity of 0.2. What specific formatting guideline is the model proposed in the paper provided for the width of the abstract text compared to the columns for the text in the body of the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method in the figure has around 30 perplexity and 0.2 average max toxicity?", "answer_anchor": "DExperts", "question_reference": "What specific formatting guideline is provided for the width of the abstract text compared to the columns for the text in the body of the paper?", "explanation_reference": "The question assesses the understanding of detailed formatting instructions for the abstract section, which is a fundamental aspect of preparing a manuscript for submission. It requires knowledge of specific measurements that ensure the abstract is formatted correctly according to the guidelines.", "evidence_reference": "Use two-column format when you begin the abstract. Type the abstract at the beginning of the first column. The width of the abstract text should be smaller than the width of the columns for the text in the body of the paper by 0.6 cm on each side."}
{"question": "Consider the paper that introduces the method that shows the lowest overall performance. What is the average number of action steps in the expert demonstrations across the ALFRED dataset for the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the lowest over performance?", "answer_anchor": "SEQ2SEQ", "question_reference": "What is the average number of action steps in the expert demonstrations across the ALFRED dataset?", "explanation_reference": "The average number of action steps in the expert demonstrations is directly stated in the content describing the dataset's expert demonstrations, indicating that each demonstration averages 50 steps.", "evidence_reference": "For 2,685 combinations of task parameters, we generate three expert demonstrations per parameter set, for a total of 8,055 unique demonstrations with an average of 50 action steps."}
{"question": "Consider the paper that introduces the methodological approach used to determine the threshold value for including sentences in aspect-based summaries within the dataset that has the largest number of instances. What specific methodological approach was used?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset with the most number of instances?", "answer_anchor": "QASUM", "question_reference": "What specific methodological approach was used to determine the threshold value for including sentences in aspect-based summaries within the OASum dataset?", "explanation_reference": "The methodological approach for determining the threshold value for including sentences in aspect-based summaries involved manually evaluating randomly selected Wikipedia pages. This process included assigning these pages to expert annotators for quality assessment, thereby ensuring the summaries' relevance and quality.", "evidence_reference": "To determine the exact value of the threshold, we try $\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]$ and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality."}
{"question": "Consider the paper that introduces the method that has a Hits@1 score ranging from 40% to 55% across the different fine-tuning samples shown in the figure. What is the unique aspect of the model's architecture proposed in the paper that differentiates it from previous works in multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has Hit@1 score range from 40% to 55% as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What is the unique aspect of the UniKGQA's model architecture that differentiates it from previous works in multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the unique aspect of UniKGQA's model architecture by highlighting the combination of a semantic matching module and a matching information propagation module, which is a distinctive approach compared to previous works in the field of multi-hop KGQA tasks. This combination allows for effective semantic matching between questions and relations and the propagation of this matching information along the directed edges on KGs, which is crucial for unifying retrieval and reasoning in the context of multi-hop KGQA.", "evidence_reference": "For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method that exhibits a FLAN-T5 score of 52.4% using SGD in 24 domains. How does it ensure the prevention of cycles in the generated subtask graph to avoid causality paradoxes?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shows FLAN-T5 score of 52.4% using SGE in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "How does the proposed method, MSG2, ensure the prevention of cycles in the generated subtask graph to avoid causality paradoxes?", "explanation_reference": "The method ensures the prevention of cycles in the generated subtask graph by assigning layers to each subtask based on their parent-child relationships inferred from the subtask state labels. This layer-wise precondition inference ensures that edges in the subtask graph are formed from lower to higher layers, preventing cycles and thus avoiding causality paradoxes.", "evidence_reference": "To avoid this problem, we perform precondition inference in a \\emph{layer-wise} fashion similar to~\\citet{sohn-iclr20}...After each subtask is assigned to its depth, we perform precondition inference at each depth in order of increasing depth...This ensures that the edge in the subtask graph is formed from the lower depth to the higher depth, which prevents the cycle."}
{"question": "Consider the paper that introduces the method that consistently achieves a higher MRR score than NodePiece. Based on the ablation studies, which dataset showed a significant performance degradation when the multi-hop neighbor information was removed from the model proposed in the paper?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method consistently shows higher MRR than NodePiece?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which dataset showed a significant performance degradation when multi-hop neighbor information was removed?", "explanation_reference": "The ablation study results indicate that removing multi-hop neighbor information ('w/o MulHop') dramatically affected the performance on the WN18RR dataset, as evidenced by the significant drop in performance metrics compared to other ablation settings.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the method in the figure represented by the 'x' (cross) marker. What is the unique aspect of the model's architecture proposed by the paper that differentiates it from previous works in multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method in the figure is demonstrated by the 'x' (cross) marker?", "answer_anchor": "UniKGQA", "question_reference": "What is the unique aspect of the UniKGQA's model architecture that differentiates it from previous works in multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the unique aspect of UniKGQA's model architecture by highlighting the combination of a semantic matching module and a matching information propagation module, which is a distinctive approach compared to previous works in the field of multi-hop KGQA tasks. This combination allows for effective semantic matching between questions and relations and the propagation of this matching information along the directed edges on KGs, which is crucial for unifying retrieval and reasoning in the context of multi-hop KGQA.", "evidence_reference": "For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the model which is placed fourth in the table. What specific aspect of its pre-training process distinguishes the model's adaptation for the legal domain from the adaptation strategies of BERT models in other specialized domains, as discussed in previous studies?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shown in the fourth row of the table?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the LEGAL-BERT's pre-training process distinguishes its adaptation for the legal domain from the adaptation strategies of BERT models in other specialized domains, as discussed in previous studies?", "explanation_reference": "The question targets the methodological uniqueness of LEGAL-BERT's adaptation process for the legal domain, specifically focusing on the creation of a new vocabulary. This aspect is crucial for adapting the model to the specialized terminology and syntax of legal texts, which is a significant methodological detail that distinguishes LEGAL-BERT from other domain-specific BERT adaptations.", "evidence_reference": "\\noindent\\\\textbf{\\\\legalbertp} has the same architecture as \\\\bertbase with 12 layers, 768 hidden units and 12 attention heads (110M parameters). We use this architecture in all our experiments unless otherwise stated. We use a newly created vocabulary of equal size to \\\\bert's vocabulary."}
{"question": "Consider the paper that introduces the method that shows the lowest overall performance. What specific challenge does the ALFRED benchmark introduce to the community that is highlighted as a significant gap in the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the lowest over performance?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific challenge does the ALFRED benchmark introduce to the community that is highlighted as a significant gap in existing models?", "explanation_reference": "The paper emphasizes that the long horizon of tasks in the ALFRED benchmark poses a significant challenge for existing models, which is a gap that needs to be addressed for improving performance on complex vision-and-language planning tasks.", "evidence_reference": "While this model is relatively competent at accomplishing some sub-goals (\\eg operating microwaves is similar across \\textbf{Heat \\& Place} tasks), the overall task success rates are poor. The long horizon of \\dataset{} tasks poses a significant challenge  with sub-problems including visual semantic navigation, object detection, referring expression grounding, and action grounding."}
{"question": "Consider the paper that introduces the method that has a METEOR cross entropy score lower than ours but higher than X-Transformer. How does the model's performance, as proposed in the paper, using ground-truth concepts for captioning compare to using predicted concepts or a mixture of both during training?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method has a METEOR score lower than (Ours) but higher than X-Transformer?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP using ground-truth concepts for captioning compare to using predicted concepts or a mixture of both during training?", "explanation_reference": "The experiment demonstrates that training with predicted concepts yields better performance than using ground-truth concepts or a mixture of both, indicating the effectiveness of the CTN in generating useful concepts for captioning.", "evidence_reference": "We experiment with different ways to train with the concept tokens. In Table~\\ref{tab:concept}, we list the results of training using GT semantic concepts encoded as tokens, GT concepts mixed with predicted concepts, and fully predicted concepts. We find that by using the predicted concepts for training leads to optimal results."}
{"question": "Consider the paper that introduces the model that scores an 81.5 in the SRL task. What specific aspect of its unified pre-training framework contributes to the model's, proposed by the paper, effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR tasks?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets score of 81.5 in SRL task?", "answer_anchor": "AMRBART", "question_reference": "What specific aspect of the unified pre-training framework contributes to its effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR tasks?", "explanation_reference": "The unified pre-training framework's effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR tasks is attributed to the introduction of a dynamic masking rate. This approach adjusts the masking probability over time, making the pre-training tasks gradually more similar to the fine-tuning tasks, thus facilitating knowledge transfer and reducing the gap between these phases.", "evidence_reference": "Different from standard masking that uses a static masking rate, we adopt a dynamic masking rate $p$ for task $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g}. Formally, at step $t$, we calculate the masking probability $p$ as:  $p = 0.1 + 0.75 * t/T$, where $0.1$ is the initial masking rate, $T$ denotes the total training step. $p$ increases as $t$ grows, as $t$ approaches to $T$, the pre-training tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} are closer to fine-tuning tasks."}
{"question": "Consider the paper that introduces the method which has the highest perplexity. What is the primary goal of discriminatively training CC-LMs with the model proposed in the paper's training?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having the highest perplexity?", "answer_anchor": "GeDi", "question_reference": "What is the primary goal of discriminatively training CC-LMs with GeDi training?", "explanation_reference": "The primary goal of discriminatively training CC-LMs with GeDi training is to enhance their capabilities as discriminators, specifically for the purpose of guiding generation in a more controlled manner. This is aimed at improving the efficiency and effectiveness of the GeDi-guided generation process.", "evidence_reference": "For this reason, we propose training CC-LMs discriminatively as classifiers with GeDi training, with the primary goal of making them better discriminators for GeDi-guided generation."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than BROS and a higher F1 score than LayoutXLM. What specific layout features are used in the model's GCN encoder to update the representation of the entity and edge, and how are these features mathematically represented?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having lower F1 score than BROS and higher F1 score than LayoutXLM?", "answer_anchor": "SERA", "question_reference": "What specific layout features are used in the GCN encoder to update the representation of the entity and edge, and how are these features mathematically represented?", "explanation_reference": "The specific layout features used in the GCN encoder to update the representation of the entity and edge are the horizontal and vertical distances between the two entity boxes, mathematically represented as [x_{i,j}, y_{i,j}]. This representation captures the spatial relationship between entities in visually rich documents, which is crucial for understanding their layout and structure.", "evidence_reference": "The edge embedding consists of 2 layout features, as the following equation shows: \\begin{equation} \\mathbf{r}_{i, j} = [x_{i,j}, y_{i,j}] \\f\\f\\f \\end{equation} where $x_{ij}$ and $y_{ij}$ are horizontal and vertical distance between the two entity boxes respectively: \\begin{equation} \\begin{aligned} \\label{ra} \\scriptsize &x_{i, j} = min(| x_i^1 - x_j^2 |, | x_j^1 - x_i^2 |) \\\\ &y_{i, j} = min(| y_i^1 - y_j^2 |, | y_j^1 - y_i^2 |) \\end{aligned} \\end{equation}"}
{"question": "Consider the paper that introduces the method that achieves a higher score than No Graph but a lower score than TOD-Flow using GPT-turbo with SGD in 24 domains. How does the model proposed in the paper, specifically the Multimodal Subtask Graph Generation (MSG^2) approach, ensure the prevention of cycles in the resulting subtask graph, thereby avoiding a causality paradox?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method has higher score than No Graph but lower score than TOD-Flow using GPT-turbo with SGD in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "How does the proposed Multimodal Subtask Graph Generation (MSG2) approach ensure the prevention of cycles in the resulting subtask graph, which could lead to a causality paradox?", "explanation_reference": "The paper addresses the potential issue of forming cycles in the resulting subtask graph, which could lead to a causality paradox, by performing precondition inference in a layer-wise fashion. This method ensures that the edge in the subtask graph is formed from the lower depth to the higher depth, preventing the formation of cycles.", "evidence_reference": "One major problem of inferring the precondition independently for each subtask is the possibility of forming a cycle in the resulting subtask graph, which leads to a causality paradox (\\ie, subtask A is a precondition of subtask B and subtask B is a precondition of subtask A). To avoid this problem, we perform precondition inference in a layer-wise fashion similar to~\\citet{sohn-iclr20}."}
{"question": "Consider the paper that introduces the model shown in the first row of the table. How does MVQG-VL-T5's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "explanation_reference": "Traditional methods for the RefCOCOg task typically involve classification over a set of visual regions, requiring task-specific architectures and objectives. In contrast, the unified framework proposed in the paper treats RefCOCOg as a text generation task, leveraging the same language modeling architecture and objective used for other vision-and-language tasks. This approach allows for more flexible architecture design and eliminates the need for task-specific modifications.", "evidence_reference": "While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously~\\cite{Yu2018,Chen2020} formulated classification task over a set of visual regions, allowing more flexible architecture design."}
{"question": "Consider the paper that introduces the method whose results are displayed in a lighter grey color in the table. What is the improvement in performance points of the model compared to the previous state-of-the-art model UNICORN on the RefCOCO+ testA set?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is using grey-ish color for showing results in the table?", "answer_anchor": "OFA-base", "question_reference": "How does OFA's performance on the RefCOCO+ testA set compare to the previous state-of-the-art model UNICORN in terms of improvement points?", "explanation_reference": "The question specifically asks for the improvement in performance points of OFA over the previous state-of-the-art model UNICORN on a particular dataset (RefCOCO+ testA). The answer directly reflects the numerical improvement in performance, which is a detail that can be extracted from the comparison of results between OFA and UNICORN.", "evidence_reference": "Compared with the previous SOTA UNICORN, OFA achieves significant improvement with a gain of 3.61, 6.65 and 4.85 points on the testA sets of RefCOCO and RefCOCO+ as well as the test-u set of RefCOCOg."}
{"question": "Consider the paper that introduces the dataset which has 1 language but 13 SM tasks. What specific aspect of the SentiEval benchmark design aims to address the challenge of prompt sensitivity in evaluating LLMs' sentiment analysis capabilities?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset has 1 language but 13 SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific aspect of the \\textsc{SentiEval} benchmark design aims to address the challenge of prompt sensitivity in evaluating LLMs' sentiment analysis capabilities?", "explanation_reference": "The \\textsc{SentiEval} benchmark is designed to address the challenge of prompt sensitivity by incorporating diverse but fixed instructions for evaluating LLMs. This approach aims to make performance comparisons more stable and reliable across different LLMs and studies, by reducing the variability and bias associated with prompt design.", "evidence_reference": "The main idea of \\textsc{SentiEval} is to: 1) break the boundary between individual sentiment analysis tasks to establish a unified testing benchmark, providing a more comprehensive assessment of a model's sentiment analysis proficiency, rather than emphasizing on specific aspects; 2) test the model using natural language instructions presented in various styles. This mimics the real use case when humans interact with the model with natural languages for solving SA tasks, instead of purely learning text-label mapping; 3) equip the benchmark with diverse but fixed instructions, making performance comparisons more stable and reliable across different LLMs and studies."}
{"question": "Consider the paper that introduces the model that shows 0 accuracy in the COGS-structural dataset. What specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis in the attention visualizations section?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model shows 0 accuracy in COGS-structural dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the evidence provided in the attention visualizations section, what specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis?", "explanation_reference": "The authors present visual evidence showing that attention head 5 in layer 5 of 6 is involved in anaphora resolution, as demonstrated by the focused attention on the word 'its' and its relation to other parts of the sentence.", "evidence_reference": "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the model shown on the penultimate line of the table, MolXPT. What specific advantage does MolXPT demonstrate over Galactica in terms of pre-training data utilization?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model shown on the penult line?", "answer_anchor": "MolXPT", "question_reference": "What specific advantage does MolXPT demonstrate over Galactica in terms of pre-training data utilization?", "explanation_reference": "The advantage is highlighted by the explicit use of 'wrapped' sequences in MolXPT's pre-training, which incorporates both molecular SMILES and surrounding text, allowing for a more effective representation by leveraging the complementary information from both modalities. This approach is contrasted with Galactica, which, despite also using SMILES and text, does not build and train on these 'wrapped' sequences, missing out on the potential benefits of such integration.", "evidence_reference": "A possible explanation of the superior performance is that the SMILES describes the component and structural information of molecules, while the text describes the general properties. They are complementary to each other, and joint training on them brings more effective representations."}
{"question": "Consider the paper that introduces the model that exhibits the most negative Spearman's Correlation Coefficient. What specific performance improvement does the model proposed in the paper, DialoGPT (345M, w/ MMI), show over the standard DialoGPT (345M) model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model in the figure has the highest Spearman's Correlation?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific performance improvement does the DialoGPT (345M, w/ MMI) model show over the standard DialoGPT (345M) model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "explanation_reference": "The improvement in METEOR score for the DialoGPT (345M, w/ MMI) model over the standard DialoGPT (345M) model is derived from the comparison of their METEOR scores in the DSTC-7 Dialogue Generation Challenge results. The standard DialoGPT (345M) model achieved a METEOR score of 8.51%, while the DialoGPT (345M, w/ MMI) model achieved a METEOR score of 11.23%, indicating a specific improvement of 3.06%.", "evidence_reference": "DialoGPT (345M) = 8.51% METEOR; DialoGPT (345M, MMI) = 11.23% METEOR"}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest dialogues. How does its curriculum learning strategy specifically address the challenge of optimizing large language models (LLMs) to learn knowledge encoded in multiple languages simultaneously?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the least number of dialogues from the table?", "answer_anchor": "Multialpaca", "question_reference": "How does the curriculum learning strategy specifically address the challenge of optimizing LLMs to learn knowledge encoded in multiple languages simultaneously?", "explanation_reference": "The curriculum learning strategy is designed to transfer general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. This is achieved by initially using the whole pre-training dataset to train a base model for commonsense generalization ability, and then transitioning to a subset of the pre-training dataset that boasts superior quality and a greater proportion of multilingual content to strengthen the model's multilingual capabilities.", "evidence_reference": "Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is a significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. To address this issue, we adopt a curriculum learning strategy that ramps up the ratio of high-quality and low-resource languages during training."}
{"question": "Consider the paper that introduces the large language model that corresponds to an HVI score of 47. What is the specific improvement in percentage points of the model proposed in the paper over its predecessor in internal adversarially-designed factuality evaluations?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the large language model that demonstrates 47 HVI scores?", "answer_anchor": "GPT-4", "question_reference": "What is the specific improvement in percentage points of GPT-4 over GPT-3.5 in internal adversarially-designed factuality evaluations?", "explanation_reference": "The improvement is directly stated as a comparison between GPT-4 and GPT-3.5, highlighting the progress made in reducing hallucinations and improving factuality.", "evidence_reference": "GPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have themselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our latest GPT-3.5 on our internal, adversarially-designed factuality evaluations."}
{"question": "Consider the paper that introduces the model depicted in the figure that exhibits the highest fluctuation. What specific transformation rule is applied by the model proposed in the paper to construct disfluent summaries for the fluency dimension in text summarization?", "answer": "", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the figure demonstrates the highest fluctuation?", "answer_anchor": "UniEval", "question_reference": "What specific transformation rule is applied to construct disfluent summaries for the fluency dimension in text summarization?", "explanation_reference": "The answer directly addresses the transformation rule used for creating disfluent summaries to evaluate the fluency dimension, as specified in the paper. This detail is part of the pseudo data construction process for the fluency dimension in text summarization, highlighting the methodological approach to generate negative samples.", "evidence_reference": "Fluency represents the quality of individual sentences. We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries."}
{"question": "Consider the paper that introduces the model that achieves a P_k score of 24.8 in the en_disease category. How does its structured summarization approach ensure the generation of valid segment boundary positions within the task semantics?", "answer": "", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model gets 24.8 P_k score in en_disease category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "How does the structured summarization approach ensure the generation of valid segment boundary positions within the task semantics?", "explanation_reference": "The paper addresses the potential issue of generating invalid segment boundary positions in the output sequence of the structured summarization models. It does so by calculating the fraction of erroneous outputs, such as non-integer-convertible components or out-of-bound segment boundary positions, and demonstrating that such errors are exceedingly rare. This indicates that the transformer decoders used in the structured summarization models are capable of accurately generating tokens that represent integer values within the semantic bounds of the task, ensuring the validity of the generated segment boundary positions.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. What is the primary limitation of using the model, EQG, for event-centric summary generation in educational question generation, as identified in the paper?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model is in the first row of the table?", "answer_anchor": "EQG", "question_reference": "What is the primary limitation of using the BART model for event-centric summary generation in educational question generation, as identified in the paper?", "explanation_reference": "The paper identifies the primary limitation of using the BART model for event-centric summary generation as the factuality error problem, indicating that sometimes the system may generate non-factual facts in terms of the original context.", "evidence_reference": "Owing to the factuality error problem of our system, we suggest to further investigate constructing structured knowledge of fairy tales and knowledge-grounded question generation for real-world applications."}
{"question": "Consider the paper that introduces the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections category. What specific performance metric does the model proposed in the paper achieve on multi-hop questions in the {\\dscf} dataset when using GPT-3 as the base model, considering instances where all associated edited facts are successfully retrieved from memory?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2305.14795", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections?", "answer_anchor": "MeLLo", "question_reference": "What specific performance metric does MeLLo achieve on multi-hop questions in the {\\dscf} dataset when using GPT-3 as the base model, considering instances where all associated edited facts are successfully retrieved from memory?", "explanation_reference": "The performance metric of 73.1% for MeLLo on multi-hop questions in the {\\dscf} dataset when using GPT-3 as the base model and considering instances where all associated edited facts are successfully retrieved from memory is directly provided in the section discussing the impact of retrieval performance. This detail highlights MeLLo's effectiveness in handling multi-hop questions under optimal retrieval conditions.", "evidence_reference": "Among those questions where all associated facts are successfully retrieved from memory, MeLLo can answer $73.1\\%$ of them correctly."}
{"question": "Consider the paper that introduces the Twitter dataset that has the most number of languages compared to all other Twitter datasets. What specific sentiment analysis sub-tasks do the authors plan to extend the model proposed in the paper to in the future?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset (Twitter) has the most number of languages compared to all Twitter datasets?", "answer_anchor": "AfriSenti", "question_reference": "What specific sentiment analysis sub-tasks do the authors plan to extend AfriSenti to in the future?", "explanation_reference": "The authors explicitly mention their future plans for AfriSenti, which include extending it to additional African languages and exploring other sentiment analysis sub-tasks, indicating a broader scope for the dataset beyond its current state.", "evidence_reference": "In the future, we plan to extend \\textit{AfriSenti} to additional African languages and other sentiment analysis sub-tasks."}
{"question": "Consider the paper that introduces the score described as a \"fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits) by the model\". What is the Pearson correlation range for cross-model estimates in CoLA?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2110.08420", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the name of the score with description 'Fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits by the model.'?", "answer_anchor": "PVI", "question_reference": "What is the Pearson correlation range for cross-model \\pvi estimates in CoLA?", "explanation_reference": "The range for the Pearson correlation of cross-model \\pvi estimates in CoLA indicates the degree of agreement between different models' assessments of instance difficulty within the CoLA dataset. This range is specifically mentioned to highlight the variability and noisiness in difficulty estimates due to CoLA's lower amount of usable information compared to SNLI.", "evidence_reference": "The cross-model Pearson correlation between \\pvi estimates of SNLI instances is very high ($r >$ 0.80). However, the cross-model Pearson correlation is lower for CoLA (0.40 $< r <$ 0.65); see Fig.~\\ref{fig:correlations_heatmap} in Appendix~\\ref{appendix:inter-epoch}."}
{"question": "Consider the paper that discusses the benchmark that corresponds to the light green color in the figure. What is the primary reason for the underestimation of language model capabilities in the corresponding paper, according to the findings related to answer-only prompting?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which benchmark is represented using the light green color from the figure?", "answer_anchor": "BBH", "question_reference": "What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to answer-only prompting?", "explanation_reference": "The paper suggests that the few-shot evaluation of PaLM 540B with answer-only prompting, which includes both a task instruction and answer options, demonstrates the effect of including these elements in the prompt. This approach outperforms the average human-rater on 6 out of 23 BBH tasks and is overall 1.4% better than the BIG-Bench reported result, indicating that the original setup underestimated language model performance.", "evidence_reference": "The few-shot evaluation of PaLM 540B  with answer-only prompting in this paper, however, outperforms the average human-rater on 6 out of 23 \\bbh{} tasks and is overall 1.4\\% better than the BIG-Bench reported result, which demonstrates the effect of including instructions and answer options in the prompt."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 80.3 on Deit-B with a Weight/Activation (W/A) precision of 6/6. What specific advantage does the twin uniform quantization, as utilized by PTQ4ViT, offer for post-softmax and post-GELU activation values in terms of hardware efficiency?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the quant method show 80.3 score on Deit-B?", "answer_anchor": "PTQ4ViT", "question_reference": "What specific advantage does the twin uniform quantization offer for post-softmax and post-GELU activation values in terms of hardware efficiency?", "explanation_reference": "The twin uniform quantization is designed to efficiently process on existing hardware devices like CPUs and GPUs by using the shift operation, which avoids the need for format transformation and additional FP32 operations that are more computationally expensive.", "evidence_reference": "Our method uses the shift operation, avoiding the format transformation and extra FP32 multiplication and FP32 addition."}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-De task in the Test2016 dataset in Previous Image-must Systems. How does its performance with hallucinated visual tokens compare to using ground-truth visual representations on the Multi30K dataset for the EN\u2192DE task using the Transformer-Tiny model?", "answer": "", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model demonstrates the highest performance in En-De task in Test2016 dataset in Previous Image-must Systems?", "answer_anchor": "VALHALLA", "question_reference": "How does the performance of the model proposed in the paper with hallucinated visual tokens compare to using ground-truth visual representations on the Multi30K dataset for the EN$\\rightarrow$DE task using the Transformer-Tiny model?", "explanation_reference": "The performance of VALHALLA with hallucinated visual tokens is very similar to when using ground-truth visual representations, demonstrating the model's strong ability to generate visual representations that are semantically consistent with the ground-truth.", "evidence_reference": "Moreover, \\ours has very similar performance with either hallucinated (\\texttt{V}) or ground-truth representation (\\texttt{VM}), showing strong ability to generate visual representations that are semantically consistent with the ground-truth."}
{"question": "Consider the paper that introduces the method that demonstrates the second lowest Acc-7 score on MOSI. What is the theoretical model complexity of the fusion method, compared to the model's own complexity?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1806.00064", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method demonstrates the lowest Acc-7 score on MOSI?", "answer_anchor": "TFN", "question_reference": "What is the theoretical model complexity of the proposed fusion method compared to the Tensor Fusion Network (TFN) model?", "explanation_reference": "The question assesses understanding of the core concept of computational complexity reduction achieved by the proposed low-rank multimodal fusion method. It specifically targets the comparison of theoretical model complexities between the proposed method and the Tensor Fusion Network (TFN) model.", "evidence_reference": "Theoretically, the model complexity of our fusion method is $O(d_y \\times r \\times \\sum_{m=1}^M d_m)$ compared to $O(d_y \\prod_{m=1}^M d_m)$ of TFN from Section \\ref{par:stupid_tensor}."}
{"question": "Consider the paper that introduces the method that is in the third row of the table. What specific mathematical operations are included in the Numeric Reasoning Knowledge category of external knowledge evidence in the benchmark?", "answer": "", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "BIRD", "question_reference": "What specific mathematical operations are included in the Numeric Reasoning Knowledge category of external knowledge evidence in the BIRD benchmark?", "explanation_reference": "The question assesses understanding of the specific types of mathematical operations that are considered as part of the Numeric Reasoning Knowledge category for external knowledge evidence in the BIRD benchmark. This detail is crucial for understanding the complexity and requirements of the benchmark in terms of the mathematical reasoning capabilities expected from text-to-SQL models.", "evidence_reference": "In our benchmark, we present 8 basic math operations, including 4 complex operations as \\citep{finqa}: \\texttt{MINUS}, \\texttt{ADDITION}, \\texttt{DIVISION}, \\texttt{MULTIPLY}."}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that has a Test Accuracy of 79.6. What specific method does its solution discrimination module employ to generate negative solutions that are variants of the ground truth solution?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which Seq2Seq model shows 79.6 Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module employ to generate negative solutions that are variants of the ground truth solution?", "explanation_reference": "The solution discrimination module uses a gradient-based manipulation method to identify the most vulnerable (important) token in the ground truth solution and then finds all possible alternatives for this token to generate negative solutions. This method is chosen to ensure that the negative solutions are hard for the model to distinguish from the correct solution, thereby enhancing the model's ability to correctly associate problems with their ground truth solutions.", "evidence_reference": "Our goal is to find variants of the ground truth solution as hard negative samples, which only manipulate the most vulnerable (important) token. Therefore, borrowing the idea from white-box evasion attack, we regard the token with the largest gradient as the most important and vulnerable one: \\begin{equation} \\label{grad} y_i = \\mathop{argmax\\;}_{y_i \\in Y}(\\nabla Dis([en_x(X),en_y(Y)]). \\end{equation}"}
{"question": "Consider the paper that introduces what model is shown in the first row of the table. What is the performance difference between \\textsc{WPM-Sep} and \\textsc{WPM-Joint} on the NT14 dataset for the En$\\Rightarrow$De translation task according to this model?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "GWLAN", "question_reference": "What is the performance difference between \\textsc{WPM-Sep} and \\textsc{WPM-Joint} on the NT14 dataset for the En$\\Rightarrow$De translation task?", "explanation_reference": "The performance difference is calculated based on the accuracy scores provided for the NT14 dataset in the En$\\Rightarrow$De translation task. \\textsc{WPM-Sep} has an accuracy of 51.46, and \\textsc{WPM-Joint} has an accuracy of 52.68. The difference is 52.68 - 51.46 = 1.22.", "evidence_reference": "4 &  \\textsc{WPM-Sep}    &        56.93     &     55.67       &      54.54       &    51.46        \\\\ 5 & \\textsc{WPM-Joint} & \\textbf{55.54}                      & \\textbf{55.85}                      & \\textbf{53.64}                      & \\textbf{54.25}                             &       \\textbf{57.84}     &    \\textbf{56.75}   &    \\textbf{56.91}     &      \\textbf{52.68}"}
{"question": "Consider the paper that introduces the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category. What is the significance of the assumption of non-stationary distribution shifts for the realism and complexity of the setup in this method, based on its problem formulation and the unique challenges it introduces?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2205.02014", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "CMR", "question_reference": "Based on the CMR problem formulation and the unique challenges it introduces, why is the assumption of non-stationary distribution shifts particularly significant for the realism and complexity of the CMR setup?", "explanation_reference": "The assumption of non-stationary distribution shifts is significant because it reflects the dynamic, unpredictable, and diverse nature of real-world data streams, making the CMR setup more aligned with practical scenarios and thus more complex and challenging to address.", "evidence_reference": "the proposed CMR formulation is essentially a boundary-agnostic CL problem in non-stationary data streams, where the distribution shifts are more dynamic, unpredictable, and diverse, yielding a more realistic yet challenging CL setup."}
{"question": "Consider the paper that introduces the method that corresponds to the brown color label in the figure. How does the model proposed in the paper ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is method demonstrated in the figure with brown color label?", "answer_anchor": "PPLM", "question_reference": "How does the proposed K2T method ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "explanation_reference": "The method described as K2T modifies the score function at each decoding step by adding a shift towards the semantic space of a given guide word. This approach encourages the explicit appearance of the guide word and appropriate context for it, without requiring additional models or fine-tuning, thus maintaining the fluency and diversity of the generated text.", "evidence_reference": "In this work, we propose \\emph{Keyword2Text} (K2T), a new and simple plug-and-play method for exerting hard control during text generation. By modifying the score function, we can incorporate a semantic shift at decoding time, without additional models or fine-tuning."}
{"question": "Consider the paper that introduces the model represented by the lavender color in the figure. What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for the model?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model is demonstrated in the lavender color?", "answer_anchor": "StarCoder", "question_reference": "What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for StarCoderBase?", "explanation_reference": "The percentage of responses flagged as toxic using a toxicity classifier for StarCoderBase in the RealToxicityPrompts evaluation is provided directly in the results table for the toxicity evaluation.", "evidence_reference": "StarCoderBase & 0.42 & 1.12"}
{"question": "Consider the paper that introduces the method that has an F1 score of 65.96. What is the F1 score improvement percentage when applying multi-task learning (MTL) with entity labeling and relation extraction tasks on the FUNSD dataset?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having 65.96 F1 score?", "answer_anchor": "SERA", "question_reference": "What is the F1 score improvement percentage when applying multi-task learning (MTL) with entity labeling and relation extraction tasks on the FUNSD dataset?", "explanation_reference": "The improvement percentage is calculated based on the F1 score improvement mentioned in the training strategies section, where it states that relation extraction task can improve by about 0.86% F1 while labeling model performance drops a little.", "evidence_reference": "Relation extraction task can improve by about 0.86\\% F1 while labeling model performance drops a little from Table~\\ref{trainstrategy}."}
{"question": "Consider the paper that introduces the method that achieves the highest Hits@1 score in the MQA-1H dataset. What is the unique aspect of the model's architecture proposed by the paper that differentiates it from previous works in multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets the highest Hits@1 score in MQA-1H dataset?", "answer_anchor": "UniKGQA", "question_reference": "What is the unique aspect of the UniKGQA's model architecture that differentiates it from previous works in multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the unique aspect of UniKGQA's model architecture by highlighting the combination of a semantic matching module and a matching information propagation module, which is a distinctive approach compared to previous works in the field of multi-hop KGQA tasks. This combination allows for effective semantic matching between questions and relations and the propagation of this matching information along the directed edges on KGs, which is crucial for unifying retrieval and reasoning in the context of multi-hop KGQA.", "evidence_reference": "For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the model that has the second lowest MCD 1 score. Which specific modification in the ablation experiments led to the most significant decrease in performance on the CLUTRR task when trained on relation lengths k=2,3?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2112.00578", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model dmonstrates the lowest MCD 1 score?", "answer_anchor": "T5-based UT", "question_reference": "In the ablation experiments, which specific modification led to the most significant decrease in performance on the CLUTRR task when trained on relation lengths k=2,3?", "explanation_reference": "The value ablation modification, which simplifies the Edge Transformer computations by computing the value \\(v_{ilj}\\) for each triangle \\((i, l, j)\\) using only the edge \\((i, l)\\) instead of using both edges \\((i, l)\\) and \\((l, j)\\), led to the most significant decrease in performance on the CLUTRR task when trained on relation lengths k=2,3. This is evidenced by the drastic drop in test performance across all tested relation lengths compared to the base model and other ablation conditions.", "evidence_reference": "Value ablation & 54.4 \\(\\pm\\) 3.4    & 44.2 \\(\\pm\\) 3.6   & 36.9 \\(\\pm\\) 3.6   & 33.9 \\(\\pm\\) 3.7   & 30.7 \\(\\pm\\) 3.6"}
{"question": "Consider the paper that introduces the method shown in the first row of the table. What is the model's, proposed by the paper, average accuracy improvement over the best baseline few-shot-CoT GPT-3 on the \\data{} dataset?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is shown in the first row of the table?", "answer_anchor": "PromptPG", "question_reference": "What is the average accuracy improvement of PromptPG over the best baseline few-shot-CoT GPT-3 on the \\data{} dataset?", "explanation_reference": "The average accuracy improvement of PromptPG over the best baseline few-shot-CoT GPT-3 is directly stated in the Experimental Results section, indicating the effectiveness of the PromptPG method in selecting in-context examples and constructing performing prompts for the test example.", "evidence_reference": "our proposed \\model learns to select performing examples with the help of policy gradient. \\model establishes a state-of-the-art performance on the \\data{} dataset: it surpasses the best baseline few-shot-CoT GPT-3 by 5.31\\% on average."}
{"question": "Consider the paper that introduces the dataset which includes 1 SM task and 4 languages. What is the Fleiss kappa inter-annotator agreement score for the Yoruba language when considering the 3-class sentiment classification?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset with 1 SM task and 4 languages?", "answer_anchor": "NaijaSenti", "question_reference": "What is the Fleiss kappa inter-annotator agreement score for the Yoruba language when considering the 3-class sentiment classification?", "explanation_reference": "The Fleiss kappa score for the Yoruba language in the 3-class sentiment classification setup indicates the level of agreement among annotators on the sentiment classification of tweets in Yoruba. This score is a measure of the reliability of agreement between annotators beyond chance.", "evidence_reference": "IAA ($\\kappa$) for Yoruba in the 3-class setup is $0.600$."}
{"question": "Consider the paper that introduces the dataset which has 1 SM task and 14 languages. What specific sentiment analysis sub-tasks do the authors plan to extend the model proposed in the paper to in the future?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What dataset has 1 SM task and 14 languages?", "answer_anchor": "AfriSenti", "question_reference": "What specific sentiment analysis sub-tasks do the authors plan to extend AfriSenti to in the future?", "explanation_reference": "The authors explicitly mention their future plans for AfriSenti, which include extending it to additional African languages and exploring other sentiment analysis sub-tasks, indicating a broader scope for the dataset beyond its current state.", "evidence_reference": "In the future, we plan to extend \\textit{AfriSenti} to additional African languages and other sentiment analysis sub-tasks."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. What is the maximum number of paragraphs the IRCoT method is designed to collect before terminating the retrieval process?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2212.10509", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "IRCoT", "question_reference": "What is the maximum number of paragraphs \\iconsys is designed to collect before terminating the retrieval process?", "explanation_reference": "The maximum number of paragraphs \\iconsys is designed to collect is directly mentioned as being capped at 15 to fit within the model's context limit.", "evidence_reference": "The \\textbf{CoT-guided retrieval step (``Retrieve'')} uses the last generated CoT sentence as a query to retrieve more paragraphs and adds them to the collected paragraphs. We cap the total number of collected paragraphs\\footnote{set to 15 in our experiments.} so as to fit in at least a few demonstrations in the model's context limit."}
{"question": "Consider the paper that introduces the method that has a Hits@1 score ranging from 40% to 55% across the different fine-tuning samples shown in the figure. What specific pre-training task is designed for both retrieval and reasoning models in the model proposed in the paper to ensure their unification in parameter learning?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has Hit@1 score range from 40% to 55% as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning?", "explanation_reference": "The specific pre-training task designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning is the question-relation matching task. This task is directly mentioned as a shared pre-training task for both models, indicating its role in unifying the parameter learning process by focusing on matching the semantics of questions with the relations in the knowledge graph.", "evidence_reference": "For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 42.0 in the Seen, Val, GC dataset. What specific performance improvement does the model proposed in the paper provide on the ALFRED benchmark's unseen test split?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the score of 42.0 in Seen, Val, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific performance improvement does pretraining and joint training with synthetic instructions provide on the ALFRED benchmark's unseen test split?", "explanation_reference": "The question focuses on the specific detail of how pretraining and joint training with synthetic instructions impact the performance on the ALFRED benchmark, particularly on the unseen test split. The answer directly reflects the improvement mentioned in the paper, emphasizing the effectiveness of the proposed methods in handling environments not encountered during training.", "evidence_reference": "Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits."}
{"question": "Consider the paper that introduces the model that corresponds to the green dashed line. What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model demonstrates in the gree dashed line?", "answer_anchor": "T5-Large", "question_reference": "What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "explanation_reference": "The span-corruption objective with an average span length of 3 slightly outperformed the i.i.d. objective on most non-translation benchmarks, as indicated by the statement 'we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks.'", "evidence_reference": "we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks."}
{"question": "Consider the paper that introduces the model that demonstrates the highest Oracle score. How does its \\ednascore metric correlate with human judgments of both faithfulness and diversity according to Spearman's rank correlation coefficient?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model that demonstrates the highest Oracle score?", "answer_anchor": "Composition", "question_reference": "How does the \\ednascore metric correlate with human judgments of both faithfulness and diversity according to Spearman's rank correlation coefficient?", "explanation_reference": "The \\ednascore metric is designed to jointly measure faithfulness and diversity in generated summaries. According to the paper, it is positively correlated with human judgments of both dimensions, indicating that as \\ednascore increases, the perceived faithfulness and diversity of summaries by human evaluators also increase. This suggests that \\ednascore is an effective metric for capturing qualities that are valued in human assessments of generated text.", "evidence_reference": "In line with previous work \\cite{maynez-etal-2020-faithfulness,kryscinski-etal-2019-neural}, we find that entailment scores are best correlated with faithfulness (moderate, $0.40 \\leq r \\leq 0.59$). Like Self-BLUE, Self-Entailment and Self-BERTScore are also strongly correlated with diversity ratings. Compared to other metrics which capture a single dimension, \\ednascore is positively correlated with both dimensions of diversity \\emph{and} faithfulness."}
{"question": "Consider the paper that introduces the method that has an accuracy of 95.20% in automatic evaluation. What is the effect of the size of re-ranked candidate tokens on the model's control performance and text fluency according to its methodology?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows 95.20 accuracy in automatic evaluation?", "answer_anchor": "Discup", "question_reference": "What is the effect of the size of re-ranked candidate tokens on the model's control performance and text fluency according to the DisCup methodology?", "explanation_reference": "The paper discusses how the control performance of the DisCup method is proportional to the size of re-ranked candidate tokens, indicating that a deeper sampling scope (larger size of candidate tokens) results in better control performance. However, it also mentions that this comes at the cost of decreased text fluency, as more selected tokens in low-probability regions lead to a decrease in fluency.", "evidence_reference": "The control performance of our method is proportional to the size of candidate tokens $\\mathcal{C}$. As shown in Figure~\\ref{top_k}, the deeper the sampling scope is, the better the control performance will be, but meanwhile the PPL deteriorates."}
{"question": "Consider the paper that introduces the method that has an F1 score of 41.3. What is the observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having 41.3 F1 score?", "answer_anchor": "SPADE", "question_reference": "what is the observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle?", "explanation_reference": "The Tail Collision Avoidance algorithm is integrated to handle the property where individual text nodes have a single incoming edge for each relation except in special documents like tables. The observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle is +1.0%.", "evidence_reference": "To push the performance further, we notice that individual text nodes have a single incoming edge for each relation except in special documents like table (Fig. \\ref{fig_paradigm}). Using this property, we integrate Tail Collision Avoidance algorithm (\\caabb) that iteratively trims the tail-sharing-edges and generate new edges until the process becomes self-consistent (Section \\ref{sec:graph_gen}). $F_1$ increases by +1.0\\% and +0.8\\% with and without the oracle upon the integration (2nd row, \\cordabb)."}
{"question": "Consider the paper that introduces the method that has an F1 score of 53.36. What specific methodological adaptation does the Doc2Graph framework employ to redefine the neighborhood aggregation strategy in its GNN layer, and how does it differ from traditional approaches?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having 53.36 F1 score?", "answer_anchor": "Doc2Graph", "question_reference": "What specific methodological adaptation does the Doc2Graph framework employ to redefine the neighborhood aggregation strategy in its GNN layer, and how does it differ from traditional approaches?", "explanation_reference": "The adaptation is detailed in the description of the GNN layer within the architecture section, where it specifies that the neighborhood aggregation at layer l for a node i considers only those neighbors j within a certain Euclidean distance threshold, normalized between 0 and 1, applying a constant scale factor c to the sum of their representations. This approach differs from traditional methods by explicitly incorporating distance-based filtering and scaling into the aggregation process, aiming to maintain locality properties in the fully connected graph structure.", "evidence_reference": "h_{N(i)}^{l+1} = \\frac{c}{|\\Upsilon(i)|} \\sum_{j \\in \\Upsilon(i)} h^{l}_{j} where $\\Upsilon(i) = \\{j \\in N(i): |i - j| < threshold\\}$, $|i - j|$ is the Euclidean distance of nodes $i$ and $j$ saved (normalized between 0 and 1) on their connecting edge, and $c$ is a constant scale factor."}
{"question": "Consider the paper that introduces the method at the rightmost part of the figure. What is the primary goal of discriminatively training CC-LMs with the training approach proposed in the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method on the right most coordinates of the figure?", "answer_anchor": "GeDi", "question_reference": "What is the primary goal of discriminatively training CC-LMs with GeDi training?", "explanation_reference": "The primary goal of discriminatively training CC-LMs with GeDi training is to enhance their capabilities as discriminators, specifically for the purpose of guiding generation in a more controlled manner. This is aimed at improving the efficiency and effectiveness of the GeDi-guided generation process.", "evidence_reference": "For this reason, we propose training CC-LMs discriminatively as classifiers with GeDi training, with the primary goal of making them better discriminators for GeDi-guided generation."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category. What is the effect of reducing the train wall time by half on the pre-training performance of this method, compared to other lifelong learning baselines?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.06311", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category?", "answer_anchor": "ELLE", "question_reference": "What is the effect of reducing the train wall time by half on the pre-training performance of ELLE compared to other lifelong learning baselines?", "explanation_reference": "When given fewer computational budgets, ELLE still outperforms all the lifelong learning baselines in both pre-training efficiency and downstream performances. However, reducing the train wall time by half leads to significant performance drops, indicating that pre-training with fewer computations harms PLMs' knowledge acquisition.", "evidence_reference": "For ELLE, when PLMs are trained with fewer computational budgets, we observe significant performance drops in both pre-training (higher AP and AP+) and downstream tasks (lower average F1). This shows that pre-training with fewer computations would harm PLMs' knowledge acquisition."}
{"question": "Consider the paper that introduces the method that achieves a score of 28.62 in the WQ-B task. How does its approach to handling instances with simple expressions differ from the paraphrasing-based approach, according to the limitations section?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets 28.62 score in WQ B task?", "answer_anchor": "DSM", "question_reference": "How does the paraphrasing-based approach differ from the proposed method in handling instances with simple expressions, according to the limitations section?", "explanation_reference": "The distinction between the paraphrasing-based approach and the proposed method is highlighted in the limitations section, where it is mentioned that for instances with simple expressions, the paraphrasing-based method may perform better by focusing on words, whereas the proposed method concentrates more on sentence structure.", "evidence_reference": "For example, the ground truth is 'What religion in Australia that influenced Arthur Schopenhauer?', the paraphrasing-based approach generates 'What faith in Australia inspired Arthur Schopenhauer?'. Our method generates  'What is the religion in Australia that influenced  Arthur Schopenhauer? '. We observe that the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the model shown in the figure corresponds to the green line. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/accuracy_figure.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model is shown in green line?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength and direction of a linear relationship between two variables. In this case, it quantifies the relationship between the word overlap (how vocabularies change over time) and the model performance for the task of political affiliation classification on Twitter data. A value of 0.9817159316285563 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that discusses the dataset located in the top left of the figure. What specific linguistic phenomenon does the paper mention requires familiarity with social media to interpret correctly, especially in the context of Chinese social media?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset located on the top left of the figure?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly, especially in the context of Chinese social media?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity in sentiment analysis with LLMs. It specifically mentions the phenomenon where a seemingly agreeable statement in Chinese, made with a respectful tone, does not necessarily indicate agreement but can be used ironically. This example illustrates the subtlety and context-dependency of language, highlighting the difficulty for models to interpret such nuances without familiarity with specific cultural or social media contexts.", "evidence_reference": "For example, on Chinese social media, a comment '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically. However, this linguistic phenomenon may require familiarity with social media to interpret correctly."}
{"question": "Consider the paper that introduces the model that has the highest P-BLEU score in the uAD dataset. How does the \\ednascore metric correlate with human judgments of both faithfulness and diversity according to Spearman's rank correlation coefficient?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets the lowest P-BLEU? score in uAD dataset?", "answer_anchor": "Composition", "question_reference": "How does the \\ednascore metric correlate with human judgments of both faithfulness and diversity according to Spearman's rank correlation coefficient?", "explanation_reference": "The \\ednascore metric is designed to jointly measure faithfulness and diversity in generated summaries. According to the paper, it is positively correlated with human judgments of both dimensions, indicating that as \\ednascore increases, the perceived faithfulness and diversity of summaries by human evaluators also increase. This suggests that \\ednascore is an effective metric for capturing qualities that are valued in human assessments of generated text.", "evidence_reference": "In line with previous work \\cite{maynez-etal-2020-faithfulness,kryscinski-etal-2019-neural}, we find that entailment scores are best correlated with faithfulness (moderate, $0.40 \\leq r \\leq 0.59$). Like Self-BLUE, Self-Entailment and Self-BERTScore are also strongly correlated with diversity ratings. Compared to other metrics which capture a single dimension, \\ednascore is positively correlated with both dimensions of diversity \\emph{and} faithfulness."}
{"question": "Consider the paper that introduces the method which corresponds to the fifth row of the table. What is the effect of the contrastive loss margin \\(\\rho\\) on the perplexity of SimCTG, when it is set to either too small or too large values?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown in the fifth row of the table?", "answer_anchor": "SimCTG", "question_reference": "What is the effect of the contrastive loss margin \\(\\rho\\) on the model perplexity when it is set to either too small or too large values?", "explanation_reference": "The paper discusses how setting the contrastive loss margin \\(\\rho\\) to either too small (e.g., \\(0.1\\)) or too large (e.g., \\(1.0\\)) values leads to a representation space that is either less or too isotropic, resulting in sub-optimal perplexity. This indicates that there is an optimal range for \\(\\rho\\) that balances the isotropy of the representation space to achieve better model perplexity.", "evidence_reference": "However, when \\(\\rho\\) is either too small (e.g., \\(0.1\\)) or large (e.g., \\(1.0\\)), the learned representation space of the model would be either less or too isotropic, leading to a sub-optimal perplexity."}
{"question": "Consider the paper that introduces the model that demonstrates the lowest zh-en score. What is the performance difference between \\textsc{WPM-Sep} and \\textsc{WPM-Joint} on the NT14 dataset for the En$\\Rightarrow$De translation task?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates the lowest zh-en score?", "answer_anchor": "GWLAN", "question_reference": "What is the performance difference between \\textsc{WPM-Sep} and \\textsc{WPM-Joint} on the NT14 dataset for the En$\\Rightarrow$De translation task?", "explanation_reference": "The performance difference is calculated based on the accuracy scores provided for the NT14 dataset in the En$\\Rightarrow$De translation task. \\textsc{WPM-Sep} has an accuracy of 51.46, and \\textsc{WPM-Joint} has an accuracy of 52.68. The difference is 52.68 - 51.46 = 1.22.", "evidence_reference": "4 &  \\textsc{WPM-Sep}    &        56.93     &     55.67       &      54.54       &    51.46        \\\\ 5 & \\textsc{WPM-Joint} & \\textbf{55.54}                      & \\textbf{55.85}                      & \\textbf{53.64}                      & \\textbf{54.25}                             &       \\textbf{57.84}     &    \\textbf{56.75}   &    \\textbf{56.91}     &      \\textbf{52.68}"}
{"question": "Consider the paper that introduces the method that has 638K tunable parameters. How does the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) compare to the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) in the proposed architecture?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has 638K tunable parameters?", "answer_anchor": "Hyperformer", "question_reference": "How does the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) compare to the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) in the proposed \\methodefficient model?", "explanation_reference": "The dimensions are specified in the Experimental Details section, indicating that the task feature embedding (\\(\\bm{z_{\\tau}}\\)) is larger (512) compared to the task embedding (\\(\\bm{I_{\\tau}}\\)) which is smaller (64).", "evidence_reference": "We set the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) to \\(t'=512\\), and the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) to \\(t=64\\)."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage category. What was the success rate for the FEVER dataset in the targeted in-context knowledge updating experiment when the prompt included Original Examples Only?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2210.09150", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage?", "answer_anchor": "IC-Retrieval", "question_reference": "In the targeted in-context knowledge updating experiment, what was the success rate for the FEVER dataset when the prompt included Original Examples Only?", "explanation_reference": "The success rate for the FEVER dataset when the prompt included only Original Examples was 44.2, indicating the percentage of times GPT-3 correctly predicted the target label for a paraphrase of the original test claim under this specific prompting condition.", "evidence_reference": "FEVER & 44.2 & 85.1 - 84.9 = 0.2"}
{"question": "Consider the paper that introduces the method that has an F1 score of 65.96. What specific layout features are used in its GCN encoder to update the representation of the entity and edge, and how are these features mathematically represented?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having 65.96 F1 score?", "answer_anchor": "SERA", "question_reference": "What specific layout features are used in the GCN encoder to update the representation of the entity and edge, and how are these features mathematically represented?", "explanation_reference": "The specific layout features used in the GCN encoder to update the representation of the entity and edge are the horizontal and vertical distances between the two entity boxes, mathematically represented as [x_{i,j}, y_{i,j}]. This representation captures the spatial relationship between entities in visually rich documents, which is crucial for understanding their layout and structure.", "evidence_reference": "The edge embedding consists of 2 layout features, as the following equation shows: \\begin{equation} \\mathbf{r}_{i, j} = [x_{i,j}, y_{i,j}] \\f\\f\\f \\end{equation} where $x_{ij}$ and $y_{ij}$ are horizontal and vertical distance between the two entity boxes respectively: \\begin{equation} \\begin{aligned} \\label{ra} \\scriptsize &x_{i, j} = min(| x_i^1 - x_j^2 |, | x_j^1 - x_i^2 |) \\\\ &y_{i, j} = min(| y_i^1 - y_j^2 |, | y_j^1 - y_i^2 |) \\end{aligned} \\end{equation}"}
{"question": "Consider the paper that introduces the method which does not have results in the CSQA2.0 dev and StrategyQA test tasks shown in the table. What specific methodological approach does the paper employ to align language models with human intentions, particularly in terms of handling sensitive content?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method does not show result in CSQA2.0 dev and StrategyQA test task?", "answer_anchor": "ChatGPT", "question_reference": "What specific methodological approach does the paper employ to align language models with human intentions, particularly in terms of handling sensitive content?", "explanation_reference": "The paper employs Reinforcement Learning from Human Feedback (RLHF) as the specific methodological approach to align language models with human intentions, especially for handling sensitive content. This approach involves training models to act in accordance with user intentions, which includes being helpful, truthful, and harmless, particularly in contexts that may involve sensitive content.", "evidence_reference": "Our labelers consist of contractors hired either through Upwork, or sourced from Scale AI. Unlike previous work on RLHF that focused mostly on the summarization domain~\\cite{ziegler2019fine,stiennon2020learning,wu2021recursively}, in this work we want humans to label a broad set of natural language prompts submitted to language models, some of which may be sensitive in nature. Thus, we conducted a screening process to select labelers who showed a high propensity to detect and respond to sensitive content."}
{"question": "Consider the paper that introduces the method that achieves a higher EA score than Fixed set but a lower EA score than Diverse KATE in the FinQA task. How does the order of in-context examples affect its performance on the NQ dataset, and what is the observed trend regarding the default and reverse orders?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets higher EA score than Fixed set but lower EA score than Diverse KATE in FinQA task?", "answer_anchor": "KATE", "question_reference": "How does the order of in-context examples affect the performance of KATE on the NQ dataset, and what is the observed trend regarding the default and reverse orders?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results on the NQ dataset showed that the reverse order, where the most similar sentences are placed closer to the test example, performed the best. This suggests that the proximity of similar sentences to the test example in the input sequence may help GPT-3 leverage the corresponding information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the model on the last line of the Seq2Seq/Tree block of the table. What specific methodological limitation does the paper acknowledge regarding the incorporation of commonsense knowledge into the model's proposed MWP solver?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model on the last line of the Seq2Seq/Tree block?", "answer_anchor": "Ana-CL", "question_reference": "What specific methodological limitation does the paper acknowledge regarding the incorporation of commonsense knowledge into the MWP solver?", "explanation_reference": "The limitation is directly acknowledged in the 'Limitations' section, where the authors mention that the MWP solver cannot solve problems requiring commonsense knowledge if such knowledge is not explicitly provided in the problem description. This points to a methodological limitation in the solver's design regarding the integration of external, implicit knowledge.", "evidence_reference": "As mentioned in \\cite{lin2020numersense,DBLP:journals/corr/abs-2107-13435}, MWP solving in the real-word scenario requires many commonsense knowledge, e.g., 1km = 1000m and one day = 24 hours. When these commonsense constants are not explicitly given in the problem description, our MWP solver has no chance to solve problems that require them."}
{"question": "Consider the paper that introduces the method which is shown in the table above the 'Magister et al' row but below the 'UL2' row. How did the introduction of a guidance mechanism impact the number of questions (\\# Q) generated by the GPT-2 model in the model proposed by the paper compared to the no-guidance scenario?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the table above method proposed by Magister et al but below UL2 method?", "answer_anchor": "DecomDistill", "question_reference": "In the ablation study, how did the introduction of a guidance mechanism impact the number of questions (\\# Q) generated by the GPT-2 model compared to the no-guidance scenario?", "explanation_reference": "The introduction of a guidance mechanism significantly improved the accuracy of the number of questions generated by the GPT-2 model, as indicated by the increase in the metric from 0.42 to 0.80. This demonstrates the effectiveness of the guidance mechanism in enhancing the model's ability to generate a more accurate number of subquestions, aligning better with the GPT-3 annotated questions for a given problem.", "evidence_reference": "No-guidance   & 51.5 & 0.78 & 0.42    \\\\ Guidance   & \\textbf{58.8} & \\bf 0.81 & \\bf 0.80"}
{"question": "Consider the paper that introduces the method that has the lowest MAE in the CH-SIMS task. In the qualitative analysis section, which specific example demonstrates a scenario where the spoken words are ambiguous, but the acoustic and visual modalities provide complementary evidence leading to a correct positive sentiment prediction by the model proposed in the paper?", "answer": "", "figure": "locality/2310.05804/result_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the highest MAE in CH-SIMS task?", "answer_anchor": "Tensor Fusion", "question_reference": "In the qualitative analysis section, which specific example demonstrates a scenario where the spoken words are ambiguous, but the acoustic and visual modalities provide complementary evidence leading to a correct positive sentiment prediction by the TFN model?", "explanation_reference": "The question focuses on a detailed part of the qualitative analysis where the TFN model's ability to integrate complementary evidence from acoustic and visual modalities, despite ambiguous spoken words, leads to a correct positive sentiment prediction. This is explicitly mentioned in the description of the second example in the qualitative analysis section.", "evidence_reference": "In the second example, the spoken words are ambiguous since the model has no clue what a B is except a token, but the acoustic and visual modalities are bringing complementary evidences. Our TFN approach correctly identify this trimodal interaction and predicts a positive sentiment."}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE-1 score equal to 44.52. What specific performance improvement (in terms of ROUGE-1 score) does the integration of normalizing flow into the model proposed in the paper achieve over using a VAE-based neural topic model without gating on the XSum test set?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method in the table that demonstrates a ROUGE 1 score equal to 44.52?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific performance improvement (in terms of ROUGE-1 score) does the integration of normalizing flow into the neural topic model achieve over using a VAE-based neural topic model without gating on the XSum test set?", "explanation_reference": "The integration of normalizing flow into the neural topic model achieves a specific performance improvement of 0.4 in terms of ROUGE-1 score over using a VAE-based neural topic model without gating on the XSum test set, as indicated by the comparison of ROUGE-1 scores between the model configurations in the ablation study.", "evidence_reference": "without the normalizing flow, the improvement that the latent vector brings is downgraded, nearly 0.4 of ROUGE-1 for using contextualized gating and 0.53 of ROUGE-1 in non-gating case."}
{"question": "Consider the paper that introduces the method that has a Hits@1 score ranging from 40% to 55% across the different fine-tuning samples shown in the figure. What is the core innovation of the model proposed in the paper in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has Hit@1 score range from 40% to 55% as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What is the core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the question by summarizing the unique approach of UniKGQA, which integrates the retrieval and reasoning processes into a single framework. This is innovative because it contrasts with previous methods that treated these stages separately, thus enhancing the efficiency and effectiveness of solving multi-hop KGQA tasks.", "evidence_reference": "In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning."}
{"question": "Consider the paper that introduces the method that has an F1 score of 41.3. What is the percentage drop in $F_1$ score observed upon the removal of data augmentation during training on the \\cord\\ dataset according to the ablation study proposed in the paper?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having 41.3 F1 score?", "answer_anchor": "SPADE", "question_reference": "What is the percentage drop in $F_1$ score observed upon the removal of data augmentation during training on the \\cord\\ dataset according to the ablation study?", "explanation_reference": "The ablation study section of the paper indicates that removing data augmentation during training on the \\cord\\ dataset results in a 2.6% drop in $F_1$ score, highlighting the importance of data augmentation in the model's performance.", "evidence_reference": "~~~~ (-) data augmentation & 81.9 (-2.6) \\\\\\\\"}
{"question": "Consider the paper that introduces the method that achieves the highest score in the 'w/o p_out' setting. What theoretical connection underlies the model proposed in the paper for detecting abnormal samples?", "answer": "", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method that demonstrates the highest score in 'w/o p_out' setting?", "answer_anchor": "MSP", "question_reference": "What theoretical connection underlies the proposed method for detecting abnormal samples in the paper?", "explanation_reference": "The paper establishes a theoretical connection between Gaussian Discriminant Analysis (GDA) and the softmax classifier, which underlies the proposed method for detecting abnormal samples. This connection is utilized to fit class-conditional Gaussian distributions to the features of a pre-trained softmax neural classifier, thereby enabling the detection of out-of-distribution and adversarial samples.", "evidence_reference": "Specifically, we assume that pre-trained features can be fitted well by a class-conditional Gaussian distribution since its posterior distribution can be shown to be equivalent to the softmax classifier under Gaussian discriminant analysis (see Section \\ref{sec:main_results_score} for our justification)."}
{"question": "Consider the paper that introduces the model that has a diversity score of 2.58 for p2. What specific architectural change in its Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model demonstrates diversity score of 2.58 in p2?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change in the Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "explanation_reference": "The paper found that sharing parameters across the encoder and decoder performed nearly as well as not sharing them, without a substantial drop in performance. This approach effectively halves the total number of parameters in the model.", "evidence_reference": "We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count."}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What specific aspect of spoken language dynamics does the Spoken Language Embedding Subnetwork of the model proposed in the paper, identified as TFN, focus on to handle the volatile nature of spoken opinions based on its critical analysis of the multimodal sentiment analysis approach?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method on the first row of the table?", "answer_anchor": "TFN", "question_reference": "Based on the critical analysis of the Tensor Fusion Network's approach to multimodal sentiment analysis, what specific aspect of spoken language dynamics does the Spoken Language Embedding Subnetwork focus on to handle the volatile nature of spoken opinions?", "explanation_reference": "The Spoken Language Embedding Subnetwork is designed to handle the volatile nature of spoken language by building models capable of operating in the presence of unreliable and idiosyncratic speech traits. It achieves this by focusing on important parts of speech, which allows the model to maintain the relevance of the utterance's meaning even when encountering unusable information or recovering usable information later in the speech.", "evidence_reference": "The first part conveys the actual message and the rest is speaker thinking out loud eventually agreeing with the first part. The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech."}
{"question": "Consider the paper that introduces the method that has a METEOR cross entropy score lower than ours but higher than X-Transformer. What specific performance gain does the model proposed in the paper achieve on the Google-CC dataset compared to the CC-12M model?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method has a METEOR score lower than (Ours) but higher than X-Transformer?", "answer_anchor": "ViTCAP", "question_reference": "What specific performance gain does ViTCAP achieve on the Google-CC dataset compared to the CC-12M model?", "explanation_reference": "The performance gain of ViTCAP over the CC-12M model on the Google-CC dataset is quantified as a +3.2 improvement in the CIDEr score, indicating that ViTCAP outperforms the CC-12M model by this margin.", "evidence_reference": "ViTCAP  &  $\\\\textbf{108.6}_\\\\text{\\\\color{darkgreen}\\\\textbf{ +3.2}}$"}
{"question": "Consider the paper that introduces the optimization method that has a BLEU score of 27.3. Based on the evidence provided in the attention visualizations section, what specific linguistic function does attention head 5 in layer 5 of 6 of the model proposed in the paper appear to be involved in, according to the authors' analysis?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What optimization method shows BLEU score of 27.3?", "answer_anchor": "Transformer base", "question_reference": "Based on the evidence provided in the attention visualizations section, what specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis?", "explanation_reference": "The authors present visual evidence showing that attention head 5 in layer 5 of 6 is involved in anaphora resolution, as demonstrated by the focused attention on the word 'its' and its relation to other parts of the sentence.", "evidence_reference": "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the method that has a METEOR cross entropy score lower than ours but higher than X-Transformer. What specific loss parameters are set for the model proposed in the paper to handle the imbalanced distribution of semantic concepts in the concept classification task?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method has a METEOR score lower than (Ours) but higher than X-Transformer?", "answer_anchor": "ViTCAP", "question_reference": "What specific loss parameters are set for the concept classification task in ViTCAP to handle the imbalanced distribution of semantic concepts?", "explanation_reference": "These parameters are chosen to handle the imbalanced distribution of semantic concepts by decoupling the decay rates of positive and negative samples, emphasizing more on the contribution of the positive samples.", "evidence_reference": "Due to the extremely imbalanced semantic concepts distribution (certain concepts appear much frequently than the rest), we adopt the simplified asymmetric focal loss which shows great performances handling sample imbalance problems for the multi-label classification task. We set parameters $\\gamma_{+}=0$ and $\\gamma_{-}=1$ as in our experiment."}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the green color. What specific methodological approach does the model, identified as DExperts, utilize to ensure that generated text aligns with desired attributes while leveraging the capabilities of both expert and anti-expert language models?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method in the figure represented by teh green color?", "answer_anchor": "DExperts", "question_reference": "What specific methodological approach does DExperts utilize to ensure that generated text aligns with desired attributes while leveraging the capabilities of both expert and anti-expert language models?", "explanation_reference": "The methodological approach utilized by DExperts to align generated text with desired attributes involves combining a pretrained language model with expert and anti-expert language models in a product of experts. This ensures that tokens only get high probability if they are considered likely by the experts and unlikely by the anti-experts, effectively steering the generation process.", "evidence_reference": "Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts."}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-Fr task in the MSCOCO dataset. What is the average BLEU score improvement of VALHALLA over the text-only baseline on the EN$\\rightarrow$DE task for the Transformer-Tiny model on the Multi30K dataset?", "answer": "", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates the highest performance in En-Fr task in MSCOCO dataset?", "answer_anchor": "VALHALLA", "question_reference": "What is the average BLEU score improvement of the model proposed in the paper over the text-only baseline on the EN$\\rightarrow$DE task for the Transformer-Tiny model on the Multi30K dataset?", "explanation_reference": "The question focuses on the specific detail of the average BLEU score improvement achieved by VALHALLA over the text-only baseline for the EN$\\rightarrow$DE task using the Transformer-Tiny model on the Multi30K dataset. The answer is directly provided in the Results on Multi30K section, where it states that using Transformer-Tiny as the backbone, VALHALLA obtains an average 35.4 BLEU in EN$\\rightarrow$DE, which is about 2.1 BLEU improvements over the text-only baseline.", "evidence_reference": "Using Transformer-Tiny as the backbone, \\ours obtains an average $35.4$ BLEU in EN$\\rightarrow$DE and $54.4$ BLEU in EN$\\rightarrow$FR, which is about $2.1$ and $1.4$ BLEU improvements over the text-only baseline."}
{"question": "Consider the paper that introduces the optimization method which exhibits the second-highest reward accuracy. What is the specific mathematical expression used to derive its (the model proposed in the paper) objective under the Bradley-Terry model?", "answer": "", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What optimization method demonstrates the second highest reward accuracy?", "answer_anchor": "DPO", "question_reference": "What is the specific mathematical expression used to derive the DPO objective under the Bradley-Terry model?", "explanation_reference": "The expression directly represents the DPO objective derived under the Bradley-Terry model, showcasing how the human preference probability is expressed in terms of the optimal policy and the reference policy, with the partition function cancelling out.", "evidence_reference": "In Section \\ref{app:derivation2} we showed that we can express the (unavailable) ground-truth reward through its corresponding optimal policy: \\begin{equation}\\label{eq:main_eq_restated} r^*(x,y) =\\beta \\log \\frac{\\pi^*(y|x)}{\\piref(y|x)} + \\beta \\log Z(x) \\end{equation}  Substituting Eq. \\ref{eq:main_eq_restated} into Eq. \\ref{eq:BT_restated} we obtain: \\begin{align*} p^*(y_1\\succ y_2|x)&=\\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2|x)}{\\piref(y_2|x)} + \\beta \\log Z(x)\\right)}\\\\ &= \\frac{1}{1+\\exp\\left(\\beta \\log \\frac{\\pi^*(y_2|x)}{\\piref(y_2|x)}-\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)}\\right)} \\\\&= \\sigma\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)} - \\beta \\log \\frac{\\pi^*(y_2|x)}{\\piref(y_2|x)}\\right). \\end{align*}"}
{"question": "Consider the paper that introduces the model represented with a dot marker. What specific methodological approach did it employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model represneted with a dot marker?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological approach did DialoGPT employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "explanation_reference": "The paper describes the implementation of a maximum mutual information (MMI) scoring function to specifically address the problem of generating bland, uninformative samples in open-domain text generation models. This method penalizes bland hypotheses by maximizing the backward model likelihood, which is a direct response to the challenge mentioned.", "evidence_reference": "To address this problem, we implement a maximum mutual information (MMI) scoring function~\\cite{li2015diversity, zhang2018generating}. MMI employs a pre-trained \\textit{backward} model to predict source sentences from given responses, i.e., $P(\\\\text{Source}|\\\\text{target})$."}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the green color. What specific formatting guideline is provided regarding the width of the abstract text compared to the columns for the text in the body of the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is method in the figure represented by teh green color?", "answer_anchor": "DExperts", "question_reference": "What specific formatting guideline is provided for the width of the abstract text compared to the columns for the text in the body of the paper?", "explanation_reference": "The guideline for the width of the abstract text specifies that it should be smaller than the width of the columns for the text in the body of the paper by 0.6 cm on each side, ensuring a clear distinction and visual hierarchy between the abstract and the main text.", "evidence_reference": "Type the abstract at the beginning of the first column. The width of the abstract text should be smaller than the width of the columns for the text in the body of the paper by 0.6 cm on each side."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than LayoutXLM and a higher F1 score than SPADE. What specific methodological adaptation does the model proposed in the paper employ to redefine the neighborhood aggregation strategy in its GNN layer, and how does it differ from traditional approaches?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having lower F1 score than LayoutXLM and high F1 score than SPADE?", "answer_anchor": "Doc2Graph", "question_reference": "What specific methodological adaptation does the Doc2Graph framework employ to redefine the neighborhood aggregation strategy in its GNN layer, and how does it differ from traditional approaches?", "explanation_reference": "The adaptation is detailed in the description of the GNN layer within the architecture section, where it specifies that the neighborhood aggregation at layer l for a node i considers only those neighbors j within a certain Euclidean distance threshold, normalized between 0 and 1, applying a constant scale factor c to the sum of their representations. This approach differs from traditional methods by explicitly incorporating distance-based filtering and scaling into the aggregation process, aiming to maintain locality properties in the fully connected graph structure.", "evidence_reference": "h_{N(i)}^{l+1} = \\frac{c}{|\\Upsilon(i)|} \\sum_{j \\in \\Upsilon(i)} h^{l}_{j} where $\\Upsilon(i) = \\{j \\in N(i): |i - j| < threshold\\}$, $|i - j|$ is the Euclidean distance of nodes $i$ and $j$ saved (normalized between 0 and 1) on their connecting edge, and $c$ is a constant scale factor."}
{"question": "Consider the paper that introduces the model that results in the highest MCD 1 score. What is the instance-wise novel compound translation error rate reduction achieved by the model proposed in the paper, \\textsc{Dangle-encdec}, with relative position embeddings compared to the base \\textsc{Transformer} model with relative position embeddings?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2110.04655", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model dmonstrates the highest MCD 1 score?", "answer_anchor": "Dangle", "question_reference": "What is the instance-wise novel compound translation error rate reduction achieved by the \\textsc{Dangle-encdec} model with relative position embeddings compared to the base \\textsc{Transformer} model with relative position embeddings?", "explanation_reference": "The instance-wise novel compound translation error rate (\\(ErrR_{\\mathrm{Inst}}\\)) reduction is calculated by subtracting the \\textsc{Dangle-encdec} model's error rate from the base \\textsc{Transformer} model's error rate, both using relative position embeddings. The base \\textsc{Transformer} model with relative position embeddings has an error rate of 30.5, and the \\textsc{Dangle-encdec} model with relative position embeddings has an error rate of 22.8. Therefore, the reduction is \\(30.5 - 22.8 = 7.7\\).", "evidence_reference": "\\textsc{Transformer} (rel) & 30.5 & 63.8  & 59.4 \\\\ \\hspace{.3cm}+\\textsc{Dangle-encdec} & \\textbf{22.8} & \\textbf{50.6}  & \\textbf{60.6}"}
{"question": "Consider the paper that introduces the supervised method that achieves the highest score in 100-shot prompting. What specific performance improvement does the model proposed in the paper provide over static masking for the MNLI-m task according to the paper's findings?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which supervised method demonstrates highest scores in 100-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does dynamic masking provide over static masking for the MNLI-m task according to the paper's findings?", "explanation_reference": "The question focuses on extracting a specific detail regarding the performance improvement dynamic masking offers over static masking for the MNLI-m task. The answer is derived from comparing the performance metrics of static and dynamic masking specifically for the MNLI-m task, where dynamic masking shows a slight improvement.", "evidence_reference": "static & 78.3 & 84.3 & 92.5 \\\\ dynamic & 78.7 & 84.0 & 92.9 \\\\"}
{"question": "Consider the paper that introduces the model that has a macro-F1 score of 27.34. What specific aspect of the LegalBERT model's architecture contributes most significantly to its efficiency in terms of training and inference speed compared to other BERT-based models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model having mac-F1 score of 27.34?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's architecture contributes most significantly to its efficiency in terms of training and inference speed compared to other BERT-based models?", "explanation_reference": "The efficiency of the \\legalbertsmall model in terms of training and inference speed is primarily attributed to its architecture, which includes fewer layers, fewer hidden units, and fewer attention heads compared to other BERT-based models. This streamlined architecture reduces computational requirements and memory usage, making it faster and more resource-efficient.", "evidence_reference": "Our hypothesis is that such a specialised \\bert model can perform well against generic \\bert models, despite its fewer parameters. [...] This light-weight model, trains approx.\\ 4 times faster, while also requiring fewer hardware resources. [...] \\textsc{legal-bert-small} & 35M    & 6  & 512   & 8    & 26       & $2.43\\times$ & $4.00\\times$     & $1.70\\times$."}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What is the average number of action steps in the expert demonstrations across the ALFRED dataset for the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "SEQ2SEQ", "question_reference": "What is the average number of action steps in the expert demonstrations across the ALFRED dataset?", "explanation_reference": "The average number of action steps in the expert demonstrations is directly stated in the content describing the dataset's expert demonstrations, indicating that each demonstration averages 50 steps.", "evidence_reference": "For 2,685 combinations of task parameters, we generate three expert demonstrations per parameter set, for a total of 8,055 unique demonstrations with an average of 50 action steps."}
{"question": "Consider the paper that introduces the method in the figure that demonstrates the highest Toxicity Probability Score when the number of samples equals 1M. What specific activation function is employed in the model's, proposed by the paper, parallel cross-attention mechanism for gated activation?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method in the figure demonstrates the highest Toxicity Probability Score when number of samples equal to 1M?", "answer_anchor": "GRIT", "question_reference": "What specific activation function is employed in the parallel cross-attention mechanism of GRIT for gated activation?", "explanation_reference": "The parallel cross-attention mechanism in GRIT uses the sigmoid activation function for gated activation, as specified in the description of the parallel cross-attention design.", "evidence_reference": "c_i^g &= \\mathrm{sigmoid}(W^g[{a^{g}_{i}}; x^{\\prime}_{i}] + b^g), \\\\ c_i^r &= \\mathrm{sigmoid}(W^r[{a^{r}_{i}}; x^{\\prime}_{i}] + b^r)."}
{"question": "Consider the paper that introduces the method shown in the figure corresponds to the solid red line. What specific transformation rule is applied by the model proposed in the paper to construct disfluent summaries for the fluency dimension in text summarization?", "answer": "", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown in the figure demonstrated by the red solid line?", "answer_anchor": "UniEval", "question_reference": "What specific transformation rule is applied to construct disfluent summaries for the fluency dimension in text summarization?", "explanation_reference": "The answer directly addresses the transformation rule used for creating disfluent summaries in the context of evaluating the fluency dimension, as specified in the paper.", "evidence_reference": "Fluency represents the quality of individual sentences. We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries."}
{"question": "Consider the paper that introduces the last method shown in the Explicit --> Retrieval-enhanced --> Multi-Stage category. What specific technique is used in the Knowledge Solver approach to reduce the GPU memory burden during the finetuning of LLMs?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2309.03118", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "Knowledge Solver", "question_reference": "What specific technique is used to reduce the GPU memory burden during the finetuning of LLMs in the Knowledge Solver approach?", "explanation_reference": "The technique used to reduce the GPU memory burden during the finetuning of LLMs in the Knowledge Solver approach is LoRA, as mentioned in the section on finetuning. LoRA stands for Low-Rank Adaptation, a method that allows for efficient finetuning of large models by only updating a small set of parameters.", "evidence_reference": "We use LoRA to finetune LLaMA-7B on 8 NVIDIA A40 GPUs, each has 48 GB memory."}
{"question": "Consider the paper that introduces the model shown in the table that has an overall score of less than 3.80. How does its approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model shown in the table with overall score less than 3.80?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "explanation_reference": "Traditional methods for the RefCOCOg task typically involve classification over a set of visual regions, requiring task-specific architectures and objectives. In contrast, the unified framework proposed in the paper treats RefCOCOg as a text generation task, leveraging the same language modeling architecture and objective used for other vision-and-language tasks. This approach allows for more flexible architecture design and eliminates the need for task-specific modifications.", "evidence_reference": "While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously~\\cite{Yu2018,Chen2020} formulated classification task over a set of visual regions, allowing more flexible architecture design."}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in few-shot prompting. What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shows the second best execuation accuracy in few-shot prompting?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "explanation_reference": "The preprocessing step mentioned specifically applies a lowercase filter and Lucene's ASCIIFoldingFilter to the code, followed by tokenization using a 3-gram tokenizer, before indexing it using Elasticsearch. This step is crucial for preparing the code for efficient and effective search functionality.", "evidence_reference": "The code itself is preprocessed using a lowercase filter and Lucene's \\texttt{ASCIIFoldingFilter}, tokenized using a 3-gram tokenizer, and indexed using the default Lucene implementation of BM25 as a similarity function."}
{"question": "Consider the paper that introduces the dataset in which KALMV achieves a score of 66.48 for the Large model. What percentage of its questions can be answered using a boolean response, and how does this compare to the percentage of questions that require a numerical response?", "answer": "", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset being tested that KALMV gets 66.48 score for Large model?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka's questions that can be answered using a boolean response, and how does this compare to the percentage of questions answerable using a numerical response?", "explanation_reference": "The paper provides specific statistics on the types of answers in the Mintaka dataset, including the percentages of questions that can be answered with different types of responses. The comparison between boolean and numerical response types directly answers the question.", "evidence_reference": "A majority (72\\%) of the questions in Mintaka can be answered using an entity. 14\\% can be answered using a boolean, in yes/no or comparative questions. 7\\% can be answered using a number, such as someone\u2019s age."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. What specific limitation does the IRCoT system have regarding the base language model's capabilities and input support, as discussed in the limitations section?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2212.10509", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "IRCoT", "question_reference": "What specific limitation does the IRCoT system have regarding the base language model's capabilities and input support, as discussed in the limitations section?", "explanation_reference": "The answer directly addresses the question by summarizing the specific limitations of the IRCoT system mentioned in the limitations section of the paper. It captures the essence of the system's dependency on the base language model's capabilities for generating Chains-of-Thought (CoT) in a zero or few-shot setting and the requirement for the model to handle long inputs, which are crucial for the interleaving retrieval and reasoning process.", "evidence_reference": "\\iconsys relies on the base LM to have a zero or few-shot CoT-generation ability. \\iconsys also relies on the base LM to support long inputs as multiple retrieved paragraphs need to fit in the LM's input, in addition to at least a few demonstrations of QA or CoT with paragraphs."}
{"question": "Consider the paper that introduces the method that achieves the highest score in 10-shot prompting. Which specific scheme achieved the highest micro F1 score in the contextual label representations experiment in the 1-shot setting for the CoNLL'03 dataset?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method demonstrates highest scores in 10-shot prompting?", "answer_anchor": "LSFS", "question_reference": "In the contextual label representations experiment, which specific scheme achieved the highest micro F1 score in the 1-shot setting for the CoNLL'03 dataset?", "explanation_reference": "The highest micro F1 score in the 1-shot setting for the CoNLL'03 dataset was achieved by the scheme where the BIO tag is combined with the label name, indicated as '(BIO-TAG) LABEL'. This scheme outperformed others by providing a more detailed context for each label, leveraging both the position (BIO tag) and the semantic meaning of the label (LABEL).", "evidence_reference": "& (\\textit{BIO-TAG}) \\textit{LABEL} & \\bf 70.8 $\\pm$ 4.2  & 76.5 $\\pm$ 1.6  & 81.2 $\\pm$ 2.0  & 84.7 $\\pm$ 1.1 \\\\"}
{"question": "Consider the paper that introduces the method that has a score of 87.3 in the SciTail dataset with 16-shot prompting. How does the performance of the model proposed in the paper with 12 source tasks compare to its performance with 6 source tasks on the MRQA and Others benchmarks?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having score of 78.6 in SciTail dataset with 16-shot prompting?", "answer_anchor": "MPT", "question_reference": "How does the performance of MPT with 12 source tasks compare to MPT with 6 source tasks on the MRQA and Others benchmarks?", "explanation_reference": "The comparison between MPT trained with 6 source tasks and MPT trained with 12 source tasks on the MRQA and Others benchmarks shows that incorporating additional diverse source tasks slightly improves the performance, indicating that MPT can effectively utilize cross-task knowledge from a larger set of source tasks for target adaptation.", "evidence_reference": "MPT (w/ 6 Source Tasks) & 72.0 & 75.8 & 77.2 & 63.7 & 72.2 & 56.5 & 96.4 & 95.5  & 93.5 & 85.5 \\n MPT (w/ 12 Source Tasks) & 72.1 & 76.4 & 77.9 & 64.0 & 72.6 & 56.6 & 96.8 & 95.9   & 92.9 & 85.6"}
{"question": "Consider the paper that introduces the method that scores higher than 69.0 but lower than 70.0 in the Forgotten Realms category. How does the model proposed in the paper ensure the selection of high-quality synthetic data over poorly generated synthetic data during the training process?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method got a score high than 69.0 but lower than 70.0 in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model ensure the selection of high-quality synthetic data over poorly generated synthetic data during the training process?", "explanation_reference": "The meta-learning mechanism in MetaBLINK is designed to automatically assign different weights to each synthetic data instance. This process allows the model to prioritize high-quality synthetic data that positively impacts the model's performance on few-shot data in the target domain, thereby enhancing the overall effectiveness of the training process.", "evidence_reference": "Through exact matching and mention rewriting, we have a large number of training instances in the few-shot domain. And in the section \\ref{entity-link-model}, we give a deep learning based entity linking framework, which can be used to train on the weakly supervised dataset. However, part of the instances are noisy and may deteriorate the entity linking model's performance in the target domain. Therefore, we need to find an effective way to select suitable instances for the training process. Previous studies have shown that meta-learning can be used to reweight the instances during the training process automatically and gained relative success in neural ranking~\\cite{DBLP:conf/icml/RenZYU18}. Therefore, we adopt a meta-learning method to reweight these synthetic data in this section."}
{"question": "Consider the paper that introduces the method that has an average score of 82.8 with zero-shot prompting. What specific feature of the model's visual embeddings proposed in the paper allows it to discriminate regions from different images when multiple images are given to it?", "answer": "", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method has average score of 82.8 with zero-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific feature of the visual embeddings allows the model to discriminate regions from different images when multiple images are given to it?", "explanation_reference": "The feature that allows the model to discriminate regions from different images when multiple images are given to it is the use of image ids. This is explicitly mentioned as a function of the image ids encoded within the visual embeddings.", "evidence_reference": "Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in \\NLVR{}~\\cite{Suhr2019}, models take two input images)."}
{"question": "Consider the paper that introduces the method that has a CoLA score equal to 55.9 on the GLUE task. What is the statistical significance level (p-value) used to determine the performance difference of the model proposed in the paper in the GLUE benchmark?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has CoLA score equal to 55.9 on GLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "What is the statistical significance level (p-value) used to determine the performance difference of the Hyperdecoder model in the GLUE benchmark?", "explanation_reference": "The statistical significance level used to determine the performance difference of the Hyperdecoder model in the GLUE benchmark is indicated by the symbol '*' next to the performance values, which is defined as p < 0.05 in the paper.", "evidence_reference": "* indicates value is statistically significant ($p < 0.05$)."}
{"question": "Consider the paper that introduces the model that achieves a P_k score of 24.8 in the en_disease category. How does its structured summarization approach ensure the generation of valid sentence boundary positions within the task semantics?", "answer": "", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets 24.8 P_k score in en_disease category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "How does the structured summarization approach ensure the generation of valid sentence boundary positions within the task semantics?", "explanation_reference": "The paper addresses the potential issue of generating invalid sentence boundary positions in the output sequence of the structured summarization models. It does so by calculating the fraction of erroneous segment boundary positions in the output sequences when tested on different datasets. The results demonstrate that transformer decoders are capable of accurately generating tokens that represent integer values within the semantic bounds of the task, thereby ensuring the generation of valid sentence boundary positions.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics."}
{"question": "Consider the paper that introduces the method which exhibits the highest accuracy on the VQA-v2 task. How does the model's performance on the RefCOCO+ testA set compare to the previous state-of-the-art model UNICORN in terms of improvement points?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method shows the highest Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "How does OFA's performance on the RefCOCO+ testA set compare to the previous state-of-the-art model UNICORN in terms of improvement points?", "explanation_reference": "The question specifically asks for the improvement in performance points of OFA over the previous state-of-the-art model UNICORN on a particular dataset (RefCOCO+ testA). The answer directly reflects the numerical improvement in performance, which is a detail that can be extracted from the comparison of results between OFA and UNICORN.", "evidence_reference": "Compared with the previous SOTA UNICORN, OFA achieves significant improvement with a gain of 3.61, 6.65 and 4.85 points on the testA sets of RefCOCO and RefCOCO+ as well as the test-u set of RefCOCOg."}
{"question": "Consider the paper that introduces the method that has an F1 score of 53.36. What specific combination of features and hyperparameters resulted in the highest Key-Value F1 score in the ablation studies of the model proposed in the paper on FUNSD, according to Table 1?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having 53.36 F1 score?", "answer_anchor": "Doc2Graph", "question_reference": "What specific combination of features and hyperparameters resulted in the highest Key-Value F1 score in the ablation studies of the Doc2Graph model on FUNSD, according to Table 1?", "explanation_reference": "The highest Key-Value F1 score is directly reported in the ablation studies table (Table 1) under the combination of features and hyperparameters that includes Geometrical, Textual, and Visual features with the Edge Predictor (EP) Inner dimension set to 300 and the Input Projector Fully Connected (IP FC) layers output dimension also set to 300. This combination resulted in a Key-Value F1 score of 0.5895.", "evidence_reference": "Geometrical and textual features make the largest contribution, while visual features bring almost three points more to the Key-Value F1 score by an important increase in terms of network parameters (2.3 times more). Textual and geometrical features remain crucial for the task at hand, and their combination increase by a large amount both of their scores when used in isolation... The hyperparameters shown in the table refer to the edge predictor (EP) inner layer input dimension and the input projector fully connected (IP FC) layers (per each modality) output dimension, respectively... These changes bring an improvement of 13 points on the key-value F1 scores, between the third and fourth line of the table where we keep the features fixed... \\cmark        & \\cmark             & \\cmark          & 300                 & 300                    & \\textbf{0.9964}                    & \\textbf{0.5895}               & \\textbf{0.7903}                & 2.68"}
{"question": "Consider the paper that introduces the method that achieves the highest score in the 'w/o p_out' setting. What is the theoretical justification for the equivalence between the posterior distribution of the generative classifier under Gaussian Discriminant Analysis (GDA) and the softmax classifier, as proposed in the paper?", "answer": "", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method that demonstrates the highest score in 'w/o p_out' setting?", "answer_anchor": "MSP", "question_reference": "What is the theoretical justification for the equivalence between the posterior distribution of the generative classifier under Gaussian Discriminant Analysis (GDA) and the softmax classifier?", "explanation_reference": "The equivalence is justified by showing that the form of the posterior distribution defined by the generative classifier under GDA, when assuming tied covariance across classes, simplifies to a linear function of the input similar to the softmax classifier. This simplification occurs because the quadratic term, which is present in the general form of the Gaussian distribution, cancels out due to the tied covariance assumption, resulting in a linear dependency on the input.", "evidence_reference": "In addition to Gaussian assumption, LDA further assumes that all classes share the same covariance matrix, i.e., $\\mathbf{\\Sigma}_c = \\mathbf{\\Sigma}$. Since the quadratic term is canceled out with this assumption, the posterior distribution of generative classifier can be represented as follows: \\begin{align*} \\l & P\\left(y=c|\\mathbf{x}\\right) = \\frac{P\\left( y = c \\right) P\\left(\\mathbf{x}| y =c \\right) }{\\sum_{c^\\prime}P\\left( y= c^\\prime \\right) P\\left(\\mathbf{x}| y= c^\\prime \\right)} \\notag = \\frac{ \\exp \\left( \\mathbf{\\mu}_c^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{x} -\\frac{1}{2} \\mathbf{\\mu}_c^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_c +\\log \\beta_c \\right) }{\\sum_{c^\\prime} \\exp \\left( \\mathbf{\\mu}_{c^\\prime}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{x} -\\frac{1}{2} \\mathbf{\\mu}_{c^\\prime}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_{c^\\prime} +\\log \\beta_{c^\\prime} \\right)}. \\end{align*} One can note that the above form of posterior distribution is equivalent to the softmax classifier by considering $\\mathbf{\\mu}_{c}^\\top \\mathbf{\\Sigma}^{-1}$ and $ -\\frac{1}{2} \\mathbf{\\mu}_c^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_c +\\log \\beta_c$ as weight and bias of it, respectively."}
{"question": "Consider the paper that introduces the method shown in the table that demonstrates the highest BLEU-1 score for the Test Seen task. What is the effect of varying the contrastive loss margin \\(\\rho\\) on the model's perplexity as proposed in the paper, and what is the optimal value of \\(\\rho\\) identified in the experiments?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the table demonstrates the highest BLEU-1 score for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the effect of varying the contrastive loss margin \\(\\rho\\) on model perplexity, and what is the optimal value of \\(\\rho\\) identified in the experiments?", "explanation_reference": "The paper analyzes the effect of varying the contrastive loss margin \\(\\rho\\) on model perplexity, finding that both too small and too large values of \\(\\rho\\) lead to sub-optimal perplexity. The optimal value of \\(\\rho\\) identified through experimentation is 0.5.", "evidence_reference": "From Figure \\ref{fig:margin_vs_ppl}, we see that the contrastive training always helps to improve the perplexity as compared with MLE. However, when \\(\\rho\\) is either too small (e.g., \\(0.1\\)) or large (e.g., \\(1.0\\)), the learned representation space of the model would be either less or too isotropic, leading to a sub-optimal perplexity. In our experiments, the most suitable margin \\(\\rho=0.5\\)."}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.717 in the FB15kET dataset. What specific aspect of the model proposed in the paper allows for the differentiated integration of neighbor content while preserving the graph structure?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets MRR score equal to 0.717 in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What specific aspect of the TET approach allows for the differentiated integration of neighbour content while preserving the graph structure?", "explanation_reference": "The context transformer is specifically designed to integrate neighbours' content in a differentiated way through information exchange between neighbour pairs, which directly addresses the question's focus on preserving the graph structure while integrating neighbour content.", "evidence_reference": "a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
{"question": "Consider the paper that introduces the model that is seventh in the table. What is the Macro-F1 score improvement of the model proposed in the paper, PCP-large without the segment token </s>, over the LDSGM model for the second-level sense 'Exp.List' in PDTB 2.0?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which is the method demonstrated in the seventh row in the table?", "answer_anchor": "PCP", "question_reference": "What is the Macro-F1 score improvement of PCP-large without the segment token </s> over the LDSGM model for the second-level sense 'Exp.List' in PDTB 2.0?", "explanation_reference": "The improvement can be calculated from the Macro-F1 scores provided for the second-level sense 'Exp.List' in PDTB 2.0. LDSGM has a Macro-F1 score of 8.98, and PCP-large without the segment token </s> achieved a Macro-F1 score of 37.50. The improvement is the difference between these two scores. 44.04-40.49=3.55", "evidence_reference": "Exp.List            & 0.0           & \\underline{8.98}  & 29.63  & \\textbf{37.50}"}
{"question": "Consider the paper that introduces the dataset represented by the smallest blue circle. What specific method did the study employ to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset represented by the smallest blue label?", "answer_anchor": "NaijaSenti", "question_reference": "What specific method did the study employ to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "explanation_reference": "The study addressed the issue of tweets being collected in a language different from the query language due to stopwords overlap by collecting tweets based on locations where a language is predominantly spoken. This method involved using the location, longitude, latitude, and radius parameters to specify a circular geographic area, thereby reducing the likelihood of collecting tweets in the wrong language.", "evidence_reference": "Stopwords overlap across indigenous languages in a multilingual society such as Nigeria. This results in tweets being collected in a language that differs from the query language. To mitigate this, we collected tweets based on locations where a language is predominantly spoken, using the location, longitude, latitude and radius parameters (25 miles) to specify a circular geographic area."}
{"question": "Consider the paper that introduces the model that corresponds to the green dashed line. What specific architectural change in the Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model demonstrates in the gree dashed line?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change in the Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "explanation_reference": "The paper found that sharing parameters across the encoder and decoder performed nearly as well as not sharing them, without a substantial drop in performance. This approach effectively halves the total number of parameters in the model.", "evidence_reference": "We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count."}
{"question": "Consider the paper that introduces the method that has an accuracy of 82.82% in the CAIL2018 task. What specific feature does the model's, proposed by the paper, fact re-encoder focus on to effectively distinguish between Article 385 and Article 163?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shows 82.82 accuracy in CAIL2018 task?", "answer_anchor": "LADAN", "question_reference": "What specific feature does the LADAN model's fact re-encoder focus on to effectively distinguish between Article 385 and Article 163?", "explanation_reference": "The fact re-encoder in the LADAN model uses the distinction vector to focus on extracting distinguishable features such as defendants' identity information, which is crucial for distinguishing the applicable law articles and charges of the cases, specifically between Article 385 and Article 163.", "evidence_reference": "our fact re-encoder focuses on extracting distinguishable features like defendants' identity information (e.g., \\textit{``company manager'' ``working in the Cadastral Unit of Luocheng Branch of Luohe City Land and Resources Bureau''} in our examples), which effectively distinguish the applicable law articles and charges of these two cases."}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE-1 score equal to 44.52. How does the application of normalizing flow in the neural topic model, as proposed in the paper, specifically contribute to the improvement of abstractive text summarization performance?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method in the table that demonstrates a ROUGE 1 score equal to 44.52?", "answer_anchor": "PEGASUS+NTM", "question_reference": "How does the application of normalizing flow in the neural topic model specifically contribute to the improvement of abstractive text summarization performance?", "explanation_reference": "The application of normalizing flow in the neural topic model contributes to the improvement of abstractive text summarization performance by enabling a more accurate approximation of the true distribution of global semantics. This enhanced approximation allows the summarization model to better understand the overall document, leading to summaries that are more informative and capture key points more effectively.", "evidence_reference": "To this end, we propose a method to adapt normalizing flow in the neural topic model to have a better approximation of true distribution and integrate it into the summarization model. Integrating flow mechanism to better approximate the true posterior has been proven to improve performance for variational inference \\cite{rezende2015variational} as well as for downstream tasks such as image synthesis \\cite{kingma2016improved}, etc."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Internet-enhanced category. What specific performance improvement range did the model proposed in the paper see in language generation tasks when conditioned on the web through few-shot prompting, compared to the closed-book few-shot approach?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.05115", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the first method shown in Explicit --> Internet-enhanced", "answer_anchor": "Internet-Fewshot", "question_reference": "What specific performance improvement range did the language generation tasks see when conditioned on the web through few-shot prompting, compared to the closed-book few-shot approach?", "explanation_reference": "The question assesses the reader's understanding of the core concept of how conditioning language models on the web can improve performance in language generation tasks, specifically in the context of few-shot prompting compared to a closed-book approach. It focuses on the detail of the relative performance increase observed in the study.", "evidence_reference": "For the language generation tasks, we see a relative performance increase of 15%-30% over the commonly used closed-book few-shot approach."}
{"question": "Consider the paper that introduces the method which is directly above the dashed line in few-shot prompting. What specific adaptation in the text embeddings allows the model proposed in the paper to build correspondence among query text, label text, and objects in grounding tasks?", "answer": "", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method is shown right above the dashed line in few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific adaptation in the text embeddings allows the model to build correspondence among query text, label text, and objects in grounding tasks?", "explanation_reference": "The specific adaptation that allows the model to build correspondence among query text, label text, and objects in grounding tasks is the use of embedding sharing. This is achieved by reusing the text embeddings of visual sentinel tokens as region id embeddings, which enables the model to establish a connection between the text and visual elements, particularly useful in grounding tasks.", "evidence_reference": "In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens \\{\\texttt{<vis\\_1>}, $\\dots$, \\texttt{<vis\\_n>}\\}, which corresponds to image regions. As illustrated in Fig.~\\ref{fig:architecture}, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec.~\\ref{sec:visual_embeddings}. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec.~\\ref{sec:pretraining}, referring expression comprehension in Sec.~\\ref{sec:refcoco})."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.6712 on the Hate dataset. What is the Pearson's correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model gets mean classification accuracy 0.6712 on Hate dataset?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the task of political affiliation classification on Twitter data. A value of 0.9817 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "Twitter, \\poliaff{} ($F_1$), 0.9817159316285563"}
{"question": "Consider the paper that introduces the method that is in the last row of the upper half of the table. How does the model proposed in the paper ensure the selection of high-quality synthetic data over poorly generated synthetic data during the training process?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is on the last row of the upper half of the table?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model ensure the selection of high-quality synthetic data over poorly generated synthetic data during the training process?", "explanation_reference": "The meta-learning mechanism in MetaBLINK is designed to automatically assign different weights to each synthetic data instance. This process allows the model to prioritize high-quality synthetic data that positively impacts the model's performance on few-shot data in the target domain, thereby enhancing the overall effectiveness of the training process.", "evidence_reference": "Through exact matching and mention rewriting, we have a large number of training instances in the few-shot domain. And in the section \\ref{entity-link-model}, we give a deep learning based entity linking framework, which can be used to train on the weakly supervised dataset. However, part of the instances are noisy and may deteriorate the entity linking model's performance in the target domain. Therefore, we need to find an effective way to select suitable instances for the training process. Previous studies have shown that meta-learning can be used to reweight the instances during the training process automatically and gained relative success in neural ranking~\\cite{DBLP:conf/icml/RenZYU18}. Therefore, we adopt a meta-learning method to reweight these synthetic data in this section."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest Method 1 accuracy. What specific architectural feature allows the verifier models, as proposed in the paper, to simultaneously perform language modeling and correctness prediction tasks?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What dataset demonstrates the highest accuracy with method 1?", "answer_anchor": "GSM8K", "question_reference": "What specific architectural feature allows the verifier models to simultaneously perform language modeling and correctness prediction tasks?", "explanation_reference": "The verifier models are designed with a unique architectural feature that enables them to handle both language modeling and correctness prediction tasks. This feature is a \u2248 that operates on the logits outputted by the language model's final unembedding layer, specifically shifting and scaling the logit corresponding to a special token in the vocabulary reserved for the verifier\u2019s predictions. This design allows the rest of the tokens to continue representing the language modeling objective, while the special token is used for correctness predictions.", "evidence_reference": "We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model\u2019s final unembedding layer. Specifically, the bias and gain shift and scale the logit corresponding to a special token in the vocabulary."}
{"question": "Consider the paper that introduces the dataset that contains 3,747,569 instances. What specific threshold value was chosen for the matching score to include an abstract sentence in an aspect-based summary in the model proposed by the paper, and how was this value determined?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset with 3,747,569 instances?", "answer_anchor": "QASUM", "question_reference": "What specific threshold value was chosen for the matching score to include an abstract sentence in an aspect-based summary in OASum, and how was this value determined?", "explanation_reference": "The threshold value of 0.5 for the matching score was chosen based on manual evaluation of dataset quality. Specifically, different threshold values were tested, and for each, a set of Wikipedia pages was evaluated by experts. The chosen value reflects the balance between content overlap and summary quality, as indicated by the manual evaluation results.", "evidence_reference": "To filter out sentences with limited content overlap, an aspect-based summary includes only abstract sentences with a matching score $\\mathcal{S}(x, a)$ greater or equal to a pre-defined threshold $\\lambda$. To determine the exact value of the threshold, we try $\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]$ and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality. The Cohen's kappa between annotators is calculated to be 0.43, showing moderate agreement. The results are shown in \\cref{tab:quality}. We then choose to use $\\lambda=0.5$."}
{"question": "Consider the paper that introduces the score described as the 'influence of any example z towards another example z' by tracking their gradient dot products,' where 'we generate the self-influence scores where z = z'. What specific aspect of the CIFAR-10 evaluation demonstrates the model's superiority in identifying mislabelled training examples compared to influence functions and representer points?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2002.08484", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the name of the score with description 'Influence of any example z towards another example z' by tracking their gradient dot products. We generate the self-influence scores where z = z''?", "answer_anchor": "TracIn", "question_reference": "Based on the experimental evaluation, what specific aspect of the CIFAR-10 evaluation demonstrates TrackIn's superiority in identifying mislabelled training examples compared to influence functions and representer points?", "explanation_reference": "The question targets a detailed comparison presented in the CIFAR-10 evaluation section, focusing on TrackIn's performance relative to other methods in identifying mislabelled training examples. The answer directly reflects TrackIn's effectiveness as quantitatively demonstrated in the experiments.", "evidence_reference": "For instance, \\trackin{} recovers more than 80\\% of the mislabelled data in the first 20\\% of the ranking, whereas the other methods recover less than 50\\% at the same point."}
{"question": "Consider the paper that introduces the method shown in the figure corresponds to the solid red line. What specific transformation rule is applied by the model proposed in the paper to construct disfluent summaries for the fluency dimension in text summarization?", "answer": "", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the figure demonstrated by the red solid line?", "answer_anchor": "UniEval", "question_reference": "What specific transformation rule is applied to construct disfluent summaries for the fluency dimension in text summarization?", "explanation_reference": "The answer directly addresses the transformation rule used for creating disfluent summaries to evaluate the fluency dimension, as specified in the paper. This detail is part of the pseudo data construction process for the fluency dimension in text summarization, highlighting the methodological approach to generate negative samples.", "evidence_reference": "Fluency represents the quality of individual sentences. We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest dialogues. What is the average improvement in accuracy that the model proposed in the paper achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the least number of dialogues from the table?", "answer_anchor": "Multialpaca", "question_reference": "What is the average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "explanation_reference": "The average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English is directly stated as 7.6% in the paper.", "evidence_reference": "For languages other than English (the multilingual column), \\textsc{Poly}LM-13B outperforms LLaMA-13B with average improvement up to 7.6%, 5.6%, 3%, and 11% on XCOPA, PAWS-X, XWinagrad, and XNLI, respectively."}
{"question": "Consider the paper that introduces the model that has a diversity score of 2.58 for p2. What specific architectural change was made to the Transformer model in its framework to potentially improve computational efficiency during unsupervised pre-training?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model demonstrates diversity score of 2.58 in p2?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change was made to the Transformer model in the T5 framework to potentially improve computational efficiency during unsupervised pre-training?", "explanation_reference": "The specific architectural change made to improve computational efficiency during unsupervised pre-training in the T5 framework was replacing entire spans of corrupted tokens with a single token. This approach is mentioned as part of the unsupervised objectives exploration, where it is noted that this method produces shorter target sequences, potentially making unsupervised pre-training more computationally efficient.", "evidence_reference": "We found that most ``denoising'' objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to-text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient."}
{"question": "Consider the paper that introduces the method that achieves a Hits@1 score of 0.281 in the YAGO43kET dataset. What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the model proposed in the paper?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets Hits@1 score equal to 0.281 in YAGO43kET datast?", "answer_anchor": "RGCN", "question_reference": "What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the CET model?", "explanation_reference": "The relevance score indicates how strongly a neighbor correlates with a candidate type for an entity according to the CET model's inference. In this case, the relevance score of 6.93 directly quantifies the strength of the relationship between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan, as determined by the CET model.", "evidence_reference": "(has won prize, Pulitzer Prize) & 6.93"}
{"question": "Consider the paper that introduces the method that has the highest score in the WQ-R task. What is the approach of DSM in addressing the challenge of generating questions with both word-level and structure-level diversity as proposed in the paper?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the highest score in WQ R task?", "answer_anchor": "DSM", "question_reference": "How does the proposed approach in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "explanation_reference": "The paper discusses leveraging external natural questions to diversify question generation, indicating that this method can generate questions with different expressions by covering a broader range of semantic patterns and expressions. This approach implicitly addresses both word-level and structure-level diversity by introducing varied semantic patterns from external sources.", "evidence_reference": "In our approach, we introduce external natural questions to diversify question generation, which can generate questions with different expressions, since these natural questions cover a wider range of semantic patterns and expressions."}
{"question": "Consider the paper that introduces the method that has a score of 3.1 in the Seen, Val, SR dataset. What is the average number of action steps in the expert demonstrations across the ALFRED dataset?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows score of 3.1 in Seen, Val, SR dataset?", "answer_anchor": "SEQ2SEQ", "question_reference": "What is the average number of action steps in the expert demonstrations across the ALFRED dataset?", "explanation_reference": "The average number of action steps in the expert demonstrations is directly stated in the content describing the dataset's expert demonstrations, indicating that each demonstration averages 50 steps.", "evidence_reference": "For 2,685 combinations of task parameters, we generate three expert demonstrations per parameter set, for a total of 8,055 unique demonstrations with an average of 50 action steps."}
{"question": "Consider the paper that introduces the method that scores a 70.1 in the 'Revised Persona' column. What specific hyperparameter controls the extent of document content selection in the Content Selection Network proposed in the paper, and how does its optimal value range affect the model's performance on the original PersonaChat dataset?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets score of 70.1 in 'Revised Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What specific hyperparameter controls the extent of document content selection in the Content Selection Network, and how does its optimal value range affect the model's performance on the original PersonaChat dataset?", "explanation_reference": "The question focuses on a detailed aspect of the model's methodology, specifically the role and optimal setting of the hyperparameter $\\gamma$ in document content selection. It requires understanding of how the model filters document content based on relevance to the conversation context and how the tuning of $\\gamma$ affects model performance.", "evidence_reference": "The hyperparameter $\\gamma$ in Equation (\\ref{eq:up1}) and (\\ref{eq:up2}) controls how much the document content is selected. ... The best setting of $\\gamma$ is around 0.3 for both CSN-sent and CSN-word, which retains an appropriate amount of relevant document content for response matching."}
{"question": "Consider the paper that introduces the method that achieves a relatively constant MRR score in the FB15k-237 dataset as entity code entropy increases. What is the relative increase in Mean Reciprocal Rank (MRR) achieved by the model proposed in the paper compared to RotatE on the same dataset using a similar parameter budget?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method shows a huge increase as entity code entropy increases in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "What is the relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "explanation_reference": "The relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset is mentioned in the section summarizing the main results, where it states that EARL uses only 62% parameters and obtains a relative increase of 4.7% on MRR in comparison with RotatE.", "evidence_reference": "Specifically, on FB15k-237, \\model~uses only 62\\% parameters and obtains a relative increase of 4.7\\% on MRR in comparison with RotatE."}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in direct prompting. How does its performance, specifically StarCoderBase, on natural language reasoning tasks on the HELM benchmark compare to other open-access models?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shows the second best execuation accuracy in direct prompting?", "answer_anchor": "StarCoder", "question_reference": "Based on the evaluation of StarCoderBase on the HELM benchmark, how does its performance on natural language reasoning tasks compare to other open-access models?", "explanation_reference": "The evaluation of StarCoderBase on the HELM benchmark for natural language reasoning tasks shows that it generally outperforms other open-access models, indicating its superior ability to leverage its natural language and code pretraining for these tasks.", "evidence_reference": "In Table~\\ref{tab:helm_results} we report the results. We compute each model's ranking on each task, and order models in the table by their average ranking across tasks. StarCoderBase generally obtains substantially stronger performance than all other models with released weights and often performs comparably to or better than much larger models."}
{"question": "Consider the paper that introduces the model that has an F1 score higher than PCP's but lower than DiscoPrompt's on PDTB-Top. How does its utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method with an F1 score higher than PCP but lower than DiscoPrompt?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF model's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact?", "explanation_reference": "The notable drop in performance when replacing the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ with its hard-label version $\\mathcal{L}_{L'}$ demonstrates the effectiveness of the scoring function in $\\mathcal{L}_{L}$, which considers more subtle semantic structures of the local hierarchy.", "evidence_reference": "Secondly, we replace the Local Hierarchy-aware Contrastive loss $\\mathcal{L}_{L}$ (Equation (\\ref{equation: soft local})) with the hard-label version $\\mathcal{L}_{L'}$ (Equation (\\ref{equation: hard local})) and find that the performance drops notably."}
{"question": "Consider the paper that introduces the LLM model that corresponds to an r score of 0.813. What is the specific improvement in percentage points over its predecessor in internal adversarially-designed factuality evaluations?", "answer": "", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the LLM model that demonstrates the r score equal to 0.813?", "answer_anchor": "GPT-4", "question_reference": "What is the specific improvement in percentage points of GPT-4 over GPT-3.5 in internal adversarially-designed factuality evaluations?", "explanation_reference": "The improvement is directly stated as a comparison between GPT-4 and GPT-3.5, highlighting the progress made in reducing hallucinations and improving factuality.", "evidence_reference": "GPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have themselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our latest GPT-3.5 on our internal, adversarially-designed factuality evaluations."}
{"question": "Consider the paper that introduces the method demonstrated by the solid lavender line. What is the model's inference time for feature extraction compared to VinVL and M2 Transformer?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method demonstrated in the lavendar solid line?", "answer_anchor": "GRIT", "question_reference": "What is the inference time for feature extraction using GRIT compared to VinVL and M2 Transformer?", "explanation_reference": "The inference time for feature extraction using GRIT is significantly lower than that of VinVL and M2 Transformer, demonstrating GRIT's computational efficiency in this aspect.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the model that achieves the best P_k score among the models in the first part of the table. How does its structured summarization approach ensure the generation of valid segment boundary positions within the task semantics?", "answer": "", "figure": "locality/2310.11772/comparison_2_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model gets the best P_k score on the upper part of the table??", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "How does the structured summarization approach ensure the generation of valid segment boundary positions within the task semantics?", "explanation_reference": "The paper addresses the potential issue of generating invalid segment boundary positions in the output sequence of the structured summarization models. It does so by calculating the fraction of erroneous outputs, such as non-integer-convertible components or out-of-bound segment boundary positions, and demonstrating that such errors are exceedingly rare. This indicates that the transformer decoders used in the structured summarization models are capable of accurately generating tokens that represent integer values within the semantic bounds of the task, ensuring the validity of the generated segment boundary positions.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics."}
{"question": "Consider the paper that introduces the dataset in the Movies domain only that has a PF field. Which dataset significantly demonstrates the effectiveness of the knowledge posterior/prior distribution learning in DuConv's conversation goal completion rate?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which dataset is in Movies domain only and has PF field?", "answer_anchor": "DuConv", "question_reference": "What specific aspect of the conversation goal completion rate significantly demonstrates the effectiveness of the knowledge posterior/prior distribution learning in the proposed generation model?", "explanation_reference": "The question focuses on a detailed aspect of the conversation goal completion rate that highlights the effectiveness of the knowledge posterior/prior distribution learning in the proposed generation model. The answer directly addresses this by pointing out the significantly higher rate of achieving score '2' in goal completion, which indicates a more engaging and coherent conversation due to effective knowledge exploitation.", "evidence_reference": "From this table, it can be seen that our proposed generation model can exploit more knowledge to achieve the conversation goal  (much higher rate on score '2'), making the conversation more engaging and coherent."}
{"question": "Consider the paper that introduces the method that achieves the highest Precision score in the Token (I-topo) category. What specific advantage does the span boundary objective (SBO) of SpanBERT provide over BERT's next sentence prediction (NSP) objective in terms of model performance on coreference resolution?", "answer": "", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets the highest Precision score in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific advantage does the span boundary objective (SBO) provide over BERT's next sentence prediction (NSP) objective in terms of model performance on coreference resolution?", "explanation_reference": "The span boundary objective (SBO), when compared to BERT's next sentence prediction (NSP) objective, specifically provides a substantial gain of 2.7% F1 on coreference resolution. This indicates that SBO is more effective for tasks requiring understanding of relationships between spans of text, such as coreference resolution, by improving the model's ability to predict entire masked spans using only the boundary token representations.", "evidence_reference": "Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone."}
{"question": "Consider the paper that discusses which dataset is represented by the smallest blue circle. What is the primary reason applying Language Adaptive Fine-tuning (LAFT) on the Twitter domain for this dataset did not improve performance?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset represented by the smallest blue label?", "answer_anchor": "NaijaSenti", "question_reference": "What is the primary reason applying Language Adaptive Fine-tuning (LAFT) on the Twitter domain did not improve performance?", "explanation_reference": "The paper explains that applying LAFT on the Twitter domain did not improve performance primarily due to the small size of the Twitter data available for pre-training, which is often short and less comprehensive compared to the general domain data.", "evidence_reference": "Interestingly, applying LAFT on the Twitter domain did not improve performance. The main reason for this is the small size of the Twitter data."}
{"question": "Consider the paper that introduces the model that has the largest number of updated parameters. What specific role does the stop-gradient operation play in the end-to-end fine-tuning process of the model proposed in the paper's subgraph retriever and reasoner?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the most number of updated parameters?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific role does the stop-gradient operation play in the end-to-end fine-tuning process of the subgraph retriever and reasoner?", "explanation_reference": "The stop-gradient operation is used to prevent the backpropagation of gradients through the reasoner's parameters during the joint training of the retriever and reasoner. This allows for the optimization of the retriever based on the feedback from the reasoner without altering the reasoner's learned parameters during this phase.", "evidence_reference": "In the end-to-end fine-tuning section, it is mentioned that '\\u2026where the stop-gradient operation \\u03b2\\u03b2\\u03a3\\u03a3 is to stop updating the parameters \\u03c6.' This indicates that the stop-gradient operation is applied to ensure that the parameters of the reasoner (\\u03c6) do not get updated while the retriever is being fine-tuned based on the feedback from the reasoner."}
{"question": "Consider the paper that introduces the method which has the highest perplexity. What specific computational advantage does the model proposed by the paper offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having the highest perplexity?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's online classification trick offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "explanation_reference": "The computational advantage is highlighted by comparing the efficiency of GeDi's method to compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token using only two parallel forward passes (one for each control code) against the requirement of a unidirectional classifier needing $|\\\\gV|$ forward passes for a vocabulary set $\\\\gV$. This comparison underlines GeDi's significant reduction in computational demand.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the model that has an overall average score of 45.68 in Previous Image-free Systems. What specific loss function is introduced in the paper to reduce the mismatch between training and inference in its framework?", "answer": "", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model demonstrates the overall average score of 45.68 in previous image-free systems?", "answer_anchor": "VALHALLA", "question_reference": "What specific loss function is introduced to reduce the mismatch between training and inference in the VALHALLA framework?", "explanation_reference": "The consistency loss is introduced to reduce the mismatch between training and inference by encouraging consistency between predictions using either ground-truth or hallucinated visual representations.", "evidence_reference": "To reduce this mismatch, we define a \\emph{consistency loss}...where $y_i^M = p(y_i | x, z, y_{<i}; \\mathbf{f}_{\\mathbf{T}})$ and $y_i^H = p(y_i | x, \\hat{z}, y_{<i}; \\mathbf{f}_{\\mathbf{T}})$ are the next word distributions from ground-truth visual tokens and hallucinated features respectively, and $\\text{KL}[y^M_i \\Vert y^H_i]$ is the Kullback-Leibler divergence between the two conditional distributions."}
{"question": "Consider the paper that introduces the model in the figure that has a more negative Spearman's Correlation than 0.60 when the alternative set size is set to 100 in the Mean Cosine setting. What specific architectural feature allows the model, proposed by the paper, to efficiently handle the full context in a computationally efficient manner?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model in the figure exceed 0.60 Spearman's Correlation as the alternative set size to 100 in Mean cosine setting?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific architectural feature of DialoGPT allows it to efficiently handle the full context in a computationally efficient manner?", "explanation_reference": "The question assesses understanding of a core concept in the architecture of DialoGPT that enables it to efficiently process and generate conversational responses by handling the full context. The multi-layer self-attentive mechanism is a fundamental architectural feature that allows for this efficiency.", "evidence_reference": "a transformer-based architecture like GPT-2, which uses a multi-layer self-attentive mechanism to allow fully-connected cross-attention to the full context in a computationally efficient manner"}
{"question": "Consider the paper that introduces the method that has a score of 87.3 in the SciTail dataset with 16-shot prompting. How does its performance with 12 source tasks compare to its performance with 6 source tasks on the MRQA benchmark?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having score of 78.6 in SciTail dataset with 16-shot prompting?", "answer_anchor": "MPT", "question_reference": "How does the performance of MPT with 12 source tasks compare to MPT with 6 source tasks on the MRQA benchmark?", "explanation_reference": "The performance of MPT with 12 source tasks is slightly better than MPT with 6 source tasks on the MRQA benchmark, as indicated by the average performance improvement shown in the results.", "evidence_reference": "MPT (w/ 6 Source Tasks) Avg. = 72.2, MPT (w/ 12 Source Tasks) Avg. = 72.6"}
{"question": "Consider the paper that introduces the model that has a macro-F1 score of 27.34. What specific aspect of the model's performance on the Contracts dataset demonstrates its efficiency compared to larger models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model having mac-F1 score of 27.34?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's performance on the \\contractsdata dataset demonstrates its efficiency compared to larger models?", "explanation_reference": "The question targets a detailed aspect of the \\legalbertsmall model's performance, specifically its efficiency and effectiveness on the \\contractsdata dataset compared to larger models. This is a critical analysis question because it asks for a comparison based on the model's size and performance, highlighting the balance between computational resources and task effectiveness.", "evidence_reference": "Most importantly, (iv) we release \\legalbert, a family of \\bert models for the legal domain, intended to assist legal \\nlp research, computational law, and legal technology applications. This family includes \\legalbertsmall, a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"question": "Consider the paper that introduces the model that corresponds to the penultimate row of the table. What specific hyperparameter values were used for the FewRel dataset in the experiments conducted by the CEAR model, and how do these values compare to those used for the TACRED dataset?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model is shown in the penultimate row in the table?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were used for the FewRel dataset in the experiments, and how do these values compare to those used for the TACRED dataset?", "explanation_reference": "The specific hyperparameter values for the FewRel and TACRED datasets are directly provided in the Implementation Details section of the paper, allowing for a direct comparison between the two sets of values.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7."}
{"question": "Consider the paper that introduces the method that has an accuracy of 82.82% in the CAIL2018 task. What specific method does the model proposed in the paper use to initially segment law articles into communities before applying the graph distillation operator?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shows 82.82 accuracy in CAIL2018 task?", "answer_anchor": "LADAN", "question_reference": "What specific method does LADAN use to initially segment law articles into communities before applying the graph distillation operator?", "explanation_reference": "The paper specifies that to initially segment law articles into communities, a fully-connected graph is constructed where the weight on the edge between a pair of law articles is defined by the cosine similarity between their TF-IDF (Term Frequency-Inverse Document Frequency) representations. This method is used before applying the graph distillation operator to identify probably confusing law articles by removing edges with weights less than a predefined threshold from the graph, resulting in several disconnected subgraphs or communities.", "evidence_reference": "To find probably confusing law articles, we first construct a fully-connected graph $G^*$ for all law articles $\\mathcal{L}$, where the weight on the edge between a pair of law article $L_i, L_j\\in \\mathcal{L}$ is defined as the cosine similarity between their TF-IDF (Term Frequency-Inverse Document Frequency) representations $\\mathbf{tf\\_idf}_i$ and $\\mathbf{tf\\_idf}_j$."}
{"question": "Consider the paper that introduces the model in the figure that has the lowest diversity score for each of p1, p2, p3, and p4. What specific architectural change was made to the Transformer's layer normalization in the model proposed in the paper compared to its originally proposed form?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model demonstrates the lowest diversity score in p1, p2, p3, and p4?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change was made to the Transformer's layer normalization in the T5 model compared to its originally proposed form?", "explanation_reference": "The T5 model made specific modifications to the original Transformer architecture's layer normalization. Specifically, it simplified layer normalization by placing it outside the residual path and removing the additive bias. This change is orthogonal to the experimental factors considered in the empirical survey of transfer learning.", "evidence_reference": "Our encoder-decoder Transformer implementation closely follows its originally-proposed form \\citep{vaswani2017attention}. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of ``blocks'', each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization \\citep{ba2016layer} is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection \\citep{he2016deep} adds each subcomponent's input to its output."}
{"question": "Consider the paper that introduces the method that has the lowest score in the 'Original Persona' column. What is the implication of the model proposed in the paper's performance on the CMUDoG dataset for future research directions in document-grounded conversation systems?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the lowest score on 'Original Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the implication of the CSN model's performance on the CMUDoG dataset for future research directions in document-grounded conversation systems?", "explanation_reference": "The performance of the CSN model on the CMUDoG dataset, which showed that both sentence-level and word-level content selection strategies work equally well, implies that future research could benefit from exploring document content selection at the topic level. This is suggested as a potential future work direction in the conclusion, indicating that the findings from the CMUDoG dataset performance could lead to the exploration of more granular or higher-level content selection mechanisms, such as topic-level selection, to further improve the effectiveness of document-grounded conversation systems.", "evidence_reference": "As a future work, it would be interesting to study if the selection can be done at topic level, in addition to sentence and word levels."}
{"question": "Consider the paper that introduces the method that has an F1 score of 64.95 on PDTB-Top. What is the primary reason the model proposed in the paper underperforms compared to it on the Temporal sense?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which is the method show a 64.95 F1 score on PDTB-Top?", "answer_anchor": "PCP", "question_reference": "What is the primary reason the PIDRP method underperforms compared to the PCP method on the Temporal sense?", "explanation_reference": "The paper suggests that the PIDRP method's underperformance, especially on the Temporal sense, is attributed to the fact that connective prediction aligns more closely with the natural language patterns that the model was exposed to during its pre-training stage, as opposed to directly predicting implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the method that scores a 70.1 in the 'Revised Persona' column. What is the primary motivation behind its implementation of a hard selection of document content in the Content Selection Network (CSN) as opposed to a soft weighting approach?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets score of 70.1 in 'Revised Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the primary motivation behind implementing a hard selection of document content in the Content Selection Network (CSN) as opposed to a soft weighting approach?", "explanation_reference": "The question assesses the understanding of the core concept behind the design of the CSN model, specifically why a hard selection mechanism is preferred over a soft weighting approach for selecting relevant document content. The answer is directly related to the fundamental observation that motivates the design of the CSN, which is that conversations typically focus on specific parts of a document rather than its entirety at any given step.", "evidence_reference": "The hard selection of document content is motivated by the following observation: although the whole conversation can cover many aspects described in the grounding document, each of the step is related to only a small part of the document content."}
{"question": "Consider the paper that introduces the dataset located at the bottom left of the figure. What specific method did the study employ to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset located on the bottom left of the figure?", "answer_anchor": "NaijaSenti", "question_reference": "What specific method did the study employ to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "explanation_reference": "The study addressed the issue of tweets being collected in a language different from the query language due to stopwords overlap by collecting tweets based on locations where a language is predominantly spoken. This method involved using the location, longitude, latitude, and radius parameters to specify a circular geographic area, thereby reducing the likelihood of collecting tweets in the wrong language.", "evidence_reference": "Stopwords overlap across indigenous languages in a multilingual society such as Nigeria. This results in tweets being collected in a language that differs from the query language. To mitigate this, we collected tweets based on locations where a language is predominantly spoken, using the location, longitude, latitude and radius parameters (25 miles) to specify a circular geographic area."}
{"question": "Consider the paper that introduces the method which exhibits an accuracy of 79.44% in the CJO22 task. What specific feature does the model's, proposed by the paper, fact re-encoder focus on to effectively distinguish between Article 385 and Article 163?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shows 79.44 accuracy in CJO22 task?", "answer_anchor": "LADAN", "question_reference": "What specific feature does the LADAN model's fact re-encoder focus on to effectively distinguish between Article 385 and Article 163?", "explanation_reference": "The fact re-encoder in the LADAN model uses the distinction vector to focus on extracting distinguishable features such as defendants' identity information, which is crucial for distinguishing the applicable law articles and charges of the cases, specifically between Article 385 and Article 163.", "evidence_reference": "our fact re-encoder focuses on extracting distinguishable features like defendants' identity information (e.g., \\textit{``company manager'' ``working in the Cadastral Unit of Luocheng Branch of Luohe City Land and Resources Bureau''} in our examples), which effectively distinguish the applicable law articles and charges of these two cases."}
{"question": "Consider the paper that introduces the model shown in the figure that is consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B. What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shown in the figure consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "explanation_reference": "The preprocessing step mentioned specifically applies a lowercase filter and Lucene's ASCIIFoldingFilter to the code, followed by tokenization using a 3-gram tokenizer, before indexing it using Elasticsearch. This step is crucial for preparing the code for efficient and effective search functionality.", "evidence_reference": "The code itself is preprocessed using a lowercase filter and Lucene's \\texttt{ASCIIFoldingFilter}, tokenized using a 3-gram tokenizer, and indexed using the default Lucene implementation of BM25 as a similarity function."}
{"question": "Consider the paper that introduces the optimization method that has the lowest BLEU score among all models in the table. What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What optimization method shows the lowest BLEU score across all models?", "answer_anchor": "Transformer base", "question_reference": "What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "explanation_reference": "The answer is directly supported by the descriptions provided in the Attention Visualizations section, where it is mentioned that many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult', and that two attention heads, also in layer 5 of 6, are apparently involved in anaphora resolution. These behaviors indicate that the attention heads have learned to perform tasks related to the structural aspects of sentences.", "evidence_reference": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb `making', completing the phrase `making...more difficult'.  Attentions here shown only for the word `making'. Different colors represent different heads. Best viewed in color. Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the model represented by the orange bar. What specific advantage does the Vanilla Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model is demonstrated by the yellow bar?", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific advantage does the Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "explanation_reference": "The advantage is highlighted by the experimental variations where changing the number of attention heads and dimensions (keeping the computational cost constant) showed that single-head attention performs worse than the optimal setting, indicating that multi-head attention improves model quality. Additionally, the design of multi-head attention ensures that the total computational cost remains similar to that of single-head attention with full dimensionality, thus not significantly increasing the computational cost.", "evidence_reference": "In Table~\\ref{tab:variations} rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section \\ref{sec:multihead}. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"question": "Consider the paper that introduces the method which is directly above the dashed line in few-shot prompting. What specific feature of the visual embeddings allows the SALAM method to discriminate regions from different images when multiple images are given to it?", "answer": "", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method is shown right above the dashed line in few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific feature of the visual embeddings allows the model to discriminate regions from different images when multiple images are given to it?", "explanation_reference": "The feature that allows the model to discriminate regions from different images when multiple images are given to it is the use of image ids. This is explicitly mentioned as a function of the image ids encoded within the visual embeddings.", "evidence_reference": "Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in \\NLVR{}~\\cite{Suhr2019}, models take two input images)."}
{"question": "Consider the paper that introduces the LLM model that demonstrates the lowest MSE score. What specific methodological difference in the model's evaluation setup for its performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "answer": "", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the LLM model that demonstrates the lowest MSE score?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard approach of directly extracting the model's letter choice from the explanation for most exam runs. Instead, for these specific exams, the approach involved sampling a letter choice at temperature 0 using the explanation already sampled, indicating a unique handling of these exams compared to others.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category, namely GRACE. How does the choice of $\\epsilon_\\text{init}$ and the edited layer impact GRACE in terms of its ability to generalize to unseen inputs while maintaining edit compression?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2211.11031", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "GRACE", "question_reference": "How does the choice of $\\epsilon_\\text{init}$ and the edited layer impact GRACE's ability to generalize to unseen inputs while maintaining edit compression?", "explanation_reference": "The choice of $\\epsilon_\\text{init}$ and the edited layer directly impacts GRACE's editing performance, with different layers offering varying degrees of generalization and memorization. Larger $\\epsilon_\\text{init}$ values allow for more general edits but can increase interference with unrelated inputs, while the choice of layer affects how semantically similar inputs are edited. This balance is crucial for maintaining high performance across a large number of edits without significantly increasing the size of the codebook.", "evidence_reference": "We first find that different blocks lead to different editing performance. Notably, editing Blocks two and four achieve high TRR, high ERR, and strong generalization for both choices of $\\epsilon_\\text{init}$. On the contrary, for Block 6 we see that since each $\\epsilon_\\text{init}$ is small, they do not generalize at all and also lead to poor ERR. As expected, when $\\epsilon_\\text{init}$ is small, TRR performance is optimal, since most edits require a new key. While creating a large codebook can be feasible, the trade-off in Holdout becomes clear, as detailed in the next finding. The relationship between layer choice, performance, and the size of the codebook likely stems from a layer's representational capacity: Layers that map semantically-equivalent inputs near one another will be easier to edit with \\method."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest Method 1 accuracy. What specific architectural feature allows the model proposed in the paper to simultaneously perform language modeling and correctness prediction tasks?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What dataset demonstrates the highest accuracy with method 1?", "answer_anchor": "GSM8K", "question_reference": "What specific architectural feature allows the verifier models to simultaneously perform language modeling and correctness prediction tasks?", "explanation_reference": "The specific architectural feature that enables verifier models to undertake both language modeling and correctness prediction tasks is the implementation of a small scalar head. This scalar head operates on the logits outputted by the language model\u2019s final unembedding layer, allowing for predictions on a per-token basis. This design choice facilitates the verifier's dual functionality by shifting and scaling the logit corresponding to a special token in the vocabulary reserved for the verifier\u2019s predictions, while other tokens continue to represent the language modeling objective.", "evidence_reference": "We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model\u2019s final unembedding layer. Specifically, the bias and gain shift and scale the logit corresponding to a special token in the vocabulary."}
{"question": "Consider the paper that introduces the model that corresponds to the penultimate row of the table. What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the model proposed in the paper?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model is shown in the penultimate row in the table?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the proposed model?", "explanation_reference": "The answer directly lists the specific hyperparameter values that were determined through grid search for the FewRel dataset, as detailed in the implementation section of the paper.", "evidence_reference": "We find the best hyperparameter values through grid search with a step of 0.1 except 0.05 for \u03c9 and 0.25 for \u03b3. The search spaces for various hyperparameters are \u03b1\u2208[0.2,0.8], \u03b2\u2208[0.1,0.5], \u03bc\u2208[0.1,1.0], \u03c9\u2208[0.05,0.25], \u03b3\u2208[1.0,2.0] and \u03bb1, \u03bb2\u2208[0.5,1.5]. Besides, we fix \u03c41 and \u03c42 to 0.1 and 0.5, respectively. The used hyperparameter values are listed below: For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the model represented by a blue line in the figure. What specific methodological adjustment did the authors make to the initialization scheme of the model proposed in the paper to account for its model depth?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model represented with a blue line in the figure?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological adjustment did the authors make to the initialization scheme of the DialoGPT model to account for its model depth?", "explanation_reference": "The authors mention modifying the initialization scheme to account for model depth as part of their methodological adjustments to the GPT-2 architecture for the DialoGPT model. This detail is a specific methodological choice aimed at improving the model's performance, likely by ensuring stability in deeper network layers.", "evidence_reference": "Our model inherits from GPT-2 \\cite{gpt2}, a 12-to-48 layer transformer with layer normalization, a initialization scheme that accounts for model depth that we modified, and byte pair encodings \\cite{bpe} for the tokenizer."}
{"question": "Consider the paper that introduces the method, with an average max toxicity of more than 0.3, is represented by a circle. What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments for this method, after analyzing its effect on perplexity and repetition scores?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method with average max toxicity more than 0.3 but with circle label?", "answer_anchor": "PPLM", "question_reference": "What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments after analyzing its effect on perplexity and repetition scores?", "explanation_reference": "The initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments is determined based on its effect on perplexity and repetition scores. The paper mentions that for \\(\\lambda_0=5\\), the average perplexity (58.4) and repetition score (3.5%) are the best among the considered values, indicating this value was chosen for subsequent experiments.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the method that exhibits a FLAN-T5 score of 52.4% using SGD in 24 domains. How does the model's recency weighting in the complexity-regularized ILP method for graph generation from subtask state labels influence the precondition inference?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shows FLAN-T5 score of 52.4% using SGE in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "How does the recency weighting in the complexity-regularized ILP method for graph generation from subtask state labels influence the precondition inference?", "explanation_reference": "The recency weighting modifies the precondition inference process by incorporating temporal information, specifically by giving more importance to recent subtask eligibility. This approach helps in dealing with the sparse and noisy data by emphasizing the most relevant information for determining subtask dependencies.", "evidence_reference": "w_{t, n} = \\max(0.1, \\lambda ^ {t_n-t}), where $0<\\lambda<1$ is the discount factor, $t_n$ is the time step when the precondition for subtask $n$ became satisfied."}
{"question": "Consider the paper that introduces the model that has a lower mean classification accuracy than VIBE but higher mean classification accuracy than UDALM on the Stance dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model has lower accuracy than VIBE but higher accuracy than UDALM?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength and direction of a linear relationship between two variables. In this case, it quantifies the relationship between the word overlap (how vocabularies change over time) and the model performance for the task of political affiliation classification on Twitter data. A value of 0.9817159316285563 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the dataset that has the largest number of Queries|Aspects in the ABS category. What threshold value was set for the 'Group' domain during the aspect discovery stage in the model proposed by the paper?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset with the most number of Queries|Aspects in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What threshold value was set for the 'Group' domain during the aspect discovery stage?", "explanation_reference": "The threshold value for the 'Group' domain during the aspect discovery stage is mentioned as part of the methodology for assigning labels to sentences based on their aspect probabilities. A lower threshold means more sentences are included as input to the summarization model, potentially increasing noise.", "evidence_reference": "We tune $\\lambda$ independently for each domain based on the performance on validation sets and set $0.5$ for \\textit{Group}, $0.8$ for \\textit{Album}, \\textit{Animal}, \\textit{Building}, \\textit{Film}, and $0.9$ for the remaining domains as the threshold values."}
{"question": "Consider the paper that introduces the model that has a macro-F1 score of 27.34. What specific aspect of its performance on the contracts dataset compared to larger models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model having mac-F1 score of 27.34?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's performance on the \\contractsdata dataset compared to larger models?", "explanation_reference": "The question targets a detailed aspect of the \\legalbertsmall model's performance, specifically its efficiency and effectiveness on the \\contractsdata dataset compared to larger models. This is a critical analysis question because it asks for a comparison based on the model's size and performance, highlighting the balance between computational resources and task effectiveness.", "evidence_reference": "Most importantly, (iv) we release \\legalbert, a family of \\bert models for the legal domain, intended to assist legal \\nlp research, computational law, and legal technology applications. This family includes \\legalbertsmall, a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"question": "Consider the paper that introduces the model that has a Recall@7 score of 92.97 for the MWOZ task. Which model shows performance differences with a fine-tuned knowledge retriever on the SMD dataset between the T5-Large and T5-3B models in terms of Entity F1 improvement?", "answer": "", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has the 92.97 score in Recall@7 for MWOZ task?", "answer_anchor": "Q-TOD", "question_reference": "How does the performance of Q-TOD with a fine-tuned knowledge retriever on the SMD dataset compare between the T5-Large and T5-3B models in terms of Entity F1 improvement?", "explanation_reference": "The question focuses on the specific detail of performance improvement when employing a fine-tuned knowledge retriever for Q-TOD on the SMD dataset, comparing the Entity F1 metric improvements between two different model sizes. The answer directly addresses the comparison by quantifying the improvement seen in both models, which is derived from the detailed experimental results provided in the paper.", "evidence_reference": "~~Q-TOD (T5-Large) & ~~~~~71.11~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~71.17 (+0.06)~~ \\\\ \\quad ~~w/ oracle knowledge & ~~~~~71.96 (\\textbf{+0.85})~~ \\\\ \\hline ~~Q-TOD (T5-3B) & ~~~~~73.44~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~74.96 (+1.52)~~ \\\\ \\quad ~~w/ oracle knowledge & ~~~~~76.20 (\\textbf{+2.76})~~ \\\\"}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the green color. What specific formatting guideline is provided for the width of the abstract text compared to the columns for the text in the body of the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method in the figure represented by teh green color?", "answer_anchor": "DExperts", "question_reference": "What specific formatting guideline is provided for the width of the abstract text compared to the columns for the text in the body of the paper?", "explanation_reference": "The question assesses the understanding of detailed formatting instructions for the abstract section, which is a fundamental aspect of preparing a manuscript for submission. It requires knowledge of specific measurements that ensure the abstract is formatted correctly according to the guidelines.", "evidence_reference": "Use two-column format when you begin the abstract. Type the abstract at the beginning of the first column. The width of the abstract text should be smaller than the width of the columns for the text in the body of the paper by 0.6 cm on each side."}
{"question": "Consider the paper that introduces the method that has a score of 61.2 in the BoolQ dataset with 32-shot prompting. What is the relative error reduction achieved by using a source prompt from MNLI for the BoolQ target task, despite its low cosine similarity?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having score of 61.2 in BoolQ dataset with 32-shot prompting?", "answer_anchor": "SPoT", "question_reference": "What is the relative error reduction achieved by using a source prompt from MNLI for the BoolQ target task, despite its low cosine similarity?", "explanation_reference": "The relative error reduction of 19.0% for BoolQ using a source prompt from MNLI, despite a low cosine similarity of 0.4, highlights the effectiveness of task transferability beyond mere task similarity. This detail underscores the nuanced dynamics of prompt transfer effectiveness, where factors beyond straightforward task similarity can significantly influence transfer outcomes.", "evidence_reference": "In some cases (e.g., on BoolQ), we observe a large relative error reduction (19.0\\%, achieved by a source prompt of MNLI) despite a low cosine similarity (0.4). This suggests that factors other than task similarity (data size, task difficulty, domain similarity, etc.) may also play a role in determining transferability."}
{"question": "Consider the paper that introduces the model that results in the highest MCD 1 score. What specific aspect of its encoder mechanism allows it to better handle the translation of novel compounds in machine translation tasks compared to the baseline model?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2110.04655", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model dmonstrates the highest MCD 1 score?", "answer_anchor": "Dangle", "question_reference": "What specific aspect of the proposed \\textsc{Dangle} model's encoder mechanism allows it to better handle the translation of novel compounds in machine translation tasks compared to the baseline model?", "explanation_reference": "The adaptive encoding mechanism of the \\textsc{Dangle} model allows it to decompose the representation problem of an unfamiliar compound phrase into sub-problems of familiar phrases, thereby effectively handling the translation of novel compounds by focusing on familiar sub-components.", "evidence_reference": "We believe this is due to the proposed adaptive encoding mechanism and its ability to decompose the representation problem of an unfamiliar compound phrase into sub-problems of familiar phrases (i.e, ``behind the small doctor'' and ``the small doctor on the floor'')."}
{"question": "Consider the paper that introduces the LLM model that demonstrates the lowest MSE score. What specific methodological difference in the evaluation setup for the model proposed in the paper's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "answer": "", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the LLM model that demonstrates the lowest MSE score?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "explanation_reference": "This methodological difference is significant because sampling at temperature 0 can lead to more deterministic outcomes based on the generated explanation, potentially affecting the model's performance on these exams in a way that differs from how choices were determined in other exams.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method that scores a 69.56 in the Forgotten Realms category. How does the model proposed in the paper ensure the selection of high-quality synthetic data over poorly generated synthetic data during the training process?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method got 69.56 score in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model ensure the selection of high-quality synthetic data over poorly generated synthetic data during the training process?", "explanation_reference": "The meta-learning mechanism in MetaBLINK is designed to automatically assign different weights to each synthetic data instance. This process allows the model to prioritize high-quality synthetic data that positively impacts the model's performance on few-shot data in the target domain, thereby enhancing the overall effectiveness of the training process.", "evidence_reference": "Through exact matching and mention rewriting, we have a large number of training instances in the few-shot domain. And in the section \\ref{entity-link-model}, we give a deep learning based entity linking framework, which can be used to train on the weakly supervised dataset. However, part of the instances are noisy and may deteriorate the entity linking model's performance in the target domain. Therefore, we need to find an effective way to select suitable instances for the training process. Previous studies have shown that meta-learning can be used to reweight the instances during the training process automatically and gained relative success in neural ranking~\\cite{DBLP:conf/icml/RenZYU18}. Therefore, we adopt a meta-learning method to reweight these synthetic data in this section."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than SERA and a higher F1 score than Doc2Graph. What specific adaptation does the model proposed in the paper make to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having lower F1 score than SERA and higher F1 score than Doc2Graph?", "answer_anchor": "LayoutXLM", "question_reference": "What specific adaptation does LayoutXLM make to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "explanation_reference": "The adaptation mentioned addresses the challenge of language-specific pre-processing by obtaining character-level bounding boxes for each token after tokenization, which allows for a unified approach to handling multilingual multimodal inputs.", "evidence_reference": "However, for LayoutXLM, this strategy is not applicable because the definition of the linguistic unit is different from language to language. To prevent the language-specific pre-processing, we decide to obtain the character-level bounding boxes. After the tokenization using SentencePiece with a unigram language model, we calculate the bounding box of each token by merging the bounding boxes of all characters it contains."}
{"question": "Consider the paper that introduces the method that has a score of 3.1 in the Seen, Val, SR dataset. What specific feature of the ALFRED dataset's expert demonstrations makes re-planning during a DAgger-style student-forcing paradigm non-trivial for the model proposed in the paper, and can lead to the inability to complete a task?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows score of 3.1 in Seen, Val, SR dataset?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific feature of the ALFRED dataset's expert demonstrations makes re-planning during a DAgger-style student-forcing paradigm non-trivial, and can lead to the inability to complete a task?", "explanation_reference": "The feature of the ALFRED dataset's expert demonstrations that makes re-planning during a DAgger-style student-forcing paradigm non-trivial, and can lead to the inability to complete a task, is the presence of irreversible actions. This is because if an action taken by the student-forcing model during a task leads to an irreversible state change that deviates from the expert demonstration (e.g., slicing the only apple in the scene when the task is to place an intact apple in the refrigerator), the task cannot be completed as planned.", "evidence_reference": "Obtaining expert demonstration actions on the fly in navigation-only datasets like R2R only requires rerunning $A^*$. In ALFRED, on the fly demonstrations requires re-planning. In some cases re-planning is not possible: if during a task of {Clean & Place, apple, refrigerator, kitchen-3} a student-forcing model slices the only apple in the scene, the action cannot be recovered from and the task cannot be completed."}
{"question": "Consider the paper that introduces the method that demonstrates the lowest EA score on the FinQA task. What is the model's average accuracy improvement over the best baseline few-shot-CoT GPT-3 on the dataset?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method demonstrates the lowest EA score in FinQA task?", "answer_anchor": "PromptPG", "question_reference": "What is the average accuracy improvement of PromptPG over the best baseline few-shot-CoT GPT-3 on the \\data{} dataset?", "explanation_reference": "The average accuracy improvement of PromptPG over the best baseline few-shot-CoT GPT-3 is directly stated in the Experimental Results section, indicating the effectiveness of the PromptPG method in selecting in-context examples and constructing performing prompts for the test example.", "evidence_reference": "our proposed \\model learns to select performing examples with the help of policy gradient. \\model establishes a state-of-the-art performance on the \\data{} dataset: it surpasses the best baseline few-shot-CoT GPT-3 by 5.31\\% on average."}
{"question": "Consider the paper that introduces the method that has three empty entries in the table for the mathematical reasoning and commonsense reasoning tasks. What is the accuracy improvement for the MAWPS dataset when using an external calculator for the model proposed in the paper compared to its baseline accuracy?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has three empty entries in mathematical reasoning and commonsense reasoning tasks?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What is the accuracy improvement for the MAWPS dataset when using an external calculator for the CoT finetuned T5 XXL model compared to its baseline accuracy?", "explanation_reference": "The improvement can be calculated from the reported accuracies in the arithmetic reasoning section. The baseline T5 XXL model's accuracy with a calculator is not directly provided, but its baseline accuracy is given as 54.15%. The CoT finetuned T5 XXL model's accuracy with a calculator is reported as 88.22%. Therefore, the improvement is 88.22% - 54.15% = 34.07%.", "evidence_reference": "The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. \\textbf{MAWPS} & 54.15 & \\textbf{70.41} & 88.22 & 93.00 & 93.66"}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the penultimate row. What is the threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the figure in the penultimate row?", "answer_anchor": "MSG^2", "question_reference": "What is the threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph?", "explanation_reference": "The threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph is mentioned as part of the layer-wise precondition inference strategy to ensure that the subtask graph does not contain any cycles. This threshold is used to determine the ancestor-descendant relationships between subtasks based on their sequence of occurrence in the videos.", "evidence_reference": "We used \\(\\delta=0.96\\) for ProceL and \\(\\delta=0.55\\) for CrossTask in the experiment."}
{"question": "Consider the paper that introduces the method that exhibits an accuracy of 79.44% in the CJO22 task. What specific methodological weakness does it address in the context of learning attention vectors for semantically close law articles, as identified in the comparison with Luo et al.'s (2017) approach?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shows 79.44 accuracy in CJO22 task?", "answer_anchor": "LADAN", "question_reference": "What specific methodological weakness does the LADAN model address in the context of learning attention vectors for semantically close law articles, as identified in the comparison with Luo et al.'s (2017) approach?", "explanation_reference": "The question targets a specific methodological weakness that the LADAN model aims to overcome, which is the challenge of distinguishing confusing charges when similar attention vectors are learned for semantically close law articles. This issue is directly addressed in the comparison with the approach by Luo et al. (2017), where the paper critiques the independent learning of each law article's attention vector, leading to the ineffectiveness in distinguishing confusing charges.", "evidence_reference": "Nevertheless, the weakness is that they learn each law article's attention vector independently, and this may result in that similar attention vectors are learned for semantically close law articles; hence, it is ineffective in distinguishing confusing charges."}
{"question": "Consider the paper that introduces the model that has the highest accuracy in the COGS-all dataset. What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model demonstrates the highest accuracy in COGS-all dataset?", "answer_anchor": "T5", "question_reference": "What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "explanation_reference": "The span-corruption objective with an average span length of 3 slightly outperformed the i.i.d. objective on most non-translation benchmarks, as indicated by the statement 'we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks.'", "evidence_reference": "we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks."}
{"question": "Consider the paper that introduces the method that has a score of 3.1 in the Seen, Val, SR dataset. What specific challenge does the ALFRED benchmark introduce to the community that is highlighted as a significant gap in the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows score of 3.1 in Seen, Val, SR dataset?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific challenge does the ALFRED benchmark introduce to the community that is highlighted as a significant gap in existing models?", "explanation_reference": "The paper emphasizes that the long horizon of tasks in the ALFRED benchmark poses a significant challenge for existing models, which is a gap that needs to be addressed for improving performance on complex vision-and-language planning tasks.", "evidence_reference": "While this model is relatively competent at accomplishing some sub-goals (\\eg operating microwaves is similar across \\textbf{Heat \\& Place} tasks), the overall task success rates are poor. The long horizon of \\dataset{} tasks poses a significant challenge  with sub-problems including visual semantic navigation, object detection, referring expression grounding, and action grounding."}
{"question": "Consider the paper that introduces the model that corresponds to the brown bars in the figure. How does the temperature rescaling phenomenon observed in Reinforcement Learning from Human Feedback (RLHF) differ when responding to creative versus factual prompts?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model is demonstrated in the brown color?", "answer_anchor": "LLaMA-30B", "question_reference": "How does the temperature rescaling phenomenon observed in RLHF differ when responding to creative versus factual prompts?", "explanation_reference": "The temperature rescaling phenomenon observed in RLHF shows that for creative prompts, an increase in temperature continues to generate diversity across various RLHF iterations, as indicated by the Self-BLEU slope pattern comparable to that of the SFT model. However, for factual prompts, despite the rising temperature, the model learns to consistently provide the same response, as shown by the diminishing Self-BLEU slope over time. This indicates that RLHF dynamically adjusts the temperature based on the type of prompt, maintaining diversity for creative prompts while reducing it for factual ones.", "evidence_reference": "For instance, when it comes to prompts associated with creativity, such as ``Write a poem,'' an increase in temperature continues to generate diversity across our various RLHF iterations. This can be observed in the Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model. On the other hand, for prompts based on factual information, such as ``What is the capital of ?'' the Self-BLEU slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to consistently provide the same response to factual prompts."}
{"question": "Consider the paper that introduces the method that exhibits the highest Accuracy@0.5 value on the RefCOCOg task. What is the improvement in grounding F1 score from the baseline to the model proposed in the paper in the grounded captioning task on the Flickr30k Entities dataset?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method demonstrates the highest Accuary@0.5 on RefCOCOg task?", "answer_anchor": "UniTAB", "question_reference": "What is the improvement in grounding F1 score from the baseline to \\modelname~in the grounded captioning task on the Flickr30k Entities dataset?", "explanation_reference": "The improvement in grounding F1 score from the baseline to \\modelname~is calculated by subtracting the baseline F1 score from the \\modelname~F1 score. The baseline F1 score is given as 8.44 (from the Cyclical model), and the \\modelname~F1 score is 12.95. Therefore, the improvement is 12.95 - 8.44 = 4.51.", "evidence_reference": "Cyclical~\\cite{ma2019learning} & 26.8 & 22.4 & 61.1 & 16.8 & 8.44 & 22.78 \\\\ \\hline \\modelname & \\textbf{30.1} & \\textbf{23.7} & \\textbf{69.7} & \\textbf{17.4} & \\textbf{12.95} & \\textbf{34.79}"}
{"question": "Consider the paper that introduces the method that consistently achieves a higher MRR score than NodePiece. Based on the ablation studies, which component's removal from the model proposed in the paper resulted in the most dramatic performance decrease on the WN18RR dataset?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method consistently shows higher MRR than NodePiece?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which component's removal resulted in the most dramatic performance decrease on the WN18RR dataset?", "explanation_reference": "The ablation study results for the WN18RR dataset show that the removal of the MulHop component resulted in the most significant performance decrease. This indicates that multi-hop neighbor information is crucial for the model's performance on this dataset.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the model that achieves a BLEURT score of 0.4126. What is the primary limitation of using it for event-centric summary generation in educational question generation, as identified in the paper?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model gets 0.4126 in BLEURT score?", "answer_anchor": "EQG", "question_reference": "What is the primary limitation of using the BART model for event-centric summary generation in educational question generation, as identified in the paper?", "explanation_reference": "The paper identifies the primary limitation of using the BART model for event-centric summary generation as the factuality error problem, indicating that sometimes the system may generate non-factual facts in terms of the original context.", "evidence_reference": "Owing to the factuality error problem of our system, we suggest to further investigate constructing structured knowledge of fairy tales and knowledge-grounded question generation for real-world applications."}
{"question": "Consider the paper that introduces the method that has a lower Hits@1 score than ReasoningLM but a higher Hits@1 score than NSM across all fine-tuning samples. What specific pre-training task is designed for the model proposed in the paper to ensure their unification in parameter learning?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has lower Hit@1 score than ReasoningLM but higher Hit@1 score than NSM as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning?", "explanation_reference": "The specific pre-training task designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning is the question-relation matching task. This task is directly mentioned as a shared pre-training task for both models, indicating its role in unifying the parameter learning process by focusing on matching the semantics of questions with the relations in the knowledge graph.", "evidence_reference": "For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies."}
{"question": "Consider the paper that introduces the model that is placed below TransferNet but above UniKGQA in the table. What specific effect does the removal of the path ending strategy have on its Hits@1 retrieval performance on the WebQSP dataset according to the paper?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shown in the table is below TransferNet but above UniKGQA?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific effect does the removal of the path ending strategy have on the Hits@1 retrieval performance on the WebQSP dataset according to the paper?", "explanation_reference": "The removal of the path ending strategy (PE) results in a significant decrease in the Hits@1 retrieval performance on the WebQSP dataset, indicating the critical role of this strategy in the model's ability to accurately retrieve relevant subgraphs.", "evidence_reference": "Table~\\ref{tb:retrieverperformance} indicates that based on \\model, Hits@1 drops 4.3-15.0\\% when removing QU (\\smodel w/o QU) and Hits@1 drops 2.1-18.5\\% when changing PE to the fixed path length $T$ (\\smodel w/o PE), where the optimal $T$ is set to 3 on both WebQSP and CWQ."}
{"question": "Consider the paper that introduces the method that scores higher than 69.0 but lower than 70.0 in the Forgotten Realms category. How does the model proposed in the paper ensure the selection of high-quality synthetic data over poor-quality data during training?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method got a score high than 69.0 but lower than 70.0 in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model ensure the selection of high-quality synthetic data over poor-quality data during training?", "explanation_reference": "The MetaBLINK model utilizes a meta-learning mechanism to differentiate the quality of synthetic data by assigning weights to them. This process is guided by the performance impact of the synthetic data on a small set of high-quality seed data in the target domain. The mechanism ensures that synthetic data which positively influences the model's performance on the seed data is given higher weight, thereby prioritizing high-quality synthetic data during training.", "evidence_reference": "Through exact matching and mention rewriting, we have a large number of training instances in the few-shot domain. And in the section \\ref{entity-link-model}, we give a deep learning based entity linking framework, which can be used to train on the weakly supervised dataset. However, part of the instances are noisy and may deteriorate the entity linking model's performance in the target domain. Therefore, we need to find an effective way to select suitable instances for the training process. Previous studies have shown that meta-learning can be used to reweight the instances during the training process automatically and gained relative success in neural ranking~\\cite{DBLP:conf/icml/RenZYU18}. Therefore, we adopt a meta-learning method to reweight these synthetic data in this section."}
{"question": "Consider the paper that introduces the method that has an accuracy of 78.1% on the VQA-v2 task. What specific strategy does the model proposed in the paper employ during finetuning and inference to address the inefficiency and potential for generating invalid labels in classification tasks?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method shows 78.1 Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "What specific strategy does OFA employ during finetuning and inference to address the inefficiency and potential for generating invalid labels in classification tasks?", "explanation_reference": "The paper mentions the use of a Trie-based search strategy to overcome the inefficiencies and potential for generating invalid labels during classification tasks in the finetuning and inference stages. This strategy enhances performance by ensuring that the model's output is constrained within the set of valid labels, addressing the mentioned problems directly.", "evidence_reference": "For inference, we apply the decoding strategies, e.g., beam search, to enhance the quality of generation. However, this paradigm has several problems in classification tasks: 1. optimizing on the entire vocabulary is unnecessary and inefficient; 2. the model may generate invalid labels out of the closed label set during inference. To overcome these issues, we introduce a search strategy based on prefix tree (Trie, \\cite{trie})."}
{"question": "Consider the paper that introduces the model that achieves a score of 21.073 in 10-shot prompting. What specific performance improvement does the model proposed in the paper provide over static masking for the MNLI-m task according to the paper's findings?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates score of 21.073 in 10-shot prompting", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does dynamic masking provide over static masking for the MNLI-m task according to the paper's findings?", "explanation_reference": "The question focuses on extracting a specific detail regarding the performance improvement dynamic masking offers over static masking for the MNLI-m task. The answer is derived from comparing the performance metrics of static and dynamic masking specifically for the MNLI-m task, where dynamic masking shows a slight improvement.", "evidence_reference": "static & 78.3 & 84.3 & 92.5 \\\\ dynamic & 78.7 & 84.0 & 92.9 \\\\"}
{"question": "Consider the paper that introduces the model that has a macro-F1 score of 27.34. What specific improvement in F1 score was observed for the 'lease details' subset when comparing its variants to the tuned BERT base?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model having mac-F1 score of 27.34?", "answer_anchor": "LegalBERT", "question_reference": "What specific improvement in F1 score was observed for the 'lease details' subset when comparing LEGAL-BERT variants to the tuned BERT base?", "explanation_reference": "The question focuses on extracting a precise statistical improvement from the experimental results section, specifically targeting the performance comparison between LEGAL-BERT variants and the tuned BERT base model on a particular subset of a dataset. The answer directly reflects the observed improvement in F1 score for the 'lease details' subset, showcasing the effectiveness of LEGAL-BERT in this specific task.", "evidence_reference": "On the contrary, we observe a more substantial improvement in the more difficult multi-label task (2.5%) indicating that the \\legalbert variations benefit from in-domain knowledge. On \\contractsdata, the drop in perplexity is larger (5.6), which is reflected in the increase in $F1$ on the \\emph{contract header} (1.8\\%) and \\emph{dispute resolution} (1.6\\%) subsets. In the \\emph{lease details} subset, we also observe an improvement (1.1\\%)."}
{"question": "Consider the paper that introduces the supervised method that results in the lowest score in 10-shot prompting. Based on the critical analysis of its (the method proposed in the paper) paper, what specific aspect of the NSP (Next Sentence Prediction) loss's implementation in the original BERT model might have led to the observed discrepancy in performance when it was removed?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which supervised method demonstrates lowest scores in 10-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "Based on the critical analysis of the RoBERTa paper, what specific aspect of the NSP (Next Sentence Prediction) loss's implementation in the original BERT model might have led to the observed discrepancy in performance when it was removed?", "explanation_reference": "The question focuses on a detailed aspect of the paper's findings regarding the NSP loss's role in BERT's performance. It critically evaluates the logical coherence of the paper's argument and evidence regarding the NSP loss's impact. The answer directly addresses the question by pinpointing a specific implementation detail that could explain the discrepancy in performance observed when the NSP loss was removed.", "evidence_reference": "It is possible that the original BERT implementation may only have removed the loss term while still retaining the segment-pair input format."}
{"question": "Consider the paper that introduces the model that scores a 91.5 in the NER task. What specific aspect of the unified pre-training framework contributes to its effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR parsing and generation tasks?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets score of 91.5 in NER task?", "answer_anchor": "AMRBART", "question_reference": "What specific aspect of the unified pre-training framework contributes to its effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR parsing and generation tasks?", "explanation_reference": "The unified pre-training framework's effectiveness in reducing the gap between pre-training and fine-tuning phases is attributed to the implementation of a dynamic masking rate, which adjusts the masking probability as training progresses. This approach makes the pre-training tasks gradually closer to the fine-tuning tasks, facilitating a smoother transition and better knowledge transfer from pre-training to fine-tuning.", "evidence_reference": "Different from standard masking that uses a static masking rate, we adopt a dynamic masking rate $p$ for task $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g}. Formally, at step $t$, we calculate the masking probability $p$ as:  $p = 0.1 + 0.75 * t/T$, where $0.1$ is the initial masking rate, $T$ denotes the total training step. $p$ increases as $t$ grows, as $t$ approaches to $T$, the pre-training tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} are closer to fine-tuning tasks."}
{"question": "Consider the paper that examines the dataset which has a test set size of 1,106. What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset having 1,106 test set size?", "answer_anchor": "ViHOS", "question_reference": "What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included in the dataset?", "explanation_reference": "The improvement in F1-score for the PhoBERT_Large model when additional clean comments are included is directly stated in the Experiments and Results section, indicating the performance enhancement due to the inclusion of additional clean comments.", "evidence_reference": "PhoBERT$_{Large}$ considerably outperforms other models in the dataset without additional clean data, achieving 0.6867 in F1-score. In addition, the best model trained on Full data is XLM-R$_{Large}$, which has an F1-score of 0.7770. We find that XLM-R$_{Large}$ increased by 0.1014 and PhoBERT$_{Large}$ increased by 0.0849."}
{"question": "Consider the paper that introduces the model in the table that has 12M updated parameters. What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model in the table has 12M updated parameters?", "answer_anchor": "UniKGQA", "question_reference": "What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning?", "explanation_reference": "The specific pre-training task designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning is the question-relation matching task. This task is directly mentioned as a shared pre-training task for both models, indicating its role in unifying the parameter learning process by focusing on matching the semantics of questions with the relations in the knowledge graph.", "evidence_reference": "For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.8173 on the Stance dataset. In its methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets mean classification accuracy 0.8173 on Stance dataset?", "answer_anchor": "DPT", "question_reference": "In the methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "explanation_reference": "The modification ensures that the calculated value reflects performance deterioration in a consistent manner, increasing as performance worsens, irrespective of whether the misalignment is due to training data being from the past or future relative to the evaluation data.", "evidence_reference": "Let $S_{t' \\shortto t}$ indicate the performance a model trained on timestamp $t'$ data and evaluated on the timestamp $t$. Let $$ {D} (t' \\shortto{} t) = -\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t), $$  In other words, ${D} (t' \\shortto{} t)$ is a modified difference in performance between a aligned and misaligned models."}
{"question": "Consider the paper that introduces the method that corresponds to the fifth row of the table. What is the effect of varying the contrastive loss margin \\(\\rho\\) on the model's perplexity, and what is the optimal value of \\(\\rho\\) identified in the experiments?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the fifth row of the table?", "answer_anchor": "SimCTG", "question_reference": "What is the effect of varying the contrastive loss margin \\(\\rho\\) on model perplexity, and what is the optimal value of \\(\\rho\\) identified in the experiments?", "explanation_reference": "The paper analyzes the effect of varying the contrastive loss margin \\(\\rho\\) on model perplexity, finding that both too small and too large values of \\(\\rho\\) lead to sub-optimal perplexity. The optimal value of \\(\\rho\\) identified through experimentation is 0.5.", "evidence_reference": "From Figure \\ref{fig:margin_vs_ppl}, we see that the contrastive training always helps to improve the perplexity as compared with MLE. However, when \\(\\rho\\) is either too small (e.g., \\(0.1\\)) or large (e.g., \\(1.0\\)), the learned representation space of the model would be either less or too isotropic, leading to a sub-optimal perplexity. In our experiments, the most suitable margin \\(\\rho=0.5\\)."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Knowledge Editing category. What specific configuration arguments are used in CMR's proposed sampling algorithm to control the dynamics of the query stream?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2205.02014", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "CMR", "question_reference": "What specific configuration arguments are used in the proposed sampling algorithm to control the dynamics of the query stream?", "explanation_reference": "The specific configuration arguments mentioned in the paper for controlling the dynamics of the query stream are \\u03b1 (alpha), \\u03b2 (beta), and \\u03b3 (gamma). These parameters are crucial for adjusting the decaying factor of in-distribution data, the transition probability of the Markov chain for the major OOD cluster, and the diversity by adding data from remaining OOD clusters, respectively.", "evidence_reference": "As shown in \\\\textbf{Alg.~\\\\ref{alg:cns}}, we have three key configuration arguments $(\\\\alpha, \\\\beta, \\\\gamma)$ for controlling the dynamics of the query stream: 1) $\\\\alpha$ is the \\\\textbf{decaying factor} for the proportion of in-distribution data, 2) $\\\\beta$ is the \\\\textbf{transition probability} of the Markov chain for deciding the index of the major OOD cluster $c_t$, and 3) $\\\\gamma$ is to control the \\\\textbf{diversity} by adding data from remaining OOD clusters; $T$ is the number of episodes and $b$ is size of $Q_t$."}
{"question": "Consider the paper that introduces the benchmark represented by the triangle marker in the figure. What is the primary reason for the underestimation of language model capabilities in the corresponding BIG-Bench paper according to the findings related to answer-only prompting?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which benchmark is represented using the triangle marker from the figure?", "answer_anchor": "BBH", "question_reference": "What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to answer-only prompting?", "explanation_reference": "The paper suggests that the few-shot evaluation of PaLM 540B with answer-only prompting, which includes both a task instruction and answer options, demonstrates the effect of including these elements in the prompt. This approach outperforms the average human-rater on 6 out of 23 BBH tasks and is overall 1.4% better than the BIG-Bench reported result, indicating that the original setup underestimated language model performance.", "evidence_reference": "The few-shot evaluation of PaLM 540B  with answer-only prompting in this paper, however, outperforms the average human-rater on 6 out of 23 \\bbh{} tasks and is overall 1.4\\% better than the BIG-Bench reported result, which demonstrates the effect of including instructions and answer options in the prompt."}
{"question": "Consider the paper that introduces the method shown in the fourth row of the table. How does the inclusion of agent ID in the agent-specific global state influence the performance of the model proposed in the paper, MAPPO, in the SMAC domain, particularly in maps where agents may assume different roles?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method showing in the fourth row of the table?", "answer_anchor": "MAPPO", "question_reference": "How does the inclusion of agent ID in the agent-specific global state influence MAPPO's performance in the SMAC domain, particularly in maps where agents may assume different roles?", "explanation_reference": "The inclusion of agent ID in the agent-specific global state allows for an agent-specific value function depending on an agent's type or role, which is particularly helpful when the environment contains heterogeneous agents. This is supported by the superior performance of the variant with agent ID compared to the variant without it in the 3s5z vs. 3s6z map, demonstrating the importance of agent-specific features in forming an effective global state.", "evidence_reference": "Including the agent id in the death mask, as is done in variant (1), is particularly important in maps which agents may take on different roles, as demonstrated by the superior performance of variant (1) compared to variant (4), which does not contain the agent ID in the death-mask zero-state, in the 3s5z vs. 3s6z map."}
{"question": "Consider the paper that introduces the method that has a lower J_k score than Random Entity Quantization but a higher J_k score than NodePiece in the FB15k-237 dataset for all values of k between 400 and 1000. Which component's removal from the model proposed in the paper resulted in the most dramatic performance decrease on the WN18RR dataset?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method shows the lower J_k score than Random Entity Quantization but higher J_k score than NodePiece for k in the range of [400, 1000] in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which component's removal resulted in the most dramatic performance decrease on the WN18RR dataset?", "explanation_reference": "The ablation study results for the WN18RR dataset show that the removal of the MulHop component resulted in the most significant performance decrease. This indicates that multi-hop neighbor information is crucial for the model's performance on this dataset.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the method that has a score of 61.2 in the BoolQ dataset with 32-shot prompting. What is the specific performance improvement (in percentage points) observed on the SuperGLUE benchmark when using the T5 XXL model compared to the baseline Prompt Tuning, as proposed by the paper?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having score of 61.2 in BoolQ dataset with 32-shot prompting?", "answer_anchor": "SPoT", "question_reference": "Based on the SPoT approach, what is the specific performance improvement (in percentage points) observed on the SuperGLUE benchmark when using the T5 XXL model compared to the baseline Prompt Tuning?", "explanation_reference": "The specific performance improvement observed on the SuperGLUE benchmark when using the T5 XXL model with the SPoT approach, compared to the baseline Prompt Tuning, is mentioned as a +2.4 point average accuracy improvement. This detail directly answers the question by specifying the improvement in performance due to the application of SPoT.", "evidence_reference": "For instance, on the SuperGLUE benchmark, we obtain +10.1 and +2.4 point average accuracy improvements using the T5 Base (220M parameter) and T5 XXL (11B parameter) models, respectively."}
{"question": "Consider the paper that introduces the method that has a GPT backbone and 7B parameters. In the context of distilling reasoning capabilities into smaller models, how does the performance of a GPT-2 Large model fine-tuned with ground truth step-by-step annotation compare to one fine-tuned with LLM-generated Chain of Thought (CoT) on the GSM8K dataset, specifically when using DecomDistill?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method used with a GPT backbone and 7B parameters?", "answer_anchor": "DecomDistill", "question_reference": "In the context of distilling reasoning capabilities into smaller models, how does the performance of a GPT-2 Large model fine-tuned with ground truth step-by-step annotation compare to one fine-tuned with LLM-generated Chain of Thought (CoT) on the GSM8K dataset?", "explanation_reference": "The comparison between the performance of models fine-tuned with different types of annotations indicates the effectiveness of the distillation approach. The specific performance metrics for the GPT-2 Large model, when fine-tuned with ground truth step-by-step annotation and LLM-generated CoT, directly answer the question by showing the relative effectiveness of these training strategies on the GSM8K dataset.", "evidence_reference": "GSM8K & Large (774M) &  4.62 & 14.10 & -& 12.85 & 17.89 & \\bf 21.08 ($\\uparrow 33\\%$) & 13.25"}
{"question": "Consider the paper that introduces the method in the table that corresponds to the highest ROUGE 2 score. How does the application of normalizing flow in the neural topic model, as proposed by the paper, specifically contribute to the improvement of abstractive text summarization performance?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method in the table that demonstrates the highest ROUGE 2 score?", "answer_anchor": "PEGASUS+NTM", "question_reference": "How does the application of normalizing flow in the neural topic model specifically contribute to the improvement of abstractive text summarization performance?", "explanation_reference": "The application of normalizing flow in the neural topic model contributes to the improvement of abstractive text summarization performance by enabling a more accurate approximation of the true distribution of global semantics. This enhanced approximation allows the summarization model to better understand the overall document, leading to summaries that are more informative and capture key points more effectively.", "evidence_reference": "To this end, we propose a method to adapt normalizing flow in the neural topic model to have a better approximation of true distribution and integrate it into the summarization model. Integrating flow mechanism to better approximate the true posterior has been proven to improve performance for variational inference \\cite{rezende2015variational} as well as for downstream tasks such as image synthesis \\cite{kingma2016improved}, etc."}
{"question": "Consider the paper that introduces the model that achieves a score of 3.84 in the Grounding task. What specific adaptation in the text embeddings allows MVQG-VL-T5 to build correspondence among query text, label text, and objects in grounding tasks?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the base model tested in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific adaptation in the text embeddings allows the model to build correspondence among query text, label text, and objects in grounding tasks?", "explanation_reference": "The specific adaptation that allows the model to build correspondence among query text, label text, and objects in grounding tasks is the use of embedding sharing. This is achieved by reusing the text embeddings of visual sentinel tokens as region id embeddings, which enables the model to establish a connection between the text and visual elements, particularly useful in grounding tasks.", "evidence_reference": "In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens \\{\\texttt{<vis\\_1>}, $\\dots$, \\texttt{<vis\\_n>}\\}, which corresponds to image regions. As illustrated in Fig.~\\ref{fig:architecture}, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec.~\\ref{sec:visual_embeddings}. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec.~\\ref{sec:pretraining}, referring expression comprehension in Sec.~\\ref{sec:refcoco})."}
{"question": "Consider the paper that introduces the model with the largest number of updated parameters. What specific performance improvement does it achieve on the CWQ dataset in terms of Hits@1 and F1 scores compared to the original NSM model?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the most number of updated parameters?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific performance improvement does the \\model+NSM model achieve on the CWQ dataset in terms of Hits@1 and F1 scores compared to the original NSM model?", "explanation_reference": "The question asks for the specific performance improvement of the \\model+NSM model over the original NSM model on the CWQ dataset, focusing on the Hits@1 and F1 metrics. The answer directly provides these specific improvements, indicating the effectiveness of the \\model when combined with NSM.", "evidence_reference": "NSM injected by \\smodel (\\model+NSM) improves 0.4% Hits@1 and 1.3% F1 on WebQSP, 3.9% Hits@1 and 4.7% F1 on CWQ compared with the original NSM."}
{"question": "Consider the paper that introduces the method that results in a score of 14.9 in the Unseen, Test, GC dataset. What specific performance improvement does the model proposed in the paper provide on the ALFRED benchmark's unseen test split?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the score of 14.9 in Unseen, Test, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific performance improvement does pretraining and joint training with synthetic instructions provide on the ALFRED benchmark's unseen test split?", "explanation_reference": "The question focuses on the specific detail of how pretraining and joint training with synthetic instructions impact the performance on the ALFRED benchmark, particularly on the unseen test split. The answer directly reflects the improvement mentioned in the paper, emphasizing the effectiveness of the proposed methods in handling environments not encountered during training.", "evidence_reference": "Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits."}
{"question": "Consider the paper that introduces the model that corresponds to an F1 score of 65.76 on PDTB-Top. What is the primary intuition behind the Multi-Head Interactive Attention (MHIA) module proposed in the GOLF framework, for IDRR?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method demonstrates 65.76 F1 score on PDTB-Top?", "answer_anchor": "GOLF", "question_reference": "What is the primary intuition behind the Multi-Head Interactive Attention (MHIA) module proposed in the GOLF framework for IDRR?", "explanation_reference": "The MHIA module is designed to facilitate bilateral multi-perspective matching between two arguments by taking one argument as Query and the other as Key and Value, and vice versa, aiming to mimic the human cognitive process of considering perspectives from both sides of the discourse.", "evidence_reference": "To this end, we propose a Multi-Head Interactive Attention (MHIA) module to facilitate bilateral multi-perspective matching between $arg_1$ and $arg_2$. The intuition behind MHIA is to simulate human\u2019s transposition thinking process: respectively considering each other\u2019s focus from the standpoint of $arg_1$ and $arg_2$."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. What is the Dev F1 score for the feature-based approach using the model proposed in the paper, BERTbase with embeddings only, on the CoNLL-2003 NER task?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model on the first row of the table?", "answer_anchor": "BERT", "question_reference": "What is the Dev F1 score for the feature-based approach using BERTbase with embeddings only on the CoNLL-2003 NER task?", "explanation_reference": "The Dev F1 score directly measures the performance of the feature-based approach using BERTbase with only embeddings on the CoNLL-2003 Named Entity Recognition task, indicating how well the model performed without fine-tuning or additional context.", "evidence_reference": "Feature-based approach (\\bertbase) &  &  \\\\ \\;\\;\\;Embeddings & 91.0 &- \\\\"}
{"question": "Consider the paper that introduces the model that has a Recall@7 score of 92.97 for the MWOZ task. What specific token is used to represent a null query in cases where no query is required, such as greetings or thanks, within the system proposed by the paper?", "answer": "", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model has the 92.97 score in Recall@7 for MWOZ task?", "answer_anchor": "Q-TOD", "question_reference": "What specific token is used to represent a null query in cases where no query is required, such as greetings or thanks, within the Q-TOD system?", "explanation_reference": "The specific token used to represent a null query in the Q-TOD system, for instances where the dialogue does not require a query (e.g., greetings or thanks), is explicitly mentioned in the paper.", "evidence_reference": "In cases where no query is required, e.g. greetings or thanks, a special token \\texttt{[NOTHING]} is used to represent the null query at this turn."}
{"question": "Consider the paper that introduces the method in the figure that has a perplexity of approximately 30 and an average max toxicity of 0.2. What specific method does the model proposed in the paper utilize to ensure that only tokens with high probability are generated, based on their likelihood as determined by both experts and anti-experts?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is method in the figure has around 30 perplexity and 0.2 average max toxicity?", "answer_anchor": "DExperts", "question_reference": "What specific method does DExperts utilize to ensure that only tokens with high probability are generated, based on their likelihood as determined by both experts and anti-experts?", "explanation_reference": "The method described for controlling the attributes of generated text involves combining pretrained language models with expert and anti-expert LMs in a product of experts. This ensures that tokens only receive high probability if they are deemed likely by the experts and unlikely by the anti-experts, thus directly answering the question.", "evidence_reference": "Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts."}
{"question": "Consider the paper that introduces the method that has the lowest score in the 'Original Persona' column. What specific hyperparameter controls the extent of document content selection in the model proposed by the paper, and how does its optimal value range affect the model's performance on the original PersonaChat dataset?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the lowest score on 'Original Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What specific hyperparameter controls the extent of document content selection in the Content Selection Network, and how does its optimal value range affect the model's performance on the original PersonaChat dataset?", "explanation_reference": "The question focuses on a detailed aspect of the model's methodology, specifically the role and optimal setting of the hyperparameter $\\gamma$ in document content selection. It requires understanding of how the model filters document content based on relevance to the conversation context and how the tuning of $\\gamma$ affects model performance.", "evidence_reference": "The hyperparameter $\\gamma$ in Equation (\\ref{eq:up1}) and (\\ref{eq:up2}) controls how much the document content is selected. ... The best setting of $\\gamma$ is around 0.3 for both CSN-sent and CSN-word, which retains an appropriate amount of relevant document content for response matching."}
{"question": "Consider the paper that introduces the dataset in the table that has an average answer length of 83.71. What specific strategy does the model proposed in the paper employ to enhance its performance on machine translation tasks during curriculum learning?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method demonstrates 83.71 average answer length from the table?", "answer_anchor": "Multialpaca", "question_reference": "What specific strategy does the model employ to enhance its performance on machine translation tasks during curriculum learning?", "explanation_reference": "The strategy of introducing more multilingual parallel data during curriculum learning is mentioned as significantly boosting the model's performance on translation tasks. This approach likely helps in better aligning the representations across different languages, thereby improving the model's ability to translate between them.", "evidence_reference": "Finally, it is worth noting that introducing more multilingual parallel data during the curriculum learning significantly boost the model performance on translation task."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. How does its performance on the AQuA task compare to beam search when utilizing the UL2-20B model with 40 reasoning paths?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "How does self-consistency compare to beam search in terms of performance on the AQuA task when using the UL2-20B model with 40 reasoning paths?", "explanation_reference": "The comparison between self-consistency and beam search on the AQuA task using the UL2-20B model with 40 reasoning paths shows that self-consistency using sampling achieves higher accuracy than beam search, indicating that self-consistency's approach to generating diverse reasoning paths and aggregating the most consistent answer leads to better performance.", "evidence_reference": "Self-consistency using sampling & 19.7 \\scriptsize{$\\pm$ 2.5} & \\textbf{24.9 \\scriptsize{$\\pm$ 2.6}} & \\textbf{25.3 \\scriptsize{$\\pm$ 1.8}} & \\textbf{26.7 \\scriptsize{$\\pm$ 1.0}} & \\textbf{26.9 \\scriptsize{$\\pm$ 0.5}} \\\\ \\midrule \\multirow{3}{*}{AQuA} & Beam search decoding (top beam) & 23.6 & 19.3 & 16.1 & 15.0 &10.2"}
{"question": "Consider the paper that introduces the model which is shown in the first row of the table. What specific feature of the visual embeddings allows the model, MVQG-VL-T5, to discriminate regions from different images when multiple images are given to it?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific feature of the visual embeddings allows the model to discriminate regions from different images when multiple images are given to it?", "explanation_reference": "The feature that allows the model to discriminate regions from different images when multiple images are given to it is the use of image ids. This is explicitly mentioned as a function of the image ids encoded within the visual embeddings.", "evidence_reference": "Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in \\NLVR{}~\\cite{Suhr2019}, models take two input images)."}
{"question": "Consider the paper that introduces the method that has an F1 score of 65.96. How does the performance improvement achieved through multi-task learning with entity labeling compare to the performance improvement achieved through data augmentation in the FUNSD dataset?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having 65.96 F1 score?", "answer_anchor": "SERA", "question_reference": "How does the performance improvement achieved through multi-task learning with entity labeling compare to the performance improvement achieved through data augmentation in the FUNSD dataset?", "explanation_reference": "The performance improvement achieved through multi-task learning with entity labeling is 0.86% F1, while the performance improvement achieved through data augmentation is about 1.1% F1. This comparison is derived from the discussion on training strategies where the effectiveness of multi-task learning and data augmentation on the FUNSD dataset is evaluated.", "evidence_reference": "Relation extraction task can improve by about 0.86% F1 while labeling model performance drops a little from Table~\\ref{trainstrategy}. Models trained on the training data after augmentation improve performance by about 1.1% F1 with auto label and 2.9% F1 with gold label."}
{"question": "Consider the paper that introduces the last method shown in the Implicit --> Continual Learning --> Continual Knowledge Editing category. What is the default value of the deferral radius $\\epsilon$ when a new codebook entry is created in this method?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2211.11031", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "GRACE", "question_reference": "What is the default value of the deferral radius $\\epsilon$ when a new codebook entry is created in GRACE?", "explanation_reference": "The default value of the deferral radius $\\epsilon$ when a new codebook entry is created in GRACE is referred to as $\\epsilon_\\text{init}$, which is a hyperparameter.", "evidence_reference": "New entries have a default value $\\epsilon_\\text{init}$, which is a hyperparameter."}
{"question": "Consider the paper that introduces the model that demonstrates the lowest zh-en score. What is the primary reason for using accuracy as the evaluation metric in its task, according to the paper?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model demonstrates the lowest zh-en score?", "answer_anchor": "GWLAN", "question_reference": "What is the primary reason for using accuracy as the evaluation metric in the GWLAN task, according to the paper?", "explanation_reference": "The paper suggests that higher prediction accuracy in the GWLAN task is directly correlated with the reduction of keystrokes required from human translators. This reduction in keystrokes leads to time savings for translators, making them more productive. Therefore, accuracy is used as the evaluation metric to reflect the effectiveness of the autocompletion in assisting human translators.", "evidence_reference": "Experiments show that the more keystrokes are reduced, the more time can be saved for translators. Since the prediction accuracy is highly correlated with the keystrokes, we think higher accuracy will make translators more productive."}
{"question": "Consider the paper that introduces the method shown in the table that achieves a PPL score of 24.466 for the Test Seen task. What is the effect of varying the contrastive loss margin \\(\\rho\\) on the model proposed in the paper's perplexity, and what is the optimal value of \\(\\rho\\) identified in the experiments?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the table gets 24.466 PPL for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the effect of varying the contrastive loss margin \\(\\rho\\) on model perplexity, and what is the optimal value of \\(\\rho\\) identified in the experiments?", "explanation_reference": "The paper analyzes the effect of varying the contrastive loss margin \\(\\rho\\) on model perplexity, finding that both too small and too large values of \\(\\rho\\) lead to sub-optimal perplexity. The optimal value of \\(\\rho\\) identified through experimentation is 0.5.", "evidence_reference": "From Figure \\ref{fig:margin_vs_ppl}, we see that the contrastive training always helps to improve the perplexity as compared with MLE. However, when \\(\\rho\\) is either too small (e.g., \\(0.1\\)) or large (e.g., \\(1.0\\)), the learned representation space of the model would be either less or too isotropic, leading to a sub-optimal perplexity. In our experiments, the most suitable margin \\(\\rho=0.5\\)."}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What is the chosen temperature \\(\\alpha\\) value for positive sentiment control in the model proposed by the paper?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "Discup", "question_reference": "What is the chosen temperature \\(\\alpha\\) value for positive sentiment control in DisCup?", "explanation_reference": "The chosen temperature \\(\\alpha\\) value for positive sentiment control in DisCup is specified in the experimental details section, indicating the parameter tuning for achieving optimal performance.", "evidence_reference": "For our approach, we search the temperature \\(\\alpha\\) over the value \\(\\{0.1, 0.01, 0.005, 0.001\\}\\), and finally chose \\(\\alpha = 0.005\\) for positive sentiment control,  \\(\\alpha = 0.01\\) for negative sentiment and detoxication."}
{"question": "Consider the paper that introduces the method that has an average score of 82.8 with zero-shot prompting. How does the model's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "answer": "", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method has average score of 82.8 with zero-shot prompting?", "answer_anchor": "SALAM", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "explanation_reference": "Traditional methods for the RefCOCOg task typically involve classification over a set of visual regions, requiring task-specific architectures and objectives. In contrast, the unified framework proposed in the paper treats RefCOCOg as a text generation task, leveraging the same language modeling architecture and objective used for other vision-and-language tasks. This approach allows for more flexible architecture design and eliminates the need for task-specific modifications.", "evidence_reference": "While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously~\\cite{Yu2018,Chen2020} formulated classification task over a set of visual regions, allowing more flexible architecture design."}
{"question": "Consider the paper that introduces the model that corresponds to the lowest BERTScore F1 score on the TellMeWhy dataset. What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model shows the lowest BERTScore F1 score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach introduced in the paper, which is a combination of predicting the distribution of question types and generating summaries focused on salient events to facilitate the generation of educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the dataset which has the largest number of Queries|Aspects in the ABS category. What is the precision achieved for aspect discovery in the Software domain with a threshold \\(\\lambda\\) set to 0.9?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset with the most number of Queries|Aspects in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What is the precision achieved for aspect discovery in the Software domain with a threshold \\(\\lambda\\) set to 0.9?", "explanation_reference": "The precision value directly answers the question by indicating the performance of aspect discovery in the Software domain with the specified threshold.", "evidence_reference": "With the threshold \\(\\lambda\\) set to 0.9, we achieved the precision of 45.1, which shows that the aspect discovery has the ability to extract aspects, but not as good at extracting \\textit{relevant} aspects for the article."}
{"question": "Consider the paper that introduces the dataset in the table that has an average answer length of 83.71. How does the curriculum learning strategy specifically address the challenge of optimizing LLMs to learn knowledge encoded in multiple languages simultaneously, as proposed by the model in the paper?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method demonstrates 83.71 average answer length from the table?", "answer_anchor": "Multialpaca", "question_reference": "How does the curriculum learning strategy specifically address the challenge of optimizing LLMs to learn knowledge encoded in multiple languages simultaneously?", "explanation_reference": "The curriculum learning strategy is designed to transfer general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. This is achieved by initially using the whole pre-training dataset to train a base model for commonsense generalization ability, and then transitioning to a subset of the pre-training dataset that boasts superior quality and a greater proportion of multilingual content to strengthen the model's multilingual capabilities.", "evidence_reference": "Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is a significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. To address this issue, we adopt a curriculum learning strategy that ramps up the ratio of high-quality and low-resource languages during training."}
{"question": "Consider the paper that introduces the model that achieves the second highest score in the Stance column. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shows consistently low accuracy than VIBE model?", "answer_anchor": "DPT", "question_reference": "Based on the findings, what is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data indicates the strength of the linear relationship between the vocabulary overlap across time periods and the performance of the model on this specific task. A value close to 1 suggests a strong positive correlation.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that scores a 69.56 in the Forgotten Realms category. How does the model proposed in the paper ensure the selection of high-quality synthetic data over poor-quality data during training?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method got 69.56 score in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model ensure the selection of high-quality synthetic data over poor-quality data during training?", "explanation_reference": "The MetaBLINK model utilizes a meta-learning mechanism to differentiate the quality of synthetic data by assigning weights to them. This process is guided by the performance impact of the synthetic data on a small set of high-quality seed data in the target domain. The mechanism ensures that synthetic data which positively influences the model's performance on the seed data is given higher weight, thereby prioritizing high-quality synthetic data during training.", "evidence_reference": "Through exact matching and mention rewriting, we have a large number of training instances in the few-shot domain. And in the section \\ref{entity-link-model}, we give a deep learning based entity linking framework, which can be used to train on the weakly supervised dataset. However, part of the instances are noisy and may deteriorate the entity linking model's performance in the target domain. Therefore, we need to find an effective way to select suitable instances for the training process. Previous studies have shown that meta-learning can be used to reweight the instances during the training process automatically and gained relative success in neural ranking~\\cite{DBLP:conf/icml/RenZYU18}. Therefore, we adopt a meta-learning method to reweight these synthetic data in this section."}
{"question": "Consider the paper that introduces the method that has three empty entries in the table for the mathematical reasoning and commonsense reasoning tasks. Based on the ablation study findings, what is the accuracy of the model proposed in the paper when finetuned on 20% of the GSM8K dataset data and using an external calculator?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has three empty entries in mathematical reasoning and commonsense reasoning tasks?", "answer_anchor": "CoT Fine-tuned", "question_reference": "Based on the ablation study findings, what is the accuracy of T5 XXL finetuned on 20% of the GSM8K dataset data when using an external calculator?", "explanation_reference": "The ablation study on dataset size reveals that finetuning T5 XXL on only 20% of the GSM8K dataset data results in an accuracy of 20.47% when an external calculator is used. This detail directly answers the question by providing the specific performance metric for a subset of the data under specified conditions.", "evidence_reference": "20\\% (1067 examples) & 11.22 & 20.47"}
{"question": "Consider the paper that introduces the method that has a lower Hits@1 score than ReasoningLM but a higher Hits@1 score than NSM across all fine-tuning samples. What is the unique aspect of the model's architecture proposed in the paper that differentiates it from previous works in multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has lower Hit@1 score than ReasoningLM but higher Hit@1 score than NSM as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What is the unique aspect of the UniKGQA's model architecture that differentiates it from previous works in multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the unique aspect of UniKGQA's model architecture by highlighting the combination of a semantic matching module and a matching information propagation module, which is a distinctive approach compared to previous works in the field of multi-hop KGQA tasks. This combination allows for effective semantic matching between questions and relations and the propagation of this matching information along the directed edges on KGs, which is crucial for unifying retrieval and reasoning in the context of multi-hop KGQA.", "evidence_reference": "For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the model shown on the penultimate line of the table. What specific finetuning strategy did the authors of the MolXPT model find to yield slightly better results for MoleculeNet tasks, and how is it characterized?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shown on the penult line?", "answer_anchor": "MolXPT", "question_reference": "What specific finetuning strategy did the authors find to yield slightly better results for MoleculeNet tasks, and how is it characterized?", "explanation_reference": "The authors explored two finetuning strategies for MoleculeNet tasks: finetuning the full prompts and finetuning the tags only. They found that finetuning the tags only yielded slightly better results, characterized by focusing the finetuning process on the classification tags at the end of the prompts rather than the entire prompt sequence.", "evidence_reference": "According to our exploration, Eqn.(\\ref{eq:finetune_label_only}) achieves slightly better results and we use it for all tasks (see Appendix \\ref{sec:moleculenet_detailed_result} for the results)."}
{"question": "Consider the paper that introduces the method that achieves a Hits@1 score of 0.281 in the YAGO43kET dataset. What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the model proposed in the paper?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets Hits@1 score equal to 0.281 in YAGO43kET datast?", "answer_anchor": "RGCN", "question_reference": "What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the CET model?", "explanation_reference": "The relevance score indicates how strongly a neighbor's attribute (in this case, having won a Pulitzer Prize) correlates with a candidate type (Pulitzer Prize winners) for a given entity (Bob Dylan). The score of 6.93 suggests a strong correlation, implying that this neighbor is a significant indicator that Bob Dylan should be typed as a Pulitzer Prize winner according to the CET model.", "evidence_reference": "(has won prize, Pulitzer Prize) & 6.93"}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What specific hyperparameter controls the extent of document content selection in the CSN-word model, and how does its optimal value range affect the model's performance on the original PersonaChat dataset?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "CSN-word", "question_reference": "What specific hyperparameter controls the extent of document content selection in the Content Selection Network, and how does its optimal value range affect the model's performance on the original PersonaChat dataset?", "explanation_reference": "The question focuses on a detailed aspect of the model's methodology, specifically the role and optimal setting of the hyperparameter $\\gamma$ in document content selection. It requires understanding of how the model filters document content based on relevance to the conversation context and how the tuning of $\\gamma$ affects model performance.", "evidence_reference": "The hyperparameter $\\gamma$ in Equation (\\ref{eq:up1}) and (\\ref{eq:up2}) controls how much the document content is selected. ... The best setting of $\\gamma$ is around 0.3 for both CSN-sent and CSN-word, which retains an appropriate amount of relevant document content for response matching."}
{"question": "Consider the paper that introduces the method that achieves the highest Hits@1 score in the MQA-1H dataset. What is the core innovation of the model proposed in the paper in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets the highest Hits@1 score in MQA-1H dataset?", "answer_anchor": "UniKGQA", "question_reference": "What is the core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the question by summarizing the unique approach of UniKGQA, which integrates the retrieval and reasoning processes into a single framework. This is innovative because it contrasts with previous methods that treated these stages separately, thus enhancing the efficiency and effectiveness of solving multi-hop KGQA tasks.", "evidence_reference": "In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning."}
{"question": "Consider the paper that introduces the LLM shown in the figure with a model size of 1.7T. What specific performance improvement does this large language model exhibit over its predecessor in the context of the Uniform Bar Exam?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the large language model that has 1.7T model size?", "answer_anchor": "GPT-4", "question_reference": "What specific performance improvement does GPT-4 exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "explanation_reference": "The question assesses understanding of GPT-4's significant improvement in performance on a professional benchmark, the Uniform Bar Exam, compared to its predecessor. This detail highlights GPT-4's advanced capabilities in understanding and generating natural language in complex scenarios.", "evidence_reference": "For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers. This contrasts with GPT-3.5, which scores in the bottom 10%."}
{"question": "Consider the paper that introduces the model that has a de-en score of 53.87. What is the primary reason according to the authors for using hard constraints over soft constraints in the human input autocompletion process?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model has 53.87 de-en score?", "answer_anchor": "GWLAN", "question_reference": "What is the primary reason for using hard constraints over soft constraints in the human input autocompletion process according to the authors?", "explanation_reference": "The authors decided to use human inputs as hard constraints in their experiments because this method was found to be efficient and simple. Despite the comparable performance of soft and hard constraint methods in preliminary experiments, the efficiency and simplicity of using hard constraints made it the preferred choice.", "evidence_reference": "Therefore, we propose to use the human inputs as hard constraints in our later experiments, because of the method's efficiency and simplicity."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. How does the performance of the model proposed in the paper, using ground-truth concepts for captioning, compare to using predicted concepts or a mixture of both during training?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method is shown in the penult row?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP using ground-truth concepts for captioning compare to using predicted concepts or a mixture of both during training?", "explanation_reference": "The experiment demonstrates that training with predicted concepts yields better performance than using ground-truth concepts or a mixture of both, indicating the effectiveness of the CTN in generating useful concepts for captioning.", "evidence_reference": "We experiment with different ways to train with the concept tokens. In Table~\\ref{tab:concept}, we list the results of training using GT semantic concepts encoded as tokens, GT concepts mixed with predicted concepts, and fully predicted concepts. We find that by using the predicted concepts for training leads to optimal results."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. What specific performance improvement does the model proposed in the paper achieve on the GSM8K task when used with PaLM-540B, compared to chain-of-thought prompting?", "answer": "", "figure": "locality/2310.09619/Math23k_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "What specific performance improvement does self-consistency achieve on the GSM8K task when used with PaLM-540B, compared to chain-of-thought prompting?", "explanation_reference": "The question targets a detailed comparison of performance improvements achieved by the self-consistency method over the chain-of-thought prompting method on a specific arithmetic reasoning task (GSM8K) using a specific language model (PaLM-540B). This detail requires understanding of the empirical results presented in the paper, specifically focusing on the exact numerical improvement in accuracy.", "evidence_reference": "Self-consistency & 93.7 {\\scriptsize(+1.8)} & 99.3 {\\scriptsize(+4.6)} & 81.9 {\\scriptsize(+7.9)} & 48.3 {\\scriptsize(+12.5)} & 86.6 {\\scriptsize(+7.6)} & 74.4 {\\scriptsize(+17.9)}"}
{"question": "Consider the paper that introduces the model represented by the lavender color in the figure. How does its performance on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model is demonstrated in the lavender color?", "answer_anchor": "StarCoder", "question_reference": "How does StarCoderBase's performance on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "explanation_reference": "The comparison between completion and insertion formats for StarCoderBase on the Asleep at the Keyboard security benchmark shows that the insertion format leads to a higher percentage of valid code generation and a slightly lower percentage of insecure code generation. This indicates that the insertion format may be more effective for generating secure and valid code.", "evidence_reference": "Completion & StarCoderBase         & 855/1000 (85.50\\%) & 340/855 (39.77\\%) \\\\ Insertion  & StarCoderBase         & 987/1000 (98.70\\%) & 354/987 (35.87\\%)"}
{"question": "Consider the paper that introduces the dataset represented by the leftmost bar in the figure, GSM8K. What specific architectural feature allows the model proposed in the paper to simultaneously perform language modeling and correctness prediction tasks?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset represented on the leftmost of the figure?", "answer_anchor": "GSM8K", "question_reference": "What specific architectural feature allows the verifier models to simultaneously perform language modeling and correctness prediction tasks?", "explanation_reference": "The verifier models are designed with a unique architectural feature that enables them to handle both language modeling and correctness prediction tasks. This feature is a \u2248 that operates on the logits outputted by the language model's final unembedding layer, specifically shifting and scaling the logit corresponding to a special token in the vocabulary reserved for the verifier\u2019s predictions. This design allows the rest of the tokens to continue representing the language modeling objective, while the special token is used for correctness predictions.", "evidence_reference": "We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model\u2019s final unembedding layer. Specifically, the bias and gain shift and scale the logit corresponding to a special token in the vocabulary."}
{"question": "Consider the paper that introduces the method represented by the green line in the figure. Which component's removal, according to the ablation studies, resulted in the most dramatic performance decrease on the WN18RR dataset?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method represented in the green line from the figure?", "answer_anchor": "EARL Quantization", "question_reference": "Based on the ablation studies, which component's removal resulted in the most dramatic performance decrease on the WN18RR dataset?", "explanation_reference": "The ablation study results for the WN18RR dataset show that the removal of the MulHop component resulted in the most significant performance decrease. This indicates that multi-hop neighbor information is crucial for the model's performance on this dataset.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the model that has the highest P-BLEU score in the uAD dataset. Why does the model proposed in the paper attain a higher \\ednascore compared to other diverse decoding strategies on both summarization datasets?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets the lowest P-BLEU? score in uAD dataset?", "answer_anchor": "Composition", "question_reference": "In the context of the paper, why does Composition Sampling with \\frost$_{\\hspace*{-.1cm}++}$ achieve a higher \\ednascore compared to other diverse decoding strategies on both summarization datasets?", "explanation_reference": "The paper states that Composition Sampling with \\frost$_{\\hspace*{-.1cm}++}$ is most effective in generating faithful summaries, as demonstrated automatically (with best entailment scores on XSum and CNN/DailyMail) and by humans (with highest ratings on XSum and CNN/DailyMail); these summaries are also diverse, achieving the highest \\ednascore scores on both summarization datasets. This indicates that the method's ability to generate summaries that are both faithful to the input and diverse in content leads to its higher \\ednascore.", "evidence_reference": "Composition(\\frost$_{\\hspace*{-.1cm}++}$) is most effective in generating faithful summaries, as demonstrated automatically (with best entailment scores on XSum and CNN/DailyMail) and by humans (with highest ratings on XSum and CNN/DailyMail); these summaries are also diverse achieving highest \\ednascore scores on both summarization datasets."}
{"question": "Consider the paper that introduces the dataset which has fewer validation set samples than GoGenSum but more samples than FactCC. What is the Spearman's correlation coefficient for the model proposed in the paper's textual entailment scores with factual human scores?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset having less val set samples than GoGenSum but more samples than FacCC?", "answer_anchor": "XSumFaith", "question_reference": "What is the Spearman's correlation coefficient for the textual entailment scores with factual human scores?", "explanation_reference": "The Spearman's correlation coefficient for textual entailment scores with factual human scores directly measures the strength and direction of association between these two variables. The value of 0.264 indicates a weak positive correlation, suggesting that as textual entailment scores increase, factual human scores tend to increase as well, albeit weakly.", "evidence_reference": "Spearman's correlation coefficient ($|r_s|$) of different metrics with faithful and factual annotations. [...] Entailment & \\textbf{0.431} & \\textbf{0.264}"}
{"question": "Consider the paper that introduces the model that has a de-en score of 53.87. What is the performance difference between \\textsc{WPM-Sep} and \\textsc{WPM-Joint} on the NT14 dataset for the En$\\Rightarrow$De translation task?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has 53.87 de-en score?", "answer_anchor": "GWLAN", "question_reference": "What is the performance difference between \\textsc{WPM-Sep} and \\textsc{WPM-Joint} on the NT14 dataset for the En$\\Rightarrow$De translation task?", "explanation_reference": "The performance difference is calculated based on the accuracy scores provided for the NT14 dataset in the En$\\Rightarrow$De translation task. \\textsc{WPM-Sep} has an accuracy of 51.46, and \\textsc{WPM-Joint} has an accuracy of 52.68. The difference is 52.68 - 51.46 = 1.22.", "evidence_reference": "4 &  \\textsc{WPM-Sep}    &        56.93     &     55.67       &      54.54       &    51.46        \\\\ 5 & \\textsc{WPM-Joint} & \\textbf{55.54}                      & \\textbf{55.85}                      & \\textbf{53.64}                      & \\textbf{54.25}                             &       \\textbf{57.84}     &    \\textbf{56.75}   &    \\textbf{56.91}     &      \\textbf{52.68}"}
{"question": "Consider the paper that introduces the model shown in the figure that performs most similarly to GPT-3.5-Turbo for the 'Plausible' and 'Foreign' scenarios. What specific condition is required for the model's DA-WR algorithm to ensure convergence to the target distribution in terms of the maximum difference between indicator functions of the initial and augmented datasets?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shown in the figure has a similar performance to GPT-3.5-Turbo?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition is required for the DA-WR algorithm to ensure convergence to the target distribution in terms of the maximum difference between indicator functions of the initial and augmented datasets?", "explanation_reference": "The condition required for the DA-WR algorithm to ensure convergence to the target distribution is that the maximum difference between the indicator functions of the initial and augmented datasets, for all x in the support, should be o(1/n). This condition ensures that the difference diminishes as the sample size increases, allowing the cumulative distribution function resulting from the DA-WR algorithm to converge in probabilities to the target distribution as n tends to infinity.", "evidence_reference": "we should say that the imbalanced phenomenon occurs when $F \\\\neq F_0$. To measure the degree of imbalance we can denote by  $\\\\widehat{F}$ and $\\\\widehat{\\\\mathbb{P}}$ the empirical estimators and we propose the following definition: we  face a $(\\\\alpha, \\\\beta)$-imbalanced regression problem if there is a set $\\\\chi\\\\subset\\\\mathcal{X}$ with  $\\\\mathbb{P}_0(\\\\boldsymbol{X}\\\\in\\\\chi) \\\\geq \\\\beta$ such that $\\\\lvert \\\\frac{\\\\widehat{\\\\mathbb{P}}(\\\\boldsymbol{X}\\\\in\\\\chi)}{\\\\mathbb{P}_0(\\\\boldsymbol{X}\\\\in\\\\chi)} - 1 \\\\rvert > \\\\alpha$."}
{"question": "Consider the paper that introduces the method that results in a token-level F1 score equal to 37.03. What specific initialization method was used for the additional position embeddings in the model proposed by the paper to support longer documents?", "answer": "", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method show a token-level F1 score equal to 37.03?", "answer_anchor": "longformer", "question_reference": "What specific initialization method was used for the additional position embeddings in Longformer to support longer documents?", "explanation_reference": "The method used for initializing the additional position embeddings in Longformer, to extend its support for longer documents beyond the 512 token limit of RoBERTa, involved copying the 512 position embeddings from RoBERTa multiple times. This approach was chosen to preserve the local structure learned by RoBERTa's pretrained weights, except at the partition boundaries, and was found to be very effective for rapid convergence of Longformer's pretraining with a small number of gradient updates.", "evidence_reference": "To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa's pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times as analysis of BERT's attention heads shows a strong learned bias to attending to local context, including the previous or next token. Using the copy initialization preserves this local structure everywhere except at the partition boundaries."}
{"question": "Consider the paper that introduces the method that has three empty entries in the table for the mathematical reasoning and commonsense reasoning tasks. What specific modification to the few-shot prompts used in its generation is highlighted as key for improving the quality of generated data?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has three empty entries in mathematical reasoning and commonsense reasoning tasks?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as key for improving the quality of generated data?", "explanation_reference": "The specific modification mentioned is crucial because it allows the large language models (LLMs) to correct small mistakes in the chain of thought (CoT), thereby improving the quality of the generated data for finetuning smaller models.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the model represented by the orange bar. What specific mechanism does it employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model is demonstrated by the yellow bar?", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific mechanism does the Transformer employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "explanation_reference": "The mechanism described directly addresses the need to prevent future information from influencing the prediction of the current position in the sequence, which is crucial for maintaining the auto-regressive nature of the model. This is achieved by modifying the self-attention sub-layer in the decoder to mask out future positions.", "evidence_reference": "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."}
{"question": "Consider the paper that introduces the model that scores a 91.5 in the NER task. What is the performance decrease in Smatch score for AMR parsing on AMR2.0 when it is further fine-tuned on silver data after pre-training?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets score of 91.5 in NER task?", "answer_anchor": "AMRBART", "question_reference": "What is the performance decrease in Smatch score for AMR parsing on AMR2.0 when further fine-tuning the model on silver data after pre-training?", "explanation_reference": "The performance decrease in Smatch score for AMR parsing on AMR2.0 when further fine-tuning the model on silver data after pre-training is calculated by comparing the Smatch score of the model without further fine-tuning on silver data (85.4) to the score after such fine-tuning (85.1), resulting in a decrease of 0.3.", "evidence_reference": "Ours (large)& 85.4 & 49.8 \\\\ \\quad + silver  & 85.1 & 49.6 \\\\"}
{"question": "Consider the paper that introduces the model that corresponds to the fifth row of the table. How does its dynamic halting mechanism compare to the fixed-depth approach in terms of model regularization and computational efficiency?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1807.03819", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model in the fifth row of the table?", "answer_anchor": "UT", "question_reference": "How does the dynamic halting mechanism in Universal Transformers compare to the fixed-depth approach in terms of model regularization and computational efficiency?", "explanation_reference": "The explanation highlights that dynamic halting not only improves computational efficiency by dynamically adjusting the number of processing steps per input symbol but also serves as a regularization technique by encouraging the model to use a minimal number of steps necessary for processing different symbols. This is contrasted with fixed-depth models, where the depth is predetermined and not adaptive to the input, potentially leading to either underfitting or overfitting depending on the task complexity.", "evidence_reference": "Our best fixed UT results used 6 steps. However, the average number of steps that the best UT with dynamic halting took on the test data over all positions and examples was $8.2 \\\\rpm 2.1$. In order to see if the dynamic model did better simply because it took more steps, we trained two fixed UT models with 8 and 9 steps respectively (see last two rows). Interestingly, these two models achieve better results compared to the model with 6 steps, but \\\\emph{do not outperform the UT with dynamic halting}. This leads us to believe that dynamic halting may act as a useful regularizer for the model via incentivizing a smaller numbers of steps for some of the input symbols, while allowing more computation for others."}
{"question": "Consider the paper that introduces the method that corresponds to the orange line in the figure. What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA, to ensure their unification in parameter learning?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method in the figure is demonstrated by the orange line?", "answer_anchor": "UniKGQA", "question_reference": "What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning?", "explanation_reference": "The specific pre-training task designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning is the question-relation matching task. This task is directly mentioned as a shared pre-training task for both models, indicating its role in unifying the parameter learning process by focusing on matching the semantics of questions with the relations in the knowledge graph.", "evidence_reference": "For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies."}
{"question": "Consider the paper that introduces the method that has the lowest MAE in the CH-SIMS task. What specific feature of spoken language does the Spoken Language Embedding Subnetwork in the model proposed by the paper focus on to handle the volatile nature of spoken opinions?", "answer": "", "figure": "locality/2310.05804/result_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has the highest MAE in CH-SIMS task?", "answer_anchor": "Tensor Fusion", "question_reference": "What specific feature of spoken language does the Spoken Language Embedding Subnetwork in the Tensor Fusion Network model focus on to handle the volatile nature of spoken opinions?", "explanation_reference": "The Spoken Language Embedding Subnetwork is designed to focus on important parts of speech to effectively deal with the volatile nature of spoken language, which often lacks proper structure and includes idiosyncratic speech traits.", "evidence_reference": "The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech."}
{"question": "Consider the paper that introduces the method that has an F1 score of 54.83. What specific adaptation does the model proposed in the paper make to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having 54.84 F1 score?", "answer_anchor": "LayoutXLM", "question_reference": "What specific adaptation does LayoutXLM make to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "explanation_reference": "The adaptation mentioned addresses the challenge of language-specific pre-processing by obtaining character-level bounding boxes for each token after tokenization, which allows for a unified approach to handling multilingual multimodal inputs.", "evidence_reference": "However, for LayoutXLM, this strategy is not applicable because the definition of the linguistic unit is different from language to language. To prevent the language-specific pre-processing, we decide to obtain the character-level bounding boxes. After the tokenization using SentencePiece with a unigram language model, we calculate the bounding box of each token by merging the bounding boxes of all characters it contains."}
{"question": "Consider the paper that introduces the method that demonstrates the second highest score in the TweetEval Irony dataset for both zero-shot and few-shot prompting. How does the model's, proposed by the paper, approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "answer": "", "figure": "locality/2310.15746/comparison_2_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method demonstrates the second highest score in TweetEval Irony dataset in both zero-shot and few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "explanation_reference": "The paper hypothesizes that the divergence in performance on the RefCOCOg task between VL-T5 and VL-BART is due to the different methods of positional encoding used by T5 and BART. Specifically, BART uses learned absolute positional embeddings, which might lead to the model memorizing the positions of training objects, resulting in high training accuracy but low validation accuracy. This hypothesis is supported by the observation of VL-BART's performance drop in the RefCOCOg task compared to VL-T5.", "evidence_reference": "We also observe that our experiments with \\oursb{} on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in self-attention layers instead. We hypothesize that \\oursb{} found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy)."}
{"question": "Consider the paper that introduces the model represented by the lavender color in the figure, specifically StarCoderBase. What is the performance comparison of this model on the HELM benchmark for natural language reasoning tasks against other open-access models?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model is demonstrated in the lavender color?", "answer_anchor": "StarCoder", "question_reference": "Based on the evaluation of StarCoderBase on the HELM benchmark, how does its performance on natural language reasoning tasks compare to other open-access models?", "explanation_reference": "The evaluation of StarCoderBase on the HELM benchmark for natural language reasoning tasks shows that it generally outperforms other open-access models, indicating its superior ability to leverage its natural language and code pretraining for these tasks.", "evidence_reference": "In Table~\\ref{tab:helm_results} we report the results. We compute each model's ranking on each task, and order models in the table by their average ranking across tasks. StarCoderBase generally obtains substantially stronger performance than all other models with released weights and often performs comparably to or better than much larger models."}
{"question": "Consider the paper that introduces the method that corresponds to the second highest Acc-5 score on MOSI. What is the mathematical representation used to calculate the shifting value for unimodal supervisions in the model proposed by the paper?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "2102.04830", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method demonstrates the highest Acc-5 score on MOSI", "answer_anchor": "Self-MM", "question_reference": "What is the mathematical representation used to calculate the shifting value for unimodal supervisions in the ULGM?", "explanation_reference": "The shifting value for unimodal supervisions in the ULGM is calculated using the formula y_s = y_m + \\alpha_s - \\alpha_m, which represents the adjustment of unimodal supervisions based on the relative distance values (\\alpha_s and \\alpha_m) from modality representations to class centers.", "evidence_reference": "y_s - y_m \\propto \\hat{y}_s - \\hat{y}_m \\propto \\alpha_s - \\alpha_m \\Rightarrow y_s = y_m + \\alpha_s - \\alpha_m"}
{"question": "Consider the paper that introduces the model that corresponds to the green dashed line. What specific architectural change was made to the Transformer model in its framework to potentially improve computational efficiency during unsupervised pre-training?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model demonstrates in the gree dashed line?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change was made to the Transformer model in the T5 framework to potentially improve computational efficiency during unsupervised pre-training?", "explanation_reference": "The specific architectural change made to improve computational efficiency during unsupervised pre-training in the T5 framework was replacing entire spans of corrupted tokens with a single token. This approach is mentioned as part of the unsupervised objectives exploration, where it is noted that this method produces shorter target sequences, potentially making unsupervised pre-training more computationally efficient.", "evidence_reference": "We found that most ``denoising'' objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to-text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient."}
{"question": "Consider the paper that introduces the method that has an F1 score of 41.3. What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having 41.3 F1 score?", "answer_anchor": "SPADE", "question_reference": "What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically mentioned as a method applied by SPADE to prevent loops and token redundancy in parses. It functions by iteratively trimming tail-sharing-edges and generating new edges until the process becomes self-consistent, with a maximum iteration limit set to 20.", "evidence_reference": "Based on this property, we apply the following simple yet powerful tail collision avoidance algorithm: (1) at each tail node having multiple incoming edges, all edges are trimmed except the one with the highest linking probability; (2) at each head node of the trimmed edges, the new tail node is found by drawing the next probable edge whose probability is larger than $p_{th}$ and belongs to the top three; (3) go back to Step 1 and repeat the routine until the process becomes self-consistent or the max iteration limit is reached (set to 20 in this paper). The algorithm prevents loops and token redundancy in parses."}
{"question": "Consider the paper that introduces the model that shows the best overall performance in the 'Foreign' scenario. What is the trimming sequence value chosen for its application?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shown the best overall performance?", "answer_anchor": "LLaMA-30B", "question_reference": "What is the trimming sequence value chosen for the application?", "explanation_reference": "The trimming sequence value chosen for the application is explicitly mentioned in the section titled \\textsc{Application_8}, indicating the specific value used for the trimming sequence in the application.", "evidence_reference": "For the application, we chose a trimming  sequence $e_{n} = \\frac{1}{10 \\times n}$"}
{"question": "Consider the paper that introduces the dataset in the Movies domain only that has a PF field. Which model demonstrated the highest improvement in performance when normalized topics were introduced, and what specific metric was most significantly affected?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which dataset is in Movies domain only and has PF field?", "answer_anchor": "DuConv", "question_reference": "Based on the experimental results, which model demonstrated the highest improvement in performance when normalized topics were introduced, and what specific metric was most significantly affected?", "explanation_reference": "The norm generation model showed the most significant improvement in performance when normalized topics were introduced, particularly in the F1/BLEU1/BLEU2 metrics, indicating a substantial enhancement in the quality and relevance of the generated responses.", "evidence_reference": "norm generation & 32.50\\% & 58.50\\% & 24.3 & \\textbf{41.84} / \\textbf{0.347} / \\textbf{0.198} & 0.057 / 0.155 & \\textbf{9.78} / 38.02 / \\textbf{15.27}"}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in few-shot prompting. How does its method specifically address the issue of multi-turn consistency in dialogue models?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shows the lowest execuation accuracy in few-shot prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "How does the GAtt method specifically address the issue of multi-turn consistency in dialogue models?", "explanation_reference": "The GAtt method addresses multi-turn consistency by ensuring that a specific instruction is respected throughout the dialogue. It does this by artificially adding the instruction to all user messages in the dialogue during training, and then selectively applying loss only to the tokens related to the most recent turn and the initial instruction, effectively teaching the model to maintain focus on the instruction across multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right). [...] Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than BROS and a higher F1 score than LayoutXLM. What specific aspect of the biaffine scorer's performance under the single-head constraint in the model proposed by the paper significantly outperforms the MLP scorer in the context of entity relation extraction from VRDs?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having lower F1 score than BROS and higher F1 score than LayoutXLM?", "answer_anchor": "SERA", "question_reference": "What specific aspect of the biaffine scorer's performance under the single-head constraint significantly outperforms the MLP scorer in the context of entity relation extraction from VRDs?", "explanation_reference": "The biaffine scorer's performance under the single-head constraint is significantly better than the MLP scorer in terms of F1 score, indicating its superior ability to balance precision and recall in the context of entity relation extraction from visually rich documents (VRDs). This is critical for achieving high overall performance in relation extraction tasks.", "evidence_reference": "Using different relation scorer, the trend between these two decoders is contrary. MLP scorer performs poorer than biaffine scorer under single-head constraint. This is because biaffine scorer is more suitable for single-head constraint which has been proved by \\citet{dozat2016deep}."}
{"question": "Consider the paper that introduces the method which does not have results in the CSQA2.0 dev and StrategyQA test tasks shown in the table. What specific methodological approach was found to mitigate performance regressions on public NLP datasets during its RLHF fine-tuning process?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method does not show result in CSQA2.0 dev and StrategyQA test task?", "answer_anchor": "ChatGPT", "question_reference": "What specific methodological approach was found to mitigate performance regressions on public NLP datasets during the RLHF fine-tuning process?", "explanation_reference": "The approach of mixing in pretraining gradients during PPO training was identified as a method to mitigate performance regressions on public NLP datasets. This method involves incorporating gradients from the pretraining data distribution into the PPO fine-tuning process, which helps maintain the capabilities of the pretrained model and addresses the performance regressions.", "evidence_reference": "We mitigate the regressions by mixing in pretraining gradients during PPO training. We use 8 times more pretraining examples than the number of the RL training episodes... For each minibatch, we compute the PPO gradients and pretraining gradients in consecutive steps and accumulate them both into the gradient buffers. We multiply the pretraining gradients by a coefficient, $\\gamma=27.8$ (see Equation~\\ref{eq2}), to control the relative strength of gradients from PPO and pretraining distributions."}
{"question": "Consider the paper that introduces the model that has the highest relative performance in few-shot prompting. How does the GAtt method specifically address the issue of multi-turn consistency in dialogue models?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model has the highest accuracy in few-shot prompting?", "answer_anchor": "LLaMA-30B", "question_reference": "How does the GAtt method specifically address the issue of multi-turn consistency in dialogue models?", "explanation_reference": "The GAtt method addresses multi-turn consistency by ensuring that a specific instruction is respected throughout the dialogue. It does this by artificially adding the instruction to all user messages in the dialogue during training, and then selectively applying loss only to the tokens related to the most recent turn and the initial instruction, effectively teaching the model to maintain focus on the instruction across multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right). [...] Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages."}
{"question": "Consider the paper that introduces the method that achieves the highest Hits@1 score in the MQA-1H dataset. What specific component of the model proposed in the paper is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets the highest Hits@1 score in MQA-1H dataset?", "answer_anchor": "UniKGQA", "question_reference": "What specific component of UniKGQA's architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "explanation_reference": "The question targets a detailed aspect of the UniKGQA architecture, specifically asking for the component that handles the propagation of semantic matching information across the knowledge graph. The answer, 'Matching information propagation module,' directly addresses this by naming the specific part of the architecture designed for this purpose.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method which is shown in the table above the 'Magister et al' row but below the 'UL2' row. What specific performance improvement does the Socratic CoT approach demonstrate over the traditional CoT approach when applied to the GPT-3 model in a single-shot setting on the GSM8K dataset, according to DecomDistill?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown in the table above method proposed by Magister et al but below UL2 method?", "answer_anchor": "DecomDistill", "question_reference": "What specific performance improvement does the Socratic CoT approach demonstrate over the traditional CoT approach when applied to the GPT-3 model in a single-shot setting on the GSM8K dataset?", "explanation_reference": "The question focuses on the comparison between the Socratic CoT and traditional CoT approaches in enhancing the reasoning capabilities of GPT-3 on the GSM8K dataset. The answer is derived from the results section, where it mentions that introducing subquestioning (Socratic CoT) boosts accuracy by over 40% compared to standard CoT prompting.", "evidence_reference": "The introduction of subquestioning boosts accuracy by over 40\\% compared to standard CoT prompting (Table \\ref{Acc:GPT-3})."}
{"question": "Consider the paper that introduces the model that performs the best on the BBBP dataset. What is the peak learning rate used during the pre-training of this model?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model perform the best in the BBBP dataset?", "answer_anchor": "MolXPT", "question_reference": "What is the peak learning rate used during the pre-training of MolXPT?", "explanation_reference": "The peak learning rate is a specific hyperparameter value used during the pre-training phase of the MolXPT model. It is mentioned directly in the section detailing the pre-training hyper-parameters.", "evidence_reference": "The peak learning rate is $0.0005$ and the warm-up steps are 20000."}
{"question": "Consider the paper that introduces the method shown in the figure corresponds to the solid red line. What specific method from continual learning is employed in the model proposed in the paper, UniEval, to address the negative transfer problem observed in several dimensions during its training process?", "answer": "", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the figure demonstrated by the red solid line?", "answer_anchor": "UniEval", "question_reference": "What specific method from continual learning is employed in UniEval to address the negative transfer problem observed in several dimensions during its training process?", "explanation_reference": "The paper mentions employing a simple and effective method from continual learning to tackle the negative transfer problem observed in several dimensions, specifically stating that whenever a new dimension is introduced, a small portion of data from all previous dimensions is added to replay. This method is known as the 'replay method' in continual learning, which helps in retaining the performance on previously learned tasks while learning new ones.", "evidence_reference": "To tackle this issue, we employ a simple and effective method from continual learning~\\cite{parisi2019continual}: whenever a new dimension is introduced, we add small portion of data from all previous dimensions to replay."}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE-1 score equal to 44.52. What specific activation function is used in the non-linear transformation within the BoW Encoder of the neural topic model proposed in the paper?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method in the table that demonstrates a ROUGE 1 score equal to 44.52?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific activation function is used in the non-linear transformation within the BoW Encoder of the neural topic model?", "explanation_reference": "The paper specifies that the non-linear transformation within the BoW Encoder uses a tanh activation function. This detail is crucial for understanding the specific architecture and functionality of the neural topic model described.", "evidence_reference": "the input $\\textbf{x}_{bow}$ is first encoded into a latent variable $\\mathbf{z}$ by a topic encoder. Each input is passed to obtain the prior mean $\\mu$ and prior standard deviation $\\sigma$ \\vspace{-2mm} \\begin{equation} \\pi = f_{MLP}(\\textbf{x}_{bow}), \\mu = f_1(\\pi), \\log \\sigma = f_2(\\pi) \\end{equation} where $f_{MLP}$ is a non-linear transformation with a $\\tanh$ activation function; $f_1$ and $f_2$ are two linear transformations with bias."}
{"question": "Consider the paper that introduces the method whose results are displayed in a lighter grey color in the table. What specific strategy does the model proposed in the paper employ during finetuning and inference to address the inefficiency and potential for generating invalid labels in classification tasks?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is using grey-ish color for showing results in the table?", "answer_anchor": "OFA-base", "question_reference": "What specific strategy does OFA employ during finetuning and inference to address the inefficiency and potential for generating invalid labels in classification tasks?", "explanation_reference": "The paper mentions the use of a Trie-based search strategy to overcome the inefficiencies and potential for generating invalid labels during classification tasks in the finetuning and inference stages. This strategy enhances performance by ensuring that the model's output is constrained within the set of valid labels, addressing the mentioned problems directly.", "evidence_reference": "For inference, we apply the decoding strategies, e.g., beam search, to enhance the quality of generation. However, this paradigm has several problems in classification tasks: 1. optimizing on the entire vocabulary is unnecessary and inefficient; 2. the model may generate invalid labels out of the closed label set during inference. To overcome these issues, we introduce a search strategy based on prefix tree (Trie, \\cite{trie})."}
{"question": "Consider the paper that introduces the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category. What specific layer of the T5 model was edited in the main results for the zsRE question-answering task?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2211.11031", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "GRACE", "question_reference": "What specific layer of the T5 model was edited in the main results for the zsRE question-answering task?", "explanation_reference": "The specific layer edited in the T5 model for the zsRE question-answering task is mentioned directly in the paper, indicating the precise location within the model's architecture where GRACE was applied.", "evidence_reference": "In our main results in Table \\ref{tab:main_results}, we edit T5's ``\\texttt{wo}'' module in its encoder's 4$^\\text{th}$ block's layer 1: \\texttt{encoder.block[4].layer[1].DenseReluDense.wo}."}
{"question": "Consider the paper that introduces the model that corresponds to the lowest BERTScore F1 score on the TellMeWhy dataset. What is the K-L divergence between the prediction results of this model, proposed in the paper, and ground-truth for question type distribution learning on the test set?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model shows the lowest BERTScore F1 score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the prediction results of the BERT-based model and ground-truth for question type distribution learning on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how well the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the method that has the second lowest overall performance for the Seen condition. How does the model's approach to handling object interactions differ from the method proposed by Shridhar et al. in terms of object class reasoning?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the second lowest over performance for Seen condition?", "answer_anchor": "MOCA", "question_reference": "How does the MOCA model's approach to handling object interactions differ from the method proposed by Shridhar et al. in terms of object class reasoning?", "explanation_reference": "MOCA's approach to object interactions involves reasoning about object classes, enabling it to interact with the correct object by leveraging object-centric localisation (OCL). This contrasts with Shridhar et al.'s method, which generates class-agnostic interaction masks, lacking the ability to reason about object categories.", "evidence_reference": "Object-Centric Localisation (OCL) allows our method to reason about object classes (Sec.~\\ref{section:target_class}) which ensures interaction with the correct object. This is in contrast with~\\cite{shridhar2020alfred} that upsamples a linear embedding via a deconvolution network and predicts class-agnostic masks, thereby not preserving any information about object category."}
{"question": "Consider the paper that introduces the model that has a 6-layer encoder and a 6-layer decoder architecture. What specific method was adopted for selecting high-quality, in-domain sentences from the larger Commoncrawl corpus for back-translation in the English to Russian translation task?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "1907.06616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model with 6-layer encoder and 6-layer decoder architecture?", "answer_anchor": "FSMT", "question_reference": "What specific method was adopted for selecting high-quality, in-domain sentences from the larger Commoncrawl corpus for back-translation in the English to Russian translation task?", "explanation_reference": "The paper mentions that to select a limited amount of high quality, in-domain sentences from the larger and noisier Commoncrawl corpus for back-translation, the method described by Moore (2010) for selecting in-domain data was adopted. This indicates that Moore's in-domain filtering method was specifically used for this purpose.", "evidence_reference": "In order to select a limited amount of high quality, in-domain sentences from the larger corpus, we adopt the method of~\\citet{moore2010intelligent} for selecting in-domain data (\\textsection\\ref{subsection:btcc})."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. How does the model's performance, proposed by the paper, with concepts extracted from captions compare to using concepts from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method is shown in the penult row?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP with concepts extracted from captions compare to using concepts from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "explanation_reference": "The paper demonstrates that leveraging concepts extracted directly from captions, as opposed to using pre-defined concepts from an object detector, results in improved performance on the COCO-caption Karpathy split. This is evidenced by the higher CIDEr score achieved when using caption-extracted concepts.", "evidence_reference": "FOCAL$_\\text{Tag+Init}$ & $10$ & $35.9$ & $28.4$  & $57.6$ & \\cellcolor{gray!40}$121.3$ & $21.9$"}
{"question": "Consider the paper that introduces the model shown in the figure corresponds to the green line. What is the Pearson's r correlation coefficient between word overlap and its performance for the task of AI venue classification?", "answer": "", "figure": "locality/2310.10191/accuracy_figure.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model is shown in green line?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of AI venue classification?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the AI venue classification task. A value of 0.9303959931770183 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also tends to increase.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method demonstrated by the solid lavender line. What specific architectural feature allows the model proposed in the paper to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method demonstrated in the lavendar solid line?", "answer_anchor": "GRIT", "question_reference": "What specific architectural feature of GRIT allows it to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "explanation_reference": "GRIT employs a Deformable DETR-based detector for extracting region features, which inherently does not require class-aware NMS (Non-Maximum Suppression) and RoI (Region of Interest) Align operations that are typically necessary in CNN-based detectors like Faster R-CNN. This architectural choice significantly reduces the computational time for feature extraction.", "evidence_reference": "On the other hand, we employ a Deformable DETR-based detector to extract region features without using all such operations. Table \\ref{tab:extraction} shows the comparison on feature extraction."}
{"question": "Consider the paper that introduces the dataset in the table that has the second shortest average question length. What specific strategy does the model proposed in the paper employ to enhance its performance on machine translation tasks during curriculum learning?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has the longest question average length?", "answer_anchor": "Multialpaca", "question_reference": "What specific strategy does the model employ to enhance its performance on machine translation tasks during curriculum learning?", "explanation_reference": "The strategy of introducing more multilingual parallel data during curriculum learning is mentioned as significantly boosting the model's performance on translation tasks. This approach likely helps in better aligning the representations across different languages, thereby improving the model's ability to translate between them.", "evidence_reference": "Finally, it is worth noting that introducing more multilingual parallel data during the curriculum learning significantly boost the model performance on translation task."}
{"question": "Consider the paper that introduces the method that achieves an F1 score with a mean of 58.86 in the TAT-QA task. What is the model's average accuracy improvement over the best baseline few-shot-CoT GPT-3 on the dataset?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method demonstrates F1 score with mean 58.86 in TAT-QA task?", "answer_anchor": "PromptPG", "question_reference": "What is the average accuracy improvement of PromptPG over the best baseline few-shot-CoT GPT-3 on the \\data{} dataset?", "explanation_reference": "The average accuracy improvement of PromptPG over the best baseline few-shot-CoT GPT-3 is directly stated in the Experimental Results section, indicating the effectiveness of the PromptPG method in selecting in-context examples and constructing performing prompts for the test example.", "evidence_reference": "our proposed \\model learns to select performing examples with the help of policy gradient. \\model establishes a state-of-the-art performance on the \\data{} dataset: it surpasses the best baseline few-shot-CoT GPT-3 by 5.31\\% on average."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest number of turns. What is the average improvement in accuracy that the model proposed in the paper achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the least number of turns from the table?", "answer_anchor": "Multialpaca", "question_reference": "What is the average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "explanation_reference": "The average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English is directly stated as 7.6% in the paper.", "evidence_reference": "For languages other than English (the multilingual column), \\textsc{Poly}LM-13B outperforms LLaMA-13B with average improvement up to 7.6%, 5.6%, 3%, and 11% on XCOPA, PAWS-X, XWinagrad, and XNLI, respectively."}
{"question": "Consider the paper that introduces the method that has the third highest Avg score on the GLUE task. How does the Hyperformer model ensure parameter efficiency while enabling task-specific adaptation in the context of multi-task learning?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the second highest Avg score on GLUE task?", "answer_anchor": "Hyperformer", "question_reference": "How does the \\methodefficient model ensure parameter efficiency while enabling task-specific adaptation in the context of multi-task learning?", "explanation_reference": "The \\methodefficient model achieves parameter efficiency through the use of shared hypernetworks across tasks and layers, which generate task-specific adapter parameters. This approach allows for knowledge sharing across tasks while enabling task-specific adaptation through the generation of task, layer id, and adapter position conditioned parameters. This method balances the need for a compact model with the flexibility required for task-specific adjustments.", "evidence_reference": "Our approach learns a task feature embedding per task, consisting of $Tt$ parameters. We additionally employ layer id and adapter position embeddings in the encoder and decoder, which require $2(2+L)t$ parameters, with a fixed embedding size of $t$ for all these feature embeddings. We consider a separate task projector networks $h'_I$ for encoder and decoder, which is in both cases a two-layer MLP, consisting of a total of $2(3te+et)$ parameters, where $e=128$ is the hidden dimension for the task-projector network. Our hypernetwork for adapters in encoder/decoder consists of $2(2thd)$ parameters and our layer normalization hypernetwork consists of $2(2th)$ parameters. In total, this results in $\\underbrace{t(T+4+2L)}_{\\text{Task features}}+\\underbrace{8te+2t(2hd+2h)}_\\text{Hypernetworks}$ parameters."}
{"question": "Consider the paper that introduces the method that achieves a score of 28.62 in the WQ-B task. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions proposed by the paper?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets 28.62 score in WQ B task?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in few-shot prompting. What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for the model proposed in the paper?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shows the second best execuation accuracy in few-shot prompting?", "answer_anchor": "StarCoder", "question_reference": "What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for StarCoderBase?", "explanation_reference": "The percentage of responses flagged as toxic using a toxicity classifier for StarCoderBase in the RealToxicityPrompts evaluation is provided directly in the results table for the toxicity evaluation.", "evidence_reference": "StarCoderBase & 0.42 & 1.12"}
{"question": "Consider the paper that introduces the method that demonstrates the lowest score in the CSQA2.0 dev/dev* task. What specific methodological change did the authors find to be optimal for mitigating performance regressions on public NLP datasets during PPO training, according to their ablations?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method demonstrates the lowest score in CSQA2.0 dev task?", "answer_anchor": "ChatGPT", "question_reference": "What specific methodological change did the authors find to be optimal for mitigating performance regressions on public NLP datasets during PPO training, according to their ablations?", "explanation_reference": "The authors conducted ablations to find an optimal method for mitigating performance regressions on public NLP datasets during PPO training. They compared using different amounts of pretraining data while keeping the pretraining loss coefficient constant. They found that using a pretraining data ratio of 8 was a middle ground between training speed and pretraining loss performance, which they chose as optimal for mitigating performance regressions.", "evidence_reference": "We compared using different amount of pretraining data, while keeping the pretraining loss coefficient constant. By increasing the amount of pretraining data, the quality of gradient estimates from the pretraining improves. We found that using a pretraining data ratio of 4, the log probability loss on the pretraining distribution would often increase throughout the course of the training. Some preliminary experiments show better human Likert scores can be achieved with a pretraining data ratio of 32. However, the training time also increases by a few fold. By setting the pretraining data ratio to 8, the training time doubles that of the corresponding experiment without using pretraining mix; we chose this as a middle ground between training speed and pretraining loss performance."}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 63.8% in the StrategyQA dataset. Based on the ablation study findings, what is the model's, proposed by the paper, accuracy when using an external calculator and finetuned on 20% of the GSM8K dataset data?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets 63.8 accuracy in StrategyQA dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "Based on the ablation study findings, what is the accuracy of T5 XXL finetuned on 20% of the GSM8K dataset data when using an external calculator?", "explanation_reference": "The ablation study on dataset size reveals that finetuning T5 XXL on only 20% of the GSM8K dataset data results in an accuracy of 20.47% when an external calculator is used. This detail directly answers the question by providing the specific performance metric for a subset of the data under specified conditions.", "evidence_reference": "20\\% (1067 examples) & 11.22 & 20.47"}
{"question": "Consider the paper that introduces the model that achieves a score of 3.84 in the Grounding task. What specific feature of the visual embeddings allows it to discriminate regions from different images when multiple images are given to it?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the base model tested in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific feature of the visual embeddings allows the model to discriminate regions from different images when multiple images are given to it?", "explanation_reference": "The feature that allows the model to discriminate regions from different images when multiple images are given to it is the use of image ids. This is explicitly mentioned as a function of the image ids encoded within the visual embeddings.", "evidence_reference": "Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in \\NLVR{}~\\cite{Suhr2019}, models take two input images)."}
{"question": "Consider the paper that introduces the model that demonstrates the highest Oracle score. What is the Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model that demonstrates the highest Oracle score?", "answer_anchor": "Composition", "question_reference": "What is the Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity?", "explanation_reference": "The Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity is provided directly in the results section discussing the correlation of automatic metrics with human judgments.", "evidence_reference": "Self-BLEU & 0.880"}
{"question": "Consider the paper that introduces the dataset in which KALMV achieves a score of 70.83 for the XL model. What is the percentage of questions that can be answered using a boolean response?", "answer": "", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset being tested that KALMV gets 70.83 score for XL model?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka questions that can be answered using a boolean response?", "explanation_reference": "The percentage of questions in the Mintaka dataset that can be answered using a boolean (yes/no) response is directly provided in the dataset statistics.", "evidence_reference": "A majority (72\\%) of the questions in Mintaka can be answered using an entity. 14\\% can be answered using a boolean, in yes/no or comparative questions."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. What specific loss parameters are set for the concept classification task in the model proposed by the paper to handle the imbalanced distribution of semantic concepts?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method is shown in the penult row?", "answer_anchor": "ViTCAP", "question_reference": "What specific loss parameters are set for the concept classification task in ViTCAP to handle the imbalanced distribution of semantic concepts?", "explanation_reference": "These parameters are chosen to handle the imbalanced distribution of semantic concepts by decoupling the decay rates of positive and negative samples, emphasizing more on the contribution of the positive samples.", "evidence_reference": "Due to the extremely imbalanced semantic concepts distribution (certain concepts appear much frequently than the rest), we adopt the simplified asymmetric focal loss which shows great performances handling sample imbalance problems for the multi-label classification task. We set parameters $\\gamma_{+}=0$ and $\\gamma_{-}=1$ as in our experiment."}
{"question": "Consider the paper that introduces the method which is listed in the table right below the PCP method. What is the model's Dice similarity score between Instances (1) and (3) in Figure 1, and how is this score calculated?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method in the table right below the PCP method?", "answer_anchor": "GOLF", "question_reference": "What is the Dice similarity score between Instances (1) and (3) in Figure 1, and how is it calculated?", "explanation_reference": "The Dice similarity score between Instances (1) and (3) is calculated based on their sense label sequences across three hierarchical levels (Top, Second, and Connective), considering six sub-paths in the hierarchies. The calculation involves the Dice coefficient for each sub-path among the hierarchical levels and takes the average as the similarity score. The paper provides a detailed example calculation leading to a score of approximately 0.7.", "evidence_reference": "Taking Instances (1) and (3) in Figure \\ref{fig: example1} as examples, their label sequences are \\emph{Top: Comparison, Sec: Contrast, Conn: but} and \\emph{Top: Comparison, Sec: Contrast, Conn: however}, respectively. Then the similarity score would be $\\frac{1}{6}(\\frac{2\\times 1}{1+1} + \\frac{2\\times 1}{1+1} + \\frac{2\\times 0}{1+1} + \\frac{2\\times 2}{2+2} + \\frac{2\\times 1}{2+2} + \\frac{2\\times 2}{3+3})\\approx0.7$."}
{"question": "Consider the paper that introduces the method that results in a token-level F1 score equal to 37.03. What specific implementation detail of the model's attention mechanism proposed in the paper allows it to efficiently handle sequences of up to 32K characters on modern GPUs?", "answer": "", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method show a token-level F1 score equal to 37.03?", "answer_anchor": "longformer", "question_reference": "What specific implementation detail of the Longformer's attention mechanism allows it to efficiently handle sequences of up to 32K characters on modern GPUs?", "explanation_reference": "The combination of windowed and a new dilated attention pattern is mentioned as a key implementation detail that enables the Longformer to efficiently process sequences of up to 32K characters on modern GPUs. This detail is crucial for understanding how the Longformer achieves its efficiency and scalability for long sequences.", "evidence_reference": "We first evaluate \\model on autoregressive character-level language modeling using a combination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs."}
{"question": "Consider the paper that introduces the method that has a Hits@1 score ranging from 40% to 55% across the different fine-tuning samples shown in the figure. What specific component of the model proposed in the paper is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has Hit@1 score range from 40% to 55% as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What specific component of UniKGQA's architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "explanation_reference": "The question targets a detailed aspect of the UniKGQA architecture, specifically asking for the component that handles the propagation of semantic matching information across the knowledge graph. The answer, 'Matching information propagation module,' directly addresses this by naming the specific part of the architecture designed for this purpose.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method that is in the second row of the table. What specific performance improvement does the DenoiseFET model achieve on the UFET benchmark when enhanced with Prior Knowledge about Labels (PKL) using BERT-large as the language model, compared to its performance without PKL?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is in the second row of the table?", "answer_anchor": "ConCN clusters", "question_reference": "What specific performance improvement does the DenoiseFET model achieve on the UFET benchmark when enhanced with Prior Knowledge about Labels (PKL) using BERT-large as the language model, compared to its performance without PKL?", "explanation_reference": "The improvement is calculated based on the F1 scores presented for the DenoiseFET model without PKL (49.8%) and with PKL (51.9%) using BERT-large. The difference between these two F1 scores (51.9% - 49.8%) indicates a 2.1% increase in performance when PKL is applied.", "evidence_reference": "DenoiseFET & Bl & 52.6 & 47.5 & 49.8\\\\ DenoiseFET + PKL & Bl & 53.8 & 50.2 & 51.9"}
{"question": "Consider the paper that introduces the model shown in the figure that is consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B. How does the performance of the model proposed in the paper on the HELM benchmark for natural language reasoning tasks compare to other open-access models?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shown in the figure consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B?", "answer_anchor": "StarCoder", "question_reference": "Based on the evaluation of StarCoderBase on the HELM benchmark, how does its performance on natural language reasoning tasks compare to other open-access models?", "explanation_reference": "The evaluation of StarCoderBase on the HELM benchmark for natural language reasoning tasks shows that it generally outperforms other open-access models, indicating its superior ability to leverage its natural language and code pretraining for these tasks.", "evidence_reference": "In Table~\\ref{tab:helm_results} we report the results. We compute each model's ranking on each task, and order models in the table by their average ranking across tasks. StarCoderBase generally obtains substantially stronger performance than all other models with released weights and often performs comparably to or better than much larger models."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 25.9 in the Seen, Val, SR dataset. How does the model proposed in the paper handle the scenario when the agent's path is blocked by immovable objects during inference?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows score of 25.9 in Seen, Val, SR dataset?", "answer_anchor": "MOCA", "question_reference": "How does the model handle the scenario when the agent's path is blocked by immovable objects during inference?", "explanation_reference": "The question focuses on a specific methodological aspect of the model, particularly how it addresses the challenge of immovable obstacles during inference. The answer directly points to the solution proposed in the paper, which is the introduction of an 'obstruction evasion' mechanism within the Action Policy Module (APM).", "evidence_reference": "To address such unanticipated situations, we propose an 'obstruction evasion' mechanism in the APM to avoid obstacles at inference time."}
{"question": "Consider the paper that introduces the method that achieves a score of 31.8 in the Seen, Test, SR dataset. How does the performance of the model proposed in the paper with OSCAR initialization and without predicting the parent object or visual region classification compare to its performance with these features on the seen validation fold?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the score of 31.8 in Seen, Test, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "How does the performance of EmBERT with OSCAR initialization and without predicting the parent object or visual region classification compare to its performance with these features on the seen validation fold?", "explanation_reference": "The performance of EmBERT on the seen validation fold is higher when it does not predict the parent object or visual region classification, as indicated by the task and GC metrics comparing the configurations with and without these features.", "evidence_reference": "OSCAR & 18 & 200 & \\cblkmark & & & \\B{37.44} (\\B{28.81}) & \\B{44.62} (\\B{36.41}) & \\B{\\phantom{0}5.73} (\\B{\\phantom{0}3.09}) & \\B{15.91} (\\B{\\phantom{0}9.33}) \\\\[1pt] OSCAR & 18 & 200 & \\cblkmark & \\cblkmark & & 36.22 (27.05) & 44.57 (35.23) & \\phantom{0}4.39 (\\phantom{0}2.21) & 13.03 (\\phantom{0}7.54)"}
{"question": "Consider the paper that introduces the large language model that achieves a lower HVI score than OPT but a higher HVI score than Alpaca. What specific performance improvement does this model exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the large language model that demonstrates lower HVI score than OPT but higher HVI score than Alpaca?", "answer_anchor": "GPT-4", "question_reference": "What specific performance improvement does GPT-4 exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "explanation_reference": "The question assesses understanding of GPT-4's significant improvement in performance on a professional benchmark, the Uniform Bar Exam, compared to its predecessor. This detail highlights GPT-4's advanced capabilities in understanding and generating natural language in complex scenarios.", "evidence_reference": "For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers. This contrasts with GPT-3.5, which scores in the bottom 10%."}
{"question": "Consider the paper that introduces the benchmark represented by the triangle marker in the figure. What specific improvement does CoT prompting provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which benchmark is represented using the triangle marker from the figure?", "answer_anchor": "BBH", "question_reference": "What specific improvement does CoT prompting provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "explanation_reference": "The improvement is calculated by comparing the performance increase from answer-only prompting to CoT prompting for the Codex model on the 'Tracking Shuffled Objects' task. This is derived from the performance metrics provided, showing a significant gain when using CoT prompting.", "evidence_reference": ""}
{"question": "Consider the paper that introduces the LLM model that corresponds to an r score of 0.813. What specific methodological difference in the evaluation of the model's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "answer": "", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the LLM model that demonstrates the r score equal to 0.813?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard procedure used for most exam runs, where the model's letter choice is extracted directly from the explanation. This approach for the USABO and SAT reading/writing runs indicates a unique handling of these exams, which could contribute to the minimal impact on the overall results, as it relies on the model's generated explanation to determine the final answer choice.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest Method 1 accuracy. In the verification ablation studies, what specific training objective combination was found to strictly improve the performance of the model proposed in the paper?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What dataset demonstrates the highest accuracy with method 1?", "answer_anchor": "GSM8K", "question_reference": "In the verification ablation studies, which specific training objective combination was found to strictly improve the performance of verifiers?", "explanation_reference": "The paper discusses an ablation study where they compare the performance of verifiers trained with two different objectives: one with only the verification objective and another combining the verification objective with a language modeling objective. The study found that including the language modeling objective is a strict improvement over using the verification objective alone. This suggests that a better understanding of the language distribution aids the verifier in discriminating between correct and incorrect solutions.", "evidence_reference": "In \\Cref{fig:fc_verifier_loss_ablation}, we ablate the objective used when training verifiers. Although both are reasonable choices, including the language modeling objective is a strict improvement."}
{"question": "Consider the paper that introduces the method that achieves a score of 31.8 in the Seen, Test, SR dataset. What specific architectural modification allows the model proposed in the paper's Segment-Level Recurrent Action Decoder to perform cross-attention over encoder states, diverging from the original TransformerXL design?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the score of 31.8 in Seen, Test, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "What specific architectural modification allows the Segment-Level Recurrent Action Decoder in EmBERT to perform cross-attention over encoder states, diverging from the original TransformerXL design?", "explanation_reference": "The question targets a detailed aspect of the Segment-Level Recurrent Action Decoder's design, specifically how it adapts the TransformerXL architecture to enable cross-attention between decoder and encoder states. This is a nuanced detail that requires understanding both the original TransformerXL limitations and the specific modifications made by EmBERT to overcome these limitations for its task.", "evidence_reference": "Therefore, we introduce two novel elements to its architecture: 1) \\textit{encoder hidden states cache}; 2) \\textit{cross-attention over encoder states}. First, our extended context is composed of both agent state representations and hidden states from the previous segment $\\mathbf{s}_i$. In addition, to perform cross-attention between decoder and encoder hidden states, we modify the TransformerXL self-attention mechanism following common practice in designing transformer decoders~\\cite{vaswani2017attention}."}
{"question": "Consider the paper that introduces the method that is in the third row of the table. What specific approach does the model, known as BIRD, employ to ensure the high quality of SQL annotation and minimize annotation errors?", "answer": "", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "BIRD", "question_reference": "What specific approach does the \\textsc{Bird} benchmark employ to ensure the high quality of SQL annotation and minimize annotation errors?", "explanation_reference": "The \\textsc{Bird} benchmark employs a double-blind annotation approach to ensure the high quality of SQL annotations. This method involves two independent SQL annotators generating SQLs for the same question without discussion, and only SQLs yielding identical results are collected. This process significantly reduces the SQL annotation error rate by minimizing the likelihood of two skilled annotators producing the same incorrect results when databases have large values.", "evidence_reference": "As shown in Figure \\ref{wkf} (b), we employ a double-blind approach \\citep{csqa2} for SQL annotation. This approach involves two independent SQL annotators who generate SQLs for the same question without discussion. The annotated SQLs are executed in databases, and those yielding identical results are gathered. Otherwise, the SQLs are checked with experts until a consensus is reached. Double-blind procedures can dramatically reduce the SQL annotation error rate, as there is a small probability for two skillful annotators to generate the same incorrect results when databases have large values."}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE L score equal to 41.39. What specific preprocessing step is applied to the Bag-of-Words (BoW) representations before training the model proposed in the paper?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method in the table that demonstrates a ROUGE L score equal to 41.39?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific preprocessing step is applied to the Bag-of-Words (BoW) representations before training the flow-based neural topic model?", "explanation_reference": "The specific preprocessing step applied to the BoW representations before training the flow-based neural topic model is the removal of stopwords. This step is crucial for reducing noise and focusing on the meaningful content of the documents.", "evidence_reference": "Following \\cite{wang2020friendly}, we preprocess to remove stopwords in the BoW representations."}
{"question": "Consider the paper that introduces the method that has an accuracy of 95.20% in automatic evaluation. How does the gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) in the simplified theoretical analysis of the model's loss function, proposed in the paper, indicate the direction of model optimization?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows 95.20 accuracy in automatic evaluation?", "answer_anchor": "Discup", "question_reference": "How does the gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) in the simplified theoretical analysis of DisCup's loss function indicate the direction of model optimization?", "explanation_reference": "The gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) being a negative value indicates that model optimization is always in the direction of decreasing the probability of the unlikely token. This is because a negative gradient value for $h_{\\text{unlike}}$ means that the optimization process will work to reduce the logit value of the unlikely token, thereby decreasing its probability in the model's output distribution.", "evidence_reference": "The gradient of logit $h_{unlike}$ could be represented as:  \\begin{equation} \\begin{aligned} \\frac{\\partial \\mathcal{L}_t}{\\partial h_{\\text{unlike}}} & =\\left(0-p_{\\text {unlike}}\\right)- \\frac{p_{\\text {unlike}}}{1-p_{\\text {unlike}}}\\left(1-p_{\\text {unlike}}\\right) \\\\ &=-2*p_{\\text {unlike}} \\end{aligned} \\end{equation}"}
{"question": "Consider the paper that introduces the method that has the third highest Avg score on the GLUE task. How does the Hyperformer's parameter efficiency compare to the adapter model in terms of scaling with the number of tasks and layers?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the second highest Avg score on GLUE task?", "answer_anchor": "Hyperformer", "question_reference": "How does the \\methodefficient model's parameter efficiency compare to the \\adapter model in terms of scaling with the number of tasks and layers?", "explanation_reference": "The \\methodefficient model is more parameter-efficient compared to the \\adapter model because, for \\methodefficient, the total number of parameters for hypernetworks remains constant, while the task feature parameters scale with the number of tasks or layers times a small factor \\(t\\), where \\(t \\ll 2hd+2h\\) and \\(T+L \\ll TL\\), making it much more efficient especially in settings with a large number of layers and tasks.", "evidence_reference": "In settings with a large number of layers and a large number of tasks, since \\(t \\ll 2hd{+}2h\\) and \\(T{+}L \\ll TL\\), our method is much more parameter-efficient compared to \\adapter. In the current setting, the term \\(hd\\) is the largest term, and the factor \\(2TL\\) for \\adapter is larger than the factor \\(t\\) for \\methodefficient."}
{"question": "Consider the paper that introduces the method that achieves the highest Hits@1 score in the MQA-1H dataset. What specific pre-training task is designed for the model proposed in the paper to ensure the unification of retrieval and reasoning models in parameter learning?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets the highest Hits@1 score in MQA-1H dataset?", "answer_anchor": "UniKGQA", "question_reference": "What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning?", "explanation_reference": "The specific pre-training task designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning is the question-relation matching task. This task is directly mentioned as a shared pre-training task for both models, indicating its role in unifying the parameter learning process by focusing on matching the semantics of questions with the relations in the knowledge graph.", "evidence_reference": "For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies."}
{"question": "Consider the paper that introduces the method that demonstrates the lowest score in the CSQA2.0 dev/dev* task. What specific methodological approach was found to mitigate performance regressions on public NLP datasets during the RLHF fine-tuning process of the model proposed in the paper?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method demonstrates the lowest score in CSQA2.0 dev task?", "answer_anchor": "ChatGPT", "question_reference": "What specific methodological approach was found to mitigate performance regressions on public NLP datasets during the RLHF fine-tuning process?", "explanation_reference": "The approach of mixing in pretraining gradients during PPO training was identified as a method to mitigate performance regressions on public NLP datasets. This method involves incorporating gradients from the pretraining data distribution into the PPO fine-tuning process, which helps maintain the capabilities of the pretrained model and addresses the performance regressions.", "evidence_reference": "We mitigate the regressions by mixing in pretraining gradients during PPO training. We use 8 times more pretraining examples than the number of the RL training episodes... For each minibatch, we compute the PPO gradients and pretraining gradients in consecutive steps and accumulate them both into the gradient buffers. We multiply the pretraining gradients by a coefficient, $\\gamma=27.8$ (see Equation~\\ref{eq2}), to control the relative strength of gradients from PPO and pretraining distributions."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than BROS and a higher F1 score than LayoutXLM. How does the performance improvement of the model proposed in the paper achieved through multi-task learning with entity labeling compare to the performance improvement achieved through data augmentation in the FUNSD dataset?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having lower F1 score than BROS and higher F1 score than LayoutXLM?", "answer_anchor": "SERA", "question_reference": "How does the performance improvement achieved through multi-task learning with entity labeling compare to the performance improvement achieved through data augmentation in the FUNSD dataset?", "explanation_reference": "The performance improvement achieved through multi-task learning with entity labeling is 0.86% F1, while the performance improvement achieved through data augmentation is about 1.1% F1. This comparison is derived from the discussion on training strategies where the effectiveness of multi-task learning and data augmentation on the FUNSD dataset is evaluated.", "evidence_reference": "Relation extraction task can improve by about 0.86% F1 while labeling model performance drops a little from Table~\\ref{trainstrategy}. Models trained on the training data after augmentation improve performance by about 1.1% F1 with auto label and 2.9% F1 with gold label."}
{"question": "Consider the paper that introduces the dataset which has fewer validation set samples than GoGenSum but more samples than FactCC. Which model demonstrated the highest percentage of factual hallucinations among the evaluated systems according to the study's findings?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset having less val set samples than GoGenSum but more samples than FacCC?", "answer_anchor": "XSumFaith", "question_reference": "Based on the study's findings, which model demonstrated the highest percentage of factual hallucinations among the evaluated systems?", "explanation_reference": "The question targets a specific detail regarding the performance of different models in producing factual hallucinations, which is a nuanced aspect of the paper's analysis on hallucinations in summarization. The \\bencdec model is identified as having the highest percentage of factual hallucinations, indicating its relative strength in integrating background knowledge to generate summaries that, while may contain hallucinated content, are factually accurate.", "evidence_reference": "In total, 34.7\\% of the \\bencdec abstracts were faithful (26.9\\%) and/or factual (+7.8\\%). This is 7.4\\% absolute better than the next-best model (\\ptgen). The number of unfaithful yet factual summaries for \\bencdec, 7.8\\%, was also the highest."}
{"question": "Consider the paper that introduces the method that is at the rightmost part of the figure. How does GeDi potentially emerge with zero-shot capability to generalize to new control codes from its training on generative classifiers?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method on the right most coordinates of the figure?", "answer_anchor": "GeDi", "question_reference": "How does GeDi's zero-shot capability to generalize to new control codes potentially emerge from its training on generative classifiers?", "explanation_reference": "GeDi's ability to generalize to new control codes in a zero-shot manner is likely because generative classifiers can classify unseen topics zero-shot from learned word embeddings. This foundational aspect of generative classifiers enables GeDi to guide generation towards a wide array of topics beyond its initial training scope.", "evidence_reference": "This ability likely emerges because generative classifiers can classify unseen topics zero-shot from learned word embeddings \\citep{yogatama2017generative}, and GeDi uses generative classifiers to guide generation."}
{"question": "Consider the paper that introduces the model shown in the figure that performs most similarly to GPT-3.5-Turbo for the 'Plausible' and 'Foreign' scenarios. What are the specific conditions (C') and (C'') assumed for the convergence of the cumulative distribution function resulting from the DA-WR algorithm as proposed in the paper?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shown in the figure has a similar performance to GPT-3.5-Turbo?", "answer_anchor": "LLaMA-30B", "question_reference": "What are the specific conditions (C') and (C'') assumed for the convergence of the cdf resulting from the DA-WR algorithm?", "explanation_reference": "The conditions (C') and (C'') are necessary assumptions for ensuring the convergence of the cumulative distribution function (cdf) resulting from the Data Augmentation - Weighted Resampling (DA-WR) algorithm to the target distribution as the sample size n approaches infinity. These conditions are related to the differences in indicators for whether a value is less than or equal to x and the differences in weights between the initial and augmented datasets, respectively, and they must tend to zero faster than 1/n.", "evidence_reference": "\\item {\\bf (C')} : $\\max_{i=1,\\cdots,n} |\\mathbb{I}_{X_i\\leq x} - \\mathbb I_{X_i'\\leq x}| = o(1/n)$ \\item {\\bf (C'')} : $\\max_{i=1,\\cdots,n} |q_i -q_i' | =o(1/n)$"}
{"question": "Consider the paper that introduces the method that achieves a Hits@1 score of 0.281 in the YAGO43kET dataset. How does the model proposed in the paper, specifically its exponentially weighted pooling method in CET, ensure that every embedding receives sufficient training?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets Hits@1 score equal to 0.281 in YAGO43kET datast?", "answer_anchor": "RGCN", "question_reference": "How does the exponentially weighted pooling method in CET ensure that every embedding receives sufficient training?", "explanation_reference": "The exponentially weighted pooling method is designed to have a similar effect to max-pooling but ensures that every input receives a gradient, thus ensuring every embedding gets sufficient training. This method addresses the limitation of max-pooling, where only a small part of the input receives a gradient, potentially leading to insufficient training of some embeddings.", "evidence_reference": "Choosing the max value as the final result makes only a small part of the input get the gradient which means some embeddings may not be sufficiently trained. As a result, the model may fail to represent every attribute of an entity accurately. In practice, we adopt an exponentially weighted pooling method similar to softpool~\\citep{DBLP:journals/corr/abs-2101-00440}: \\begin{align} & R_{u, i}=\\mathrm{pool}(\\{R_{u, i}^{Agg2T}, R_{(n_r, n_e), i}^{N2T} \\notag \\\\ & \\vert \\; \\forall (n_e, n_r)\\in \\mathcal{N}(u)\\}),for\\; i \\in 1,2,\\dots, L, \\end{align} \\begin{equation} \\mathrm{pool}(\\{x_1, x_2,...,x_n\\})=\\sum_{i=1}^{n}w_ix_i, \\end{equation} \\begin{equation} \\label{equation:weight} w_i = \\frac{\\exp \\alpha x_i}{\\sum_{k=1}^{n}\\exp \\alpha x_k}, \\end{equation} where $R_{u, i}\\in \\mathbb{R}$ is the relevance score between entity $u$ and type i. $\\alpha \\in \\mathbb{R}^+$ is a hyperparameter that controls the temperature of the pooling process. The higher $R_{u, i}$ means entity $u$ is more likely to have type i. This pooling method has a similar effect to max-pooling but can generate a gradient for every input which ensures every embedding gets sufficient training."}
{"question": "Consider the paper that introduces the model that achieves the highest score on the SST-2 dataset. In its ablation study for different masking procedures, what was the MNLI Dev set result when using a 100% 'Mask' strategy with no 'Same' or 'Rnd' strategies?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model that demonstrates the highest score on SST-2 dataset?", "answer_anchor": "BERT", "question_reference": "In the ablation study for different masking procedures, what was the MNLI Dev set result when using a 100% 'Mask' strategy with no 'Same' or 'Rnd' strategies?", "explanation_reference": "The question specifically targets the results of an ablation study focused on different masking strategies used during the pre-training of BERT. The answer, 84.3, directly corresponds to the MNLI Dev set result for the scenario where 100% of the target tokens were replaced with the [MASK] symbol, with no tokens kept the same or replaced with a random token. This detail is explicitly provided in the table under the ablation study for different masking procedures, making it a precise answer derived from the paper's content.", "evidence_reference": "100%&0%&0%&84.3&94.9&94.0"}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, GC dataset. What specific architectural feature allows the model proposed in the paper, EmBERT, to handle long-horizon planning effectively?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the best score in Seen, Val, GC dataset?", "answer_anchor": "EmBERT", "question_reference": "What specific architectural feature allows EmBERT to handle long-horizon planning effectively?", "explanation_reference": "The Segment-Level Recurrent Action Decoder is designed to model long trajectories with recurrent segment-level state reuse, effectively handling the quadratic complexity of the self-attention mechanism in transformers for long sequences. This architectural feature specifically addresses the challenge of long-horizon planning in the ALFRED benchmark.", "evidence_reference": "Inspired by the TransformerXL model, we design the Segment-Level Recurrent Action Decoder architecture that models long trajectories with recurrent segment-level state reuse. At training time we divide trajectories into temporal segments of size $s$. Given two consecutive segments, $\\mathbf{s}_i$ and $\\mathbf{s}_{i+1}$, \\modelac\\ caches the representations generated for segment $\\mathbf{s}_i$. The computed gradient does not flow from $\\mathbf{s}_{i+1}$ to $\\mathbf{s}_i$, but cached representations are used as extended context. When predicting the next action, the model can still perform self-attention over the previous segment representations, effectively incorporating additional contextual information that spans a high number of previous timesteps."}
{"question": "Consider the paper that examines the dataset which has the largest number of Queries|Aspects in the ABS category. What specific aspect of the 'Software' domain in the model proposed by the paper exhibits a notable discrepancy in precision during aspect discovery, and how is this discrepancy quantitatively demonstrated?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset with the most number of Queries|Aspects in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What specific aspect of the 'Software' domain exhibits a notable discrepancy in precision during aspect discovery, and how is this discrepancy quantitatively demonstrated?", "explanation_reference": "The question focuses on a detailed part of the paper that discusses the aspect discovery performance within the 'Software' domain. It asks for a specific quantitative measure (precision) related to aspect discovery, which is directly answered by the paper's findings on the precision rate achieved under a certain threshold setting.", "evidence_reference": "We observed a general trend of low precision for aspect discovery. We hypothesize that this is due to limited target aspects for each article; correctly extracted aspects affect negatively to precision if they do not exist in the target article. To quantify this, 10 random articles are selected from the validation set in Software domain. For each article, we extract 10 sentences labeled with the highest confidence for each of the 10 aspects, resulting in 1,000 sentences in total. Each sentence is annotated with binary labels indicating whether it is correctly associated with the aspect or not. With the threshold $\\lambda$ set to 0.9, we achieved the precision of 45.1, which shows that the aspect discovery has the ability to extract aspects, but not as good at extracting \\textit{relevant} aspects for the article."}
{"question": "Consider the paper that introduces the dataset represented by the leftmost bar in the figure. What specific architectural feature allows the model proposed in the paper to simultaneously perform language modeling and correctness prediction tasks?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset represented on the leftmost of the figure?", "answer_anchor": "GSM8K", "question_reference": "What specific architectural feature allows the verifier models to simultaneously perform language modeling and correctness prediction tasks?", "explanation_reference": "The specific architectural feature that enables verifier models to undertake both language modeling and correctness prediction tasks is the implementation of a small scalar head. This scalar head operates on the logits outputted by the language model\u2019s final unembedding layer, allowing for predictions on a per-token basis. This design choice facilitates the verifier's dual functionality by shifting and scaling the logit corresponding to a special token in the vocabulary reserved for the verifier\u2019s predictions, while other tokens continue to represent the language modeling objective.", "evidence_reference": "We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model\u2019s final unembedding layer. Specifically, the bias and gain shift and scale the logit corresponding to a special token in the vocabulary."}
{"question": "Consider the paper that introduces the model that exhibits the most negative Spearman's Correlation Coefficient. What specific methodological adjustment did the authors make to the initialization scheme of the model proposed in the paper to account for its model depth?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model in the figure has the highest Spearman's Correlation?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological adjustment did the authors make to the initialization scheme of the DialoGPT model to account for its model depth?", "explanation_reference": "The authors mention modifying the initialization scheme to account for model depth as part of their methodological adjustments to the GPT-2 architecture for the DialoGPT model. This detail is a specific methodological choice aimed at improving the model's performance, likely by ensuring stability in deeper network layers.", "evidence_reference": "Our model inherits from GPT-2 \\cite{gpt2}, a 12-to-48 layer transformer with layer normalization, a initialization scheme that accounts for model depth that we modified, and byte pair encodings \\cite{bpe} for the tokenizer."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than Doc2Graph and a higher F1 score than GNN+MLP. What specific algorithmic enhancement does the model proposed in the paper apply to handle the issue of tail-sharing edges in the spatial dependency graphs, and what is its impact on the F1 score for the \\cord\\ dataset?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having lower F1 score than Doc2Graph and higher F1 score than GNN+MLP?", "answer_anchor": "SPADE", "question_reference": "What specific algorithmic enhancement does SPADE apply to handle the issue of tail-sharing edges in the spatial dependency graphs, and what is its impact on the F1 score for the \\cord\\ dataset?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically designed to address the issue of tail-sharing edges in spatial dependency graphs by iteratively trimming these edges and generating new ones until the process becomes self-consistent. This algorithmic enhancement is critical for improving the model's ability to accurately parse and extract information from documents, as evidenced by the reported improvements in the F1 score for the \\cord\\ dataset.", "evidence_reference": "Using this property, we integrate Tail Collision Avoidance algorithm (\\caabb) that iteratively trims the tail-sharing-edges and generate new edges until the process becomes self-consistent (Section \\ref{sec:graph_gen}). $F_1$ increases by +1.0\\% and +0.8\\% with and without the oracle upon the integration (2nd row, \\cordabb)."}
{"question": "Consider the paper that introduces the score described as the 'influence of any example z towards another example z' by tracking their gradient dot products,' where 'we generate the self-influence scores where z = z'. What is the Pearson correlation between the first-order approximate influences and heuristic influences over multiple checkpoints with projections of different sizes, as measured by TracIn?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2002.08484", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the name of the score with description 'Influence of any example z towards another example z' by tracking their gradient dot products. We generate the self-influence scores where z = z''?", "answer_anchor": "TracIn", "question_reference": "What is the Pearson correlation between the first-order approximate TrackIn influences and heuristic influences over multiple checkpoints with projections of different sizes?", "explanation_reference": "The Pearson correlation value of 0.978 indicates a very high degree of linear correlation between the first-order approximate TrackIn influences and heuristic influences over multiple checkpoints with projections of different sizes, suggesting that the first-order approximation is a reliable method for estimating the influence of training examples on test examples.", "evidence_reference": "The overall Pearson correlation between the two quantities is 0.978, which is sufficiently high."}
{"question": "Consider the paper that introduces the method which is represented by the lavender color. How does the model's, proposed by the paper, zero-shot capability to generalize to new control codes potentially emerge from its training on generative classifiers?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method represented by the lavender color?", "answer_anchor": "GeDi", "question_reference": "How does GeDi's zero-shot capability to generalize to new control codes potentially emerge from its training on generative classifiers?", "explanation_reference": "GeDi's ability to generalize to new control codes in a zero-shot manner is likely because generative classifiers can classify unseen topics zero-shot from learned word embeddings. This foundational aspect of generative classifiers enables GeDi to guide generation towards a wide array of topics beyond its initial training scope.", "evidence_reference": "This ability likely emerges because generative classifiers can classify unseen topics zero-shot from learned word embeddings \\citep{yogatama2017generative}, and GeDi uses generative classifiers to guide generation."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. What is the K-L divergence between its predicted and ground-truth question type distributions on the test set?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model is in the first row of the table?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the predicted and ground-truth question type distributions on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how closely the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the method that achieves a score of 31.8 in the Seen, Test, SR dataset. What specific architectural feature allows the model proposed in the paper to handle long-horizon planning effectively?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the score of 31.8 in Seen, Test, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "What specific architectural feature allows EmBERT to handle long-horizon planning effectively?", "explanation_reference": "The Segment-Level Recurrent Action Decoder is designed to model long trajectories with recurrent segment-level state reuse, effectively handling the quadratic complexity of the self-attention mechanism in transformers for long sequences. This architectural feature specifically addresses the challenge of long-horizon planning in the ALFRED benchmark.", "evidence_reference": "Inspired by the TransformerXL model, we design the Segment-Level Recurrent Action Decoder architecture that models long trajectories with recurrent segment-level state reuse. At training time we divide trajectories into temporal segments of size $s$. Given two consecutive segments, $\\mathbf{s}_i$ and $\\mathbf{s}_{i+1}$, \\modelac\\ caches the representations generated for segment $\\mathbf{s}_i$. The computed gradient does not flow from $\\mathbf{s}_{i+1}$ to $\\mathbf{s}_i$, but cached representations are used as extended context. When predicting the next action, the model can still perform self-attention over the previous segment representations, effectively incorporating additional contextual information that spans a high number of previous timesteps."}
{"question": "Consider the paper that introduces the method that is in the last row of the Full Training category. How does the paraphrasing-based approach differ from DSM, in handling instances with simple expressions, according to the limitations section?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is in the last row of teh Full Training category?", "answer_anchor": "DSM", "question_reference": "How does the paraphrasing-based approach differ from the proposed method in handling instances with simple expressions, according to the limitations section?", "explanation_reference": "The distinction between the paraphrasing-based approach and the proposed method is highlighted in the limitations section, where it is mentioned that for instances with simple expressions, the paraphrasing-based method may perform better by focusing on words, whereas the proposed method concentrates more on sentence structure.", "evidence_reference": "For example, the ground truth is 'What religion in Australia that influenced Arthur Schopenhauer?', the paraphrasing-based approach generates 'What faith in Australia inspired Arthur Schopenhauer?'. Our method generates  'What is the religion in Australia that influenced  Arthur Schopenhauer? '. We observe that the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 12.79% TP. What is the inference time for feature extraction using this model compared to VinVL and M2 Transformer?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shows 12.79% TP?", "answer_anchor": "GRIT", "question_reference": "What is the inference time for feature extraction using GRIT compared to VinVL and M2 Transformer?", "explanation_reference": "The inference time for feature extraction using GRIT is significantly lower than that of VinVL and M2 Transformer, demonstrating GRIT's computational efficiency in this aspect.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that has a Test Accuracy of 79.6. What specific methodological limitation does the paper acknowledge regarding the model's ability to incorporate commonsense knowledge, and how does it suggest addressing this limitation in future work?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which Seq2Seq model shows 79.6 Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific methodological limitation does the paper acknowledge regarding the solver's ability to incorporate commonsense knowledge, and how does it suggest addressing this limitation in future work?", "explanation_reference": "The paper acknowledges a limitation in the solver's ability to incorporate commonsense knowledge, which is essential for solving real-world MWP scenarios that require understanding of concepts like '1km = 1000m' or 'one day = 24 hours'. The suggested approach to address this limitation in future work is by injecting commonsense knowledge into MWP solvers, indicating a direction for enhancing the solver's capability to handle problems requiring such knowledge.", "evidence_reference": "As mentioned in \\cite{lin2020numersense,DBLP:journals/corr/abs-2107-13435}, MWP solving in the real-word scenario requires many commonsense knowledge, e.g., 1km = 1000m and one day = 24 hours. When these commonsense constants are not explicitly given in the problem description, our MWP solver has no chance to solve problems that require them. A future direction could be injecting commonsense knowledge into MWP solvers."}
{"question": "Consider the paper that introduces the benchmark that achieves a 'Generation Token Length' near 400 when the 'Compression Ratio' is 1. What is the average human-rater performance for the 'Temporal Sequences' task in the suite proposed by the paper?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which benchmark gets 'Generation Token Length' near 400 when 'Compression Ratio' is 1?", "answer_anchor": "BBH", "question_reference": "What is the average human-rater performance for the 'Temporal Sequences' task in the BIG-Bench Hard (BBH) suite?", "explanation_reference": "The average human-rater performance for the 'Temporal Sequences' task is directly provided in the detailed results table for the BBH tasks.", "evidence_reference": "Temporal Sequences & 25.0 & 52.2 & 90.8 & 100 & 33.6 & 67.2 & 77.6 & 96.8 & 39.6 & 78.8"}
{"question": "Consider the paper that introduces the score described as a \"fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits) by the model\". What is the reason, according to the paper, that the Predictive $\\mathcal{V}$-Information (PVI) is more sensitive to over-fitting than traditional performance metrics like test accuracy?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2110.08420", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the name of the score with description 'Fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits by the model.'?", "answer_anchor": "PVI", "question_reference": "Based on the predictive $\\mathcal{V}$-information framework, why does the paper argue that $\\mathcal{V}$-usable information is more sensitive to over-fitting than traditional performance metrics like test accuracy?", "explanation_reference": "The paper discusses that as models start over-fitting, they become less certain about the correct label, which causes the conditional $\\mathcal{V}$-entropy ($H_\\mathcal{V}(Y|X)$) to rise and thus $\\mathcal{V}$-information to decline. This decline in $\\mathcal{V}$-information occurs even while the majority of the probability mass is still on the correct label, making $\\mathcal{V}$-usable information a more sensitive indicator of over-fitting than test accuracy, which might only slightly decline from its peak.", "evidence_reference": "At epoch 10, the $\\mathcal{V}$-information is at its lowest for all models, although the SNLI test accuracy has only declined slightly from its peak, as seen in Figure \\ref{fig:vinfo-vs-accuracy}. This is because the models start becoming less certain about the correct label long before they start predicting the wrong label. This causes $H_\\mathcal{V}(Y|X)$ to rise---and thus \\vinfo to decline---even while most of the probability mass is still placed on the correct label."}
{"question": "Consider the paper that introduces the method that is missing a result for the WQ-M task in the table. What specific aspect of the paraphrasing-based approach does the paper identify as potentially more effective for instances with simple expressions compared to the model proposed in the paper?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method does not show a result for WQ M task?", "answer_anchor": "DSM", "question_reference": "What specific aspect of the paraphrasing-based approach does the paper identify as potentially more effective for instances with simple expressions compared to their method?", "explanation_reference": "The paper identifies that for instances with simple expressions, the paraphrasing-based approach may achieve better performance because it focuses on words, implying a more effective word-level diversity compared to their method which focuses more on the structure of the sentences.", "evidence_reference": "However, for instances with simple expressions, the paraphrasing-based method may achieve better performance...the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the method that has a score of 73.6 in the CB dataset with 4-shot prompting. How does the performance of the model proposed in the paper with 12 source tasks compare to its performance with 6 source tasks on the MRQA benchmark?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having score of 73.6 in CB dataset with 4-shot prompting?", "answer_anchor": "MPT", "question_reference": "How does the performance of MPT with 12 source tasks compare to MPT with 6 source tasks on the MRQA benchmark?", "explanation_reference": "The performance of MPT with 12 source tasks is slightly better than MPT with 6 source tasks on the MRQA benchmark, as indicated by the average performance improvement shown in the results.", "evidence_reference": "MPT (w/ 6 Source Tasks) Avg. = 72.2, MPT (w/ 12 Source Tasks) Avg. = 72.6"}
{"question": "Consider the paper that introduces the method that has a score of 71.4 in the CB dataset with 4-shot prompting. Why was the LM adapted version of T5 chosen for prompt tuning in the experimental setup of the model proposed in the paper, despite the original version showing better performance in preliminary experiments for model tuning approaches?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having score of 71.4 in CB dataset with 4-shot prompting?", "answer_anchor": "SPoT", "question_reference": "In the experimental setup for SPoT, why was the LM adapted version of T5 chosen over the original version for prompt tuning, despite the original version showing better performance in preliminary experiments for model tuning approaches?", "explanation_reference": "The LM adapted version of T5 was chosen for prompt tuning because it was found to be easier to optimize for PromptTuning, despite the original version of T5 1.1 showing better performance in preliminary experiments for model tuning approaches.", "evidence_reference": "Our frozen models are built on top of the pre-trained T5 checkpoints of all sizes... In our experiments with SPoT, we leverage the LM adapted version of T5, which was found to be easier to optimize for PromptTuning. In preliminary experiments, we found that using the original version of T5 1.1 (which was pre-trained exclusively on span corruption) for model tuning approaches results in better performance than using the LM adapted version."}
{"question": "Consider the paper that introduces the method which exhibits an accuracy of 79.44% in the CJO22 task. What specific method does the model, proposed in the paper, use to initially segment law articles into communities before applying the graph distillation operator?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shows 79.44 accuracy in CJO22 task?", "answer_anchor": "LADAN", "question_reference": "What specific method does LADAN use to initially segment law articles into communities before applying the graph distillation operator?", "explanation_reference": "The paper specifies that to initially segment law articles into communities, a fully-connected graph is constructed where the weight on the edge between a pair of law articles is defined by the cosine similarity between their TF-IDF (Term Frequency-Inverse Document Frequency) representations. This method is used before applying the graph distillation operator to identify probably confusing law articles by removing edges with weights less than a predefined threshold from the graph, resulting in several disconnected subgraphs or communities.", "evidence_reference": "To find probably confusing law articles, we first construct a fully-connected graph $G^*$ for all law articles $\\mathcal{L}$, where the weight on the edge between a pair of law article $L_i, L_j\\in \\mathcal{L}$ is defined as the cosine similarity between their TF-IDF (Term Frequency-Inverse Document Frequency) representations $\\mathbf{tf\\_idf}_i$ and $\\mathbf{tf\\_idf}_j$."}
{"question": "Consider the paper that introduces the method which is placed below the row for R-Former but above the row for NeurJudge. What specific methodological limitation does the reliance on the graph construction layer's threshold setting introduce in the model's ability to distinguish confusing law articles?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown below the row of R-Former but above the row of NeurJudge?", "answer_anchor": "LADAN", "question_reference": "What specific methodological limitation does the reliance on the graph construction layer's threshold setting introduce in the LADAN model's ability to distinguish confusing law articles?", "explanation_reference": "The reliance on the graph construction layer's threshold setting for dividing law articles into communities introduces a methodological limitation in terms of accuracy. If the threshold is not optimally set, it can lead to inaccurate community detection, which in turn affects the model's ability to effectively distinguish between confusing law articles. This is because the graph distillation operator's performance is contingent on the accuracy of the law article communities obtained by the graph construction layer.", "evidence_reference": "GCL is more critical than GDO because GDO has a limited performance when the law article communities obtained by GCL are not accurate. When removing both GCL and GDO, the accuracy of LADAN decreases to that of HARNN+MTL, which powerfully demonstrates the effectiveness of our method exploiting differences among similar law articles."}
{"question": "Consider the paper that introduces the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. What specific limitation does the Alpaca-LoRA model face in comparison when applied to the datasets mentioned in the paper?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2309.03118", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "Knowledge Solver", "question_reference": "What specific limitation does the Alpaca-LoRA model face in comparison to the Knowledge Solver (KSL) approach when applied to the datasets mentioned in the paper?", "explanation_reference": "The Alpaca-LoRA model has a maximum sequence length of 512, which is a specific limitation when compared to the Knowledge Solver (KSL) approach. This limitation is significant because the interactive inference method used by KSL generally requires input sequence lengths longer than 512, which Alpaca-LoRA cannot accommodate. This constraint hinders Alpaca-LoRA's performance on the datasets mentioned in the paper, making KSL's approach more effective for tasks requiring longer input sequences.", "evidence_reference": "It's worth noting that Alpaca-LoRA's maximum sequence length is 512, while for our interactive inference method, the input sequence length is generally longer than 512."}
{"question": "Consider the paper that introduces the model that corresponds to the fifth row of the table. What specific mechanism does the Universal Transformer employ to dynamically modulate the number of computational steps needed to process each input symbol?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1807.03819", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model in the fifth row of the table?", "answer_anchor": "UT", "question_reference": "What specific mechanism does the Universal Transformer employ to dynamically modulate the number of computational steps needed to process each input symbol?", "explanation_reference": "The Universal Transformer employs the Adaptive Computation Time (ACT) mechanism to dynamically modulate the number of computational steps (referred to as 'ponder time') needed to process each input symbol based on a scalar halting probability predicted by the model at each step.", "evidence_reference": "Adaptive Computation Time (ACT) \\citep{graves2016adaptive} is a mechanism for dynamically modulating the number of computational steps needed to process each input symbol (called the ``ponder time'') in standard recurrent neural networks based on a scalar \\emph{halting probability} predicted by the model at each step."}
{"question": "Consider the paper that introduces the model that shows 0 accuracy in the COGS-structural dataset. What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model shows 0 accuracy in COGS-structural dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "explanation_reference": "The answer is directly supported by the descriptions provided in the Attention Visualizations section, where it is mentioned that many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult', and that two attention heads, also in layer 5 of 6, are apparently involved in anaphora resolution. These behaviors indicate that the attention heads have learned to perform tasks related to the structural aspects of sentences.", "evidence_reference": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb `making', completing the phrase `making...more difficult'.  Attentions here shown only for the word `making'. Different colors represent different heads. Best viewed in color. Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that discusses the dataset which has 1 language but 13 SM tasks. What specific linguistic phenomenon, as mentioned in the paper, requires familiarity with social media to interpret correctly, especially in the context of the SentiEval dataset?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset has 1 language but 13 SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly, especially in the context of Chinese social media?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity in sentiment analysis with LLMs. It specifically mentions the phenomenon where a seemingly agreeable statement in Chinese, made with a respectful tone, does not necessarily indicate agreement but can be used ironically. This example illustrates the subtlety and context-dependency of language, highlighting the difficulty for models to interpret such nuances without familiarity with specific cultural or social media contexts.", "evidence_reference": "For example, on Chinese social media, a comment '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically. However, this linguistic phenomenon may require familiarity with social media to interpret correctly."}
{"question": "Consider the paper that introduces the model which has a lower mac-F1 score than Longformer but a higher mac-F1 score than CaselawBERT. What specific aspect of its performance on the Contracts dataset is compared to larger models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model having lower mac-F1 score than Longformer but higher mac-F1 score than CaselawBERT?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's performance on the \\contractsdata dataset compared to larger models?", "explanation_reference": "The question targets a detailed aspect of the \\legalbertsmall model's performance, specifically its efficiency and effectiveness on the \\contractsdata dataset compared to larger models. This is a critical analysis question because it asks for a comparison based on the model's size and performance, highlighting the balance between computational resources and task effectiveness.", "evidence_reference": "Most importantly, (iv) we release \\legalbert, a family of \\bert models for the legal domain, intended to assist legal \\nlp research, computational law, and legal technology applications. This family includes \\legalbertsmall, a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"question": "Consider the paper that introduces the method that does not have results in the CSQA2.0 dev and StrategyQA test tasks shown in the table. What specific methodological change did the authors find to be optimal for mitigating performance regressions on public NLP datasets during PPO training, according to their ablations?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method does not show result in CSQA2.0 dev and StrategyQA test task?", "answer_anchor": "ChatGPT", "question_reference": "What specific methodological change did the authors find to be optimal for mitigating performance regressions on public NLP datasets during PPO training, according to their ablations?", "explanation_reference": "The authors conducted ablations to find an optimal method for mitigating performance regressions on public NLP datasets during PPO training. They compared using different amounts of pretraining data while keeping the pretraining loss coefficient constant. They found that using a pretraining data ratio of 8 was a middle ground between training speed and pretraining loss performance, which they chose as optimal for mitigating performance regressions.", "evidence_reference": "We compared using different amount of pretraining data, while keeping the pretraining loss coefficient constant. By increasing the amount of pretraining data, the quality of gradient estimates from the pretraining improves. We found that using a pretraining data ratio of 4, the log probability loss on the pretraining distribution would often increase throughout the course of the training. Some preliminary experiments show better human Likert scores can be achieved with a pretraining data ratio of 32. However, the training time also increases by a few fold. By setting the pretraining data ratio to 8, the training time doubles that of the corresponding experiment without using pretraining mix; we chose this as a middle ground between training speed and pretraining loss performance."}
{"question": "Consider the paper that introduces the method demonstrated by the solid lavender line. What specific computational advantage does the model proposed in the paper have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method demonstrated in the lavendar solid line?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "explanation_reference": "GRIT's computational advantage in feature extraction time over VinVL and ${\\cal M}^2$ Transformer is quantitatively significant, reducing the time to 31 ms compared to 304 ms and 736 ms, respectively. This efficiency is achieved by utilizing a Deformable DETR-based detector that eliminates the need for computationally expensive regional operations such as NMS and RoI Align, which are used in the other methods.", "evidence_reference": "Table \\ref{tab:extraction} shows the comparison on feature extraction. VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms. ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms. \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms."}
{"question": "Consider the paper that introduces the method in the figure that demonstrates the highest Toxicity Probability Score when the number of samples equals 1M. What specific computational advantage does the model proposed in the paper, GRIT, have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method in the figure demonstrates the highest Toxicity Probability Score when number of samples equal to 1M?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "explanation_reference": "GRIT's computational advantage in feature extraction time over VinVL and ${\\cal M}^2$ Transformer is quantitatively significant, reducing the time to 31 ms compared to 304 ms and 736 ms, respectively. This efficiency is achieved by utilizing a Deformable DETR-based detector that eliminates the need for computationally expensive regional operations such as NMS and RoI Align, which are used in the other methods.", "evidence_reference": "Table \\ref{tab:extraction} shows the comparison on feature extraction. VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms. ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms. \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. What is the maximum number of paragraphs that IRCoT can collect during its retrieval process?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2212.10509", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "IRCoT", "question_reference": "What is the maximum number of paragraphs that IRCoT can collect during its retrieval process?", "explanation_reference": "The maximum number of paragraphs that IRCoT can collect is directly mentioned as being capped at 15 to fit within the model's context limit, allowing for the inclusion of a few demonstrations.", "evidence_reference": "We cap the total number of collected paragraphs so as to fit in at least a few demonstrations in the model's context limit."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 12.79% TP. What specific computational advantage does it have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shows 12.79% TP?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "explanation_reference": "The question focuses on the detailed part of the computational efficiency of GRIT compared to other methods, specifically in the context of feature extraction inference time. This detail highlights GRIT's significant improvement in computational speed.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the dataset that has the largest number of instances. What specific threshold value was chosen for the matching score to ensure the inclusion of abstract sentences in the aspect-based summaries during its construction, and how was this value determined?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset with the most number of instances?", "answer_anchor": "QASUM", "question_reference": "What specific threshold value was chosen for the matching score to ensure the inclusion of abstract sentences in the aspect-based summaries during the dataset construction, and how was this value determined?", "explanation_reference": "The chosen threshold value for the matching score, which determines the inclusion of abstract sentences in the aspect-based summaries, was 0.5. This value was determined through manual evaluation, where 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold were randomly picked and assigned to 5 experts for evaluating the dataset quality. The threshold of 0.5 was selected based on these evaluations.", "evidence_reference": "To filter out sentences with limited content overlap, an aspect-based summary includes only abstract sentences with a matching score \\(\\mathcal{S}(x, a)\\) greater or equal to a pre-defined threshold \\(\\lambda\\). To determine the exact value of the threshold, we try \\(\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]\\) and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality. We then choose to use \\(\\lambda=0.5\\)."}
{"question": "Consider the paper that introduces the quant method that achieves a lower score than APQ-ViT but still scores higher than 76.0 on Deit-S with a Weight/Activation (W/A) precision of 6/6. How does its twin uniform quantization approach, as proposed in the paper, address the issue of quantizing post-GELU activation values with a highly asymmetric distribution?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the quant method shows lower score than APQ-ViT but higher than 76.0 on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "How does the twin uniform quantization method proposed in PTQ4ViT address the issue of quantizing post-GELU activation values with a highly asymmetric distribution?", "explanation_reference": "The twin uniform quantization method effectively addresses the issue of quantizing post-GELU activation values, which have a highly asymmetric distribution, by employing two separate quantization ranges. This approach allows for the quantization of positive and negative values with different scaling factors, thereby reducing the quantization error and improving the overall performance of the quantized network.", "evidence_reference": "For activation values after GELU, negative values are located in R1 = $[-2^{k-1}\\Delta_{\\text{R1}}^{g},0]$ and positive values are located in R2=$[0, 2^{k-1}\\Delta_{\\text{R2}}^{g}]$. We also keep $\\Delta_{\\text{R1}}^{g}$ fixed to make R1 just cover the entire range of negative numbers. Since different quantization parameters are used for positive and negative values respectively, the quantization error can be effectively reduced."}
{"question": "Consider the paper that introduces the method that results in a score of 22.4 in the GSM8K dataset. What specific dynamic programming algorithm does the paper tweak for aligning the GPT tokenizer and the T5 tokenizer?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2301.12726", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method demonstrates score of 22.4 in GSM8K dataset?", "answer_anchor": "SpecialFT", "question_reference": "What specific dynamic programming algorithm does the paper tweak for aligning the GPT tokenizer and the T5 tokenizer?", "explanation_reference": "The paper mentions tweaking a textbook dynamic programming algorithm used in bioinformatics for sequence alignment, specifically citing the Needleman\u2013Wunsch algorithm as an example of such an algorithm that their approach is a slight tweak of.", "evidence_reference": "Our dynamic program is a slight tweak of the textbook dynamic programming algorithms used in bioinformatics for sequence alignment (such as the Needleman\u2013Wunsch algorithm~\\cite{Needleman1970AGM}) and in signal processing (such as dynamic time wrapping~\\cite{Senin2008DynamicTW})."}
{"question": "Consider the paper that introduces the dataset which has more dev set samples than UIT-VSMEC but fewer dev set samples than ViSpamReviews. What specific error type in the error analysis indicates a failure due to the model's inability to understand comments with indirect and disrespectful references?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset having more dev set samples than UIT-VSMEC but less dev set samples than ViSpamReviews?", "answer_anchor": "ViHOS", "question_reference": "What specific error type in the error analysis indicates a failure due to the model's inability to understand comments with indirect and disrespectful references?", "explanation_reference": "The error type 'Allusion' directly addresses the model's failure to correctly interpret comments that refer to another person or subject in an indirect and disrespectful manner, indicating a specific kind of error where the model's understanding of context and indirect references is inadequate.", "evidence_reference": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way."}
{"question": "Consider the paper that introduces the model that achieves a BLEURT score of 0.4126. What is the K-L divergence between its prediction results and the ground-truth for question type distribution learning on the test set?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model gets 0.4126 in BLEURT score?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the prediction results of the BERT-based model and ground-truth for question type distribution learning on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how well the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the model that achieves the best P_k score among the models in the first part of the table. What specific approach does the paper employ to improve the model proposed in the paper's ability to generate accurate sentence positions for generative segmentation?", "answer": "", "figure": "locality/2310.11772/comparison_2_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets the best P_k score on the upper part of the table??", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What specific approach does the paper employ to improve the model's ability to generate accurate sentence positions for generative segmentation?", "explanation_reference": "The paper describes a method to enhance the model's implicit ability to convey position information from input to its decoder, crucial for producing accurate sentence positions in generative segmentation. This is achieved by substituting the fixed BOS token index with the $i^\\text{th}$ vocabulary token embedding for the $i^\\text{th}$ sentence, thereby providing unambiguous position information to the decoder without employing custom schemes like dedicated sentence position embeddings.", "evidence_reference": "At the encoder input for the $i^\\text{th}$ sentence,  we use the $i^\\text{th}$ vocabulary token embedding in place of a fixed BOS token index. Formally, in contrast to \\eqref{eqn:bos}, we set \\begin{equation*} t_{1_1} = 0, \\hspace{2mm}  t_{2_1} = 1, \\hspace{2mm} \\ldots,  t_{|S|_1} = |S|-1. \\end{equation*}"}
{"question": "Consider the paper that introduces the method that is missing a result for the WQ-M task in the table. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions proposed by the paper?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method does not show a result for WQ M task?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the model which is placed fourth in the table. What specific aspect of the model's performance on the Contracts dataset demonstrates its efficiency compared to larger models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shown in the fourth row of the table?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's performance on the \\contractsdata dataset demonstrates its efficiency compared to larger models?", "explanation_reference": "The question targets a detailed aspect of the \\legalbertsmall model's performance, specifically its efficiency and effectiveness on the \\contractsdata dataset compared to larger models. This is a critical analysis question because it asks for a comparison based on the model's size and performance, highlighting the balance between computational resources and task effectiveness.", "evidence_reference": "Most importantly, (iv) we release \\legalbert, a family of \\bert models for the legal domain, intended to assist legal \\nlp research, computational law, and legal technology applications. This family includes \\legalbertsmall, a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"question": "Consider the paper that introduces the method that demonstrates the second lowest Acc-7 score on MOSI. What specific future application of low-rank tensors in the context of attention models, as proposed in the paper, is suggested for exploration based on the study's findings?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1806.00064", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method demonstrates the lowest Acc-7 score on MOSI?", "answer_anchor": "TFN", "question_reference": "What specific future application of low-rank tensors in the context of attention models is suggested for exploration based on the findings of this study?", "explanation_reference": "The suggestion for future work implies that the findings and methodologies of the current study could be beneficially applied to the development of more memory and computationally efficient attention models by utilizing low-rank tensors. This inference is drawn from the concluding remarks about potential future directions.", "evidence_reference": "Future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive."}
{"question": "Consider the paper that introduces the supervised method that achieves the highest score in 100-shot prompting. Based on the critical analysis of its paper, what specific aspect of the NSP (Next Sentence Prediction) loss's implementation in the original BERT model might have led to the observed discrepancy in performance when it was removed?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which supervised method demonstrates highest scores in 100-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "Based on the critical analysis of the RoBERTa paper, what specific aspect of the NSP (Next Sentence Prediction) loss's implementation in the original BERT model might have led to the observed discrepancy in performance when it was removed?", "explanation_reference": "The question focuses on a detailed aspect of the paper's findings regarding the NSP loss's role in BERT's performance. It critically evaluates the logical coherence of the paper's argument and evidence regarding the NSP loss's impact. The answer directly addresses the question by pinpointing a specific implementation detail that could explain the discrepancy in performance observed when the NSP loss was removed.", "evidence_reference": "It is possible that the original BERT implementation may only have removed the loss term while still retaining the segment-pair input format."}
{"question": "Consider the paper that introduces the model shown in the figure represented by the pink line. What specific condition is required for the DA-WR algorithm to ensure convergence to the target distribution in terms of the maximum difference between indicator functions of the initial and augmented datasets?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shown in the figure is represented by the pink line?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition is required for the DA-WR algorithm to ensure convergence to the target distribution in terms of the maximum difference between indicator functions of the initial and augmented datasets?", "explanation_reference": "The condition required for the DA-WR algorithm to ensure convergence to the target distribution is that the maximum difference between the indicator functions of the initial and augmented datasets, for all x in the support, should be o(1/n). This condition ensures that the difference diminishes as the sample size increases, allowing the cumulative distribution function resulting from the DA-WR algorithm to converge in probabilities to the target distribution as n tends to infinity.", "evidence_reference": "we should say that the imbalanced phenomenon occurs when $F \\\\neq F_0$. To measure the degree of imbalance we can denote by  $\\\\widehat{F}$ and $\\\\widehat{\\\\mathbb{P}}$ the empirical estimators and we propose the following definition: we  face a $(\\\\alpha, \\\\beta)$-imbalanced regression problem if there is a set $\\\\chi\\\\subset\\\\mathcal{X}$ with  $\\\\mathbb{P}_0(\\\\boldsymbol{X}\\\\in\\\\chi) \\\\geq \\\\beta$ such that $\\\\lvert \\\\frac{\\\\widehat{\\\\mathbb{P}}(\\\\boldsymbol{X}\\\\in\\\\chi)}{\\\\mathbb{P}_0(\\\\boldsymbol{X}\\\\in\\\\chi)} - 1 \\\\rvert > \\\\alpha$."}
{"question": "Consider the paper that introduces the method that results in a token-level F1 score equal to 37.03. How does the performance change in the ablation study for WikiHop when the model proposed in the paper is pretrained with only the extra position embeddings being trainable compared to the full model configuration?", "answer": "", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method show a token-level F1 score equal to 37.03?", "answer_anchor": "longformer", "question_reference": "In the ablation study for WikiHop, how does the performance change when \\model is pretrained with only the extra position embeddings being trainable compared to the full \\model configuration?", "explanation_reference": "The ablation study for WikiHop shows that when \\model is pretrained with only the extra position embeddings being trainable, the performance difference compared to the full \\model configuration is -0.3. This indicates a slight decrease in performance when limiting the pretraining to only the extra position embeddings.", "evidence_reference": "\\model (pretrain extra position embed. only) & 73.5 / -0.3"}
{"question": "Consider the paper that introduces the method that achieves an F1 score with a mean of 58.86 in the TAT-QA task. What specific feature of the BERT model's output is utilized in the policy network for dynamic prompting via policy gradient in the model proposed by the paper?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method demonstrates F1 score with mean 58.86 in TAT-QA task?", "answer_anchor": "PromptPG", "question_reference": "What specific feature of the BERT model's output is utilized in the policy network for dynamic prompting via policy gradient in PromptPG?", "explanation_reference": "The policy network in PromptPG uses the BERT model's output for generating contextualized representations of the given problem and candidate examples. Specifically, it utilizes the [CLS] token representation from BERT's output as the problem encoding, which is then used to learn both the semantic similarity provided by the pre-trained BERT model and the hidden logical similarity among the math problems.", "evidence_reference": "To get the contextualized representation of the given problem and candidate examples, we use the BERT~[CLS] token representation as the problem encoding. We add a small linear layer on top of the BERT final pooling layer."}
{"question": "Consider the paper that introduces the dataset located at the bottom left of the figure. What is the primary reason applying Language Adaptive Fine-tuning (LAFT) on the NaijaSenti dataset did not improve performance in the Twitter domain?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset located on the bottom left of the figure?", "answer_anchor": "NaijaSenti", "question_reference": "What is the primary reason applying Language Adaptive Fine-tuning (LAFT) on the Twitter domain did not improve performance?", "explanation_reference": "The paper explains that applying LAFT on the Twitter domain did not improve performance primarily due to the small size of the Twitter data available for pre-training, which is often short and less comprehensive compared to the general domain data.", "evidence_reference": "Interestingly, applying LAFT on the Twitter domain did not improve performance. The main reason for this is the small size of the Twitter data."}
{"question": "Consider the paper that introduces the model that achieves an F1 score of 73.1 in the en_city category. What is the erroneous output fraction for structured summarization models, as proposed by the paper, when tested on the Wiki-727k dataset?", "answer": "", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets 73.1 F1 score in en_city category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What is the erroneous output fraction for structured summarization models when tested on the Wiki-727k dataset?", "explanation_reference": "The erroneous output fraction indicates how frequently the model produces an invalid sentence boundary position. For the QMSum dataset, the structured summarization models did not produce any erroneous segment boundary positions, indicating a high level of accuracy in generating valid sentence indices.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics. \\begin{table}[h] \\small \\centering \\s\\t \\renewcommand{\\arraystretch}{1.4} \\begin{tabular}{cccc} \\toprule Wiki-727K & en\\_city & en\\_disease & QMSum \\\\ \\hline 0.0001 & 0.0025 & 0 & 0 \\\\ \\bottomrule \\end{tabular} \\caption{Fraction of examples with at least one erroneous segment boundary position. This is for the structured summarization models, when tested on the respective test set.} \\label{table:sentpos_nonnumeric} \\end{table}"}
{"question": "Consider the paper that introduces the model that results in the highest Self-BLEU score on the TellMeWhy dataset. What is the primary limitation of using it for event-centric summary generation in educational question generation, as identified in the paper?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model shows the highest Self-BLEU score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What is the primary limitation of using the BART model for event-centric summary generation in educational question generation, as identified in the paper?", "explanation_reference": "The paper identifies the primary limitation of using the BART model for event-centric summary generation as the factuality error problem, indicating that sometimes the system may generate non-factual facts in terms of the original context.", "evidence_reference": "Owing to the factuality error problem of our system, we suggest to further investigate constructing structured knowledge of fairy tales and knowledge-grounded question generation for real-world applications."}
{"question": "Consider the paper that introduces the method that is in the last row of the Full Training category. How does the model proposed in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is in the last row of teh Full Training category?", "answer_anchor": "DSM", "question_reference": "How does the proposed approach in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "explanation_reference": "The paper discusses leveraging external natural questions to diversify question generation, indicating that this method can generate questions with different expressions by covering a broader range of semantic patterns and expressions. This approach implicitly addresses both word-level and structure-level diversity by introducing varied semantic patterns from external sources.", "evidence_reference": "In our approach, we introduce external natural questions to diversify question generation, which can generate questions with different expressions, since these natural questions cover a wider range of semantic patterns and expressions."}
{"question": "Consider the paper that introduces the method which is listed in the table right below the PCP method. How does the utilization of the Multi-Head Interactive Attention (MHIA) module specifically contribute to the performance of this method, in its application to implicit discourse relation recognition?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method in the table right below the PCP method?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF model's utilization of the Multi-Head Interactive Attention (MHIA) module specifically contribute to its performance in implicit discourse relation recognition?", "explanation_reference": "The ablation study demonstrates that each component of the GOLF model, including the MHIA module, plays a crucial role in achieving the model's state-of-the-art performance. The specific impact of removing the MHIA module is quantified in terms of reduced accuracy and F1 scores, indicating its importance in capturing the interaction between discourse arguments effectively.", "evidence_reference": "Table \\ref{tab: ablation} indicates that eliminating any of the four modules would hurt the performance across all three levels and reduce the consistency among multi-level label predictions. At the same time, the Local Hierarchy-aware Contrastive loss contributes mostly."}
{"question": "Consider the paper that introduces the model represented by the blue bar. What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model is demonstrated by the blue bar?", "answer_anchor": "T5", "question_reference": "What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "explanation_reference": "The span-corruption objective with an average span length of 3 slightly outperformed the i.i.d. objective on most non-translation benchmarks, as indicated by the statement 'we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks.'", "evidence_reference": "we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 42.0 in the Seen, Val, GC dataset. What specific technique does the model proposed in the paper leverage to decouple the understanding of visual appearance from the variations in natural language instructions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the score of 42.0 in Seen, Val, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer leverage to decouple the understanding of visual appearance from the variations in natural language instructions?", "explanation_reference": "The question focuses on a detailed aspect of the methodology employed by the Episodic Transformer to address the challenge of understanding complex human instructions in dynamic environments. The answer directly addresses this by specifying the use of synthetic instructions as an intermediate representation, which is a strategy to separate the process of understanding the visual aspects of an environment from the variations inherent in natural language instructions.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest dialogues. What specific strategy does the model proposed in the paper employ to enhance its performance on machine translation tasks during curriculum learning?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has the least number of dialogues from the table?", "answer_anchor": "Multialpaca", "question_reference": "What specific strategy does the model employ to enhance its performance on machine translation tasks during curriculum learning?", "explanation_reference": "The strategy of introducing more multilingual parallel data during curriculum learning is mentioned as significantly boosting the model's performance on translation tasks. This approach likely helps in better aligning the representations across different languages, thereby improving the model's ability to translate between them.", "evidence_reference": "Finally, it is worth noting that introducing more multilingual parallel data during the curriculum learning significantly boost the model performance on translation task."}
{"question": "Consider the paper that introduces the method that has a perplexity of 60. What specific computational advantage does this method have over a unidirectional classifier in terms of the number of forward passes required for computing classification probabilities for next tokens?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having 60 perplexity?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's method of computing classification probabilities for next tokens have over a unidirectional classifier in terms of the number of forward passes required?", "explanation_reference": "The computational advantage is highlighted by the fact that GeDi can compute the classification probabilities for every possible next token using significantly fewer computations compared to a unidirectional classifier, which would require a forward pass for each token in the vocabulary. This efficiency is achieved through GeDi's method of applying Bayes rule for partial sequences during generation, which allows for the computation to be done in two parallel forward passes for the desired and anti-control codes, significantly reducing the number of necessary computations.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-De task in the Test2016 dataset in Previous Image-must Systems. How does its performance with hallucinated visual tokens (\\texttt{V}) compare to using ground-truth visual representations (\\texttt{VM}) on the Multi30K dataset for the Transformer-Tiny model in terms of BLEU score?", "answer": "", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model demonstrates the highest performance in En-De task in Test2016 dataset in Previous Image-must Systems?", "answer_anchor": "VALHALLA", "question_reference": "How does the performance of the model proposed in the paper with hallucinated visual tokens (\\texttt{V}) compare to using ground-truth visual representations (\\texttt{VM}) on the Multi30K dataset for the Transformer-Tiny model in terms of BLEU score?", "explanation_reference": "The performance of \\ours with hallucinated visual tokens (\\texttt{V}) is very similar to when using ground-truth visual representations (\\texttt{VM}), demonstrating the model's strong ability to generate visual representations that are semantically consistent with the ground-truth.", "evidence_reference": "Using Transformer-Tiny as the backbone, \\ours obtains an average $35.4$ BLEU in EN$\\rightarrow$DE and $54.4$ BLEU in EN$\\rightarrow$FR, which is about $2.1$ and $1.4$ BLEU improvements over the text-only baseline. Moreover, \\ours has very similar performance with either hallucinated (\\texttt{V}) or ground-truth representation (\\texttt{VM}), showing strong ability to generate visual representations that are semantically consistent with the ground-truth."}
{"question": "Consider the paper that introduces the model represented by the blue bar. What specific architectural change was made to the Transformer's layer normalization compared to its originally proposed form?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model is demonstrated by the blue bar?", "answer_anchor": "T5", "question_reference": "What specific architectural change was made to the Transformer's layer normalization in the T5 model compared to its originally proposed form?", "explanation_reference": "The T5 model made specific modifications to the original Transformer architecture's layer normalization. Specifically, it simplified layer normalization by placing it outside the residual path and removing the additive bias. This change is orthogonal to the experimental factors considered in the empirical survey of transfer learning.", "evidence_reference": "Our encoder-decoder Transformer implementation closely follows its originally-proposed form \\citep{vaswani2017attention}. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of ``blocks'', each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization \\citep{ba2016layer} is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection \\citep{he2016deep} adds each subcomponent's input to its output."}
{"question": "Consider the paper that introduces the model that corresponds to an F1 score of 65.76 on PDTB-Top. How does the model's utilization of the Multi-Head Interactive Attention (MHIA) module specifically contribute to its performance in implicit discourse relation recognition?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method demonstrates 65.76 F1 score on PDTB-Top?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF model's utilization of the Multi-Head Interactive Attention (MHIA) module specifically contribute to its performance in implicit discourse relation recognition?", "explanation_reference": "The ablation study demonstrates that each component of the GOLF model, including the MHIA module, plays a crucial role in achieving the model's state-of-the-art performance. The specific impact of removing the MHIA module is quantified in terms of reduced accuracy and F1 scores, indicating its importance in capturing the interaction between discourse arguments effectively.", "evidence_reference": "Table \\ref{tab: ablation} indicates that eliminating any of the four modules would hurt the performance across all three levels and reduce the consistency among multi-level label predictions. At the same time, the Local Hierarchy-aware Contrastive loss contributes mostly."}
{"question": "Consider the paper that introduces the model shown on the first line of the table. In the ablation study for different masking procedures, what was the MNLI Dev set result when using a 100% 'Mask' strategy with no 'Same' or 'Rnd' strategies?", "answer": "", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shown on the first line?", "answer_anchor": "BERT", "question_reference": "In the ablation study for different masking procedures, what was the MNLI Dev set result when using a 100% 'Mask' strategy with no 'Same' or 'Rnd' strategies?", "explanation_reference": "The question specifically targets the results of an ablation study focused on different masking strategies used during the pre-training of BERT. The answer, 84.3, directly corresponds to the MNLI Dev set result for the scenario where 100% of the target tokens were replaced with the [MASK] symbol, with no tokens kept the same or replaced with a random token. This detail is explicitly provided in the table under the ablation study for different masking procedures, making it a precise answer derived from the paper's content.", "evidence_reference": "100%&0%&0%&84.3&94.9&94.0"}
{"question": "Consider the paper that introduces the model that scores a 91.5 in the NER task. How does its performance change when further fine-tuning on silver data after already being used for pre-training?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets score of 91.5 in NER task?", "answer_anchor": "AMRBART", "question_reference": "How does the performance change when further fine-tuning the model on silver data after it has already been used for pre-training?", "explanation_reference": "The performance decreases when further fine-tuning the model on silver data after it has already been used for pre-training, indicating that this additional fine-tuning step does not bring improvement and might even harm the model's performance due to the noise in the silver data.", "evidence_reference": "As discussed in Section~\\ref{sec:main-res}, we find that graph pre-training is a better way to make use of silver data compared with fine-tuning. We further investigate whether fine-tuning our model on silver data can still bring improvement. As shown in Table~\\ref{tab:silverdata}, our models achieve the best performance on all tasks and datasets, indicating that further fine-tuning our models on silver data decreases the performance."}
{"question": "Consider the paper that introduces the method shown in the fourth row of the table. What is the effect of using a death mask on the model's performance in the SMAC domain compared to ignoring states in which an agent is dead when computing GAE?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method showing in the fourth row of the table?", "answer_anchor": "MAPPO", "question_reference": "What is the effect of using a death mask on MAPPO's performance in the SMAC domain compared to ignoring states in which an agent is dead when computing GAE?", "explanation_reference": "The ablation studies demonstrate that using a death mask, which involves replacing the value state for a dead agent with a zero state containing the agent's ID, results in superior performance compared to other options, including ignoring states in which an agent is dead when computing GAE. This suggests that handling the non-stationarity introduced by agent deaths appropriately is crucial for MAPPO's performance in the SMAC domain.", "evidence_reference": "Fig.~\\ref{fig:app-Ablation-death} and Fig. \\ref{fig:app-Ablation-ignore} illustrates the influence of the death mask on MAPPO's performance in the SMAC domain."}
{"question": "Consider the paper that introduces the model that scores an 81.5 in the SRL task. What is the performance decrease in Smatch score for AMR parsing on AMR2.0 when it is further fine-tuned on silver data after pre-training?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets score of 81.5 in SRL task?", "answer_anchor": "AMRBART", "question_reference": "What is the performance decrease in Smatch score for AMR parsing on AMR2.0 when further fine-tuning the model on silver data after pre-training?", "explanation_reference": "The performance decrease in Smatch score for AMR parsing on AMR2.0 when further fine-tuning the model on silver data after pre-training is calculated by comparing the Smatch score of the model without further fine-tuning on silver data (85.4) to the score after such fine-tuning (85.1), resulting in a decrease of 0.3.", "evidence_reference": "Ours (large)& 85.4 & 49.8 \\\\ \\quad + silver  & 85.1 & 49.6 \\\\"}
{"question": "Consider the paper that introduces the method that has an F1 score of 53.36. What specific architectural component in the model proposed by the paper is responsible for mapping the representation of each node into the number of target classes?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having 53.36 F1 score?", "answer_anchor": "Doc2Graph", "question_reference": "What specific architectural component in Doc2Graph is responsible for mapping the representation of each node into the number of target classes?", "explanation_reference": "The Node Predictor is explicitly mentioned as the component responsible for mapping the representation of each node into the number of target classes, indicating its role in the classification process within the Doc2Graph framework.", "evidence_reference": "Node Predictor: this is a FC layer, that maps the representation of each node into the number of target classes;"}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 18.4 on the GSM8K dataset. What is the accuracy improvement for the MAWPS dataset when using an external calculator for the model, proposed by the paper, compared to its baseline accuracy?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets 18.4 accuracy in GSM8K dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What is the accuracy improvement for the MAWPS dataset when using an external calculator for the CoT finetuned T5 XXL model compared to its baseline accuracy?", "explanation_reference": "The improvement can be calculated from the reported accuracies in the arithmetic reasoning section. The baseline T5 XXL model's accuracy with a calculator is not directly provided, but its baseline accuracy is given as 54.15%. The CoT finetuned T5 XXL model's accuracy with a calculator is reported as 88.22%. Therefore, the improvement is 88.22% - 54.15% = 34.07%.", "evidence_reference": "The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. \\textbf{MAWPS} & 54.15 & \\textbf{70.41} & 88.22 & 93.00 & 93.66"}
{"question": "Consider the paper that introduces the optimization method that exhibits an R2 score of 0.191. What specific mathematical derivation allows this optimization method to bypass the explicit reward modeling step while optimizing under existing models of human preferences?", "answer": "", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What optimization method show 0.191 R2 score?", "answer_anchor": "DPO", "question_reference": "What specific mathematical derivation allows the DPO algorithm to bypass the explicit reward modeling step while optimizing under existing models of human preferences?", "explanation_reference": "The DPO algorithm leverages an analytical mapping from reward functions to optimal policies, which allows for the transformation of a loss function over reward functions into a loss function over policies. This change-of-variables approach enables the DPO algorithm to bypass the explicit reward modeling step while still optimizing under existing models of human preferences, such as the Bradley-Terry model.", "evidence_reference": "Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop."}
{"question": "Consider the paper that discusses the score described as a \"fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits) by the model\". What is the theoretical basis for the violation of the data processing inequality in the context of PVI, as discussed in the paper?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2110.08420", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the name of the score with description 'Fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits by the model.'?", "answer_anchor": "PVI", "question_reference": "What is the theoretical basis for the violation of the data processing inequality in the context of $\\mathcal{V}$-usable information, as discussed in the paper?", "explanation_reference": "The violation of the data processing inequality is explained by the usefulness of certain types of processing, such as representation learning, which can make prediction easier by making information more accessible, even though it does not increase the Shannon information.", "evidence_reference": "Processing the input with $\\tau$ (e.g., by decrypting the text) can make prediction easier, allowing $I_\\mathcal{V}(\\tau(X) \\to Y) \\geq I_\\mathcal{V}(X \\to Y)$. Although this violates the data processing inequality, it explains the usefulness of certain types of processing, such as representation learning."}
{"question": "Consider the paper that introduces the dataset in the table that has the second shortest average question length. How does the curriculum learning strategy specifically address the challenge of optimizing the model proposed in the paper to learn knowledge encoded in multiple languages simultaneously?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the longest question average length?", "answer_anchor": "Multialpaca", "question_reference": "How does the curriculum learning strategy specifically address the challenge of optimizing LLMs to learn knowledge encoded in multiple languages simultaneously?", "explanation_reference": "The curriculum learning strategy is designed to transfer general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. This is achieved by initially using the whole pre-training dataset to train a base model for commonsense generalization ability, and then transitioning to a subset of the pre-training dataset that boasts superior quality and a greater proportion of multilingual content to strengthen the model's multilingual capabilities.", "evidence_reference": "Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is a significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. To address this issue, we adopt a curriculum learning strategy that ramps up the ratio of high-quality and low-resource languages during training."}
{"question": "Consider the paper that identifies the dataset with the largest number of instances in the ABS category. What specific aspect of the model's baseline performance does the paper identify as a limitation in handling time-sensitive events across multiple documents?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset with the most number of instances in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What specific aspect of the WikiAsp dataset's baseline models' performance does the paper identify as a limitation in handling time-sensitive events across multiple documents?", "explanation_reference": "The paper discusses the challenges faced by the baseline models in summarizing aspects that require content to be presented in a particular order, such as time series events. It specifically mentions that maintaining chronological order when aggregating information from scattered sources across multiple documents adds extra difficulty, highlighting this as a limitation of the models' performance in handling time-sensitive events.", "evidence_reference": "For example, aspects that require summarizing contents in a particular order (\\textit{e.g.}, time series events) in a multi-document setting adds extra difficulty because of the need for correctly ordering scattered (and possibly duplicate) pieces of information from different sources."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. What was the MNLI Dev set result for BERT, specifically in the ablation study when employing a 100% 'Mask' strategy without incorporating any 'Same' or 'Rnd' strategies?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model on the first row of the table?", "answer_anchor": "BERT", "question_reference": "In the ablation study for different masking procedures, what was the MNLI Dev set result when using a 100% 'Mask' strategy with no 'Same' or 'Rnd' strategies?", "explanation_reference": "The question specifically targets the results of an ablation study focused on different masking strategies used during the pre-training of BERT. The answer, 84.3, directly corresponds to the MNLI Dev set result for the scenario where 100% of the target tokens were replaced with the [MASK] symbol, with no tokens kept the same or replaced with a random token. This detail is explicitly provided in the table under the ablation study for different masking procedures, making it a precise answer derived from the paper's content.", "evidence_reference": "100%&0%&0%&84.3&94.9&94.0"}
{"question": "Consider the paper that introduces the model that achieves a 16.5 Top-1 score on the SQuAD dataset. How does its \\ednascore metric correlate with human judgments of both faithfulness and diversity according to Spearman's rank correlation coefficient?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets 16.5 Top-1 score in SQuAD dataset?", "answer_anchor": "Composition", "question_reference": "How does the \\ednascore metric correlate with human judgments of both faithfulness and diversity according to Spearman's rank correlation coefficient?", "explanation_reference": "The \\ednascore metric is designed to jointly measure faithfulness and diversity in generated summaries. According to the paper, it is positively correlated with human judgments of both dimensions, indicating that as \\ednascore increases, the perceived faithfulness and diversity of summaries by human evaluators also increase. This suggests that \\ednascore is an effective metric for capturing qualities that are valued in human assessments of generated text.", "evidence_reference": "In line with previous work \\cite{maynez-etal-2020-faithfulness,kryscinski-etal-2019-neural}, we find that entailment scores are best correlated with faithfulness (moderate, $0.40 \\leq r \\leq 0.59$). Like Self-BLUE, Self-Entailment and Self-BERTScore are also strongly correlated with diversity ratings. Compared to other metrics which capture a single dimension, \\ednascore is positively correlated with both dimensions of diversity \\emph{and} faithfulness."}
{"question": "Consider the paper that introduces the method that is in the last row of the upper half of the table. How does the MetaBINK model leverage the generated synthetic data to improve the quality of few-shot entity linking?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is on the last row of the upper half of the table?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model leverage the generated synthetic data to improve the quality of few-shot entity linking?", "explanation_reference": "The MetaBLINK model improves the quality of few-shot entity linking by adopting a meta-learning mechanism that automatically assigns different weights to each synthetic data instance. This approach allows the model to differentiate the quality of synthetic data for more effective training.", "evidence_reference": "To further differentiate the quality of each synthetic data instance for model training, we design a meta-learning mechanism that can automatically assign different weights to the synthetic data."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. What specific performance gain does the model proposed in the paper, ViTCAP, achieve on the Google-CC dataset compared to the CC-12M model?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method is shown in the penult row?", "answer_anchor": "ViTCAP", "question_reference": "What specific performance gain does ViTCAP achieve on the Google-CC dataset compared to the CC-12M model?", "explanation_reference": "The performance gain of ViTCAP over the CC-12M model on the Google-CC dataset is quantified as a +3.2 improvement in the CIDEr score, indicating that ViTCAP outperforms the CC-12M model by this margin.", "evidence_reference": "ViTCAP  &  $\\\\textbf{108.6}_\\\\text{\\\\color{darkgreen}\\\\textbf{ +3.2}}$"}
{"question": "Consider the paper that introduces the dataset in the table that has the second shortest average question length. What specific advantage does the curriculum learning strategy offer for the model's performance on low-resource languages according to the paper?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the longest question average length?", "answer_anchor": "Multialpaca", "question_reference": "What specific advantage does the curriculum learning strategy offer for \\textsc{Poly}LM's performance on low-resource languages according to the paper?", "explanation_reference": "The curriculum learning strategy is designed to initially focus on high-resource language data (English) and gradually increase the proportion of high-quality, low-resource language data during training. This method facilitates the transfer of learned general knowledge from English to other languages, which is particularly beneficial for improving the model's capabilities in low-resource languages.", "evidence_reference": "The model with curriculum learning has achieved stable progress in mainly all languages in both NLU and MT tasks. First of all, the model performance is enhanced in most low-resource languages, indicating that the general knowledge can be effectively transferred to these languages through raising data proportion."}
{"question": "Consider the paper that introduces the dataset located at the bottom left of the figure. What specific methodological approach did the authors use to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset located on the bottom left of the figure?", "answer_anchor": "NaijaSenti", "question_reference": "What specific methodological approach did the authors use to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "explanation_reference": "The authors addressed the issue of stopwords overlap causing tweets to be collected in an unintended language by collecting tweets based on the geographical locations where the target language is predominantly spoken. This methodological approach leverages the geographical parameters (location, longitude, latitude, and radius) to specify a circular geographic area for tweet collection, thus improving the accuracy of language-specific data collection.", "evidence_reference": "Stopwords overlap across indigenous languages in a multilingual society such as Nigeria. This results in tweets being collected in a language that differs from the query language. To mitigate this, we collected tweets based on locations where a language is predominantly spoken, using the location, longitude, latitude and radius parameters (25 miles) to specify a circular geographic area."}
{"question": "Consider the paper that introduces the method that corresponds to the second highest Acc-5 score on MOSI. What is the primary limitation of the pre-processed features for audio and vision modalities as identified in the paper, and how does it affect the significance of the generated labels for these modalities?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "2102.04830", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method demonstrates the highest Acc-5 score on MOSI", "answer_anchor": "Self-MM", "question_reference": "What is the primary limitation of the pre-processed features for audio and vision modalities as identified in the paper, and how does it affect the significance of the generated labels for these modalities?", "explanation_reference": "The primary limitation identified in the paper regarding the pre-processed features for audio and vision modalities is that these features limit the significance of the generated labels for these modalities. This limitation affects the model's ability to generate meaningful and significant labels for audio and vision modalities, which is crucial for learning modality-specific representations.", "evidence_reference": "We also find that the generated audio and vision labels are not significant enough limited by the pre-processed features."}
{"question": "Consider the paper that introduces the model that achieves a score of 3.84 in the Grounding task. How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the model proposed in the paper and VL-BART models, and what is hypothesized as the reason for this divergence?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the base model tested in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "explanation_reference": "The paper hypothesizes that the divergence in performance on the RefCOCOg task between VL-T5 and VL-BART is due to the different methods of positional encoding used by T5 and BART. Specifically, BART uses learned absolute positional embeddings, which might lead to the model memorizing the positions of training objects, resulting in high training accuracy but low validation accuracy. This hypothesis is supported by the observation of VL-BART's performance drop in the RefCOCOg task compared to VL-T5.", "evidence_reference": "We also observe that our experiments with \\oursb{} on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in self-attention layers instead. We hypothesize that \\oursb{} found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy)."}
{"question": "Consider the paper that introduces the model that achieves a 16.5 Top-1 score on the SQuAD dataset. Why does the model proposed in the paper, specifically Composition Sampling with \\(\\text{FROST}_{++}\\), achieve a higher \\(\\text{EDNA}\\) score compared to other diverse decoding strategies on both summarization datasets?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets 16.5 Top-1 score in SQuAD dataset?", "answer_anchor": "Composition", "question_reference": "In the context of the paper, why does Composition Sampling with \\frost$_{\\hspace*{-.1cm}++}$ achieve a higher \\ednascore compared to other diverse decoding strategies on both summarization datasets?", "explanation_reference": "The paper states that Composition Sampling with \\frost$_{\\hspace*{-.1cm}++}$ is most effective in generating faithful summaries, as demonstrated automatically (with best entailment scores on XSum and CNN/DailyMail) and by humans (with highest ratings on XSum and CNN/DailyMail); these summaries are also diverse, achieving the highest \\ednascore scores on both summarization datasets. This indicates that the method's ability to generate summaries that are both faithful to the input and diverse in content leads to its higher \\ednascore.", "evidence_reference": "Composition(\\frost$_{\\hspace*{-.1cm}++}$) is most effective in generating faithful summaries, as demonstrated automatically (with best entailment scores on XSum and CNN/DailyMail) and by humans (with highest ratings on XSum and CNN/DailyMail); these summaries are also diverse achieving highest \\ednascore scores on both summarization datasets."}
{"question": "Consider the paper that introduces the method in the table that corresponds to the highest ROUGE 2 score. What specific preprocessing step is applied to the Bag-of-Words (BoW) representations before training the model proposed in the paper's flow-based neural topic model?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method in the table that demonstrates the highest ROUGE 2 score?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific preprocessing step is applied to the Bag-of-Words (BoW) representations before training the flow-based neural topic model?", "explanation_reference": "The specific preprocessing step applied to the BoW representations before training the flow-based neural topic model is the removal of stopwords. This step is crucial for reducing noise and focusing on the meaningful content of the documents.", "evidence_reference": "Following \\cite{wang2020friendly}, we preprocess to remove stopwords in the BoW representations."}
{"question": "Consider the paper that introduces the dataset which has the largest number of instances in the ABS category. What specific methodological limitation is highlighted by the authors regarding the aspect discovery stage's performance in the Software domain?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset with the most number of instances in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What specific methodological limitation is highlighted by the authors regarding the aspect discovery stage's performance in the Software domain?", "explanation_reference": "The authors highlight a methodological limitation in the aspect discovery stage, specifically mentioning that despite the model's ability to extract aspects, it struggles with extracting relevant aspects for the article, as evidenced by the low precision observed in the Software domain. This indicates a limitation in the model's aspect classification accuracy, affecting its overall performance in aspect-based summarization tasks.", "evidence_reference": "We observed a general trend of low precision for aspect discovery. We hypothesize that this is due to limited target aspects for each article; correctly extracted aspects affect negatively to precision if they do not exist in the target article."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category. What is the effect of reducing the train wall time by half on the pre-training performance of this method, specifically when growing BERT from L6_D384 to L12_D768?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.06311", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category?", "answer_anchor": "ELLE", "question_reference": "What is the effect of reducing the train wall time by half on the pre-training performance of ELLE when growing BERT from L6_D384 to L12_D768?", "explanation_reference": "Reducing the train wall time by half for the domains Ns, Rev, Bio, and CS results in higher average perplexity (AP) and average increased perplexity (AP+), indicating slower knowledge acquisition and more knowledge forgetting. Additionally, the average F1 score on downstream tasks decreases, showing that less computational time harms the model's ability to apply the learned knowledge effectively.", "evidence_reference": "when given fewer computational budgets, we observe significant performance drops in both pre-training (higher AP and AP+) and downstream tasks (lower average F1). This shows that pre-training with fewer computations would harm PLMs' knowledge acquisition."}
{"question": "Consider the paper that introduces the dataset which includes 1 SM task and 4 languages. What specific method did the study employ to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset with 1 SM task and 4 languages?", "answer_anchor": "NaijaSenti", "question_reference": "What specific method did the study employ to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "explanation_reference": "The study addressed the issue of tweets being collected in a language different from the query language due to stopwords overlap by collecting tweets based on locations where a language is predominantly spoken. This method involved using the location, longitude, latitude, and radius parameters to specify a circular geographic area, thereby reducing the likelihood of collecting tweets in the wrong language.", "evidence_reference": "Stopwords overlap across indigenous languages in a multilingual society such as Nigeria. This results in tweets being collected in a language that differs from the query language. To mitigate this, we collected tweets based on locations where a language is predominantly spoken, using the location, longitude, latitude and radius parameters (25 miles) to specify a circular geographic area."}
{"question": "Consider the paper that introduces the model that achieves a score of 3.84 in the Grounding task. How does the MVQG-VL-T5's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the base model tested in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "explanation_reference": "Traditional methods for the RefCOCOg task typically involve classification over a set of visual regions, requiring task-specific architectures and objectives. In contrast, the unified framework proposed in the paper treats RefCOCOg as a text generation task, leveraging the same language modeling architecture and objective used for other vision-and-language tasks. This approach allows for more flexible architecture design and eliminates the need for task-specific modifications.", "evidence_reference": "While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously~\\cite{Yu2018,Chen2020} formulated classification task over a set of visual regions, allowing more flexible architecture design."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. How does the performance of the model proposed in the paper, IRCoT, compare with Flan-T5-XXL and GPT3 models when a separate reader is not used for the IIRC dataset?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2212.10509", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "IRCoT", "question_reference": "How does the performance of IRCoT QA with Flan-T5-XXL and GPT3 models compare when a separate reader is not used for the IIRC dataset?", "explanation_reference": "The comparison of IRCoT QA's performance with and without a separate reader for the IIRC dataset shows that for Flan-T5-XXL, having a separate reader is significantly better, while for GPT3, the performance is slightly better without a separate reader. This indicates the effectiveness of using a separate reader varies based on the model and dataset.", "evidence_reference": "For \\texttt{Flan-T5-XXL} having a separate reader is significantly better. For GPT3, this is not always true, but at least a model with a separate reader is always better or close to the one without. So overall we go with the choice of using the reader for the experiments in this paper."}
{"question": "Consider the paper that introduces the model that has the second lowest MCD 1 score. How does the Edge Transformer's triangular attention mechanism, specifically in this model, contribute to its ability to perform relational reasoning compared to classical Transformers?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2112.00578", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model dmonstrates the lowest MCD 1 score?", "answer_anchor": "T5-based UT", "question_reference": "How does the Edge Transformer's triangular attention mechanism specifically contribute to its ability to perform relational reasoning compared to classical Transformers?", "explanation_reference": "The triangular attention mechanism of the Edge Transformer updates edge representations by aggregating information across all pairs of edges that share a node, directly inspired by the unification process in logic programming. This mechanism allows the model to effectively reason about relations between entities, a capability that classical Transformers, which focus on node (entity) representations, are less equipped for.", "evidence_reference": "The updates of each edge are computed using all adjacent edges in a way that is directly inspired by unification in logic programming."}
{"question": "Consider the paper that introduces the model that is placed below TransferNet but above UniKGQA in the table. What specific performance improvement does the model proposed in the paper, when combined with NSM, achieve on the CWQ dataset in terms of Hits@1 and F1 scores compared to the original NSM model?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shown in the table is below TransferNet but above UniKGQA?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific performance improvement does the \\model+NSM model achieve on the CWQ dataset in terms of Hits@1 and F1 scores compared to the original NSM model?", "explanation_reference": "The question asks for the specific performance improvement of the \\model+NSM model over the original NSM model on the CWQ dataset, focusing on the Hits@1 and F1 metrics. The answer directly provides these specific improvements, indicating the effectiveness of the \\model when combined with NSM.", "evidence_reference": "NSM injected by \\smodel (\\model+NSM) improves 0.4% Hits@1 and 1.3% F1 on WebQSP, 3.9% Hits@1 and 4.7% F1 on CWQ compared with the original NSM."}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in direct prompting. What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shows the second best execuation accuracy in direct prompting?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "explanation_reference": "The preprocessing step mentioned specifically applies a lowercase filter and Lucene's ASCIIFoldingFilter to the code, followed by tokenization using a 3-gram tokenizer, before indexing it using Elasticsearch. This step is crucial for preparing the code for efficient and effective search functionality.", "evidence_reference": "The code itself is preprocessed using a lowercase filter and Lucene's \\texttt{ASCIIFoldingFilter}, tokenized using a 3-gram tokenizer, and indexed using the default Lucene implementation of BM25 as a similarity function."}
{"question": "Consider the paper that introduces the method that is in the first block and has an F1 score of 50.4. What specific performance improvement does the model proposed in the paper achieve on the UFET benchmark when enhanced with Prior Knowledge about Labels (PKL) using BERT-large as the language model, compared to its performance without PKL?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is in the first block while having 50.4 F1 score?", "answer_anchor": "ConCN clusters", "question_reference": "What specific performance improvement does the DenoiseFET model achieve on the UFET benchmark when enhanced with Prior Knowledge about Labels (PKL) using BERT-large as the language model, compared to its performance without PKL?", "explanation_reference": "The improvement is calculated based on the F1 scores presented for the DenoiseFET model without PKL (49.8%) and with PKL (51.9%) using BERT-large. The difference between these two F1 scores (51.9% - 49.8%) indicates a 2.1% increase in performance when PKL is applied.", "evidence_reference": "DenoiseFET & Bl & 52.6 & 47.5 & 49.8\\\\ DenoiseFET + PKL & Bl & 53.8 & 50.2 & 51.9"}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.717 in the FB15kET dataset. What specific role does the context transformer play in enhancing entity representation in the model proposed by the paper?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets MRR score equal to 0.717 in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What specific role does the context transformer play in the TET approach for enhancing entity representation?", "explanation_reference": "The answer directly addresses the question by specifying the unique function of the context transformer within the TET framework, which is to differentiate how neighbours' information is integrated through pairwise exchanges, ensuring the graph's structural integrity is maintained.", "evidence_reference": "a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
{"question": "Consider the paper that introduces the method that achieves the lowest J_k scores in the WN18RR dataset. How is the initial representation for non-reserved entities generated before being input into the GNN in the model's entity-agnostic encoding process?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method shows the lowest J_k score in WN18RR dataset?", "answer_anchor": "EARL", "question_reference": "In the EARL model's entity-agnostic encoding process, how is the initial representation for non-reserved entities generated before being input into the GNN?", "explanation_reference": "The initial representation for non-reserved entities in the EARL model is generated by first encoding the ConRel and $k$NResEnt information separately and then combining these two encoded pieces of information. This combination is achieved through a concatenation followed by a transformation using a 2-layer MLP, as described in the Entity-Agnostic Encoding section.", "evidence_reference": "In our GNN framework, similar to previous works \\cite{CompGCN, MaKEr}, we use a linear transformation on the concatenation of entity and relation representations to aggregate the neighbor information. Specifically, the message aggregation for the entity $e$ is: \\begin{equation} \\begin{aligned} \\mathbf{m}_{e}^{l} = \\sum_{(r, t) \\in \\mathcal{O}(e)} \\mathbf{W}_{\\text{out}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_t] + \\sum_{(r,h) \\in \\mathcal{I}(e)} \\mathbf{W}_{\\text{in}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_h], \\label{eq:gnn-agg} \\end{aligned} \\end{equation} where $\\mathcal{O}(e)$ denotes the out-going relation-entity pair set of $e$ and $\\mathcal{I}(e)$ denotes the in-going relation-entity pair set. $\\mathbf{W}_{\\text{out}}^l$ and $\\mathbf{W}_{\\text{in}}^l$ are transformation matrices for out-going and in-going pairs. $l \\in [0, \\dots, L]$ denotes the layer of GNN and $L$ is the total number of GNN layers. The input entity representations are calculated in Equation (\\ref{eq:info-combine}), and the input relation representations (e.g., $\\mathbf{h}_{r}^{0}$) are looked up in a trainable relation embedding matrix $\\mathbf{R} \\in \\mathbb{R}^{|\\mathcal{R}|\\times d}$.  The entity representation of $e$ in the GNN is updated as follows: \\begin{equation} \\mathbf{h}_{e}^{l+1} = \\sigma \\left( \\frac{1}{c}\\mathbf{m}_{e}^{l} + \\mathbf{W}_{\\text{self}}^{l} \\mathbf{h}_{e}^{l} \\right), \\label{eq:gnn-update} \\end{equation} where $c=|\\mathcal{I}(e)+\\mathcal{O}(e)|$ is a normalization constant. $\\mathbf{W}_{\\rm self}^{l}$ is a matrix for self representation update, and $\\sigma$ is an activation function. Furthermore, relation representations will also be updated in each layer: $\\mathbf{h}_{r}^{l+1} = \\sigma \\left( \\mathbf{W}_{\\text{rel}}^{l} \\mathbf{h}_{r}^{l} \\right)$. We use the output representations in the $L$-th layer for entities and relations as their embeddings to calculate scores next."}
{"question": "Consider the paper that introduces the method that corresponds to the leftmost bar in the figure. How does the model's performance, proposed by the paper, in detecting out-of-distribution samples using ResNet on CIFAR-10 when SVHN is used as OOD compare to the baseline and ODIN methods in terms of TNR at TPR 95%?", "answer": "", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown on the leftmost bar in the figure?", "answer_anchor": "MSP", "question_reference": "How does the proposed method's performance in detecting out-of-distribution samples using ResNet on CIFAR-10 when SVHN is used as OOD compare to the baseline and ODIN methods in terms of TNR at TPR 95%?", "explanation_reference": "The proposed method outperforms both the baseline and ODIN methods in detecting out-of-distribution samples using ResNet on CIFAR-10 when SVHN is used as OOD, achieving a higher TNR at TPR 95%.", "evidence_reference": "Baseline: 32.47%, ODIN: 86.55%, Mahalanobis (ours): 96.42%"}
{"question": "Consider the paper that introduces the score described as the 'influence of any example z towards another example z' by tracking their gradient dot products,' where 'we generate the self-influence scores where z = z'. What is the theoretical foundation that inspired the development of the model proposed in the paper for computing the influence of training examples?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2002.08484", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the name of the score with description 'Influence of any example z towards another example z' by tracking their gradient dot products. We generate the self-influence scores where z = z''?", "answer_anchor": "TracIn", "question_reference": "What is the theoretical foundation that inspired the development of TrackIn for computing the influence of training examples?", "explanation_reference": "TrackIn is inspired by the fundamental theorem of calculus, which decomposes the difference between a function at two points using the gradients along the path between the two points. Similarly, TrackIn decomposes the difference between the loss of the test point at the end of training versus at the beginning of training along the path taken by the training process.", "evidence_reference": "TrackIn is inspired by the \\emph{fundamental theorem of calculus}. The fundamental theorem of calculus decomposes the difference between a function at two points using the gradients along the path between the two points. Analogously, TrackIn decomposes the difference between the loss of the test point at the end of training versus at the beginning of training along the path taken by the training process."}
{"question": "Consider the paper that introduces the dataset which has the fewest number of languages but the most number of SM tasks. What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly, especially in the context of Chinese social media?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset has the fewest number of languages but the most number of SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly, especially in the context of Chinese social media?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity in sentiment analysis with LLMs. It specifically mentions the phenomenon where a seemingly agreeable statement in Chinese, made with a respectful tone, does not necessarily indicate agreement but can be used ironically. This example illustrates the subtlety and context-dependency of language, highlighting the difficulty for models to interpret such nuances without familiarity with specific cultural or social media contexts.", "evidence_reference": "For example, on Chinese social media, a comment '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically. However, this linguistic phenomenon may require familiarity with social media to interpret correctly."}
{"question": "Consider the paper that introduces the method that corresponds to the second highest Acc-5 score on MOSI. What is the primary method used by the model proposed in the paper to generate unimodal labels based on the self-supervised multi-task learning strategy?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "2102.04830", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method demonstrates the highest Acc-5 score on MOSI", "answer_anchor": "Self-MM", "question_reference": "Based on the self-supervised multi-task learning strategy proposed in the paper, what is the primary method used to generate unimodal labels?", "explanation_reference": "The primary method used to generate unimodal labels in the self-supervised multi-task learning strategy involves calculating the relative distance value from modality representations to class centers. This approach is central to the Unimodal Label Generation Module (ULGM), which determines the offset for unimodal supervisions based on these relative distance values.", "evidence_reference": "Therefore, the ULGM calculates the offset according to the relative distance from modality representations to class centers, shown as Figure \\ref{fig: DistExp}. [...] Then, we define the relative distance value, which evaluates the relative distance from the modality representation to the positive center and the negative center."}
{"question": "Consider the paper that introduces the method that corresponds to the leftmost bar in the figure. How does the model's performance, proposed by the paper, in detecting out-of-distribution samples using ResNet trained on CIFAR-10 compare when SVHN is used as OOD, in terms of True Negative Rate at 95% True Positive Rate, against the baseline and ODIN methods?", "answer": "", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown on the leftmost bar in the figure?", "answer_anchor": "MSP", "question_reference": "How does the proposed method's performance in detecting out-of-distribution samples using ResNet trained on CIFAR-10 compare when SVHN is used as OOD, in terms of True Negative Rate at 95% True Positive Rate, against the baseline and ODIN methods?", "explanation_reference": "The proposed method outperforms both the baseline and ODIN methods in detecting out-of-distribution samples when SVHN is used as OOD, with a True Negative Rate at 95% True Positive Rate of 96.42%, indicating its superior detection capability.", "evidence_reference": "Baseline \\citep{hendrycks2016baseline} & - & - & 32.47 & 89.88 & 85.06 & 85.40 & 93.96 \\\\ \\midrule ODIN \\citep{liang2017principled} & - & - & 86.55 & 96.65 & 91.08 & 92.54 & 98.52 \\\\ \\midrule \\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}} Mahalanobis \\\\ (ours) \\end{tabular}}& - & - & 54.51 & 93.92 & 89.13 & 91.56 & 95.95 \\\\ & - & \\checkmark & 92.26 & 98.30 & 93.72 & 96.01 & 99.28 \\\\ & \\checkmark & - & 91.45 & 98.37 & 93.55 & 96.43 & 99.35 \\\\ & \\checkmark & \\checkmark & {\\bf 96.42} & {\\bf 99.14} & {\\bf 95.75} & {\\bf 98.26} & {\\bf  99.60}"}
{"question": "Consider the paper that introduces the model that has an overall average score of 45.68 in Previous Image-free Systems. What is the average BLEU score improvement of the model proposed in the paper over the text-only baseline on the EN$\\rightarrow$DE task for the Transformer-Tiny model on the Multi30K dataset?", "answer": "", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates the overall average score of 45.68 in previous image-free systems?", "answer_anchor": "VALHALLA", "question_reference": "What is the average BLEU score improvement of the model proposed in the paper over the text-only baseline on the EN$\\rightarrow$DE task for the Transformer-Tiny model on the Multi30K dataset?", "explanation_reference": "The question focuses on the specific detail of the average BLEU score improvement achieved by VALHALLA over the text-only baseline for the EN$\\rightarrow$DE task using the Transformer-Tiny model on the Multi30K dataset. The answer is directly provided in the Results on Multi30K section, where it states that using Transformer-Tiny as the backbone, VALHALLA obtains an average 35.4 BLEU in EN$\\rightarrow$DE, which is about 2.1 BLEU improvements over the text-only baseline.", "evidence_reference": "Using Transformer-Tiny as the backbone, \\ours obtains an average $35.4$ BLEU in EN$\\rightarrow$DE and $54.4$ BLEU in EN$\\rightarrow$FR, which is about $2.1$ and $1.4$ BLEU improvements over the text-only baseline."}
{"question": "Consider the paper that examines the dataset in which KALMV achieves a score of 66.48 for the Large model. What is the percentage of questions from this dataset that can be answered using a boolean response?", "answer": "", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset being tested that KALMV gets 66.48 score for Large model?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka questions that can be answered using a boolean response?", "explanation_reference": "The percentage is directly provided in the dataset statistics, indicating the proportion of questions in Mintaka that can be answered with a boolean (yes/no) response.", "evidence_reference": "A majority (72%) of the questions in Mintaka can be answered using an entity. 14% can be answered using a boolean, in yes/no or comparative questions."}
{"question": "Consider the paper that focuses on the ViHOS dataset associated with the task 'Hate Speech Spans Detection (HSSD)'. What specific linguistic phenomenon can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset with task `Hate Speech Spans Detection (HSSD)`?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "explanation_reference": "The paper discusses how non-diacritical marks comments can trick annotators into needing to re-read the text multiple times due to the absence of punctuation, diacritics, and the presence of ambiguous text that could be interpreted inappropriately in multiple ways. This specific linguistic phenomenon is highlighted as a challenge in understanding and annotating the dataset.", "evidence_reference": "Non-diacritical marks comments might trick annotators a little bit... there are some problems causing annotators to re-read multiple times as no punctuation, diacritic, and the text 'con ng' could be considered as 'crazy girl' or 'the type (of human)' and both of these meanings is inappropriate."}
{"question": "Consider the paper that introduces the method that results in a better score than MOCA but worse score than LACMA in the Seen, Val, SR dataset. What specific method does the model proposed in the paper leverage to decouple the understanding of visual appearance from the variations in natural language instructions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the score better than MOCA but worse than LACMA in Seen, Val, SR dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer leverage to decouple the understanding of visual appearance from the variations in natural language instructions?", "explanation_reference": "The question focuses on a detailed aspect of the methodology employed by the Episodic Transformer to address the challenge of understanding complex human instructions in dynamic environments. The answer directly addresses this by specifying the use of synthetic instructions as an intermediate representation, which is a strategy to separate the process of understanding the visual aspects of an environment from the variations inherent in natural language instructions.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the model that has the highest P-BLEU score in the uAD dataset. What is the Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity for the model proposed in the paper?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets the lowest P-BLEU? score in uAD dataset?", "answer_anchor": "Composition", "question_reference": "What is the Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity?", "explanation_reference": "The Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity is provided directly in the results section discussing the correlation of automatic metrics with human judgments.", "evidence_reference": "Self-BLEU & 0.880"}
{"question": "Consider the paper that introduces the method that exhibits a FLAN-T5 score of 52.4% using SGD in 24 domains. How does the model proposed in the paper ensure the prevention of cycles in the resulting subtask graph to avoid a causality paradox?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shows FLAN-T5 score of 52.4% using SGE in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "How does the proposed Multimodal Subtask Graph Generation (MSG2) approach ensure the prevention of cycles in the resulting subtask graph, which could lead to a causality paradox?", "explanation_reference": "The paper addresses the potential issue of forming cycles in the resulting subtask graph, which could lead to a causality paradox, by performing precondition inference in a layer-wise fashion. This method ensures that the edge in the subtask graph is formed from the lower depth to the higher depth, preventing the formation of cycles.", "evidence_reference": "One major problem of inferring the precondition independently for each subtask is the possibility of forming a cycle in the resulting subtask graph, which leads to a causality paradox (\\ie, subtask A is a precondition of subtask B and subtask B is a precondition of subtask A). To avoid this problem, we perform precondition inference in a layer-wise fashion similar to~\\citet{sohn-iclr20}."}
{"question": "Consider the paper that introduces the model in the figure corresponds to the grey line with a star marker. How does its performance, specifically StarCoderBase, on natural language reasoning tasks on the HELM benchmark compare to other open-access models?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shown in the figure represented by grey line with star marker?", "answer_anchor": "StarCoder", "question_reference": "Based on the evaluation of StarCoderBase on the HELM benchmark, how does its performance on natural language reasoning tasks compare to other open-access models?", "explanation_reference": "The evaluation of StarCoderBase on the HELM benchmark for natural language reasoning tasks shows that it generally outperforms other open-access models, indicating its superior ability to leverage its natural language and code pretraining for these tasks.", "evidence_reference": "In Table~\\ref{tab:helm_results} we report the results. We compute each model's ranking on each task, and order models in the table by their average ranking across tasks. StarCoderBase generally obtains substantially stronger performance than all other models with released weights and often performs comparably to or better than much larger models."}
{"question": "Consider the paper that introduces the dataset which has more dev set samples than UIT-VSMEC but fewer dev set samples than ViSpamReviews. What specific linguistic phenomenon is used as an example in the paper to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset having more dev set samples than UIT-VSMEC but less dev set samples than ViSpamReviews?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon is used as an example to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "explanation_reference": "The paper discusses various linguistic phenomena that pose challenges to hate speech detection, including the use of puns, where phrases only make sense when read backwards. This is highlighted in the example of 'B\u1ed3n K\u1ef3 L\u1eafc', which, when read backwards, becomes 'B\u1eafc K\u1ef3 L\u1ed3n', illustrating how puns can be used to convey hate speech in a concealed manner.", "evidence_reference": "Some comments use phrases that only read them backwards, they make sense. As in the example, 'B\u1ed3n K\u1ef3 L\u1eafc', if this phrase is read backwards, it is 'B\u1eafc K\u1ef3 L\u1ed3n' (pussy north)."}
{"question": "Consider the paper that introduces the model which demonstrates the lowest accuracy in the SLOG-all dataset. What specific advantage does the Vanilla Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model demonstrates the lowest accuracy in SLGO-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific advantage does the Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "explanation_reference": "The advantage is highlighted by the experimental variations where changing the number of attention heads and dimensions (keeping the computational cost constant) showed that single-head attention performs worse than the optimal setting, indicating that multi-head attention improves model quality. Additionally, the design of multi-head attention ensures that the total computational cost remains similar to that of single-head attention with full dimensionality, thus not significantly increasing the computational cost.", "evidence_reference": "In Table~\\ref{tab:variations} rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section \\ref{sec:multihead}. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the penultimate row. What is the discount factor (\\(\\lambda\\)) used in the model proposed in the paper for recency weighting in subtask graph inference from real-world data?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the figure in the penultimate row?", "answer_anchor": "MSG^2", "question_reference": "What is the discount factor (\\(\\lambda\\)) used in the recency weighting for subtask graph inference from real-world data?", "explanation_reference": "The discount factor (\\(\\lambda\\)) used in the recency weighting for subtask graph inference from real-world data is mentioned as part of the method to improve graph generation by taking temporal information into account, specifically assigning higher weight if a subtask has been eligible more recently.", "evidence_reference": "we assign higher weight if a subtask has been eligible more recently: $w_{t, n} = \\max(0.1, \\lambda ^ {t_n-t}), where $0<\\lambda<1$ is the discount factor, $t_n$ is the time step when the precondition for subtask $n$ became satisfied. We used $\\alpha=0.2$ and $\\lambda=0.7$ in our experiments."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than LayoutXLM and a higher F1 score than SPADE. What specific architectural component in the model proposed in the paper is responsible for mapping the representation of each node into the number of target classes?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having lower F1 score than LayoutXLM and high F1 score than SPADE?", "answer_anchor": "Doc2Graph", "question_reference": "What specific architectural component in Doc2Graph is responsible for mapping the representation of each node into the number of target classes?", "explanation_reference": "The Node Predictor is explicitly mentioned as the component responsible for mapping the representation of each node into the number of target classes, indicating its role in the classification process within the Doc2Graph framework.", "evidence_reference": "Node Predictor: this is a FC layer, that maps the representation of each node into the number of target classes;"}
{"question": "Consider the paper that introduces the model that achieves a higher TP score than GIT but a lower TP score than LLaVA. What specific computational advantage does it have over VinVL and \\({\\cal M}^2\\) Transformer in terms of feature extraction inference time?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which models shows higher TP score than GIT but lower TP score than LLaVA?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "explanation_reference": "The question focuses on the detailed part of the computational efficiency of GRIT compared to other methods, specifically in the context of feature extraction inference time. This detail highlights GRIT's significant improvement in computational speed.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE L score equal to 41.39. What specific performance improvement, in terms of ROUGE-1 score, does the integration of normalizing flow into the model proposed in the paper, PEGASUS+NTM, achieve over using a VAE-based neural topic model without gating on the XSum test set?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method in the table that demonstrates a ROUGE L score equal to 41.39?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific performance improvement (in terms of ROUGE-1 score) does the integration of normalizing flow into the neural topic model achieve over using a VAE-based neural topic model without gating on the XSum test set?", "explanation_reference": "The integration of normalizing flow into the neural topic model achieves a specific performance improvement of 0.4 in terms of ROUGE-1 score over using a VAE-based neural topic model without gating on the XSum test set, as indicated by the comparison of ROUGE-1 scores between the model configurations in the ablation study.", "evidence_reference": "without the normalizing flow, the improvement that the latent vector brings is downgraded, nearly 0.4 of ROUGE-1 for using contextualized gating and 0.53 of ROUGE-1 in non-gating case."}
{"question": "Consider the paper that introduces the model that results in the highest MCD 1 score. How does its approach to handling the entanglement problem in machine translation specifically address the challenge posed by the novel compound 'behind the small doctor on the floor'?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2110.04655", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model dmonstrates the highest MCD 1 score?", "answer_anchor": "Dangle", "question_reference": "How does the \\textsc{Dangle} model's approach to handling the entanglement problem in machine translation specifically address the challenge posed by the novel compound 'behind the small doctor on the floor'?", "explanation_reference": "The \\textsc{Dangle} model's adaptive encoding mechanism allows it to effectively handle the translation of novel compounds by breaking down the representation of complex, unfamiliar phrases into smaller, familiar components. This approach enables the model to accurately translate phrases that combine elements in ways not seen during training.", "evidence_reference": "We believe this is due to the proposed adaptive encoding mechanism and its ability to decompose the representation problem of an unfamiliar compound phrase into sub-problems of familiar phrases (i.e, 'behind the small doctor' and 'the small doctor on the floor')."}
{"question": "Consider the paper that introduces the supervised method that results in the lowest score in 10-shot prompting. What specific performance improvement does the model proposed in the paper offer through dynamic masking over static masking for the SQuAD 2.0 task according to the paper's findings?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which supervised method demonstrates lowest scores in 10-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does dynamic masking offer over static masking for the SQuAD 2.0 task according to the paper's findings?", "explanation_reference": "The improvement is calculated based on the reported median accuracy for the MNLI-m task using static and dynamic masking. The paper reports a median accuracy of 84.3% for static masking and 84.0% for dynamic masking on the MNLI-m task. The difference indicates a 0.4% improvement in favor of static masking, contrary to expectations.", "evidence_reference": "static & 78.3 & 84.3 & 92.5 \\\\ dynamic & 78.7 & 84.0 & 92.9 \\\\"}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What is the primary motivation behind implementing a hard selection of document content in the CSN-word, as opposed to a soft weighting approach in the Content Selection Network (CSN)?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "CSN-word", "question_reference": "What is the primary motivation behind implementing a hard selection of document content in the Content Selection Network (CSN) as opposed to a soft weighting approach?", "explanation_reference": "The question assesses the understanding of the core concept behind the design of the CSN model, specifically why a hard selection mechanism is preferred over a soft weighting approach for selecting relevant document content. The answer is directly related to the fundamental observation that motivates the design of the CSN, which is that conversations typically focus on specific parts of a document rather than its entirety at any given step.", "evidence_reference": "The hard selection of document content is motivated by the following observation: although the whole conversation can cover many aspects described in the grounding document, each of the step is related to only a small part of the document content."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, GC dataset. What specific architectural modification allows its Segment-Level Recurrent Action Decoder to perform cross-attention over encoder states, diverging from the original TransformerXL design?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the best score in Seen, Val, GC dataset?", "answer_anchor": "EmBERT", "question_reference": "What specific architectural modification allows the Segment-Level Recurrent Action Decoder in EmBERT to perform cross-attention over encoder states, diverging from the original TransformerXL design?", "explanation_reference": "The question targets a detailed aspect of the Segment-Level Recurrent Action Decoder's design, specifically how it adapts the TransformerXL architecture to enable cross-attention between decoder and encoder states. This is a nuanced detail that requires understanding both the original TransformerXL limitations and the specific modifications made by EmBERT to overcome these limitations for its task.", "evidence_reference": "Therefore, we introduce two novel elements to its architecture: 1) \\textit{encoder hidden states cache}; 2) \\textit{cross-attention over encoder states}. First, our extended context is composed of both agent state representations and hidden states from the previous segment $\\mathbf{s}_i$. In addition, to perform cross-attention between decoder and encoder hidden states, we modify the TransformerXL self-attention mechanism following common practice in designing transformer decoders~\\cite{vaswani2017attention}."}
{"question": "Consider the paper that introduces the method which has fewer 'rounds to completion' than GPT-4 + Belief but more 'rounds to completion' than the CBS planner. What is the effect of using a death mask on the model's performance in the SMAC domain compared to ignoring states in which an agent is dead when computing GAE?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having lower `rounds to completion` than GPT-4 + Belief but higher `rounds to completion` than CBS planner?", "answer_anchor": "MAPPO", "question_reference": "What is the effect of using a death mask on MAPPO's performance in the SMAC domain compared to ignoring states in which an agent is dead when computing GAE?", "explanation_reference": "The ablation studies demonstrate that using a death mask, which involves replacing the value state for a dead agent with a zero state containing the agent's ID, results in superior performance compared to other options, including ignoring states in which an agent is dead when computing GAE. This suggests that handling the non-stationarity introduced by agent deaths appropriately is crucial for MAPPO's performance in the SMAC domain.", "evidence_reference": "Fig.~\\ref{fig:app-Ablation-death} and Fig. \\ref{fig:app-Ablation-ignore} illustrates the influence of the death mask on MAPPO's performance in the SMAC domain."}
{"question": "Consider the paper that introduces the model which corresponds to the fifth row of the table. What is the average number of steps that the best model, indicated as UT with dynamic halting, took on the test data over all positions and examples for the LAMBADA task?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1807.03819", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model in the fifth row of the table?", "answer_anchor": "UT", "question_reference": "What is the average number of steps that the best UT with dynamic halting took on the test data over all positions and examples for the LAMBADA task?", "explanation_reference": "The average number of steps that the best UT with dynamic halting took on the test data over all positions and examples for the LAMBADA task is directly provided in the paper content.", "evidence_reference": "Our best fixed UT results used 6 steps. However, the average number of steps that the best UT with dynamic halting took on the test data over all positions and examples was $8.2 \\rpm 2.1$."}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, GC dataset. How does its performance with OSCAR initialization and without predicting the parent object or visual region classification compare on the Seen and Unseen validation folds for the Task and GC metrics?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the best score in Seen, Val, GC dataset?", "answer_anchor": "EmBERT", "question_reference": "How does the performance of EmBERT with OSCAR initialization and without predicting the parent object or visual region classification compare on the Seen and Unseen validation folds for the Task and GC metrics?", "explanation_reference": "The question specifically targets the performance metrics of EmBERT under a particular configuration\u2014using OSCAR initialization but without predicting the parent object ('P(O)') or utilizing the visual region classification (VRC) loss. The answer directly corresponds to the performance metrics provided in the validation fold performance table for this specific configuration.", "evidence_reference": "OSCAR & 18 & 200 & \\cblkmark & & & \\B{37.44} (\\B{28.81}) & \\B{44.62} (\\B{36.41}) & \\B{\\phantom{0}5.73} (\\B{\\phantom{0}3.09}) & \\B{15.91} (\\B{\\phantom{0}9.33})"}
{"question": "Consider the paper that discusses which dataset has a test set size of 1,106. What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues such as lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset having 1,106 test set size?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "explanation_reference": "The paper discusses how non-diacritical marks comments can trick annotators into needing to re-read the text multiple times due to the absence of punctuation, diacritics, and the presence of ambiguous text that could be interpreted inappropriately in multiple ways. This specific linguistic phenomenon is highlighted as a challenge in understanding and annotating the dataset.", "evidence_reference": "Non-diacritical marks comments might trick annotators a little bit... there are some problems causing annotators to re-read multiple times as no punctuation, diacritic, and the text 'con ng' could be considered as 'crazy girl' or 'the type (of human)' and both of these meanings is inappropriate."}
{"question": "Consider the paper that introduces the model that achieves a P_k score of 24.8 in the en_disease category. What specific method did the authors employ to encode sentence positions in the encoder input of the model proposed in the paper, and how does this approach differ from using a fixed BOS token index?", "answer": "", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets 24.8 P_k score in en_disease category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What specific method did the authors employ to encode sentence positions in the encoder input, and how does it differ from using a fixed BOS token index?", "explanation_reference": "This method directly encodes sentence positions by utilizing the sequence of vocabulary token embeddings as a means to represent the position of each sentence in the input sequence. This approach is distinct from using a fixed Beginning Of Sentence (BOS) token index for every sentence, which does not convey unique position information. The method aims to provide the decoder with unambiguous position information that can be exploited for producing sentence indices at the decoder output, without employing custom schemes like dedicated sentence position embeddings.", "evidence_reference": "At the encoder input for the $i^{th}$ sentence,  we use the $i^{th}$ vocabulary token embedding in place of a fixed BOS token index. Formally, in contrast to \\eqref{eqn:bos}, we set \\begin{equation*} t_{1_1} = 0, \\hspace{2mm}  t_{2_1} = 1, \\hspace{2mm} \\ldots,  t_{|S|_1} = |S|-1. \\end{equation*}"}
{"question": "Consider the paper that introduces the method that has an F1 score of 41.3. What specific algorithmic enhancement does it apply to address the issue of tail-sharing edges in spatial dependency graphs, and what is its impact on the F1 score for the \\cord\\ dataset?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having 41.3 F1 score?", "answer_anchor": "SPADE", "question_reference": "What specific algorithmic enhancement does SPADE apply to handle the issue of tail-sharing edges in the spatial dependency graphs, and what is its impact on the F1 score for the \\cord\\ dataset?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically designed to address the issue of tail-sharing edges in spatial dependency graphs by iteratively trimming these edges and generating new ones until the process becomes self-consistent. This algorithmic enhancement is critical for improving the model's ability to accurately parse and extract information from documents, as evidenced by the reported improvements in the F1 score for the \\cord\\ dataset.", "evidence_reference": "Using this property, we integrate Tail Collision Avoidance algorithm (\\caabb) that iteratively trims the tail-sharing-edges and generate new edges until the process becomes self-consistent (Section \\ref{sec:graph_gen}). $F_1$ increases by +1.0\\% and +0.8\\% with and without the oracle upon the integration (2nd row, \\cordabb)."}
{"question": "Consider the paper that introduces the method that has the lowest score in the 'Original Persona' column. What is the primary motivation behind implementing a hard selection of document content in the model proposed in the paper, the Content Selection Network (CSN), as opposed to a soft weighting approach?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has the lowest score on 'Original Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the primary motivation behind implementing a hard selection of document content in the Content Selection Network (CSN) as opposed to a soft weighting approach?", "explanation_reference": "The question assesses the understanding of the core concept behind the design of the CSN model, specifically why a hard selection mechanism is preferred over a soft weighting approach for selecting relevant document content. The answer is directly related to the fundamental observation that motivates the design of the CSN, which is that conversations typically focus on specific parts of a document rather than its entirety at any given step.", "evidence_reference": "The hard selection of document content is motivated by the following observation: although the whole conversation can cover many aspects described in the grounding document, each of the step is related to only a small part of the document content."}
{"question": "Consider the paper that introduces the model that scores higher than ACA but lower than RationaleCL in the 'T5' column. What is the observed accuracy drop percentage for the model proposed in the paper on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets score higher than ACA but lower than RationalCL in 'T5' column?", "answer_anchor": "CEAR", "question_reference": "What is the observed accuracy drop percentage for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "explanation_reference": "The accuracy drop for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00) is directly reported in the empirical study results table.", "evidence_reference": "CRL & [0.85, 1.00) & 71.1 & 9.7 & 64.8 & 11.4"}
{"question": "Consider the paper that discusses what model is shown in the first row of the table. What is the primary reason for using accuracy as the evaluation metric in the task associated with this model, according to the paper?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "GWLAN", "question_reference": "What is the primary reason for using accuracy as the evaluation metric in the GWLAN task, according to the paper?", "explanation_reference": "The paper suggests that higher prediction accuracy in the GWLAN task is directly correlated with the reduction of keystrokes required from human translators. This reduction in keystrokes leads to time savings for translators, making them more productive. Therefore, accuracy is used as the evaluation metric to reflect the effectiveness of the autocompletion in assisting human translators.", "evidence_reference": "Experiments show that the more keystrokes are reduced, the more time can be saved for translators. Since the prediction accuracy is highly correlated with the keystrokes, we think higher accuracy will make translators more productive."}
{"question": "Consider the paper that introduces the model that achieves a BLEURT score of 0.4126. What specific methodological approach does the paper propose for generating high-cognitive-demand educational questions from children's storybooks?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model gets 0.4126 in BLEURT score?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach of combining question type prediction with event-centric summarization, which is the core methodological innovation of the study for generating educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the method that achieves the highest score in 10-shot prompting. What is the F1 score improvement of the model proposed in the paper using BERT label encoder over the model using GloVe label encoder in the 1-shot setting for the NCBI-disease dataset?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method demonstrates highest scores in 10-shot prompting?", "answer_anchor": "LSFS", "question_reference": "What is the F1 score improvement of the model using BERT label encoder over the model using GloVe label encoder in the 1-shot setting for the NCBI-disease dataset?", "explanation_reference": "The improvement can be calculated by subtracting the F1 score of the model using GloVe label encoder from the F1 score of the model using BERT label encoder in the 1-shot setting for the NCBI-disease dataset. 10.7-15.1=15.6", "evidence_reference": "Our model - GloVe & 15.1 $\\pm$ 8.7 & ... & ... & ... & ... & ... \\\\ &  Our model - BERT & \\bf 30.7 $\\pm$ 9.1 & ... & ... & ... & ... & ... \\\\"}
{"question": "Consider the paper that introduces the score described as the 'influence of any example z towards another example z' by tracking their gradient dot products,' where 'we generate the self-influence scores where z = z'. What specific aspect of the CIFAR-10 dataset's training process does the TracIn method, described as leveraging this score, leverage to identify mislabelled data more effectively compared to influence functions and representer points?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2002.08484", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the name of the score with description 'Influence of any example z towards another example z' by tracking their gradient dot products. We generate the self-influence scores where z = z''?", "answer_anchor": "TracIn", "question_reference": "Based on the experimental evaluation, what specific aspect of the CIFAR-10 dataset's training process does the TrackIn method leverage to identify mislabelled data more effectively compared to influence functions and representer points?", "explanation_reference": "The effectiveness of TrackIn in identifying mislabelled data on the CIFAR-10 dataset, as compared to influence functions and representer points, is attributed to its method of computing a weighted average across checkpoints. This approach leverages the information contained in different checkpoints to better identify mislabelled examples, highlighting the importance of sampling checkpoints in the training process.", "evidence_reference": "Next, we discuss the contributions of the different checkpoints to the scores produced by TrackIn; recall that TrackIn computes a weighted average across checkpoints (see the definition of TrackInCP). We find that different checkpoints contain different information. We identify the number of mislabelled examples from each class (the true class, not the mislabelled class) within the first 10% of the training data in Figure~\\ref{fig:cifar-ckpt-compare} (in the supplementary material). We show results for the 30th, 150th and 270th checkpoint. We find that the mix of classes is different between the checkpoints. The 30th checkpoint has a larger fraction (and absolute number) of mislabelled deer and frogs, while the 150th emphasizes trucks. This is likely because the model learns to identify different classes at different points in training process, \\emph{highlighting the importance of sampling checkpoints.}"}
{"question": "Consider the paper that introduces the LLM shown in the figure with a model size of 1.7T. What is the specific improvement in percentage points of this LLM over its predecessor in internal adversarially-designed factuality evaluations?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the large language model that has 1.7T model size?", "answer_anchor": "GPT-4", "question_reference": "What is the specific improvement in percentage points of GPT-4 over GPT-3.5 in internal adversarially-designed factuality evaluations?", "explanation_reference": "The improvement is directly stated as a comparison between GPT-4 and GPT-3.5, highlighting the progress made in reducing hallucinations and improving factuality.", "evidence_reference": "GPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have themselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our latest GPT-3.5 on our internal, adversarially-designed factuality evaluations."}
{"question": "Consider the paper that introduces the model that has a 6-layer encoder and a 6-layer decoder architecture. What specific method was adopted for selecting high-quality, in-domain sentences from the Commoncrawl corpus for back-translation in the English to Russian translation task?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "1907.06616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model with 6-layer encoder and 6-layer decoder architecture?", "answer_anchor": "FSMT", "question_reference": "What specific method was adopted for selecting high-quality, in-domain sentences from the Commoncrawl corpus for back-translation in the English to Russian translation task?", "explanation_reference": "The answer directly addresses the question by specifying the method used to filter the Commoncrawl corpus for back-translation, which is a detail specific to the English to Russian translation task. This method is critical for enhancing the quality of the back-translated data by ensuring it is relevant and high-quality.", "evidence_reference": "We experiment with methods to identify a subset of Commoncrawl that is most similar to Newscrawl. Specifically, we use the in-domain filtering method described in~\\citet{moore2010intelligent}."}
{"question": "Consider the paper that introduces the model which achieves the highest score on the SST-2 dataset. What specific strategy does the model use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model that demonstrates the highest score on SST-2 dataset?", "answer_anchor": "BERT", "question_reference": "What specific strategy does BERT use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "explanation_reference": "The question assesses understanding of BERT's pre-training strategy designed to mitigate the mismatch between the pre-training and fine-tuning stages, specifically focusing on the detailed probabilities of the mixed masking strategy employed.", "evidence_reference": "In Section~\\ref{sec:pretraining_tasks}, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective... The following is an ablation study to evaluate the effect of different masking strategies... \\begin{table}[ht] \\begin{center} {\\small \\begin{tabular}{@{}rrrccc@{}} \\toprule \\multicolumn{3}{c}{Masking Rates} & \\multicolumn{3}{c}{Dev Set Results}  \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.2cm}){4-6} \\textsc{Mask} &\\textsc{Same}&\\textsc{Rnd}& {MNLI} &\\multicolumn{2}{c}{NER} &  & & {\\footnotesize Fine-tune} &   {\\footnotesize Fine-tune}& {\\footnotesize Feature-based} \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.1cm}r{0.1cm}){4-4} \\cmidrule(l{0.2cm}){5-6}  80\\%&10\\%&10\\%&84.2&95.4&94.9... \\bottomrule \\end{tabular} } \\end{center} \\caption{\\label{tab:mask_ablation} Ablation over different masking strategies.} \\end{table}"}
{"question": "Consider the paper that introduces the model placed fourth in the table. What specific aspect of the LegalBERT model's architecture contributes most significantly to its efficiency in terms of training and inference speed compared to other BERT-based models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model shown in the fourth row of the table?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's architecture contributes most significantly to its efficiency in terms of training and inference speed compared to other BERT-based models?", "explanation_reference": "The efficiency of the \\legalbertsmall model in terms of training and inference speed is primarily attributed to its architecture, which includes fewer layers, fewer hidden units, and fewer attention heads compared to other BERT-based models. This streamlined architecture reduces computational requirements and memory usage, making it faster and more resource-efficient.", "evidence_reference": "Our hypothesis is that such a specialised \\bert model can perform well against generic \\bert models, despite its fewer parameters. [...] This light-weight model, trains approx.\\ 4 times faster, while also requiring fewer hardware resources. [...] \\textsc{legal-bert-small} & 35M    & 6  & 512   & 8    & 26       & $2.43\\times$ & $4.00\\times$     & $1.70\\times$."}
{"question": "Consider the paper that introduces the method that is in the second row of the table. What specific performance metric is used to evaluate the effectiveness of the model proposed in the paper, specifically the conceptual neighborhood classifier, in the post-processing strategy?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is in the second row of the table?", "answer_anchor": "ConCN clusters", "question_reference": "What specific performance metric is used to evaluate the effectiveness of the conceptual neighbourhood classifier in the post-processing strategy?", "explanation_reference": "The effectiveness of the conceptual neighbourhood classifier in the post-processing strategy is evaluated based on its precision-oriented approach. This is because the strategy focuses on removing labels that are conceptual neighbours, thus aiming to increase the precision of the label predictions by ensuring that mutually exclusive labels are not predicted for the same entity.", "evidence_reference": "we treat the notion of conceptual neighbourhood in a more informal fashion... Note that this is a precision-oriented strategy."}
{"question": "Consider the paper that introduces the model which performs the second best in the ClinTox dataset. What specific finetuning strategy did the authors find to yield slightly better results for MoleculeNet tasks, and how is it characterized?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model perform the second best in the ClinTox dataset?", "answer_anchor": "MolXPT", "question_reference": "What specific finetuning strategy did the authors find to yield slightly better results for MoleculeNet tasks, and how is it characterized?", "explanation_reference": "The authors explored two finetuning strategies for MoleculeNet tasks: finetuning the full prompts and finetuning the tags only. They found that finetuning the tags only yielded slightly better results, characterized by focusing the finetuning process on the classification tags at the end of the prompts rather than the entire prompt sequence.", "evidence_reference": "According to our exploration, Eqn.(\\ref{eq:finetune_label_only}) achieves slightly better results and we use it for all tasks (see Appendix \\ref{sec:moleculenet_detailed_result} for the results)."}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.679 in the FB15kET dataset. What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the model proposed in the paper?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets MRR score equal to 0.679 in FB15kET datast?", "answer_anchor": "RGCN", "question_reference": "What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the CET model?", "explanation_reference": "The relevance score indicates how strongly a neighbor's attribute (in this case, having won a Pulitzer Prize) correlates with a candidate type (Pulitzer Prize winners) for a given entity (Bob Dylan). The score of 6.93 suggests a strong correlation, implying that this neighbor is a significant indicator that Bob Dylan should be typed as a Pulitzer Prize winner according to the CET model.", "evidence_reference": "(has won prize, Pulitzer Prize) & 6.93"}
{"question": "Consider the paper that introduces the model that achieves a P_k score of 24.8 in the en_disease category. What is the erroneous output fraction for the model proposed in the paper when tested on the Wiki-727k dataset?", "answer": "", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets 24.8 P_k score in en_disease category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What is the erroneous output fraction for structured summarization models when tested on the Wiki-727k dataset?", "explanation_reference": "The erroneous output fraction indicates how frequently the model produces an invalid sentence boundary position. For the QMSum dataset, the structured summarization models did not produce any erroneous segment boundary positions, indicating a high level of accuracy in generating valid sentence indices.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics. \\begin{table}[h] \\small \\centering \\s\\t \\renewcommand{\\arraystretch}{1.4} \\begin{tabular}{cccc} \\toprule Wiki-727K & en\\_city & en\\_disease & QMSum \\\\ \\hline 0.0001 & 0.0025 & 0 & 0 \\\\ \\bottomrule \\end{tabular} \\caption{Fraction of examples with at least one erroneous segment boundary position. This is for the structured summarization models, when tested on the respective test set.} \\label{table:sentpos_nonnumeric} \\end{table}"}
{"question": "Consider the paper that introduces the method shown in the table that achieves a PPL score of 24.466 for the Test Seen task. What specific aspect of the model's representation space, proposed by the paper, potentially allows contrastive search to be directly applicable without contrastive training?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown in the table gets 24.466 PPL for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What specific aspect of the Chinese language model's representation space potentially allows contrastive search to be directly applicable without contrastive training?", "explanation_reference": "The intrinsic property of the Chinese language model, which naturally represents text by characters, might lead to a representation space that displays a high level of isotropy. This property potentially allows contrastive search to be directly applicable without the need for contrastive training.", "evidence_reference": "We see that in all layers (including the final layer), the MLE model displays a similar self-similarity with respect to SimCTG. This observation is quite different from what we see from English language models... We conjecture that this discrepancy might come from the intrinsic property of different languages... For English, current state-of-the-art methods always represent the text into subword units... On the other hand, languages like Chinese are naturally represented by basic units, i.e., characters... As a result, even the vanilla MLE objective can obtain a representation space that displays a high level of isotropy."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 76.3 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What specific advantage does its twin uniform quantization offer for post-softmax and post-GELU activation values in terms of hardware efficiency?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the quant method show 76.3 score on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What specific advantage does the twin uniform quantization offer for post-softmax and post-GELU activation values in terms of hardware efficiency?", "explanation_reference": "The twin uniform quantization is designed to efficiently process on existing hardware devices like CPUs and GPUs by using the shift operation, which avoids the need for format transformation and additional FP32 operations that are more computationally expensive.", "evidence_reference": "Our method uses the shift operation, avoiding the format transformation and extra FP32 multiplication and FP32 addition."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest Method 1 accuracy. What specific performance improvement does using dropout as a regularizer provide for solution-level verifiers compared to token-level verifiers?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What dataset demonstrates the highest accuracy with method 1?", "answer_anchor": "GSM8K", "question_reference": "What specific performance improvement does using dropout as a regularizer provide for solution-level verifiers compared to token-level verifiers?", "explanation_reference": "The paper mentions that using dropout significantly improves solution-level verifiers, mitigating the overfitting that occurs in the unregularized baseline, and notably reaches a similar level of performance as token-level verifiers. This indicates that while token-level verifiers are less susceptible to overfitting and thus might not see as significant an improvement from dropout, solution-level verifiers benefit substantially, enough to match the performance of the more robust token-level verifiers.", "evidence_reference": "In \\Cref{fig:single_token_dropout}, we see that dropout significantly improves solution-level verifiers, mitigating the overfitting that occurs in the unregularized baseline. Notably, using dropout with solution-level verifiers reaches a similar level of performance as token-level verifiers."}
{"question": "Consider the paper that introduces the dataset which has the largest number of instances. What is the average number of aspects per article in the model proposed by the paper, and what percentage of articles have less than 9 aspects?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset with the most number of instances?", "answer_anchor": "QASUM", "question_reference": "What is the average number of aspects per article in the OASum dataset, and what percentage of articles have less than 9 aspects?", "explanation_reference": "The question specifically asks for detailed statistics regarding the distribution of aspects per article within the OASum dataset. The answer directly addresses this by providing the average number of aspects per article and the percentage of articles with fewer than 9 aspects, which are key details extracted from the 'Data Statistics and Analysis' section of the paper.", "evidence_reference": "In \\cref{tab:big_table}, we compare \\textbf{\\DATANAME} with other query/aspect-based summarization datasets. \\textbf{\\DATANAME} contains a significantly larger amount of aspect types. On average, there are 1.82 aspects per article and 99% articles have less than 9 aspects per single document."}
{"question": "Consider the paper that introduces the model that demonstrates the lowest accuracy in the SLOG-all dataset. What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 of the model proposed in the paper exhibit, as shown in the attention visualizations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model demonstrates the lowest accuracy in SLGO-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "explanation_reference": "The answer is directly supported by the descriptions provided in the Attention Visualizations section, where it is mentioned that many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult', and that two attention heads, also in layer 5 of 6, are apparently involved in anaphora resolution. These behaviors indicate that the attention heads have learned to perform tasks related to the structural aspects of sentences.", "evidence_reference": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb `making', completing the phrase `making...more difficult'.  Attentions here shown only for the word `making'. Different colors represent different heads. Best viewed in color. Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the method under which the BIRD benchmark results in the highest execution accuracy. What specific approach does the model proposed in the paper employ to ensure the high quality of SQL annotation and minimize annotation errors?", "answer": "", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Under this method, which benchmark demonstrates the highest execution accuracy?", "answer_anchor": "BIRD", "question_reference": "What specific approach does the \\textsc{Bird} benchmark employ to ensure the high quality of SQL annotation and minimize annotation errors?", "explanation_reference": "The \\textsc{Bird} benchmark employs a double-blind annotation approach to ensure the high quality of SQL annotations. This method involves two independent SQL annotators generating SQLs for the same question without discussion, and only SQLs yielding identical results are collected. This process significantly reduces the SQL annotation error rate by minimizing the likelihood of two skilled annotators producing the same incorrect results when databases have large values.", "evidence_reference": "As shown in Figure \\ref{wkf} (b), we employ a double-blind approach \\citep{csqa2} for SQL annotation. This approach involves two independent SQL annotators who generate SQLs for the same question without discussion. The annotated SQLs are executed in databases, and those yielding identical results are gathered. Otherwise, the SQLs are checked with experts until a consensus is reached. Double-blind procedures can dramatically reduce the SQL annotation error rate, as there is a small probability for two skillful annotators to generate the same incorrect results when databases have large values."}
{"question": "Consider the paper that introduces the LLM model that corresponds to an r score of 0.813. What specific performance improvement does the model proposed in the paper exhibit over its predecessor in the context of the Uniform Bar Exam?", "answer": "", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the LLM model that demonstrates the r score equal to 0.813?", "answer_anchor": "GPT-4", "question_reference": "What specific performance improvement does GPT-4 exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "explanation_reference": "The question assesses understanding of GPT-4's significant improvement in performance on a professional benchmark, the Uniform Bar Exam, compared to its predecessor. This detail highlights GPT-4's advanced capabilities in understanding and generating natural language in complex scenarios.", "evidence_reference": "For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers. This contrasts with GPT-3.5, which scores in the bottom 10%."}
{"question": "Consider the paper that introduces the method that is in the second row of the table. What specific performance metric improvement does the DenoiseFET model achieve when enhanced with Prior Knowledge about Labels (PKL) using BERT-large, compared to its performance without PKL on the UFET benchmark?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is in the second row of the table?", "answer_anchor": "ConCN clusters", "question_reference": "What specific performance metric improvement does the DenoiseFET model achieve when enhanced with Prior Knowledge about Labels (PKL) using BERT-large, compared to its performance without PKL on the UFET benchmark?", "explanation_reference": "The question focuses on the detailed performance improvement of the DenoiseFET model with PKL using BERT-large on the UFET benchmark, which is a specific detail that requires understanding of the experimental results presented in the paper. The answer directly addresses the improvement in F1 score, which is a critical performance metric in entity typing tasks.", "evidence_reference": "DenoiseFET & Bl & 52.6 & 47.5 & 49.8\\\\ DenoiseFET + PKL & Bl & 53.8 & 50.2 & 51.9"}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in few-shot prompting. How does its base version perform on the Asleep at the Keyboard security benchmark when comparing completion and insertion formats in terms of generating valid and insecure code?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shows the second best execuation accuracy in few-shot prompting?", "answer_anchor": "StarCoder", "question_reference": "How does StarCoderBase's performance on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "explanation_reference": "The comparison between completion and insertion formats for StarCoderBase on the Asleep at the Keyboard security benchmark shows that the insertion format leads to a higher percentage of valid code generation and a slightly lower percentage of insecure code generation. This indicates that the insertion format may be more effective for generating secure and valid code.", "evidence_reference": "Completion & StarCoderBase         & 855/1000 (85.50\\%) & 340/855 (39.77\\%) \\\\ Insertion  & StarCoderBase         & 987/1000 (98.70\\%) & 354/987 (35.87\\%)"}
{"question": "Consider the paper that introduces the model that has the highest relative performance in few-shot prompting. What specific method does the model proposed in the paper employ to ensure the attention mechanism focuses on maintaining dialogue control over multiple turns?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model has the highest accuracy in few-shot prompting?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific method does GAtt employ to ensure the attention mechanism focuses on maintaining dialogue control over multiple turns?", "explanation_reference": "GAtt, or Ghost Attention, is designed to improve dialogue control over multiple turns by manipulating the fine-tuning data in a way that enhances the model's focus on relevant instructions throughout a conversation. This method is a strategic intervention in the training process to address the challenge of models forgetting initial instructions after several dialogue turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process."}
{"question": "Consider the paper that introduces the model that demonstrates the lowest zh-en score. What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for tasks where the GWLAN model shows?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model demonstrates the lowest zh-en score?", "answer_anchor": "GWLAN", "question_reference": "What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "explanation_reference": "The \\textsc{WPM-Joint} model, by being a single model trained on multiple related tasks, demonstrates the advantage of simpler deployment compared to the \\textsc{WPM-Sep} model, which requires training and deploying four separate models for different translation contexts.", "evidence_reference": "Compared with \\textsc{WPM-Sep}, \\textsc{WPM-Joint} shows two advantages. On one hand, even there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment."}
{"question": "Consider the paper that introduces the method that shows the lowest overall performance. What specific feature of the ALFRED dataset's expert demonstrations differentiates the model proposed in the paper from using a simple object class prediction approach for interactions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the lowest over performance?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific feature of the ALFRED dataset's expert demonstrations differentiates it from using a simple object class prediction approach for interactions?", "explanation_reference": "The ALFRED dataset's expert demonstrations involve specifying a pixelwise interaction mask of the target object for interactions, which is more realistic and detailed compared to a simple object class prediction approach where localization is treated as a solved problem. This feature allows for precise interaction with objects in the environment, highlighting the dataset's complexity and its focus on realistic simulation for training models.", "evidence_reference": "Motivated by work in robotics on segmentation-based grasping~\\cite{mousavian2019graspnet}, agents in \\dataset{} interact with objects visually, specifying a pixelwise interaction mask of the target object."}
{"question": "Consider the paper that introduces the model shown in the figure that is consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B. What specific preprocessing steps were applied to the XML files during the data curation process for its base version?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shown in the figure consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing steps were applied to the XML files during the data curation process for StarCoderBase?", "explanation_reference": "The question focuses on the specific method used to preprocess XML files during the data curation process for StarCoderBase. The answer directly addresses this by specifying the implementation of a simple XML filter, which is a detailed and specific part of the preprocessing steps mentioned in the paper.", "evidence_reference": "As we inspected the data, we noticed that certain extensions often consisted of XML files. For example, the .sld extension had more than 50% of its files in XML format. To address this, we implemented a simple XML filter that checked for the presence of '<?xml version=' within the first 100 characters of the file."}
{"question": "Consider the paper that introduces the model that corresponds to an F1 score of 65.76 on PDTB-Top. How does its utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact, and what does this indicate about the model's proposed scoring function's effectiveness?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method demonstrates 65.76 F1 score on PDTB-Top?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF framework's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact, and what does this indicate about the scoring function's effectiveness?", "explanation_reference": "The question focuses on the comparison between the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ and its hard-label version $\\mathcal{L}_{L'}$, specifically asking about the performance impact of replacing $\\mathcal{L}_{L}$ with $\\mathcal{L}_{L'}$. The answer highlights that such a replacement leads to a significant decrease in performance, which underscores the importance of the scoring function used in $\\mathcal{L}_{L}$. This function accounts for the nuanced semantic structures within the local hierarchy, thereby contributing to the model's overall effectiveness.", "evidence_reference": "Secondly, we replace the Local Hierarchy-aware Contrastive loss $\\mathcal{L}_{L}$ (Equation (\\ref{equation: soft local})) with the hard-label version $\\mathcal{L}_{L'}$ (Equation (\\ref{equation: hard local})) and find that the performance drops notably."}
{"question": "Consider the paper that introduces the large language model that has the second lowest HVI score among those in the figure corresponding to a purple bar. What specific methodological difference in the evaluation setup for the model's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the large language model that demonstrates the second lowest HVI score shown in purple bar?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "explanation_reference": "This methodological difference is significant because sampling at temperature 0 can lead to more deterministic outcomes based on the generated explanation, potentially affecting the model's performance on these exams in a way that differs from how choices were determined in other exams.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.679 in the FB15kET dataset. How does the model proposed in the paper ensure that every embedding receives sufficient training through its exponentially weighted pooling method in CET?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets MRR score equal to 0.679 in FB15kET datast?", "answer_anchor": "RGCN", "question_reference": "How does the exponentially weighted pooling method in CET ensure that every embedding receives sufficient training?", "explanation_reference": "The exponentially weighted pooling method is designed to have a similar effect to max-pooling but ensures that every input receives a gradient, thus ensuring every embedding gets sufficient training. This method addresses the limitation of max-pooling, where only a small part of the input receives a gradient, potentially leading to insufficient training of some embeddings.", "evidence_reference": "Choosing the max value as the final result makes only a small part of the input get the gradient which means some embeddings may not be sufficiently trained. As a result, the model may fail to represent every attribute of an entity accurately. In practice, we adopt an exponentially weighted pooling method similar to softpool~\\citep{DBLP:journals/corr/abs-2101-00440}: \\begin{align} & R_{u, i}=\\mathrm{pool}(\\{R_{u, i}^{Agg2T}, R_{(n_r, n_e), i}^{N2T} \\notag \\\\ & \\vert \\; \\forall (n_e, n_r)\\in \\mathcal{N}(u)\\}),for\\; i \\in 1,2,\\dots, L, \\end{align} \\begin{equation} \\mathrm{pool}(\\{x_1, x_2,...,x_n\\})=\\sum_{i=1}^{n}w_ix_i, \\end{equation} \\begin{equation} \\label{equation:weight} w_i = \\frac{\\exp \\alpha x_i}{\\sum_{k=1}^{n}\\exp \\alpha x_k}, \\end{equation} where $R_{u, i}\\in \\mathbb{R}$ is the relevance score between entity $u$ and type i. $\\alpha \\in \\mathbb{R}^+$ is a hyperparameter that controls the temperature of the pooling process. The higher $R_{u, i}$ means entity $u$ is more likely to have type i. This pooling method has a similar effect to max-pooling but can generate a gradient for every input which ensures every embedding gets sufficient training."}
{"question": "Consider the paper that introduces the method which is listed in the table right below the PCP method. What is the primary intuition behind the Multi-Head Interactive Attention (MHIA) module proposed in the model, specifically within the GOLF framework, for IDRR?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method in the table right below the PCP method?", "answer_anchor": "GOLF", "question_reference": "What is the primary intuition behind the Multi-Head Interactive Attention (MHIA) module proposed in the GOLF framework for IDRR?", "explanation_reference": "The MHIA module is designed to facilitate bilateral multi-perspective matching between two arguments by taking one argument as Query and the other as Key and Value, and vice versa, aiming to mimic the human cognitive process of considering perspectives from both sides of the discourse.", "evidence_reference": "To this end, we propose a Multi-Head Interactive Attention (MHIA) module to facilitate bilateral multi-perspective matching between $arg_1$ and $arg_2$. The intuition behind MHIA is to simulate human\u2019s transposition thinking process: respectively considering each other\u2019s focus from the standpoint of $arg_1$ and $arg_2$."}
{"question": "Consider the paper that introduces the method that has a score of 73.6 in the CB dataset with 4-shot prompting. What specific strategy is employed during target adaptation to manage the learning rates for the task-shared and task-specific components in the model proposed in the paper?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having score of 73.6 in CB dataset with 4-shot prompting?", "answer_anchor": "MPT", "question_reference": "What specific strategy is employed during target adaptation to manage the learning rates for the task-shared and task-specific components in \\ours?", "explanation_reference": "The paper specifies that during target adaptation, a strategy of two-speed learning rates is used for the task-shared and task-specific components, indicating a differentiated approach to updating these components.", "evidence_reference": "During target adaptation, we use a strategy of two-speed learning rates for those two components, as in~\\citet{ponti2022combining}. Specifically, we set the learning rate to $0.3$ and $0.4$ for the task-shared and task-specific components, respectively, during target task adaptation."}
{"question": "Consider the paper that introduces the transformer-based method that achieves the highest MRR score on the FB15kET dataset. What specific role does the context transformer play in the approach for enhancing entity representation?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which transformer-based method gets the highest MRR score in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What specific role does the context transformer play in the TET approach for enhancing entity representation?", "explanation_reference": "The answer directly addresses the question by specifying the unique function of the context transformer within the TET framework, which is to differentiate how neighbours' information is integrated through pairwise exchanges, ensuring the graph's structural integrity is maintained.", "evidence_reference": "a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
{"question": "Consider the paper that introduces the method that is in the third row of the table. What is the specific performance gap in execution accuracy between GPT-4 only on the testing data of the model proposed in the paper with and without external knowledge?", "answer": "", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "BIRD", "question_reference": "Based on the experimental results, what is the specific performance gap in execution accuracy between GPT-4 only on the \\textsc{Bird} testing data with and without external knowledge?", "explanation_reference": "The performance gap in execution accuracy between the best-performing LLM (GPT-4 + DIN-SQL) on the \\textsc{Bird} testing data with and without external knowledge can be inferred from the context that GPT-4 + DIN-SQL achieves a state-of-the-art result with external knowledge but does not provide a performance metric without external knowledge. Given the human performance improvement of 20.59% with external knowledge, and considering the context of LLM improvements with external knowledge, a specific performance gap can be estimated. However, the exact figure is not directly provided, making the question challenging and requiring an understanding of the impact of external knowledge on LLM performance.", "evidence_reference": "Table 2"}
{"question": "Consider the paper that introduces the method in the figure that demonstrates the highest Toxicity Probability Score when the number of samples equals 1M. What specific computational advantage does the model proposed in the paper, GRIT, have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method in the figure demonstrates the highest Toxicity Probability Score when number of samples equal to 1M?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "explanation_reference": "The question focuses on the detailed part of the computational efficiency of GRIT compared to other methods, specifically in the context of feature extraction inference time. This detail highlights GRIT's significant improvement in computational speed.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the method that achieves a relatively constant MRR score in the FB15k-237 dataset as entity code entropy increases. Which dataset demonstrated a significant performance degradation in the model proposed by the paper when multi-hop neighbor information was removed?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method shows a huge increase as entity code entropy increases in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which dataset showed a significant performance degradation when multi-hop neighbor information was removed?", "explanation_reference": "The ablation study results indicate that removing multi-hop neighbor information ('w/o MulHop') dramatically affected the performance on the WN18RR dataset, as evidenced by the significant drop in performance metrics compared to other ablation settings.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the method that has an F1 score of 54.83. What specific adaptation does the model proposed in the paper employ to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having 54.84 F1 score?", "answer_anchor": "LayoutXLM", "question_reference": "What specific adaptation does the LayoutXLM model employ to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "explanation_reference": "The adaptation mentioned is specifically designed to handle the diversity in the definition of linguistic units across different languages, which is a challenge for language-specific pre-processing. By calculating the bounding box of each token by merging the bounding boxes of all characters it contains, LayoutXLM efficiently unifies the multilingual multimodal inputs without needing language-specific pre-processing.", "evidence_reference": "However, for LayoutXLM, this strategy is not applicable because the definition of the linguistic unit is different from language to language. To prevent the language-specific pre-processing, we decide to obtain the character-level bounding boxes. After the tokenization using SentencePiece with a unigram language model, we calculate the bounding box of each token by merging the bounding boxes of all characters it contains."}
{"question": "Consider the paper that examines the dataset which has the largest number of Queries|Aspects in the ABS category. What specific aspect of the baseline models' performance does the paper identify as a limitation in handling time-sensitive events across multiple documents?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset with the most number of Queries|Aspects in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What specific aspect of the WikiAsp dataset's baseline models' performance does the paper identify as a limitation in handling time-sensitive events across multiple documents?", "explanation_reference": "The paper discusses the challenges faced by the baseline models in summarizing aspects that require content to be presented in a particular order, such as time series events. It specifically mentions that maintaining chronological order when aggregating information from scattered sources across multiple documents adds extra difficulty, highlighting this as a limitation of the models' performance in handling time-sensitive events.", "evidence_reference": "For example, aspects that require summarizing contents in a particular order (\\textit{e.g.}, time series events) in a multi-document setting adds extra difficulty because of the need for correctly ordering scattered (and possibly duplicate) pieces of information from different sources."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. How does the performance of the model proposed in the paper using contextualized label representations with the scheme '(BIO-TAG) LABEL' compare to the model using only label names in the 1-shot setting on the FEW-NERD-Person dataset?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shown in the penult row of the table?", "answer_anchor": "LSFS", "question_reference": "How does the performance of the model using contextualized label representations with the scheme '(\\textit{BIO-TAG}) \\textit{LABEL}' compare to the model using only label names in the 1-shot setting on the FEW-NERD-Person dataset?", "explanation_reference": "The performance of the model using contextualized label representations with the scheme '(\\textit{BIO-TAG}) \\textit{LABEL}' is lower than the model using only label names in the 1-shot setting on the FEW-NERD-Person dataset, as indicated by the micro F1 scores.", "evidence_reference": "FN-Person & TransferBERT & 13.2 $\\pm$ 5.0  & 24.0 $\\pm$ 7.4  & 48.7 $\\pm$ 3.4  & 66.9 $\\pm$ 3.0 \\\\ & Ours, label name only & \\bf 32.5 $\\pm$ 8.1  & \\bf 51.0 $\\pm$ 7.0  & \\bf 66.2 $\\pm$ 2.0  & \\bf 72.0 $\\pm$ 0.7 \\\\ \\cmidrule(lr){2-7} & (\\textit{BIO-TAG}) \\textit{LABEL} & 29.0 $\\pm$ 7.2  & 50.6 $\\pm$ 6.3  & \\bf 66.2 $\\pm$ 2.0  & 71.2 $\\pm$ 0.9 \\\\"}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.679 in the FB15kET dataset. What specific methodological limitation does the exponentially weighted pooling method it employs address in the context of entity typing inference, and how does this approach compare to max-pooling in terms of model training?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets MRR score equal to 0.679 in FB15kET datast?", "answer_anchor": "RGCN", "question_reference": "What specific methodological limitation does the exponentially weighted pooling method address in the context of entity typing inference, and how does it compare to max-pooling in terms of model training?", "explanation_reference": "The exponentially weighted pooling method is designed to overcome a specific limitation of max-pooling, where only a small part of the input receives a gradient during backpropagation. This can lead to insufficient training of some embeddings, as they may not represent every attribute of an entity accurately. By ensuring that every input receives a gradient, the exponentially weighted pooling method allows for more comprehensive training of the embeddings, potentially leading to more accurate entity typing results.", "evidence_reference": "Choosing the max value as the final result makes only a small part of the input get the gradient which means some embeddings may not be sufficiently trained. As a result, the model may fail to represent every attribute of an entity accurately. In practice, we adopt an exponentially weighted pooling method similar to softpool~\\citep{DBLP:journals/corr/abs-2101-00440}: \\begin{align} & R_{u, i}=\\mathrm{pool}(\\{R_{u, i}^{Agg2T}, R_{(n_r, n_e), i}^{N2T} \\notag \\\\ & \\vert \\; \\forall (n_e, n_r)\\in \\mathcal{N}(u)\\}),for\\; i in 1,2,\\dots, L, \\end{align} \\begin{equation} \\mathrm{pool}(\\{x_1, x_2,...,x_n\\})=\\sum_{i=1}^{n}w_ix_i, \\end{equation} \\begin{equation} \\label{equation:weight} w_i = \\frac{\\exp \\alpha x_i}{\\sum_{k=1}^{n}\\exp \\alpha x_k}, \\end{equation} where $R_{u, i}\\in \\mathbb{R}$ is the relevance score between entity $u$ and type i. $\\alpha \\in \\mathbb{R}^+$ is a hyperparameter that controls the temperature of the pooling process. The higher $R_{u, i}$ means entity $u$ is more likely to have type i. This pooling method has a similar effect to max-pooling but can generate a gradient for every input which ensures every embedding gets sufficient training."}
{"question": "Consider the paper that introduces SimCTG, the method shown in the table that achieves a PPL score of 24.466 for the Test Seen task. What is the effect of the contrastive loss margin \\(\\rho\\) on the perplexity of SimCTG, when it is set to either too small or too large values?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown in the table gets 24.466 PPL for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the effect of the contrastive loss margin \\(\\rho\\) on the model perplexity when it is set to either too small or too large values?", "explanation_reference": "The paper discusses how setting the contrastive loss margin \\(\\rho\\) to either too small (e.g., \\(0.1\\)) or too large (e.g., \\(1.0\\)) values leads to a representation space that is either less or too isotropic, resulting in sub-optimal perplexity. This indicates that there is an optimal range for \\(\\rho\\) that balances the isotropy of the representation space to achieve better model perplexity.", "evidence_reference": "However, when \\(\\rho\\) is either too small (e.g., \\(0.1\\)) or large (e.g., \\(1.0\\)), the learned representation space of the model would be either less or too isotropic, leading to a sub-optimal perplexity."}
{"question": "Consider the paper that introduces the model that results in the second lowest accuracy in the COGS-all dataset. What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model demonstrates the second lowest accuracy in COGS-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "explanation_reference": "The answer is directly supported by the descriptions provided in the Attention Visualizations section, where it is mentioned that many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult', and that two attention heads, also in layer 5 of 6, are apparently involved in anaphora resolution. These behaviors indicate that the attention heads have learned to perform tasks related to the structural aspects of sentences.", "evidence_reference": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb `making', completing the phrase `making...more difficult'.  Attentions here shown only for the word `making'. Different colors represent different heads. Best viewed in color. Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the method that has a perplexity of approximately 30 and an average max toxicity of around 0.4. How does the model proposed in the paper ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is method with around 30 perplexity and around 0.4 average max toxicity?", "answer_anchor": "PPLM", "question_reference": "How does the proposed K2T method ensure the appearance of guide words without negatively impacting the fluency and diversity of the generated text?", "explanation_reference": "The method described as K2T modifies the score function at each decoding step by adding a shift towards the semantic space of a given guide word. This approach encourages the explicit appearance of the guide word and appropriate context for it, without requiring additional models or fine-tuning, thus maintaining the fluency and diversity of the generated text.", "evidence_reference": "In this work, we propose \\emph{Keyword2Text} (K2T), a new and simple plug-and-play method for exerting hard control during text generation. By modifying the score function, we can incorporate a semantic shift at decoding time, without additional models or fine-tuning."}
{"question": "Consider the paper that introduces the Seq2Exp model marked with the Club citation symbol. How does its performance on the extended FinQA dataset compare to its performance on the original FinQA dataset in terms of Execution Accuracy (Exec Acc) and Program Accuracy (Prog Acc)?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which Seq2Exp model is with Club citation mark?", "answer_anchor": "Elastic", "question_reference": "How does the ELASTIC model's performance on the extended FinQA dataset compare to its performance on the original FinQA dataset in terms of Execution Accuracy (Exec Acc) and Program Accuracy (Prog Acc)?", "explanation_reference": "The question directly asks for a comparison of the ELASTIC model's performance metrics on different subsets of the FinQA dataset. The answer is derived from the reported performance metrics, indicating a decrease in performance on the combined dataset but an increase on the extended dataset alone, showcasing the model's adaptability to diverse operators.", "evidence_reference": "For the performance on the combined test data (original FinQA + extended FinQA), ELASTIC (RoBERTa-large) achieves slightly lower scores (64.5 of Exec Acc and 63.8 of Prog Acc), compared to the results of ELASTIC (RoBERTa-large) achieved on original FinQA dataset (68.96 of Exec Acc and 65.21 of Prog Acc). We also report the metric scores of ELASTIC (RoBERTa-large) achieved on test data from the extended FinQA dataset: 90.0 on both Exec Acc and Prog Acc."}
{"question": "Consider the paper that introduces the model that shows the best overall performance in the 'Foreign' scenario. What specific condition is required for the DA-WR algorithm to ensure convergence to the target distribution in terms of the maximum difference between indicator functions of the initial and augmented datasets?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shown the best overall performance?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition is required for the DA-WR algorithm to ensure convergence to the target distribution in terms of the maximum difference between indicator functions of the initial and augmented datasets?", "explanation_reference": "The condition required for the DA-WR algorithm to ensure convergence to the target distribution is that the maximum difference between the indicator functions of the initial and augmented datasets, for all x in the support, should be o(1/n). This condition ensures that the difference diminishes as the sample size increases, allowing the cumulative distribution function resulting from the DA-WR algorithm to converge in probabilities to the target distribution as n tends to infinity.", "evidence_reference": "we should say that the imbalanced phenomenon occurs when $F \\\\neq F_0$. To measure the degree of imbalance we can denote by  $\\\\widehat{F}$ and $\\\\widehat{\\\\mathbb{P}}$ the empirical estimators and we propose the following definition: we  face a $(\\\\alpha, \\\\beta)$-imbalanced regression problem if there is a set $\\\\chi\\\\subset\\\\mathcal{X}$ with  $\\\\mathbb{P}_0(\\\\boldsymbol{X}\\\\in\\\\chi) \\\\geq \\\\beta$ such that $\\\\lvert \\\\frac{\\\\widehat{\\\\mathbb{P}}(\\\\boldsymbol{X}\\\\in\\\\chi)}{\\\\mathbb{P}_0(\\\\boldsymbol{X}\\\\in\\\\chi)} - 1 \\\\rvert > \\\\alpha$."}
{"question": "Consider the paper that introduces the method shown in the fifth row of the table. How does the order of in-context examples influence the performance of the model proposed in the paper on the NQ dataset, and what is the observed effect when the examples are arranged in reverse order?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is shown in the fifth row in the table?", "answer_anchor": "KATE", "question_reference": "How does the order of in-context examples influence the performance of KATE on the NQ dataset, and what is the observed effect when the examples are arranged in reverse order?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results on the NQ dataset revealed that arranging the examples in reverse order, where the most similar sentences are placed closer to the test example, yielded the best performance. This suggests that the proximity of similar sentences to the test example may help GPT-3 leverage the corresponding information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the large language model which has the second lowest HVI score among those in the figure corresponding to a purple bar. What specific methodological difference in the evaluation of the model's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the large language model that demonstrates the second lowest HVI score shown in purple bar?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard procedure used for most exam runs, where the model's letter choice is extracted directly from the explanation. This approach for the USABO and SAT reading/writing runs indicates a unique handling of these exams, which could contribute to the minimal impact on the overall results, as it relies on the model's generated explanation to determine the final answer choice.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the Seq2Exp model marked with the Club citation symbol. What specific mechanism does the model proposed in the paper employ to prevent the interactive distraction between operators and operands during the generation of numerical reasoning programs?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which Seq2Exp model is with Club citation mark?", "answer_anchor": "Elastic", "question_reference": "What specific mechanism does ELASTIC employ to prevent the interactive distraction between operators and operands during the generation of numerical reasoning programs?", "explanation_reference": "ELASTIC's design to separate the generation of operators and operands helps in preventing potential interactive distractions between them, which in turn minimizes cascading errors during the generation of complex numerical reasoning programs.", "evidence_reference": "Because ELASTIC separates the generation procedures for operators and operands, which prevents the potential interactive distraction between operators and operands. This makes ELASTIC less liable to being influenced by the cascading error."}
{"question": "Consider the paper that introduces the method that has an F1 score of 75. What specific advantage does the integration of Coordinate Convolution (CoordConv) provide to the model in terms of its performance on the Entity Labeling and Entity Linking tasks?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having 75 F1 score?", "answer_anchor": "MSAU-PAF", "question_reference": "What specific advantage does the integration of Coordinate Convolution (CoordConv) provide to the MSAU-PAF model in terms of model performance on the Entity Labeling and Entity Linking tasks?", "explanation_reference": "The integration of Coordinate Convolution (CoordConv) into the MSAU-PAF model allows the convolution to access its own input coordinates by concatenating extra coordinate channels, which provides translational information that enhances the model's performance. This specific advantage is quantified as a total gain of 0.02 in F1 score for both the Entity Labeling and Entity Linking tasks, indicating an improvement in the model's ability to accurately label and link entities.", "evidence_reference": "As shown in Table \\ref{table:ablation}, when making the convolution access to its own input coordinates through concatenating extra coordinate channels, MSAU-PAF with Coordinate Convolution experienced a total gain of 0.02 in F1 score in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the method that is shown in the fifth row of the table, specifically when applied in the ablation study on the effect of in-context example orders for GPT-3 on the NQ dataset using the model proposed in the paper$_{\\text{nli+sts-b}}$. Which specific order of in-context examples yielded the highest EM score?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is shown in the fifth row in the table?", "answer_anchor": "KATE", "question_reference": "In the ablation study on the effect of in-context example orders for GPT-3 on the NQ dataset using KATE$_{\\\\text{nli+sts-b}}$, which specific order of in-context examples yielded the highest EM score?", "explanation_reference": "The question focuses on the detailed part of the paper where the authors conducted an ablation study to understand how the order of in-context examples affects the performance of GPT-3. The highest EM score indicates the most effective order for arranging in-context examples to improve GPT-3's performance on the NQ dataset.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best."}
{"question": "Consider the paper that introduces the model that has an F1 score higher than PCP's but lower than DiscoPrompt's on PDTB-Top. What is the primary intuition behind the Multi-Head Interactive Attention (MHIA) module in the model proposed by the paper for IDRR?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method with an F1 score higher than PCP but lower than DiscoPrompt?", "answer_anchor": "GOLF", "question_reference": "What is the primary intuition behind the Multi-Head Interactive Attention (MHIA) module proposed in the GOLF framework for IDRR?", "explanation_reference": "The MHIA module is designed to facilitate bilateral multi-perspective matching between two arguments by taking one argument as Query and the other as Key and Value, and vice versa, aiming to mimic the human cognitive process of considering perspectives from both sides of the discourse.", "evidence_reference": "To this end, we propose a Multi-Head Interactive Attention (MHIA) module to facilitate bilateral multi-perspective matching between $arg_1$ and $arg_2$. The intuition behind MHIA is to simulate human\u2019s transposition thinking process: respectively considering each other\u2019s focus from the standpoint of $arg_1$ and $arg_2$."}
{"question": "Consider the paper that introduces the method that achieves the highest score in 10-shot prompting. How does the performance of the model proposed in the paper using contextualized label representations with the scheme '(BIO-TAG) LABEL' compare to the model using only label names in the 1-shot setting on the FEW-NERD-Person dataset?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method demonstrates highest scores in 10-shot prompting?", "answer_anchor": "LSFS", "question_reference": "How does the performance of the model using contextualized label representations with the scheme '(\\textit{BIO-TAG}) \\textit{LABEL}' compare to the model using only label names in the 1-shot setting on the FEW-NERD-Person dataset?", "explanation_reference": "The performance of the model using contextualized label representations with the scheme '(\\textit{BIO-TAG}) \\textit{LABEL}' is lower than the model using only label names in the 1-shot setting on the FEW-NERD-Person dataset, as indicated by the micro F1 scores.", "evidence_reference": "FN-Person & TransferBERT & 13.2 $\\pm$ 5.0  & 24.0 $\\pm$ 7.4  & 48.7 $\\pm$ 3.4  & 66.9 $\\pm$ 3.0 \\\\ & Ours, label name only & \\bf 32.5 $\\pm$ 8.1  & \\bf 51.0 $\\pm$ 7.0  & \\bf 66.2 $\\pm$ 2.0  & \\bf 72.0 $\\pm$ 0.7 \\\\ \\cmidrule(lr){2-7} & (\\textit{BIO-TAG}) \\textit{LABEL} & 29.0 $\\pm$ 7.2  & 50.6 $\\pm$ 6.3  & \\bf 66.2 $\\pm$ 2.0  & 71.2 $\\pm$ 0.9 \\\\"}
{"question": "Consider the paper that introduces the method that has three empty entries in the table for the mathematical reasoning and commonsense reasoning tasks. What key modification did the authors make to the few-shot prompts to improve the quality of chain of thought reasoning generated by the teacher model?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has three empty entries in mathematical reasoning and commonsense reasoning tasks?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What key modification to the few-shot prompts did the authors make to improve the quality of chain of thought reasoning generated by the teacher model?", "explanation_reference": "The key modification mentioned in the paper aimed to improve the quality of the generated chain of thought by guiding the teacher model with the target answer, which helps in correcting small mistakes in the CoT reasoning.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the method which has a lower F1 score than TPP and a higher F1 score than BROS. What specific advantage does the incorporation of Part-Intensity Fields (PIF) and Part-Association Fields (PAF) modules provide to the model, in terms of entity linking?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having lower F1 score than TPP and higher F1 score than BROS?", "answer_anchor": "MSAU-PAF", "question_reference": "What specific advantage does the incorporation of Part-Intensity Fields (PIF) and Part-Association Fields (PAF) modules provide to the MSAU-PAF model in terms of entity linking?", "explanation_reference": "The PIF-PAF modules, by encoding fine-grained information and confidently predicting confidence association between entities, address the challenge of linking entities in dense and occluded documents. This is particularly important for the Entity Linking task, where the model needs to determine the associations between entities that are not necessarily close to each other and could be far away. The effectiveness of PIF-PAF in handling such scenarios contributes to the overall performance improvement of the MSAU-PAF model.", "evidence_reference": "The PIF-PAF heads could particularly handle well in densely and occluded documents via effectively encoding fine-grained information and confidently predicting confidence association between two entities with composite field structure."}
{"question": "Consider the paper that introduces the method represented by the blue line in the figure. How is the initial representation for non-reserved entities generated before being input into the GNN in its entity-agnostic encoding process?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method represented in the blue line from the figure?", "answer_anchor": "EARL", "question_reference": "In the EARL model's entity-agnostic encoding process, how is the initial representation for non-reserved entities generated before being input into the GNN?", "explanation_reference": "The initial representation for non-reserved entities in the EARL model is generated by first encoding the ConRel and $k$NResEnt information separately and then combining these two encoded pieces of information. This combination is achieved through a concatenation followed by a transformation using a 2-layer MLP, as described in the Entity-Agnostic Encoding section.", "evidence_reference": "In our GNN framework, similar to previous works \\cite{CompGCN, MaKEr}, we use a linear transformation on the concatenation of entity and relation representations to aggregate the neighbor information. Specifically, the message aggregation for the entity $e$ is: \\begin{equation} \\begin{aligned} \\mathbf{m}_{e}^{l} = \\sum_{(r, t) \\in \\mathcal{O}(e)} \\mathbf{W}_{\\text{out}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_t] + \\sum_{(r,h) \\in \\mathcal{I}(e)} \\mathbf{W}_{\\text{in}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_h], \\label{eq:gnn-agg} \\end{aligned} \\end{equation} where $\\mathcal{O}(e)$ denotes the out-going relation-entity pair set of $e$ and $\\mathcal{I}(e)$ denotes the in-going relation-entity pair set. $\\mathbf{W}_{\\text{out}}^l$ and $\\mathbf{W}_{\\text{in}}^l$ are transformation matrices for out-going and in-going pairs. $l \\in [0, \\dots, L]$ denotes the layer of GNN and $L$ is the total number of GNN layers. The input entity representations are calculated in Equation (\\ref{eq:info-combine}), and the input relation representations (e.g., $\\mathbf{h}_{r}^{0}$) are looked up in a trainable relation embedding matrix $\\mathbf{R} \\in \\mathbb{R}^{|\\mathcal{R}|\\times d}$.  The entity representation of $e$ in the GNN is updated as follows: \\begin{equation} \\mathbf{h}_{e}^{l+1} = \\sigma \\left( \\frac{1}{c}\\mathbf{m}_{e}^{l} + \\mathbf{W}_{\\text{self}}^{l} \\mathbf{h}_{e}^{l} \\right), \\label{eq:gnn-update} \\end{equation} where $c=|\\mathcal{I}(e)+\\mathcal{O}(e)|$ is a normalization constant. $\\mathbf{W}_{\\rm self}^{l}$ is a matrix for self representation update, and $\\sigma$ is an activation function. Furthermore, relation representations will also be updated in each layer: $\\mathbf{h}_{r}^{l+1} = \\sigma \\left( \\mathbf{W}_{\\text{rel}}^{l} \\mathbf{h}_{r}^{l} \\right)$. We use the output representations in the $L$-th layer for entities and relations as their embeddings to calculate scores next."}
{"question": "Consider the paper that introduces the method that exhibits an accuracy score of 71.0 on the VQA-v2 task. How does the introduction of the $\\tiny{<}obj\\tiny{>}$ token in the model's output sequence design proposed in the paper impact its performance on the referring expression comprehension task?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows 71.0 accuracy score on VQA-v2 task?", "answer_anchor": "UniTAB", "question_reference": "How does the introduction of the $\\tiny{<}obj\\tiny{>}$ token in UniTAB's output sequence design impact the model's performance on the referring expression comprehension task?", "explanation_reference": "The introduction of the $\\tiny{<}obj\\tiny{>}$ token not only naturally represents the word-box alignments but also simplifies the sequence prediction by providing hints of the text-box code-switching, which in turn helps improve the model's performance on tasks like referring expression comprehension by around 1%.", "evidence_reference": "Table~\\ref{table:objtoken} shows the experiments on the Refcocog dataset~\\cite{mao2016generation}. The \\modelname$_\\text{Separate}$ baseline inserts a pair of $\\tiny{<}obj\\tiny{>}$ and ${{\\tiny<}{\\tiny\\backslash}\\text{obj}{\\tiny>}}$ tokens before and after a word-box token segment. We experiment with removing the ${{\\tiny<}{\\tiny\\backslash}\\text{obj}{\\tiny>}}$ token, or both special tokens. We observe an around $1\\%$ accuracy improvement by adding $\\tiny{<}obj\\tiny{>}$ tokens."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest Method 1 accuracy. What specific aspect of the token-level verifiers' training procedure is likely responsible for their initial uncertainty in solution correctness, as observed in the verifier visualization section?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What dataset demonstrates the highest accuracy with method 1?", "answer_anchor": "GSM8K", "question_reference": "What specific aspect of the token-level verifiers' training procedure is likely responsible for their initial uncertainty in solution correctness, as observed in the verifier visualization section?", "explanation_reference": "The observed initial uncertainty of the token-level verifiers in assessing solution correctness is attributed to the training procedure, where a significant portion of the training data consists of incorrect model-generated samples. This exposure to a high volume of incorrect samples during training likely conditions the verifier to start with a baseline of uncertainty, gradually adjusting its confidence as more of the solution is revealed and it can better assess correctness.", "evidence_reference": "Note that the model is initially unsure about whether the solution is correct and gradually gains certainty as the solution progresses: this is likely a property of the verifier training procedure, where it trains on a large fraction of incorrect model-generated samples."}
{"question": "Consider the paper that introduces the model that achieves a BLEURT score of 0.4126. What is the K-L divergence between the predicted and ground-truth question type distributions on the test set?", "answer": "", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model gets 0.4126 in BLEURT score?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the predicted and ground-truth question type distributions on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how closely the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the method listed in the table below the VL-BART method and above the OFA-base method. What is the improvement in grounding F1 score from the baseline to the UniTAB model in the grounded captioning task on the Flickr30k Entities dataset?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the table below VL-BART method and above OFA-base method", "answer_anchor": "UniTAB", "question_reference": "What is the improvement in grounding F1 score from the baseline to \\modelname~in the grounded captioning task on the Flickr30k Entities dataset?", "explanation_reference": "The improvement in grounding F1 score from the baseline to \\modelname~is calculated by subtracting the baseline F1 score from the \\modelname~F1 score. The baseline F1 score is given as 8.44 (from the Cyclical model), and the \\modelname~F1 score is 12.95. Therefore, the improvement is 12.95 - 8.44 = 4.51.", "evidence_reference": "Cyclical~\\cite{ma2019learning} & 26.8 & 22.4 & 61.1 & 16.8 & 8.44 & 22.78 \\\\ \\hline \\modelname & \\textbf{30.1} & \\textbf{23.7} & \\textbf{69.7} & \\textbf{17.4} & \\textbf{12.95} & \\textbf{34.79}"}
{"question": "Consider the paper that introduces the method that has a perplexity of 60. How does the model's, proposed by the paper, zero-shot capability to generalize to new control codes potentially emerge from its training on generative classifiers?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having 60 perplexity?", "answer_anchor": "GeDi", "question_reference": "How does GeDi's zero-shot capability to generalize to new control codes potentially emerge from its training on generative classifiers?", "explanation_reference": "GeDi's ability to generalize to new control codes in a zero-shot manner is likely because generative classifiers can classify unseen topics zero-shot from learned word embeddings. This foundational aspect of generative classifiers enables GeDi to guide generation towards a wide array of topics beyond its initial training scope.", "evidence_reference": "This ability likely emerges because generative classifiers can classify unseen topics zero-shot from learned word embeddings \\citep{yogatama2017generative}, and GeDi uses generative classifiers to guide generation."}
{"question": "Consider the paper that introduces the benchmark that has a higher 'Generation Token Length' than ShareGPT and GSM8k. What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to answer-only prompting?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which benchmark has the higher 'Generation Token Length' than ShareGPT and GSM8k?", "answer_anchor": "BBH", "question_reference": "What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to answer-only prompting?", "explanation_reference": "The paper suggests that the few-shot evaluation of PaLM 540B with answer-only prompting, which includes both a task instruction and answer options, demonstrates the effect of including these elements in the prompt. This approach outperforms the average human-rater on 6 out of 23 BBH tasks and is overall 1.4% better than the BIG-Bench reported result, indicating that the original setup underestimated language model performance.", "evidence_reference": "The few-shot evaluation of PaLM 540B  with answer-only prompting in this paper, however, outperforms the average human-rater on 6 out of 23 \\bbh{} tasks and is overall 1.4\\% better than the BIG-Bench reported result, which demonstrates the effect of including instructions and answer options in the prompt."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.6712 on the Hate dataset. In its methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets mean classification accuracy 0.6712 on Hate dataset?", "answer_anchor": "DPT", "question_reference": "In the methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "explanation_reference": "The modification ensures that the calculated value reflects performance deterioration in a consistent manner, increasing as performance worsens, irrespective of whether the misalignment is due to training data being from the past or future relative to the evaluation data.", "evidence_reference": "Let $S_{t' \\shortto t}$ indicate the performance a model trained on timestamp $t'$ data and evaluated on the timestamp $t$. Let $$ {D} (t' \\shortto{} t) = -\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t), $$  In other words, ${D} (t' \\shortto{} t)$ is a modified difference in performance between a aligned and misaligned models."}
{"question": "Consider the paper that introduces the model that results in the highest MCD 1 score. What specific mechanism does it employ to ensure the encoder exploits specialized information for each prediction?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2110.04655", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model dmonstrates the highest MCD 1 score?", "answer_anchor": "Dangle", "question_reference": "What specific mechanism does the \\textsc{Dangle} model employ to ensure the encoder exploits specialized information for each prediction?", "explanation_reference": "The mechanism of adaptively re-encoding the source input by conditioning the source representations on the newly decoded target context ensures that the encoder exploits specialized information relevant only to the current prediction. This approach is designed to improve disentanglement and compositional generalization by allowing the model to focus on relevant information for each prediction rather than trying to capture all semantic factors in a single forward pass.", "evidence_reference": "To alleviate this issue, we propose to learn specialized source representations for different predictions by adaptively re-encoding the source input at every step of the decoding."}
{"question": "Consider the paper that introduces the method which is represented by the green line in the figure. What is the relative increase in Mean Reciprocal Rank (MRR) achieved by the model proposed in the paper, EARL Quantization, compared to RotatE on the FB15k-237 dataset using a similar parameter budget?\n\nA) 5%\nB) 10%\nC) 15%\nD) 20%", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method represented in the green line from the figure?", "answer_anchor": "EARL Quantization", "question_reference": "What is the relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "explanation_reference": "The relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset is mentioned in the section summarizing the main results, where it states that EARL uses only 62% parameters and obtains a relative increase of 4.7% on MRR in comparison with RotatE.", "evidence_reference": "Specifically, on FB15k-237, \\model~uses only 62\\% parameters and obtains a relative increase of 4.7\\% on MRR in comparison with RotatE."}
{"question": "Consider the paper that introduces the model that corresponds to the brown bars in the figure. How does its GAtt method specifically address the issue of multi-turn consistency in dialogue models?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model is demonstrated in the brown color?", "answer_anchor": "LLaMA-30B", "question_reference": "How does the GAtt method specifically address the issue of multi-turn consistency in dialogue models?", "explanation_reference": "The GAtt method addresses multi-turn consistency by ensuring that a specific instruction is respected throughout the dialogue. It does this by artificially adding the instruction to all user messages in the dialogue during training, and then selectively applying loss only to the tokens related to the most recent turn and the initial instruction, effectively teaching the model to maintain focus on the instruction across multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right). [...] Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages."}
{"question": "Consider the paper that introduces the method that results in a score of 22.4 in the GSM8K dataset. What specific dynamic programming algorithm does the model proposed in the paper slightly tweak for its tokenizer alignment method's implementation?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2301.12726", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method demonstrates score of 22.4 in GSM8K dataset?", "answer_anchor": "SpecialFT", "question_reference": "What specific dynamic programming algorithm does the paper's tokenizer alignment method slightly tweak for its implementation?", "explanation_reference": "The paper mentions tweaking a textbook dynamic programming algorithm used in bioinformatics for sequence alignment, specifically citing the Needleman\u2013Wunsch algorithm, to solve the tokenizer alignment problem between GPT and T5 tokenizers.", "evidence_reference": "Our dynamic program is a slight tweak of the textbook dynamic programming algorithms used in bioinformatics for sequence alignment (such as the Needleman\u2013Wunsch algorithm~\\cite{Needleman1970AGM}) and in signal processing (such as dynamic time wrapping~\\cite{Senin2008DynamicTW})."}
{"question": "Consider the paper that introduces the method which is directly above the dashed line in few-shot prompting. How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "answer": "", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method is shown right above the dashed line in few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "explanation_reference": "The paper hypothesizes that the divergence in performance on the RefCOCOg task between VL-T5 and VL-BART is due to the different methods of positional encoding used by T5 and BART. Specifically, BART uses learned absolute positional embeddings, which might lead to the model memorizing the positions of training objects, resulting in high training accuracy but low validation accuracy. This hypothesis is supported by the observation of VL-BART's performance drop in the RefCOCOg task compared to VL-T5.", "evidence_reference": "We also observe that our experiments with \\oursb{} on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in self-attention layers instead. We hypothesize that \\oursb{} found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy)."}
{"question": "Consider the paper that introduces the LLM model that has a test accuracy of 50.7. Why does using beam search to decode each reasoning path in the context of the model proposed in the paper result in worse performance compared to sampling?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which LLM model shows test accuracy by 50.7?", "answer_anchor": "Self-Consistency", "question_reference": "In the context of the self-consistency method, why does using beam search to decode each reasoning path result in worse performance compared to sampling?", "explanation_reference": "The paper explains that the key to better performance in self-consistency is the diversity of the reasoning paths. Beam search, by its nature, tends to produce less diverse outputs compared to sampling methods, which is why its performance is worse when used within the self-consistency framework.", "evidence_reference": "Note self-consistency can also adopt beam search to decode each reasoning path (results are shown as ``Self-consistency using beam search''), but its performance is worse compared to self-consistency with sampling. The reason is that beam search yields a lower diversity in the outputs."}
{"question": "Consider the paper that introduces the LLM model that has a test accuracy of 50.7. Which LLM model has a reported accuracy using sampling for the AQuA task on the UL2-20B model with a beam size of 40?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which LLM model shows test accuracy by 50.7?", "answer_anchor": "Self-Consistency", "question_reference": "What is the reported accuracy of self-consistency using sampling for the AQuA task on the UL2-20B model with beam size 40?", "explanation_reference": "The reported accuracy directly answers the question by providing the specific performance metric of self-consistency using sampling for the AQuA task on the UL2-20B model.", "evidence_reference": "Self-consistency using sampling & 19.7 \\scriptsize{$\\pm$ 2.5} & \\textbf{24.9 \\scriptsize{$\\pm$ 2.6}} & \\textbf{25.3 \\scriptsize{$\\pm$ 1.8}} & \\textbf{26.7 \\scriptsize{$\\pm$ 1.0}} & \\textbf{26.9 \\scriptsize{$\\pm$ 0.5}}"}
{"question": "Consider the paper that introduces the Twitter dataset that has the most number of languages compared to all other Twitter datasets. What specific method was used to address the challenge of collecting tweets in languages that share geographic locations but have no curated stopword lists in the model proposed by the paper, and how was it validated?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset (Twitter) has the most number of languages compared to all Twitter datasets?", "answer_anchor": "AfriSenti", "question_reference": "What specific method was used to address the challenge of collecting tweets in languages that share geographic locations but have no curated stopword lists, and how was it validated?", "explanation_reference": "The question focuses on the detailed methodology used to collect tweets for languages without curated stopword lists, specifically asking about the approach taken to address this challenge and the validation process. The answer directly addresses this by mentioning the specific method (word co-occurrence-based approach) and the validation process (verification by native speakers), which are both detailed in the paper.", "evidence_reference": "We also used a word co-occurrence-based approach to extract stopwords using text sources from different domains. We lower-cased and removed punctuation marks and numbers, constructed a co-occurrence graph, and filtered out the words that occurred most often. Native speakers verified the generated lists before use."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.8173 on the Stance dataset. What is the Pearson's correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model gets mean classification accuracy 0.8173 on Stance dataset?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the task of political affiliation classification on Twitter data. A value of 0.9817 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "Twitter, \\poliaff{} ($F_1$), 0.9817159316285563"}
{"question": "Consider the paper that introduces the method that corresponds to the fifth row of the table. What specific aspect of the model's representation space, as proposed in the paper, potentially allows contrastive search to be directly applicable to the Chinese language model without the need for contrastive training?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown in the fifth row of the table?", "answer_anchor": "SimCTG", "question_reference": "What specific aspect of the Chinese language model's representation space potentially allows contrastive search to be directly applicable without contrastive training?", "explanation_reference": "The intrinsic property of the Chinese language model, which naturally represents text by characters, might lead to a representation space that displays a high level of isotropy. This property potentially allows contrastive search to be directly applicable without the need for contrastive training.", "evidence_reference": "We see that in all layers (including the final layer), the MLE model displays a similar self-similarity with respect to SimCTG. This observation is quite different from what we see from English language models... We conjecture that this discrepancy might come from the intrinsic property of different languages... For English, current state-of-the-art methods always represent the text into subword units... On the other hand, languages like Chinese are naturally represented by basic units, i.e., characters... As a result, even the vanilla MLE objective can obtain a representation space that displays a high level of isotropy."}
{"question": "Consider the paper that introduces the model which exhibits the highest score on the BC5CDR (Big Dict) and BC5CDR (Small Dict) datasets. What is the Dev F1 score for its feature-based approach using only embeddings on the CoNLL-2003 NER task?", "answer": "", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model demonstrates the highest score on BC5CDR (Big Dict) and BC5CDR (Small Dict) Dataset?", "answer_anchor": "BERT", "question_reference": "What is the Dev F1 score for the feature-based approach using BERTbase with embeddings only on the CoNLL-2003 NER task?", "explanation_reference": "The Dev F1 score directly measures the performance of the feature-based approach using BERTbase with only embeddings on the CoNLL-2003 Named Entity Recognition task, indicating how well the model performed without fine-tuning or additional context.", "evidence_reference": "Feature-based approach (\\bertbase) &  &  \\\\ \\;\\;\\;Embeddings & 91.0 &- \\\\"}
{"question": "Consider the paper that introduces the method that results in a score of 14.9 in the Unseen, Test, GC dataset. What is the primary purpose of leveraging synthetic instructions in the training of the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the score of 14.9 in Unseen, Test, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What is the primary purpose of leveraging synthetic instructions in the training of the Episodic Transformer?", "explanation_reference": "The question assesses the understanding of a specific strategy employed in the training process of the Episodic Transformer, which is the use of synthetic instructions. This strategy is aimed at addressing a particular challenge in the training phase, which is to separate the process of understanding the visual aspects of an environment from the complexities introduced by the variations in natural language instructions. This is a detail that directly relates to the core concept of how the Episodic Transformer is trained to handle the dual challenges of visual and language understanding in navigation tasks.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the method that corresponds to the brown color label in the figure. What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments after analyzing its effect on perplexity and repetition scores?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method demonstrated in the figure with brown color label?", "answer_anchor": "PPLM", "question_reference": "What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments after analyzing its effect on perplexity and repetition scores?", "explanation_reference": "The initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments is determined based on its effect on perplexity and repetition scores. The paper mentions that for \\(\\lambda_0=5\\), the average perplexity (58.4) and repetition score (3.5%) are the best among the considered values, indicating this value was chosen for subsequent experiments.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the method which demonstrates the highest BLEU-1 score for the Test Seen task as shown in the table. What is the recommended selection range of \\(k\\) and \\(\\alpha\\) for contrastive search based on the ablation study of the model proposed in the paper?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the table demonstrates the highest BLEU-1 score for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the recommended selection range of \\(k\\) and \\(\\alpha\\) for contrastive search based on the ablation study?", "explanation_reference": "The recommended selection range for \\(k\\) and \\(\\alpha\\) is based on the ablation study's findings that these settings produce results more similar to human-written texts as judged by generation diversity and generation perplexity.", "evidence_reference": "In practice, our recommended selection range of \\(k\\) and \\(\\alpha\\) are \\(k\\in [5,10]\\) and \\(\\alpha\\in[0.5,0.8]\\), as these settings produce results that are more similar to human-written texts as judged by generation diversity and generation perplexity."}
{"question": "Consider the paper that introduces the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections category. What specific method does the model proposed in the paper use to ensure that the tentative answers generated during the multi-hop question answering process do not contradict the edited facts stored in memory?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2305.14795", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections?", "answer_anchor": "MeLLo", "question_reference": "What specific method does MeLLo use to ensure that the tentative answers generated during the multi-hop question answering process do not contradict the edited facts stored in memory?", "explanation_reference": "MeLLo uses a retrieval model to fetch the most relevant edited fact from memory based on the subquestion and then prompts the model to self-check if the retrieved fact contradicts the generated answer. If a contradiction is found, the model adjusts the intermediate answer accordingly. This process ensures that the answers are consistent with the edited facts.", "evidence_reference": "Third, to assess whether the generated answer conflicts with any new knowledge edits, the subquestion is used as a query to retrieve a most relevant editing statement from the edited facts saved in memory. Fourth, the model is prompted to self-check if the retrieved fact contradicts the generated answer. If it does, the model adjusts the intermediate answer to this subquestion using the retrieved statement."}
{"question": "Consider the paper that introduces the model which performs the second best in the ClinTox dataset. What is the exact improvement in ROC-AUC score for the ClinTox dataset achieved by the model proposed in the paper over GEM?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model perform the second best in the ClinTox dataset?", "answer_anchor": "MolXPT", "question_reference": "What is the exact improvement in ROC-AUC score for the ClinTox dataset achieved by MolXPT over GEM?", "explanation_reference": "The improvement can be calculated by subtracting the ROC-AUC score of GEM on the ClinTox dataset from that of MolXPT. GEM achieved a score of 90.1, while MolXPT achieved 95.3. Therefore, the improvement is 95.3 - 90.1 = 5.2.", "evidence_reference": "GEM & $72.4\\pm0.4$ & \\textbf{78.1 $\\pm$ 0.1} & $90.1\\pm1.3$ & \\textbf{80.6 $\\pm$ 0.9} & $85.6\\pm1.1$ & $67.2\\pm0.4$ & $79.0$ \\\\ \\hline \\ourM{} & \\textbf{80.0 $\\pm$ 0.5} &  $77.1\\pm0.2$  & \\textbf{95.3 $\\pm$ 0.2} & $78.1\\pm0.4$ & \\textbf{88.4 $\\pm$ 1.0} & \\textbf{71.7 $\\pm$ 0.2} & \\textbf{81.9} \\\\"}
{"question": "Consider the paper that introduces the model which exhibits the highest score on the BC5CDR (Big Dict) and BC5CDR (Small Dict) datasets. How does the performance of BERT compare on the NER task when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "answer": "", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model demonstrates the highest score on BC5CDR (Big Dict) and BC5CDR (Small Dict) Dataset?", "answer_anchor": "BERT", "question_reference": "How does the performance of BERT on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "explanation_reference": "The performance comparison between fine-tuning and feature-based approaches for the NER task, under various masking strategies during pre-training, shows that the fine-tuning approach consistently outperforms the feature-based approach. This is evident from the Dev set results for NER, where the fine-tuning approach yields higher F1 scores compared to the feature-based approach across all masking strategies.", "evidence_reference": "In the table presented in the Ablation for Different Masking Procedures section, the Dev set results for NER under different masking strategies show that the fine-tuning approach (95.4, 94.9, 95.2, 95.2 for different strategies) consistently achieves higher F1 scores than the feature-based approach (94.9, 94.0, 94.6, 94.7 for the same strategies), indicating better performance."}
{"question": "Consider the paper that introduces the method that exhibits an accuracy score of 71.0 on the VQA-v2 task. What is the role of the $\\tiny{<}obj\\tiny{>}$ token in the output sequence design of the model proposed in the paper?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows 71.0 accuracy score on VQA-v2 task?", "answer_anchor": "UniTAB", "question_reference": "What is the role of the $\\tiny{<}obj\\tiny{>}$ token in the UniTAB model's output sequence design?", "explanation_reference": "The $\\tiny{<}obj\\tiny{>}$ token is introduced to simplify the sequence generation by providing hints of the code-switching between text and box tokens, and naturally represents word-box alignments within the output sequence. This means that the words and box within a pair of $\\tiny{<}obj\\tiny{>}$ tokens refer to the same entity, facilitating the model's ability to align predicted words with boxes.", "evidence_reference": "We introduce a special $\\tiny{<}obj\\tiny{>}$ token inserted before the text word to be grounded, and after the generated box tokens. The $\\tiny{<}obj\\tiny{>}$ token simplifies the sequence generation by providing hints of the code-switching, and naturally represents word-box alignments."}
{"question": "Consider the paper that introduces the model which has a lower mac-F1 score than Longformer but a higher mac-F1 score than CaselawBERT. What specific aspect of the model proposed in the paper's performance on the \\contractsdata dataset demonstrates its efficiency compared to larger models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model having lower mac-F1 score than Longformer but higher mac-F1 score than CaselawBERT?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's performance on the \\contractsdata dataset demonstrates its efficiency compared to larger models?", "explanation_reference": "The question targets a detailed aspect of the \\legalbertsmall model's performance, specifically its efficiency and effectiveness on the \\contractsdata dataset compared to larger models. This is a critical analysis question because it asks for a comparison based on the model's size and performance, highlighting the balance between computational resources and task effectiveness.", "evidence_reference": "Most importantly, (iv) we release \\legalbert, a family of \\bert models for the legal domain, intended to assist legal \\nlp research, computational law, and legal technology applications. This family includes \\legalbertsmall, a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint."}
{"question": "Consider the paper that introduces the method which has 11.0 rounds to completion. What specific implementation detail is suggested to improve the stability of policy and value learning in the model proposed by the paper, when dealing with the non-stationarity of MARL environments?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having 11.0 rounds to Completion?", "answer_anchor": "MAPPO", "question_reference": "What specific implementation detail is suggested to improve the stability of policy and value learning in MAPPO when dealing with the non-stationarity of MARL environments?", "explanation_reference": "The suggestion to use fewer epochs per update in MAPPO is aimed at improving the stability of policy and value learning by limiting the non-stationarity inherent in MARL environments. This approach is hypothesized to mitigate the effects of rapidly changing policies among agents, which can destabilize learning.", "evidence_reference": "However, we find that in multi-agent domains, MAPPO's performance degrades when samples are re-used too often. Thus, we use 15 epochs for easy tasks, and 10 or 5 epochs for difficult tasks. We hypothesize that this pattern could be a consequence of non-stationarity in MARL: using fewer epochs per update limits the change in the agents' policies, which could improve the stability of policy and value learning."}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that shows the highest Test Accuracy. What specific methodological limitation does the paper acknowledge regarding the incorporation of commonsense knowledge into the model proposed in the paper as the MWP solver?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which Seq2Seq model shows the higest Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific methodological limitation does the paper acknowledge regarding the incorporation of commonsense knowledge into the MWP solver?", "explanation_reference": "The limitation is directly acknowledged in the 'Limitations' section, where the authors mention that the MWP solver cannot solve problems requiring commonsense knowledge if such knowledge is not explicitly provided in the problem description. This points to a methodological limitation in the solver's design regarding the integration of external, implicit knowledge.", "evidence_reference": "As mentioned in \\cite{lin2020numersense,DBLP:journals/corr/abs-2107-13435}, MWP solving in the real-word scenario requires many commonsense knowledge, e.g., 1km = 1000m and one day = 24 hours. When these commonsense constants are not explicitly given in the problem description, our MWP solver has no chance to solve problems that require them."}
{"question": "Consider the paper that introduces the model represented by the blue bar. What specific architectural change was made to the Transformer model in its framework to potentially improve computational efficiency during unsupervised pre-training?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model is demonstrated by the blue bar?", "answer_anchor": "T5", "question_reference": "What specific architectural change was made to the Transformer model in the T5 framework to potentially improve computational efficiency during unsupervised pre-training?", "explanation_reference": "The specific architectural change made to improve computational efficiency during unsupervised pre-training in the T5 framework was replacing entire spans of corrupted tokens with a single token. This approach is mentioned as part of the unsupervised objectives exploration, where it is noted that this method produces shorter target sequences, potentially making unsupervised pre-training more computationally efficient.", "evidence_reference": "We found that most ``denoising'' objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to-text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient."}
{"question": "Consider the paper that introduces the model that is seventh in the table. What is the primary reason for the poor performance of the PIDRP method compared to this model, across all four top-level senses of the PDTB?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which is the method demonstrated in the seventh row in the table?", "answer_anchor": "PCP", "question_reference": "What is the primary reason for the poor performance of the PIDRP method compared to the PCP method across all four top-level senses of the PDTB?", "explanation_reference": "The paper suggests that the main reason for the PIDRP method's inferior performance is that predicting connectives aligns more closely with the natural language patterns encountered during the pre-training stage of the model, as opposed to directly predicting implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the dataset that has 1 SM task and 14 languages. What specific challenge did annotators face when annotating tweets in Mozambican Portuguese and Xitsonga, and how did this challenge affect the final dataset?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What dataset has 1 SM task and 14 languages?", "answer_anchor": "AfriSenti", "question_reference": "What specific challenge did annotators face when annotating tweets in Mozambican Portuguese and Xitsonga, and how did it affect the final dataset?", "explanation_reference": "The challenge of code-mixing and sarcasm in tweets made it difficult for annotators to determine the intended meaning, leading to the exclusion of many tweets from the final dataset due to disagreements among annotators on the presence of sarcasm.", "evidence_reference": "One of the significant challenges for the Mozambican Portuguese and Xitsonga data annotators was the presence of code-mixed and sarcastic tweets. Code-mixing in tweets made it challenging for the annotators to determine the intended meaning of the tweet as it involved multiple languages spoken in Mozambique that some annotators were unfamiliar with. Similarly, the presence of two variants of Xitsonga spoken in Mozambique (Changana and Ronga) added to the complexity of the annotation task. Additionally. we excluded many tweets from the final dataset as sarcasm present in tweets was another source of disagreement among the annotators."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 42.0 in the Seen, Val, GC dataset. What is the primary purpose of leveraging synthetic instructions in the training of the model proposed in the paper?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the score of 42.0 in Seen, Val, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What is the primary purpose of leveraging synthetic instructions in the training of the Episodic Transformer?", "explanation_reference": "The question assesses the understanding of a specific strategy employed in the training process of the Episodic Transformer, which is the use of synthetic instructions. This strategy is aimed at addressing a particular challenge in the training phase, which is to separate the process of understanding the visual aspects of an environment from the complexities introduced by the variations in natural language instructions. This is a detail that directly relates to the core concept of how the Episodic Transformer is trained to handle the dual challenges of visual and language understanding in navigation tasks.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the dataset in the Movies domain only that has a PF field. What is the percentage of dialogues in the model proposed by the paper that achieved a goal completion score of 2?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which dataset is in Movies domain only and has PF field?", "answer_anchor": "DuConv", "question_reference": "What is the percentage of dialogues in the norm generation model that achieved a goal completion score of 2?", "explanation_reference": "The percentage of dialogues that achieved a goal completion score of 2 in the norm generation model is directly reported in the analysis of goal completion and knowledge exploitation.", "evidence_reference": "goal completion & 2 & 43%"}
{"question": "Consider the paper that introduces the method that has an F1 score of 53.36. What specific methodological adjustment does the model proposed in the paper make to the GraphSAGE aggregation strategy to accommodate the unique challenges of document graph structures?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having 53.36 F1 score?", "answer_anchor": "Doc2Graph", "question_reference": "What specific methodological adjustment does the Doc2Graph model make to the GraphSAGE aggregation strategy to accommodate the unique challenges of document graph structures?", "explanation_reference": "The adjustment to the GraphSAGE aggregation strategy is made to address the challenge of naturally defining graph structures in documents, which is not straightforward. By redefining the neighborhood aggregation to consider only neighbors within a certain Euclidean distance threshold, the model aims to maintain locality during the message passing algorithm, which is crucial for document graphs where the spatial arrangement of elements is significant.", "evidence_reference": "Then, given a document, we redefine the above equation as: \\begin{equation} \\label{eq:graph-sage} h_{N(i)}^{l+1} = \\frac{c}{|\\Upsilon(i)|} \\sum_{j \\in \\Upsilon(i)} h^{l}_{j} \\end{equation} where $\\Upsilon(i) = \\{j \\in N(i): |i - j| < threshold\\}$, $|i - j|$ is the Euclidean distance of nodes $i$ and $j$ saved (normalized between 0 and 1) on their connecting edge, and $c$ is a constant scale factor."}
{"question": "Consider the paper that introduces the method that has a score of 87.3 in the SciTail dataset with 16-shot prompting. What is the model's, proposed by the paper, relative improvement percentage over vanilla PT on the SuperGLUE benchmark?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having score of 78.6 in SciTail dataset with 16-shot prompting?", "answer_anchor": "MPT", "question_reference": "What is the relative improvement percentage of \\ours over vanilla PT on the SuperGLUE benchmark?", "explanation_reference": "The relative improvement percentage is calculated based on the performance improvement of \\ours over vanilla PT on the SuperGLUE benchmark, as mentioned in the Results and Analysis section.", "evidence_reference": "When compared to vanilla PT~\\citep{lester2021power}, \\ours obtains a relative improvement of $13\\%$ on GLUE and  $16\\%$ on SuperGLUE with the same number of task-specific parameters, highlighting the benefits of transferring knowledge from multiple source tasks."}
{"question": "Consider the paper that introduces the method that achieves sentence-level precision of 60.32. What specific initialization method was used for the additional position embeddings in the model proposed by the paper to support longer documents?", "answer": "", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method show a sentence-level precision equal to 60.32?", "answer_anchor": "longformer", "question_reference": "What specific initialization method was used for the additional position embeddings in Longformer to support longer documents?", "explanation_reference": "The method used for initializing the additional position embeddings in Longformer, to extend its support for longer documents beyond the 512 token limit of RoBERTa, involved copying the 512 position embeddings from RoBERTa multiple times. This approach was chosen to preserve the local structure learned by RoBERTa's pretrained weights, except at the partition boundaries, and was found to be very effective for rapid convergence of Longformer's pretraining with a small number of gradient updates.", "evidence_reference": "To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa's pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times as analysis of BERT's attention heads shows a strong learned bias to attending to local context, including the previous or next token. Using the copy initialization preserves this local structure everywhere except at the partition boundaries."}
{"question": "Consider the paper that introduces the method that achieves a higher EA score than Fixed set but a lower EA score than Diverse KATE in the FinQA task. In the ablation study on the effect of in-context example orders for GPT-3 on the NQ dataset using the model proposed in the paper, which specific order of in-context examples yielded the highest EM score?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets higher EA score than Fixed set but lower EA score than Diverse KATE in FinQA task?", "answer_anchor": "KATE", "question_reference": "In the ablation study on the effect of in-context example orders for GPT-3 on the NQ dataset using KATE$_{\\\\text{nli+sts-b}}$, which specific order of in-context examples yielded the highest EM score?", "explanation_reference": "The question focuses on the detailed part of the paper where the authors conducted an ablation study to understand how the order of in-context examples affects the performance of GPT-3. The highest EM score indicates the most effective order for arranging in-context examples to improve GPT-3's performance on the NQ dataset.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 80.3 on Deit-B with a Weight/Activation (W/A) precision of 6/6. What is the specific reason that using twin uniform quantization without the Hessian guided metric significantly decreases the model's, proposed by the paper, top-1 accuracy on ViT-S/224 at 8-bit quantization compared to using both methods together?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the quant method show 80.3 score on Deit-B?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific reason that using twin uniform quantization without the Hessian guided metric significantly decreases the top-1 accuracy on ViT-S/224 at 8-bit quantization compared to using both methods together?", "explanation_reference": "The significant decrease in top-1 accuracy when using twin uniform quantization without the Hessian guided metric is attributed to the inaccuracy of metrics that only consider local information. The Hessian guided metric, which considers a broader scope of information, including the impact of quantization on the task loss, provides a more accurate determination of the optimal scaling factors. This leads to better quantization performance and higher accuracy.", "evidence_reference": "For instance, the top-1 accuracy on ViT-S/224 achieves 81.00% with both Hessian guided metric and twin uniform quantization at 8-bit quantization, while it decreases to 79.25% without Hessian guided metric, which is even lower than basic PTQ with 80.47% top-1 accuracy. This is also evidence that the metric considering only the local information is inaccurate."}
{"question": "Consider the paper that introduces the method that has a lower Hits@1 score than ReasoningLM but a higher Hits@1 score than NSM across all fine-tuning samples. What specific component of the model's architecture, proposed by the paper, is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has lower Hit@1 score than ReasoningLM but higher Hit@1 score than NSM as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What specific component of UniKGQA's architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "explanation_reference": "The question targets a detailed aspect of the UniKGQA architecture, specifically asking for the component that handles the propagation of semantic matching information across the knowledge graph. The answer, 'Matching information propagation module,' directly addresses this by naming the specific part of the architecture designed for this purpose.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the large language model which achieves a lower HVI score than OPT but a higher HVI score than Alpaca. What specific methodological difference in the evaluation of the model's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the large language model that demonstrates lower HVI score than OPT but higher HVI score than Alpaca?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard procedure used for most exam runs, where the model's letter choice is extracted directly from the explanation. This approach for the USABO and SAT reading/writing runs indicates a unique handling of these exams, which could contribute to the minimal impact on the overall results, as it relies on the model's generated explanation to determine the final answer choice.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the model that exhibits a 64.1 F1 score in the WebQSP dataset. What specific role does the stop-gradient operation play in the end-to-end fine-tuning process of the model proposed in the paper's subgraph retriever and reasoner?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows 64.1 F1 score in WebQSP dataset?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific role does the stop-gradient operation play in the end-to-end fine-tuning process of the subgraph retriever and reasoner?", "explanation_reference": "The stop-gradient operation is used to prevent the backpropagation of gradients through the reasoner's parameters during the joint training of the retriever and reasoner. This allows for the optimization of the retriever based on the feedback from the reasoner without altering the reasoner's learned parameters during this phase.", "evidence_reference": "In the end-to-end fine-tuning section, it is mentioned that '\\u2026where the stop-gradient operation \\u03b2\\u03b2\\u03a3\\u03a3 is to stop updating the parameters \\u03c6.' This indicates that the stop-gradient operation is applied to ensure that the parameters of the reasoner (\\u03c6) do not get updated while the retriever is being fine-tuned based on the feedback from the reasoner."}
{"question": "Consider the paper that introduces the dataset that corresponds to the second chart from the left. What percentage of questions associated with it can be answered using a boolean response?", "answer": "", "figure": "locality/2310.12836/ratio_figure.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which dataset lies on the second left in the figure?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka questions that can be answered using a boolean response?", "explanation_reference": "The percentage is directly provided in the dataset statistics, indicating the proportion of questions in Mintaka that can be answered with a boolean (yes/no) response.", "evidence_reference": "A majority (72%) of the questions in Mintaka can be answered using an entity. 14% can be answered using a boolean, in yes/no or comparative questions."}
{"question": "Consider the paper that introduces the model that has a de-en score of 53.87. What is the primary reason for using accuracy as the evaluation metric in its task, according to the paper?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model has 53.87 de-en score?", "answer_anchor": "GWLAN", "question_reference": "What is the primary reason for using accuracy as the evaluation metric in the GWLAN task, according to the paper?", "explanation_reference": "The paper suggests that higher prediction accuracy in the GWLAN task is directly correlated with the reduction of keystrokes required from human translators. This reduction in keystrokes leads to time savings for translators, making them more productive. Therefore, accuracy is used as the evaluation metric to reflect the effectiveness of the autocompletion in assisting human translators.", "evidence_reference": "Experiments show that the more keystrokes are reduced, the more time can be saved for translators. Since the prediction accuracy is highly correlated with the keystrokes, we think higher accuracy will make translators more productive."}
{"question": "Consider the paper that introduces the model that achieves a 16.5 Top-1 score on the SQuAD dataset. What is the Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity in the study presented by the paper?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets 16.5 Top-1 score in SQuAD dataset?", "answer_anchor": "Composition", "question_reference": "What is the Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity?", "explanation_reference": "The Spearman's rank correlation coefficient between Self-BLEU and human assessments of diversity is provided directly in the results section discussing the correlation of automatic metrics with human judgments.", "evidence_reference": "Self-BLEU & 0.880"}
{"question": "Consider the paper that introduces the model that results in the second lowest accuracy in the COGS-all dataset. What specific achievement does the model proposed in the paper demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model demonstrates the second lowest accuracy in COGS-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the Transformer model's performance on English constituency parsing, what specific achievement does it demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "explanation_reference": "The paper highlights that, unlike RNN sequence-to-sequence models, the Transformer model outperforms the BerkeleyParser even when trained only on the WSJ training set of 40K sentences. This indicates the Transformer's superior ability to handle tasks with strong structural constraints and significantly longer outputs than inputs, even with limited training data.", "evidence_reference": "In contrast to RNN sequence-to-sequence models [KVparse15], the Transformer outperforms the BerkeleyParser [petrov-EtAl:2006:ACL] even when training only on the WSJ training set of 40K sentences."}
{"question": "Consider the paper that introduces the methodological approach employed by the generation-based model to mimic human knowledge selection in dialogue response generation on the dataset shown in the second row of the table. What specific methodological approach does the generation-based model employ to mimic human knowledge selection in dialogue response generation?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset shown in the second row of the table?", "answer_anchor": "DuConv", "question_reference": "What specific methodological approach does the generation-based model employ to mimic human knowledge selection in dialogue response generation?", "explanation_reference": "The generation-based model uses a methodological approach of minimizing the Kullback-Leibler Divergence (KLDivLoss) between two distributions: the prior distribution (knowledge reasoned by machines) and the posterior distribution (knowledge reasoned by humans). This approach is employed to force the model to mimic human knowledge selection during dialogue response generation.", "evidence_reference": "we introduce two different distributions: 1) the \\emph{prior distribution} $p(k_i | x)$ and the \\emph{posterior distribution} $p(k_i | x, y)$. We take the prior distribution $p(k_i | x)$ as the knowledge reasoned by machines and the posterior distribution $p(k_i | x, y)$ as the knowledge reasoned by humans, and then force the machine to mimic human by minimizing the KLDivLoss between those two distributions"}
{"question": "Consider the paper that introduces the method that has the second-highest Avg score on the SuperGLUE task. What specific performance improvement does the model proposed in the paper achieve on the GLUE benchmark when using T5 v1.1 + LM as the underlying model compared to the Task Hypernet method?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the second highest Avg score on SuperGLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "What specific performance improvement does the Hyperdecoder approach achieve on the GLUE benchmark when using T5 v1.1 + LM as the underlying model compared to the Task Hypernet method?", "explanation_reference": "The Hyperdecoder approach's performance improvement over the Task Hypernet method is calculated based on their average GLUE benchmark scores when using T5 v1.1 + LM as the underlying model. The Task Hypernet method has an average score of 54.3, while the Hyperdecoder approach achieves an average score of 86.5. The improvement is calculated as ((86.5 - 54.3) / 54.3) * 100% = 32.2%.", "evidence_reference": "Task Hypernet & 2.7% & 0.0 & 82.1 & 16.4 / 16.4 & 70.4 / 81.4 & 89.8 / 86.5 & 56.6 & 64.8 & 50.7 & 54.3 \\\\ \\textbf{Hyperdecoder (ours)} & 2.9% & 58.7$_{2.3}$ & \\textbf{95.9}$_{0.4}$* & 91.8$_{0.7}$ / \\textbf{92.0}$_{0.4}$* & 89.2$_{1.5}$ / 92.0$_{0.9}$ & 91.1$_{0.2}$ / 88.3$_{0.4}$ & \\textbf{90.0}$_{0.2}$* & \\textbf{94.2}$_{0.4}$ & \\textbf{80.8}$_{2.2}$ & \\textbf{86.5}${_{0.5}}\\\\dagger$ \\\\"}
{"question": "Consider the paper that introduces the method which has a GPT backbone and 7B parameters. What impact did the introduction of a guidance mechanism have on the number of questions (\\# Q) generated by the GPT-2 model, compared to the no-guidance scenario, when employing this method in the ablation study?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method used with a GPT backbone and 7B parameters?", "answer_anchor": "DecomDistill", "question_reference": "In the ablation study, how did the introduction of a guidance mechanism impact the number of questions (\\# Q) generated by the GPT-2 model compared to the no-guidance scenario?", "explanation_reference": "The introduction of a guidance mechanism significantly improved the accuracy of the number of questions generated by the GPT-2 model, as indicated by the increase in the metric from 0.42 to 0.80. This demonstrates the effectiveness of the guidance mechanism in enhancing the model's ability to generate a more accurate number of subquestions, aligning better with the GPT-3 annotated questions for a given problem.", "evidence_reference": "No-guidance   & 51.5 & 0.78 & 0.42    \\\\ Guidance   & \\textbf{58.8} & \\bf 0.81 & \\bf 0.80"}
{"question": "Consider the paper that introduces the dataset that corresponds to the second chart from the left. What is the percentage of the model's questions that can be answered using a boolean response, and how does this compare to the percentage of questions answerable using a numerical response?", "answer": "", "figure": "locality/2310.12836/ratio_figure.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which dataset lies on the second left in the figure?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka's questions that can be answered using a boolean response, and how does this compare to the percentage of questions answerable using a numerical response?", "explanation_reference": "The paper provides specific statistics on the types of answers in the Mintaka dataset, including the percentages of questions that can be answered with different types of responses. The comparison between boolean and numerical response types directly answers the question.", "evidence_reference": "A majority (72\\%) of the questions in Mintaka can be answered using an entity. 14\\% can be answered using a boolean, in yes/no or comparative questions. 7\\% can be answered using a number, such as someone\u2019s age."}
{"question": "Consider the paper that introduces the method that demonstrates the second highest score in the TweetEval Irony dataset for both zero-shot and few-shot prompting. What specific improvement in percentage points did the model proposed in the paper achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "answer": "", "figure": "locality/2310.15746/comparison_2_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method demonstrates the second highest score in TweetEval Irony dataset in both zero-shot and few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific improvement in percentage points did the generative models achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "explanation_reference": "The improvement is highlighted in the comparison between generative and discriminative models for the VQA task, specifically on the out-of-domain subset. The generative models (\\ourst{} and \\oursb{}) improved upon the discriminative baselines by 6 and 6.2 percentage points respectively, demonstrating the effectiveness of using generative modeling for questions with answers not included in the top-K answer candidates.", "evidence_reference": "This improvement is more significant on the out-of-domain subset, where the generative \\ourst{} and \\oursb{} achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling."}
{"question": "Consider the paper that introduces the method that exhibits the lowest BLEU score in the De->En task over Average Lagging from 5 to 11. How does the model proposed in the paper ensure that all attention heads do not deviate significantly in their reading speeds during training?", "answer": "", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the lowest BLEU score in De-En task over Average Lagging from 5 to 11?", "answer_anchor": "MMA", "question_reference": "How does the MMA model ensure that all attention heads do not deviate significantly in their reading speeds during training?", "explanation_reference": "The head divergence loss, L_{var}, is designed to minimize the variance in expected delays across all attention heads, thereby ensuring that no single head reads significantly faster or slower than the others. This regularization term effectively controls the divergence of the heads, making sure they operate at similar speeds.", "evidence_reference": "The final objective function is presented in \\autoref{eq:objective}:  \\begin{equation} L(\\theta) = -\\log(\\vy \\mid \\vx; \\theta) + \\lambda_{avg} L_{avg} + \\lambda_{var} L_{var} \\label{eq:objective} \\end{equation}  where $\\lambda_{avg}$, $\\lambda_{var}$ are hyperparameters that control both losses. Intuitively, while $\\lambda_{avg}$ controls the overall speed, $\\lambda_{var}$ controls the divergence of the heads."}
{"question": "Consider the paper that introduces the model that has a diversity score of 2.58 for p2. What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model demonstrates diversity score of 2.58 in p2?", "answer_anchor": "T5-Large", "question_reference": "What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "explanation_reference": "The span-corruption objective with an average span length of 3 slightly outperformed the i.i.d. objective on most non-translation benchmarks, as indicated by the statement 'we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks.'", "evidence_reference": "we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks."}
{"question": "Consider the paper that introduces the method which has 11.0 rounds to completion. What specific form of global state input does the model, proposed by the paper, utilize in the SMAC domain to address the lack of critical local information?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having 11.0 rounds to Completion?", "answer_anchor": "MAPPO", "question_reference": "What specific form of global state input to the value function does MAPPO utilize in the SMAC domain to address the lack of critical local information?", "explanation_reference": "The paper specifies that to address the lack of critical local information in the environment-provided global state, MAPPO utilizes an Agent-Specific Global State (AS) for each agent in the SMAC domain. This AS state is formed by concatenating the environment-provided global state with the local observation for each agent, providing the value function with a more comprehensive description of the environment state.", "evidence_reference": "To address the weaknesses of the \\emph{CL} and \\emph{EP} states, we allow the value function to leverage both global and local information by forming an \\textbf{Agent-Specific Global State (AS)} which creates a global state for agent $i$ by concatenating the \\emph{EP} state and $o_i$, the local observation for agent $i$."}
{"question": "Consider the paper that introduces the model which performs the second best in the ClinTox dataset. What specific advantage does it demonstrate over Galactica in terms of pre-training data utilization?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model perform the second best in the ClinTox dataset?", "answer_anchor": "MolXPT", "question_reference": "What specific advantage does MolXPT demonstrate over Galactica in terms of pre-training data utilization?", "explanation_reference": "The advantage is highlighted by the explicit use of 'wrapped' sequences in MolXPT's pre-training, which incorporates both molecular SMILES and surrounding text, allowing for a more effective representation by leveraging the complementary information from both modalities. This approach is contrasted with Galactica, which, despite also using SMILES and text, does not build and train on these 'wrapped' sequences, missing out on the potential benefits of such integration.", "evidence_reference": "A possible explanation of the superior performance is that the SMILES describes the component and structural information of molecules, while the text describes the general properties. They are complementary to each other, and joint training on them brings more effective representations."}
{"question": "Consider the paper that introduces the model shown in the figure that performs most similarly to GPT-3.5-Turbo for the 'Plausible' and 'Foreign' scenarios. What specific condition under the DA-WR algorithm ensures the convergence of the cumulative distribution function resulting from the data augmentation process to the target distribution?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shown in the figure has a similar performance to GPT-3.5-Turbo?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition under the DA-WR algorithm ensures the convergence of the cumulative distribution function resulting from the data augmentation process to the target distribution?", "explanation_reference": "The specific conditions (C)-(C'') ensure the convergence of the cumulative distribution function resulting from the DA-WR algorithm to the target distribution by addressing the maximum difference in indicators and weights between the initial and augmented datasets, ensuring these differences diminish as the sample size increases.", "evidence_reference": "Assume that {\\bf (C)-(C'')} hold and that the support of  $F$ contains the support of  $F_0$. Then for all $x \\in {\\cal X}$, the cdf resulting from the DA-WR algorithm converges in probabilities to $F_0(x)$ as $n \\to +\\infty$."}
{"question": "Consider the paper that introduces SPADE, which method is in the second row of the table. What is the observed improvement in $F_1$ score upon the integration of SPADE for the \\cord dataset with oracle?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method in the second row of the table?", "answer_anchor": "SPADE", "question_reference": "what is the observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle?", "explanation_reference": "The Tail Collision Avoidance algorithm is integrated to handle the property where individual text nodes have a single incoming edge for each relation except in special documents like tables. The observed improvement in $F_1$ score upon its integration for the \\cord dataset with oracle is +1.0%.", "evidence_reference": "To push the performance further, we notice that individual text nodes have a single incoming edge for each relation except in special documents like table (Fig. \\ref{fig_paradigm}). Using this property, we integrate Tail Collision Avoidance algorithm (\\caabb) that iteratively trims the tail-sharing-edges and generate new edges until the process becomes self-consistent (Section \\ref{sec:graph_gen}). $F_1$ increases by +1.0\\% and +0.8\\% with and without the oracle upon the integration (2nd row, \\cordabb)."}
{"question": "Consider the paper that introduces the model that scores an 84.2 in the 'T10' column. What specific hyperparameter values were used for the FewRel dataset in its experiments, and how do these values compare to those used for the TACRED dataset?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets score of 84.2 in 'T10' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were used for the FewRel dataset in the experiments, and how do these values compare to those used for the TACRED dataset?", "explanation_reference": "The specific hyperparameter values for the FewRel and TACRED datasets are directly provided in the Implementation Details section of the paper, allowing for a direct comparison between the two sets of values.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. How does it compare to beam search in terms of performance on the AQuA task when utilizing the UL2-20B model with 40 reasoning paths?", "answer": "", "figure": "locality/2310.09619/Math23k_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "How does self-consistency compare to beam search in terms of performance on the AQuA task when using the UL2-20B model with 40 reasoning paths?", "explanation_reference": "The comparison between self-consistency and beam search on the AQuA task using the UL2-20B model with 40 reasoning paths shows that self-consistency using sampling achieves higher accuracy than beam search, indicating that self-consistency's approach to generating diverse reasoning paths and aggregating the most consistent answer leads to better performance.", "evidence_reference": "Self-consistency using sampling & 19.7 \\scriptsize{$\\pm$ 2.5} & \\textbf{24.9 \\scriptsize{$\\pm$ 2.6}} & \\textbf{25.3 \\scriptsize{$\\pm$ 1.8}} & \\textbf{26.7 \\scriptsize{$\\pm$ 1.0}} & \\textbf{26.9 \\scriptsize{$\\pm$ 0.5}} \\\\ \\midrule \\multirow{3}{*}{AQuA} & Beam search decoding (top beam) & 23.6 & 19.3 & 16.1 & 15.0 &10.2"}
{"question": "Consider the paper that introduces the method that achieves the lowest J_k scores in the WN18RR dataset. Based on the ablation studies, which component's removal from the model proposed in the paper resulted in the most dramatic performance decrease on the same dataset?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method shows the lowest J_k score in WN18RR dataset?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which component's removal resulted in the most dramatic performance decrease on the WN18RR dataset?", "explanation_reference": "The ablation study results for the WN18RR dataset show that the removal of the MulHop component resulted in the most significant performance decrease. This indicates that multi-hop neighbor information is crucial for the model's performance on this dataset.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category. What specific performance improvement does it achieve on the TACRED dataset in terms of micro $F_1$ score compared to RoBERTa + multitask?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2002.01808", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category?", "answer_anchor": "K-Adapter", "question_reference": "What specific performance improvement does \\textsc{K-Adapter} (F+L) achieve on the TACRED dataset in terms of micro $F_1$ score compared to RoBERTa + multitask?", "explanation_reference": "The question focuses on comparing the micro $F_1$ score improvements of \\textsc{K-Adapter} (F+L) over RoBERTa + multitask specifically on the TACRED dataset. The answer is derived from the reported micro $F_1$ scores for both models, where \\textsc{K-Adapter} (F+L) achieves a score of 72.04% and RoBERTa + multitask achieves a score of 71.62%. The difference between these two scores represents the specific performance improvement.", "evidence_reference": "RoBERTa +  multitask & 70.18 & 73.11 & 71.62 \\\\  \\textsc{K-Adapter} (F) & 69.39 & 74.59 & 71.89 \\\\ \\textsc{K-Adapter} (L) & 68.85 & 75.37 & 71.96 \\\\ \\textsc{K-Adapter} (F+L) & 70.14 & 74.04 & \\textbf{72.04} \\\\"}
{"question": "Consider the paper that introduces the method that has the second-highest Avg score on the SuperGLUE task. What was the model's, proposed by the paper, average GLUE benchmark performance in the ablation study when using regular adapters in both the encoder and decoder?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the second highest Avg score on SuperGLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "In the ablation study comparing different adapter configurations, what was the average GLUE benchmark performance when using regular adapters in both the encoder and decoder?", "explanation_reference": "The question specifically targets the results from the ablation study that varied adapter configurations across the encoder and decoder. The average GLUE benchmark performance for the configuration using regular adapters in both the encoder and decoder (referred to as 'Manual-Manual' in the detailed ablation results) directly answers the question.", "evidence_reference": "Manual-Manual & 2.9% & 58.5 & 95.3 & 91.7 / 91.4 & 89.7 / 92.5 & 91.2 / 88.3 & 89.8 & 94.0 & 81.2 & 86.4"}
{"question": "Consider the paper that introduces the dataset located in the top left of the figure. What specific aspect of its benchmark design aims to address the challenge of prompt sensitivity in evaluating Large Language Models' (LLMs) sentiment analysis capabilities?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset located on the top left of the figure?", "answer_anchor": "SentiEval", "question_reference": "What specific aspect of the \\textsc{SentiEval} benchmark design aims to address the challenge of prompt sensitivity in evaluating LLMs' sentiment analysis capabilities?", "explanation_reference": "The \\textsc{SentiEval} benchmark is designed to address the challenge of prompt sensitivity by incorporating diverse but fixed instructions for evaluating LLMs. This approach aims to make performance comparisons more stable and reliable across different LLMs and studies, by reducing the variability and bias associated with prompt design.", "evidence_reference": "The main idea of \\textsc{SentiEval} is to: 1) break the boundary between individual sentiment analysis tasks to establish a unified testing benchmark, providing a more comprehensive assessment of a model's sentiment analysis proficiency, rather than emphasizing on specific aspects; 2) test the model using natural language instructions presented in various styles. This mimics the real use case when humans interact with the model with natural languages for solving SA tasks, instead of purely learning text-label mapping; 3) equip the benchmark with diverse but fixed instructions, making performance comparisons more stable and reliable across different LLMs and studies."}
{"question": "Consider the paper that introduces the method that scores a 69.56 in the Forgotten Realms category. What specific method does the model proposed in the paper use to generate high-quality synthetic data for few-shot entity linking?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method got 69.56 score in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "What specific method does MetaBLINK use to generate high-quality synthetic data for few-shot entity linking?", "explanation_reference": "The paper specifies that to improve the quality of synthetic data, they adopt a mention rewriting strategy using the T5 model. This approach is chosen to alleviate the bias introduced by exact matching and generate more effective synthetic samples by rewriting mentions to be semantically similar to entity descriptions.", "evidence_reference": "To further improve the quality of generated data, we adopt the T5 model to generate more semantic-like data samples... In our model, we suppose the mention contains part of semantic information of the corresponding entities, so we add the prefix 'summarize': to the entity's description to force the model to summarize the entity in a few words."}
{"question": "Consider the paper that introduces the method that has the second lowest overall performance for the Seen condition. What specific augmentation methods are adopted for the egocentric observations in the model's training proposed by the paper to address the sample insufficiency of imitation learning?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the second lowest over performance for Seen condition?", "answer_anchor": "MOCA", "question_reference": "What specific augmentation methods are adopted for the egocentric observations in MOCA's training to address the sample insufficiency of imitation learning?", "explanation_reference": "The paper specifies two augmentation methods used to generate perturbed trajectories for each trajectory in training: color swapping, which randomizes the order of the RGB channels, and AutoAugment, which applies predefined image operations such as rotation, shearing, and auto-contrast.", "evidence_reference": "We adopt data augmentation for the egocentric observations, $\\{I_t\\}_{t=1}^T$, to address the sample insufficiency of imitation learning in each trajectory. Specifically, we exploit two augmentation methods; color swapping and AutoAugment."}
{"question": "Consider the paper that introduces the method that demonstrates the lowest EA score on the FinQA task. What specific aspect of the policy gradient strategy allows the model proposed in the paper, PromptPG, to outperform heuristic-based example selection strategies in the context of few-shot GPT-3?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method demonstrates the lowest EA score in FinQA task?", "answer_anchor": "PromptPG", "question_reference": "What specific aspect of the policy gradient strategy allows PromptPG to outperform heuristic-based example selection strategies in the context of few-shot GPT-3?", "explanation_reference": "The policy gradient strategy, as applied in PromptPG, enables the model to learn and select the most effective in-context examples for few-shot GPT-3 dynamically. This approach is contrasted with heuristic-based strategies that rely on predefined rules, which may not always align with the nuances of each problem. By learning from the training data, PromptPG can adapt its selection strategy to optimize performance, leading to its superior results.", "evidence_reference": "Compared to random selection, selecting the same question or answer type of examples helps the model to take the task-relevant examples as the prompt, thus improving the accuracy and reducing the variance. \\model shows its effectiveness in selecting optimal in-context examples over other strategies and largely reduces the instability."}
{"question": "Consider the paper that introduces the model that demonstrates the lowest zh-en score. What is the primary reason for using hard constraints over soft constraints in the human input autocompletion process according to the authors?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model demonstrates the lowest zh-en score?", "answer_anchor": "GWLAN", "question_reference": "What is the primary reason for using hard constraints over soft constraints in the human input autocompletion process according to the authors?", "explanation_reference": "The authors decided to use human inputs as hard constraints in their experiments because this method was found to be efficient and simple. Despite the comparable performance of soft and hard constraint methods in preliminary experiments, the efficiency and simplicity of using hard constraints made it the preferred choice.", "evidence_reference": "Therefore, we propose to use the human inputs as hard constraints in our later experiments, because of the method's efficiency and simplicity."}
{"question": "Consider the paper that discusses the dataset in the table that has a validation set size of 1250. What specific aspect of pretrained models contributes to their superior performance in generating factual summaries on the XSumFaith dataset, compared to non-pretrained models?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset having 1250 val set from the table?", "answer_anchor": "XSumFaith", "question_reference": "Based on the findings, what specific aspect of pretrained models contributes to their superior performance in generating factual summaries compared to non-pretrained models?", "explanation_reference": "The superior performance of pretrained models in generating factual summaries is attributed to their exposure to vast amounts of text during pretraining. This exposure allows them to better integrate background knowledge with generation, leading to more factual content even in the presence of hallucinations.", "evidence_reference": "The superior performance of \\bencdec is most likely due to its exposure to vast amount of text through pretraining, allowing it to integrate background knowledge with generation. Even so, over 90\\% of \\bencdec hallucinations are  erroneous."}
{"question": "Consider the paper that introduces the dataset which corresponds to the largest blue circle label. What specific sentiment analysis sub-tasks do the authors plan to extend their model to in the future?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What dataset is with the largest blue circle label?", "answer_anchor": "AfriSenti", "question_reference": "What specific sentiment analysis sub-tasks do the authors of AfriSenti plan to extend their dataset to in the future?", "explanation_reference": "The authors explicitly mention their future plans to extend AfriSenti to additional African languages and other sentiment analysis sub-tasks, indicating a broader scope beyond the current dataset's focus.", "evidence_reference": "In the future, we plan to extend \\textit{AfriSenti} to additional African languages and other sentiment analysis sub-tasks."}
{"question": "Consider the paper that introduces the method that achieves the lowest J_k scores in the WN18RR dataset. What is the relative increase in MRR achieved by the model proposed in the paper compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method shows the lowest J_k score in WN18RR dataset?", "answer_anchor": "EARL", "question_reference": "What is the relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "explanation_reference": "The relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset is mentioned in the section summarizing the main results, where it states that EARL uses only 62% parameters and obtains a relative increase of 4.7% on MRR in comparison with RotatE.", "evidence_reference": "Specifically, on FB15k-237, \\model~uses only 62\\% parameters and obtains a relative increase of 4.7\\% on MRR in comparison with RotatE."}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in direct prompting. What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for the model proposed in the paper?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shows the second best execuation accuracy in direct prompting?", "answer_anchor": "StarCoder", "question_reference": "What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for StarCoderBase?", "explanation_reference": "The percentage of responses flagged as toxic using a toxicity classifier for StarCoderBase in the RealToxicityPrompts evaluation is provided directly in the results table for the toxicity evaluation.", "evidence_reference": "StarCoderBase & 0.42 & 1.12"}
{"question": "Consider the paper that introduces the method that demonstrates the lowest score in the CSQA2.0 dev/dev* task. What is the impact, according to the paper's findings, of incorporating pretraining data mix during PPO training on the performance regressions observed on public NLP datasets?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method demonstrates the lowest score in CSQA2.0 dev task?", "answer_anchor": "ChatGPT", "question_reference": "Based on the paper's findings, what is the impact of incorporating pretraining data mix during PPO training on the performance regressions observed on public NLP datasets?", "explanation_reference": "The paper discusses that incorporating pretraining data mix during PPO training mitigates the performance regressions observed on public NLP datasets, suggesting that this approach helps maintain the capabilities of the pretrained model while aligning it with human preferences.", "evidence_reference": "We sweep a range of pretraining loss coefficient ($\\gamma$ in Equation~\\ref{eq2}) to see its effects on the performance of public NLP datasets and validation reward. The results are shown in Figure~\\ref{fig:public-nlp-evals-v-pretrain}. By setting pretraining loss coefficient to greater or equal ~20, the regression on these tasks can be recovered, on the 1.3B model. We also noticed that the sensitivity to pretraining loss coefficient varies across tasks. Although increasing the pretraining loss coefficient causes the validation reward to drop, a single value of 27.8 seems to work well across model sizes, from 1.3B to 175B parameter count. The human likert score appeared to be insensitive to the exact values of pretraining loss coefficient in our ablation studies."}
{"question": "Consider the paper that introduces the model specifically Multialpaca. How does its performance on the TyDiQA-GoldP benchmark in Arabic compare to its performance in Korean?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method demonstrates 83.71 average answer length from the table?", "answer_anchor": "Multialpaca", "question_reference": "How does the performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark in Arabic compare to its performance in Korean?", "explanation_reference": "The performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark is higher in Arabic (50.7 F1 score) compared to Korean (30.1 F1 score), indicating better comprehension and response generation capabilities in Arabic.", "evidence_reference": "BLOOMZ-MT-7.1B   & 22.4          & 36.6 & 26.9 & 5.8  & 9.1  & 20.2    & 26.7 & 2.4  & 14.4 & 26.5 & 17.5    \\nLLaMA-Alpaca-13B & \\textbf{59.2} & 20.8 & 48.6 & 19.3 & 37.7 & 37.1    & 11.0 & 50.6 & 20.7 & 5.7  & 22.0    \\n\\textsc{Poly}LM-\\mySFTDatasetName-13B & 58.7 & \\textbf{50.7} & \\textbf{52.1} & \\textbf{30.1} & \\textbf{40.3} & \\textbf{46.4} & 2.5 & 8.5 & 4.6 & 1.9 & 4.4"}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that has a Test Accuracy of 79.6. What specific methodological limitation does the paper acknowledge regarding the incorporation of commonsense knowledge into the Ana-CL model?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which Seq2Seq model shows 79.6 Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific methodological limitation does the paper acknowledge regarding the incorporation of commonsense knowledge into the MWP solver?", "explanation_reference": "The limitation is directly acknowledged in the 'Limitations' section, where the authors mention that the MWP solver cannot solve problems requiring commonsense knowledge if such knowledge is not explicitly provided in the problem description. This points to a methodological limitation in the solver's design regarding the integration of external, implicit knowledge.", "evidence_reference": "As mentioned in \\cite{lin2020numersense,DBLP:journals/corr/abs-2107-13435}, MWP solving in the real-word scenario requires many commonsense knowledge, e.g., 1km = 1000m and one day = 24 hours. When these commonsense constants are not explicitly given in the problem description, our MWP solver has no chance to solve problems that require them."}
{"question": "Consider the paper that introduces the model that shows the best overall performance in the 'Foreign' scenario. What are the specific conditions (C') and (C'') assumed for the convergence of the cdf resulting from the DA-WR algorithm?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shown the best overall performance?", "answer_anchor": "LLaMA-30B", "question_reference": "What are the specific conditions (C') and (C'') assumed for the convergence of the cdf resulting from the DA-WR algorithm?", "explanation_reference": "The conditions (C') and (C'') are necessary assumptions for ensuring the convergence of the cumulative distribution function (cdf) resulting from the Data Augmentation - Weighted Resampling (DA-WR) algorithm to the target distribution as the sample size n approaches infinity. These conditions are related to the differences in indicators for whether a value is less than or equal to x and the differences in weights between the initial and augmented datasets, respectively, and they must tend to zero faster than 1/n.", "evidence_reference": "\\item {\\bf (C')} : $\\max_{i=1,\\cdots,n} |\\mathbb{I}_{X_i\\leq x} - \\mathbb I_{X_i'\\leq x}| = o(1/n)$ \\item {\\bf (C'')} : $\\max_{i=1,\\cdots,n} |q_i -q_i' | =o(1/n)$"}
{"question": "Consider the paper that introduces which model is shown on the first line of the table. What specific strategy does BERT use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "answer": "", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shown on the first line?", "answer_anchor": "BERT", "question_reference": "What specific strategy does BERT use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "explanation_reference": "The question assesses understanding of BERT's pre-training strategy designed to mitigate the mismatch between the pre-training and fine-tuning stages, specifically focusing on the detailed probabilities of the mixed masking strategy employed.", "evidence_reference": "In Section~\\ref{sec:pretraining_tasks}, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective... The following is an ablation study to evaluate the effect of different masking strategies... \\begin{table}[ht] \\begin{center} {\\small \\begin{tabular}{@{}rrrccc@{}} \\toprule \\multicolumn{3}{c}{Masking Rates} & \\multicolumn{3}{c}{Dev Set Results}  \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.2cm}){4-6} \\textsc{Mask} &\\textsc{Same}&\\textsc{Rnd}& {MNLI} &\\multicolumn{2}{c}{NER} &  & & {\\footnotesize Fine-tune} &   {\\footnotesize Fine-tune}& {\\footnotesize Feature-based} \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.1cm}r{0.1cm}){4-4} \\cmidrule(l{0.2cm}){5-6}  80\\%&10\\%&10\\%&84.2&95.4&94.9... \\bottomrule \\end{tabular} } \\end{center} \\caption{\\label{tab:mask_ablation} Ablation over different masking strategies.} \\end{table}"}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage category. What was the success rate in the targeted in-context knowledge updating experiments for the FEVER dataset when the prompt included Original Examples Only?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2210.09150", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage?", "answer_anchor": "IC-Retrieval", "question_reference": "In the targeted in-context knowledge updating experiments, what was the success rate for the FEVER dataset when the prompt included Original Examples Only?", "explanation_reference": "The success rate for the FEVER dataset when the prompt included only Original Examples was 44.2, indicating the percentage of paraphrases of the original test claim for which GPT-3 predicted the target label for editing correctly.", "evidence_reference": "FEVER & 44.2 & 85.1 - 84.9 = 0.2"}
{"question": "Consider the paper that introduces the method that corresponds to the leftmost bar in the figure. What is the theoretical justification for the equivalence between the posterior distribution of the MSP, under Gaussian Discriminant Analysis (GDA) and the softmax classifier?", "answer": "", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown on the leftmost bar in the figure?", "answer_anchor": "MSP", "question_reference": "What is the theoretical justification for the equivalence between the posterior distribution of the generative classifier under Gaussian Discriminant Analysis (GDA) and the softmax classifier?", "explanation_reference": "The equivalence is justified by showing that the form of the posterior distribution defined by the generative classifier under GDA, when assuming tied covariance across classes, simplifies to a linear function of the input similar to the softmax classifier. This simplification occurs because the quadratic term, which is present in the general form of the Gaussian distribution, cancels out due to the tied covariance assumption, resulting in a linear dependency on the input.", "evidence_reference": "In addition to Gaussian assumption, LDA further assumes that all classes share the same covariance matrix, i.e., $\\mathbf{\\Sigma}_c = \\mathbf{\\Sigma}$. Since the quadratic term is canceled out with this assumption, the posterior distribution of generative classifier can be represented as follows: \\begin{align*} \\l & P\\left(y=c|\\mathbf{x}\\right) = \\frac{P\\left( y = c \\right) P\\left(\\mathbf{x}| y =c \\right) }{\\sum_{c^\\prime}P\\left( y= c^\\prime \\right) P\\left(\\mathbf{x}| y= c^\\prime \\right)} \\notag = \\frac{ \\exp \\left( \\mathbf{\\mu}_c^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{x} -\\frac{1}{2} \\mathbf{\\mu}_c^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_c +\\log \\beta_c \\right) }{\\sum_{c^\\prime} \\exp \\left( \\mathbf{\\mu}_{c^\\prime}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{x} -\\frac{1}{2} \\mathbf{\\mu}_{c^\\prime}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_{c^\\prime} +\\log \\beta_{c^\\prime} \\right)}. \\end{align*} One can note that the above form of posterior distribution is equivalent to the softmax classifier by considering $\\mathbf{\\mu}_{c}^\\top \\mathbf{\\Sigma}^{-1}$ and $ -\\frac{1}{2} \\mathbf{\\mu}_c^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{\\mu}_c +\\log \\beta_c$ as weight and bias of it, respectively."}
{"question": "Consider the paper that introduces the benchmark that achieves a 'Generation Token Length' near 400 when the 'Compression Ratio' is 1. What specific criteria were used to exclude tasks from its subset, known as BIG-Bench Hard (BBH), due to their reliance on specialized knowledge or being outside the scope of the work?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which benchmark gets 'Generation Token Length' near 400 when 'Compression Ratio' is 1?", "answer_anchor": "BBH", "question_reference": "What specific criteria were used to exclude tasks from the BIG-Bench Hard (BBH) subset due to their reliance on specialized knowledge or being outside the scope of the work?", "explanation_reference": "The criteria for excluding tasks from the BBH subset due to their reliance on specialized knowledge or being outside the scope of the work were explicitly listed in the appendix under the heading 'Criteria: Task is outside the scope of this work.'", "evidence_reference": "Criteria: Task is outside the scope of this work: not solvable by authors within 60 minutes, requires specialized knowledge, or not even worth attempting with chain-of-thought."}
{"question": "Consider the paper that introduces the method which achieves a higher score than No Graph but a lower score than TOD-Flow using GPT-turbo with SGD in 24 domains. What is the threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph for the model proposed in the paper?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method has higher score than No Graph but lower score than TOD-Flow using GPT-turbo with SGD in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "What is the threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph?", "explanation_reference": "The threshold value used for transition purity in the ProceL dataset to avoid forming a cycle in the resulting subtask graph is mentioned as part of the layer-wise precondition inference strategy to ensure that the subtask graph does not contain any cycles. This threshold is used to determine the ancestor-descendant relationships between subtasks based on their sequence of occurrence in the videos.", "evidence_reference": "We used \\(\\delta=0.96\\) for ProceL and \\(\\delta=0.55\\) for CrossTask in the experiment."}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What is the best setting of the hyperparameter \\(\\gamma\\) for CSN-sent and why, on the original PersonaChat dataset?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "CSN-word", "question_reference": "What is the best setting of the hyperparameter \\(\\gamma\\) for both CSN-sent and CSN-word on the original PersonaChat dataset, and why?", "explanation_reference": "The best setting of \\(\\gamma\\) is around 0.3 for both CSN-sent and CSN-word because it retains an appropriate amount of relevant document content for response matching, optimizing the balance between filtering out irrelevant content and keeping sufficient information for effective response selection.", "evidence_reference": "The best setting of \\(\\gamma\\) is around 0.3 for both CSN-sent and CSN-word, which retains an appropriate amount of relevant document content for response matching."}
{"question": "Consider the paper that introduces the model that has a de-en score of 53.87. What specific advantage does it demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model has 53.87 de-en score?", "answer_anchor": "GWLAN", "question_reference": "What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "explanation_reference": "The advantage is highlighted by the fact that even though there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment. This suggests that the joint model's ability to handle multiple related tasks with a single model architecture simplifies the deployment process compared to having separate models for each context type.", "evidence_reference": "Compared with \\textsc{WPM-Sep}, \\textsc{WPM-Joint} shows two advantages. On one hand, even there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment."}
{"question": "Consider the paper that introduces the model shown in the figure that is represented by the pink line. What is the trimming sequence value chosen for its application?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shown in the figure is represented by the pink line?", "answer_anchor": "LLaMA-30B", "question_reference": "What is the trimming sequence value chosen for the application?", "explanation_reference": "The trimming sequence value chosen for the application is explicitly mentioned in the section titled \\textsc{Application_8}, indicating the specific value used for the trimming sequence in the application.", "evidence_reference": "For the application, we chose a trimming  sequence $e_{n} = \\frac{1}{10 \\times n}$"}
{"question": "Consider the paper that introduces the dataset that corresponds to the largest blue circle label. What specific sentiment analysis sub-tasks do the authors plan to extend the model proposed in the paper to in the future?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What dataset is with the largest blue circle label?", "answer_anchor": "AfriSenti", "question_reference": "What specific sentiment analysis sub-tasks do the authors plan to extend AfriSenti to in the future?", "explanation_reference": "The authors explicitly mention their future plans for AfriSenti, which include extending it to additional African languages and exploring other sentiment analysis sub-tasks, indicating a broader scope for the dataset beyond its current state.", "evidence_reference": "In the future, we plan to extend \\textit{AfriSenti} to additional African languages and other sentiment analysis sub-tasks."}
{"question": "Consider the paper that introduces the method which has a GPT backbone and 7B parameters. What specific performance improvement does the Socratic CoT approach demonstrate over the traditional CoT approach when applied to this method in a single-shot setting on the GSM8K dataset?", "answer": "", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method used with a GPT backbone and 7B parameters?", "answer_anchor": "DecomDistill", "question_reference": "What specific performance improvement does the Socratic CoT approach demonstrate over the traditional CoT approach when applied to the GPT-3 model in a single-shot setting on the GSM8K dataset?", "explanation_reference": "The question focuses on the comparison between the Socratic CoT and traditional CoT approaches in enhancing the reasoning capabilities of GPT-3 on the GSM8K dataset. The answer is derived from the results section, where it mentions that introducing subquestioning (Socratic CoT) boosts accuracy by over 40% compared to standard CoT prompting.", "evidence_reference": "The introduction of subquestioning boosts accuracy by over 40\\% compared to standard CoT prompting (Table \\ref{Acc:GPT-3})."}
{"question": "Consider the paper that introduces the method that achieves a Hits@1 score of 0.281 in the YAGO43kET dataset. How does the model proposed in the paper's CET method's N2T mechanism specifically handle the embeddings of neighbor relations and entities to infer an entity's type?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets Hits@1 score equal to 0.281 in YAGO43kET datast?", "answer_anchor": "RGCN", "question_reference": "How does the proposed CET method's N2T mechanism specifically handle the embeddings of neighbor relations and entities to infer an entity's type?", "explanation_reference": "The N2T mechanism of the CET method focuses on using each neighbor independently to infer the missing types of central entities. Specifically, it calculates the difference between the embeddings of the neighbor entity and the neighbor relation, applies a non-linear activation function (ReLU in this context), and then sends this result through a linear layer to produce the relevance score for type inference. This process is designed to reduce the interference of irrelevant information on entity typing by focusing on individual attributes represented by neighbors.", "evidence_reference": "In practice, CET follows the translating assumption in TransE to obtain the neighbor embedding, then conducts non-linear activation on neighbor embedding and sent it to a linear layer: \\(\\vec{R}_{(n_r, n_e)}^{N2T} = \\matrix{W}\\mathrm{Relu}(\\vec{n}_e - \\vec{n}_r) + \\vec{b}\\), where \\(\\matrix{W}\\in\\mathbb{R}^{L\\times k}, \\vec{b}\\in\\mathbb{R}^{L}\\) are the learning parameters and \\(\\vec{R}_{(n_r, n_e)}^{N2T} \\in \\mathbb{R}^{L}\\) is the relevance score calculated by the N2T mechanism, where the i-th entry represents the relevance score between neighbor \\((n_e, n_r)\\) and type i."}
{"question": "Consider the paper that introduces the method that achieves a relatively constant MRR score in the FB15k-237 dataset as entity code entropy increases. What is the primary reason for the performance variance in the model's, proposed by the paper, ablation studies across different datasets?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method shows a huge increase as entity code entropy increases in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "What is the primary reason for the performance variance in EARL's ablation studies across different datasets?", "explanation_reference": "The variance in performance across different datasets in the ablation studies of EARL is attributed to the data statistics of these datasets, particularly the number of relations they contain. Datasets with more relations provide sufficient distinguishable information for entity embeddings, which influences the impact of removing certain components (e.g., Reserved Entity, ConRel, $k$NResEnt) on the model's performance.", "evidence_reference": "FB15k-237 and CoDEx-L have more relations than WN18RR and YAGO3-10, and diverse relations provide enough distinguishable information for entity embeddings. Thus, even in the 'w/o Reserved Entity' and 'w/o $k$NResEnt', performance is not affected dramatically since ConRel information still exists."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Internet-enhanced category. What specific performance improvement does the Internet-Fewshot model demonstrate over the simpler baseline model across all datasets, and how does this relate to the method's effectiveness in leveraging multiple retrieved evidences?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.05115", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the first method shown in Explicit --> Internet-enhanced", "answer_anchor": "Internet-Fewshot", "question_reference": "What specific performance improvement does the \\obsearchpoe\\ model demonstrate over the simpler \\obsearch\\ model across all datasets, and how does this relate to the method's effectiveness in leveraging multiple retrieved evidences?", "explanation_reference": "The specific performance improvement demonstrated by the \\obsearchpoe\\ model over the simpler \\obsearch\\ model across all datasets is that it consistently widens the performance gap between closed-book and Google-conditioned models. This indicates the method's effectiveness in leveraging multiple retrieved evidences by using a reranking stage that utilizes scores computed by the same LMs, thus showcasing a more sophisticated use of the retrieved information to improve answer accuracy.", "evidence_reference": "Finally, our probabilistic reranking further improves performance: across all datasets,  ~\\obsearchpoe\\~ outperforms consistently the simpler \\obsearch\\~, widening the gap between closed-book and Google-conditioned model."}
{"question": "Consider the paper that introduces the dataset that corresponds to the last row of the table. What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset shown in the last row of the table?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "explanation_reference": "The paper discusses how non-diacritical marks comments can trick annotators into needing to re-read the text multiple times due to the absence of punctuation, diacritics, and the presence of ambiguous text that could be interpreted inappropriately in multiple ways. This specific linguistic phenomenon is highlighted as a challenge in understanding and annotating the dataset.", "evidence_reference": "Non-diacritical marks comments might trick annotators a little bit... there are some problems causing annotators to re-read multiple times as no punctuation, diacritic, and the text 'con ng' could be considered as 'crazy girl' or 'the type (of human)' and both of these meanings is inappropriate."}
{"question": "Consider the paper that introduces the method that has an F1 score of 75. What specific performance gain does the integration of both CoordConv and Corner Pooling modules contribute to the Entity Linking task, according to the ablation study?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having 75 F1 score?", "answer_anchor": "MSAU-PAF", "question_reference": "What specific performance gain does the integration of both CoordConv and Corner Pooling modules contribute to the Entity Linking task in the MSAU-PAF model, according to the ablation study?", "explanation_reference": "The integration of both CoordConv and Corner Pooling modules into the MSAU-PAF model results in a performance gain of 0.03 in the F1 score for the Entity Linking task, as indicated by the ablation study. This improvement demonstrates the effectiveness of these modules in enhancing the model's ability to predict entity links by exploiting spatial correlations more effectively.", "evidence_reference": "The last row shows the results that we test on combining all improvements, which improves the F1 score by 0.03 by the total in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. What is the F1 score improvement of the model proposed in the paper using BERT label encoder over the model using GloVe label encoder in the 1-shot setting for the NCBI-disease dataset?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shown in the penult row of the table?", "answer_anchor": "LSFS", "question_reference": "What is the F1 score improvement of the model using BERT label encoder over the model using GloVe label encoder in the 1-shot setting for the NCBI-disease dataset?", "explanation_reference": "The improvement can be calculated by subtracting the F1 score of the model using GloVe label encoder from the F1 score of the model using BERT label encoder in the 1-shot setting for the NCBI-disease dataset. 10.7-15.1=15.6", "evidence_reference": "Our model - GloVe & 15.1 $\\pm$ 8.7 & ... & ... & ... & ... & ... \\\\ &  Our model - BERT & \\bf 30.7 $\\pm$ 9.1 & ... & ... & ... & ... & ... \\\\"}
{"question": "Consider the paper that introduces the method that has a score of 87.3 in the SciTail dataset with 16-shot prompting. What specific strategy is employed during target adaptation to manage the learning rates for the task-shared and task-specific components in the model proposed by the paper?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having score of 78.6 in SciTail dataset with 16-shot prompting?", "answer_anchor": "MPT", "question_reference": "What specific strategy is employed during target adaptation to manage the learning rates for the task-shared and task-specific components in \\ours?", "explanation_reference": "The paper specifies that during target adaptation, a strategy of two-speed learning rates is used for the task-shared and task-specific components, indicating a differentiated approach to updating these components.", "evidence_reference": "During target adaptation, we use a strategy of two-speed learning rates for those two components, as in~\\citet{ponti2022combining}. Specifically, we set the learning rate to $0.3$ and $0.4$ for the task-shared and task-specific components, respectively, during target task adaptation."}
{"question": "Consider the paper that introduces the model that results in the second lowest accuracy in the COGS-all dataset. According to the authors' analysis, what specific linguistic function does attention head 5 in layer 5 of 6 of the model proposed in the paper appear to be involved in, based on the evidence provided in the attention visualizations section?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model demonstrates the second lowest accuracy in COGS-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the evidence provided in the attention visualizations section, what specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis?", "explanation_reference": "The authors present visual evidence showing that attention head 5 in layer 5 of 6 is involved in anaphora resolution, as demonstrated by the focused attention on the word 'its' and its relation to other parts of the sentence.", "evidence_reference": "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the dataset in the Movies domain only that has a PF field. What specific distribution statistics were analyzed in the paper to assess goal completion and knowledge exploitation?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which dataset is in Movies domain only and has PF field?", "answer_anchor": "DuConv", "question_reference": "What specific distribution statistics were analyzed to assess goal completion and knowledge exploitation in the paper?", "explanation_reference": "The question assesses understanding of the detailed analysis conducted in the paper regarding how well the conversation goals were achieved and how the knowledge was exploited in the dialogue process. It focuses on the specific aspects of 'goal completion' and 'knowledge used', which are key to evaluating the effectiveness of the proposed models in utilizing the knowledge graph for proactive conversation.", "evidence_reference": "Analysis on goal completion and knowledge exploitation."}
{"question": "Consider the paper that introduces the model which corresponds to the first row of the table. What specific strategy does this model use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model on the first row of the table?", "answer_anchor": "BERT", "question_reference": "What specific strategy does BERT use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "explanation_reference": "The question assesses understanding of BERT's pre-training strategy designed to mitigate the mismatch between the pre-training and fine-tuning stages, specifically focusing on the detailed probabilities of the mixed masking strategy employed.", "evidence_reference": "In Section~\\ref{sec:pretraining_tasks}, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective... The following is an ablation study to evaluate the effect of different masking strategies... \\begin{table}[ht] \\begin{center} {\\small \\begin{tabular}{@{}rrrccc@{}} \\toprule \\multicolumn{3}{c}{Masking Rates} & \\multicolumn{3}{c}{Dev Set Results}  \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.2cm}){4-6} \\textsc{Mask} &\\textsc{Same}&\\textsc{Rnd}& {MNLI} &\\multicolumn{2}{c}{NER} &  & & {\\footnotesize Fine-tune} &   {\\footnotesize Fine-tune}& {\\footnotesize Feature-based} \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.1cm}r{0.1cm}){4-4} \\cmidrule(l{0.2cm}){5-6}  80\\%&10\\%&10\\%&84.2&95.4&94.9... \\bottomrule \\end{tabular} } \\end{center} \\caption{\\label{tab:mask_ablation} Ablation over different masking strategies.} \\end{table}"}
{"question": "Consider the paper that introduces the method which is represented by the blue line in the figure. What is the primary reason for the performance variance in the model proposed in the paper, across different datasets in its ablation studies?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method represented in the blue line from the figure?", "answer_anchor": "EARL", "question_reference": "What is the primary reason for the performance variance in EARL's ablation studies across different datasets?", "explanation_reference": "The variance in performance across different datasets in the ablation studies of EARL is attributed to the data statistics of these datasets, particularly the number of relations they contain. Datasets with more relations provide sufficient distinguishable information for entity embeddings, which influences the impact of removing certain components (e.g., Reserved Entity, ConRel, $k$NResEnt) on the model's performance.", "evidence_reference": "FB15k-237 and CoDEx-L have more relations than WN18RR and YAGO3-10, and diverse relations provide enough distinguishable information for entity embeddings. Thus, even in the 'w/o Reserved Entity' and 'w/o $k$NResEnt', performance is not affected dramatically since ConRel information still exists."}
{"question": "Consider the paper that introduces the method which is placed below the row for R-Former but above the row for NeurJudge. What specific method does the model proposed in the paper use to initially segment law articles into communities before applying the graph distillation operator?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown below the row of R-Former but above the row of NeurJudge?", "answer_anchor": "LADAN", "question_reference": "What specific method does LADAN use to initially segment law articles into communities before applying the graph distillation operator?", "explanation_reference": "The paper specifies that to initially segment law articles into communities, a fully-connected graph is constructed where the weight on the edge between a pair of law articles is defined by the cosine similarity between their TF-IDF (Term Frequency-Inverse Document Frequency) representations. This method is used before applying the graph distillation operator to identify probably confusing law articles by removing edges with weights less than a predefined threshold from the graph, resulting in several disconnected subgraphs or communities.", "evidence_reference": "To find probably confusing law articles, we first construct a fully-connected graph $G^*$ for all law articles $\\mathcal{L}$, where the weight on the edge between a pair of law article $L_i, L_j\\in \\mathcal{L}$ is defined as the cosine similarity between their TF-IDF (Term Frequency-Inverse Document Frequency) representations $\\mathbf{tf\\_idf}_i$ and $\\mathbf{tf\\_idf}_j$."}
{"question": "Consider the paper that introduces the method that is placed directly above the PHA method in the table. What specific performance improvement does the model proposed in the paper, HyperDecoder, achieve on the GLUE benchmark when using T5 v1.1 + LM as the underlying model compared to the Task Hypernet method?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shown in the table is right above PHA method?", "answer_anchor": "HyperDecoder", "question_reference": "What specific performance improvement does the Hyperdecoder approach achieve on the GLUE benchmark when using T5 v1.1 + LM as the underlying model compared to the Task Hypernet method?", "explanation_reference": "The Hyperdecoder approach's performance improvement over the Task Hypernet method is calculated based on their average GLUE benchmark scores when using T5 v1.1 + LM as the underlying model. The Task Hypernet method has an average score of 54.3, while the Hyperdecoder approach achieves an average score of 86.5. The improvement is calculated as ((86.5 - 54.3) / 54.3) * 100% = 32.2%.", "evidence_reference": "Task Hypernet & 2.7% & 0.0 & 82.1 & 16.4 / 16.4 & 70.4 / 81.4 & 89.8 / 86.5 & 56.6 & 64.8 & 50.7 & 54.3 \\\\ \\textbf{Hyperdecoder (ours)} & 2.9% & 58.7$_{2.3}$ & \\textbf{95.9}$_{0.4}$* & 91.8$_{0.7}$ / \\textbf{92.0}$_{0.4}$* & 89.2$_{1.5}$ / 92.0$_{0.9}$ & 91.1$_{0.2}$ / 88.3$_{0.4}$ & \\textbf{90.0}$_{0.2}$* & \\textbf{94.2}$_{0.4}$ & \\textbf{80.8}$_{2.2}$ & \\textbf{86.5}${_{0.5}}\\\\dagger$ \\\\"}
{"question": "Consider the paper that introduces the score described as a \"fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits) by the model\". What is the theoretical justification for the violation of the data processing inequality in the context of $\\mathcal{V}$-usable information when processing the input with $\\tau$?", "answer": "", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2110.08420", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the name of the score with description 'Fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits by the model.'?", "answer_anchor": "PVI", "question_reference": "What is the theoretical justification for the violation of the data processing inequality in the context of $\\mathcal{V}$-usable information when processing the input with $\\tau$?", "explanation_reference": "The paper justifies the violation of the data processing inequality by highlighting that certain types of input processing can make the information more accessible to the model, thereby increasing the $\\mathcal{V}$-usable information without increasing the Shannon information. This is crucial for understanding how representation learning and other forms of input processing contribute to model performance.", "evidence_reference": "Processing the input with $\\tau$ (e.g., by decrypting the text) can make prediction easier, allowing $I_\\mathcal{V}(\\tau(X) \\to Y) \\geq I_\\mathcal{V}(X \\to Y)$. Although this violates the data processing inequality, it explains the usefulness of certain types of processing, such as representation learning."}
{"question": "Consider the paper that introduces the optimization method that has the lowest BLEU score among all models in the table. What specific achievement does the model proposed in the paper demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What optimization method shows the lowest BLEU score across all models?", "answer_anchor": "Transformer base", "question_reference": "Based on the Transformer model's performance on English constituency parsing, what specific achievement does it demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "explanation_reference": "The paper highlights that, unlike RNN sequence-to-sequence models, the Transformer model outperforms the BerkeleyParser even when trained only on the WSJ training set of 40K sentences. This indicates the Transformer's superior ability to handle tasks with strong structural constraints and significantly longer outputs than inputs, even with limited training data.", "evidence_reference": "In contrast to RNN sequence-to-sequence models [KVparse15], the Transformer outperforms the BerkeleyParser [petrov-EtAl:2006:ACL] even when training only on the WSJ training set of 40K sentences."}
{"question": "Consider the paper that introduces the LLM model that has a test accuracy of 50.7. What specific performance improvement does the model proposed in the paper achieve on the GSM8K task when used with PaLM-540B, compared to chain-of-thought prompting?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which LLM model shows test accuracy by 50.7?", "answer_anchor": "Self-Consistency", "question_reference": "What specific performance improvement does self-consistency achieve on the GSM8K task when used with PaLM-540B, compared to chain-of-thought prompting?", "explanation_reference": "The question targets a detailed comparison of performance improvements achieved by the self-consistency method over the chain-of-thought prompting method on a specific arithmetic reasoning task (GSM8K) using a specific language model (PaLM-540B). This detail requires understanding of the empirical results presented in the paper, specifically focusing on the exact numerical improvement in accuracy.", "evidence_reference": "Self-consistency & 93.7 {\\scriptsize(+1.8)} & 99.3 {\\scriptsize(+4.6)} & 81.9 {\\scriptsize(+7.9)} & 48.3 {\\scriptsize(+12.5)} & 86.6 {\\scriptsize(+7.6)} & 74.4 {\\scriptsize(+17.9)}"}
{"question": "Consider the paper that introduces the model shown in the first row of the table. What specific adaptation in the text embeddings allows the MVQG-VL-T5 to build correspondence among query text, label text, and objects in grounding tasks?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific adaptation in the text embeddings allows the model to build correspondence among query text, label text, and objects in grounding tasks?", "explanation_reference": "The specific adaptation that allows the model to build correspondence among query text, label text, and objects in grounding tasks is the use of embedding sharing. This is achieved by reusing the text embeddings of visual sentinel tokens as region id embeddings, which enables the model to establish a connection between the text and visual elements, particularly useful in grounding tasks.", "evidence_reference": "In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens \\{\\texttt{<vis\\_1>}, $\\dots$, \\texttt{<vis\\_n>}\\}, which corresponds to image regions. As illustrated in Fig.~\\ref{fig:architecture}, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec.~\\ref{sec:visual_embeddings}. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec.~\\ref{sec:pretraining}, referring expression comprehension in Sec.~\\ref{sec:refcoco})."}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that shows the highest Test Accuracy. What specific method does the model's solution discrimination module use to encode the equation representation of a solution?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which Seq2Seq model shows the higest Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module use to encode the equation representation of a solution?", "explanation_reference": "The solution discrimination module encodes the equation representation of a solution using a gate-recurrent-unit (GRU), as specified in the description of how the discrimination score is measured.", "evidence_reference": "We define a discrimination score that measures the association of a problem $X$ with its ground truth solution $Y$: \\begin{equation} \\label{eq:s_d} s_d =  Dis([en_x(X),en_y(Y)]) \\end{equation} where $en_y(Y)$ is the representation of equation $Y$ encoded by a gate-recurrent-unit (GRU) \\cite{chung2014empirical}."}
{"question": "Consider the paper that introduces the Seq2Exp model that exhibits the highest test accuracy. What is the maximum Memory Departing Distance (M-MDD) value for which the model proposed in the paper with Memory Register (MR) outperforms the model without MR on the MathQA dataset according to Figure \\ref{fig:Cache-MCDP-MathQA}?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which Seq2Exp model shows the highest test accuracy?", "answer_anchor": "Elastic", "question_reference": "What is the maximum Memory Departing Distance (M-MDD) value for which ELASTIC with Memory Register (MR) outperforms ELASTIC without MR on the MathQA dataset according to Figure \\ref{fig:Cache-MCDP-MathQA}?", "explanation_reference": "The maximum Memory Departing Distance (M-MDD) value for which ELASTIC with Memory Register (MR) outperforms ELASTIC without MR on the MathQA dataset is 5. This is indicated by the performance improvement observed in Figure \\ref{fig:Cache-MCDP-MathQA}, where ELASTIC with MR shows better performance than without MR when M-MDD is larger than 5.", "evidence_reference": "From Figure \\ref{fig:Cache-MCDP-MathQA}, the ELASTIC with Memory Register performs better than ELASTIC without it at each M-MDD on FinQA and MathQA datasets. Particularly in the MathQA dataset, when M-MDD is larger than 5, ELASTIC with Memory Register can achieve better results than the ELASTIC without it."}
{"question": "Consider the paper that introduces the large language model which corresponds to an HVI score of 47. What specific methodological difference in the evaluation of the model's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the large language model that demonstrates 47 HVI scores?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard procedure used for most exam runs, where the model's letter choice is extracted directly from the explanation. This approach for the USABO and SAT reading/writing runs indicates a unique handling of these exams, which could contribute to the minimal impact on the overall results, as it relies on the model's generated explanation to determine the final answer choice.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method represented by the purple line. What specific mechanism does the model, known as MMA-IL, use to calculate the expected attention for each head, and how does it differ from the approach used by MMA-H?", "answer": "", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is demonstrated by the purple line?", "answer_anchor": "MMA", "question_reference": "What specific mechanism does MMA-IL use to calculate the expected attention for each head, and how does it differ from the approach used by MMA-H?", "explanation_reference": "MMA-IL's approach to calculating expected attention involves using a SoftEnergy function, which allows each attention head to attend to all previous encoder states, leveraging more information for translation. This is in contrast to MMA-H, which uses MonotonicEnergy for a hard attention calculation, limiting each head to attend to one encoder state at a time. This distinction highlights the flexibility of MMA-IL in accessing more comprehensive source information compared to the more efficiency-oriented MMA-H.", "evidence_reference": "For MMA-H, we use \\autoref{eq:monotonic_attention} in order to calculate the expected alignment for each layer each head, given $p_{i,j}^{l, h}$. For MMA-IL, we calculate the softmax energy for each head as follows: \\x \\begin{eqnarray} u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j} \\end{eqnarray} and then use \\autoref{eq:milk_recurent} to calculate the expected attention."}
{"question": "Consider the paper that introduces the model which has a lower mac-F1 score than Longformer but a higher mac-F1 score than CaselawBERT. What specific improvement in F1 score was observed for the 'lease details' subset when comparing the model's variants to the tuned BERT base?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model having lower mac-F1 score than Longformer but higher mac-F1 score than CaselawBERT?", "answer_anchor": "LegalBERT", "question_reference": "What specific improvement in F1 score was observed for the 'lease details' subset when comparing LEGAL-BERT variants to the tuned BERT base?", "explanation_reference": "The question focuses on extracting a precise statistical improvement from the experimental results section, specifically targeting the performance comparison between LEGAL-BERT variants and the tuned BERT base model on a particular subset of a dataset. The answer directly reflects the observed improvement in F1 score for the 'lease details' subset, showcasing the effectiveness of LEGAL-BERT in this specific task.", "evidence_reference": "On the contrary, we observe a more substantial improvement in the more difficult multi-label task (2.5%) indicating that the \\legalbert variations benefit from in-domain knowledge. On \\contractsdata, the drop in perplexity is larger (5.6), which is reflected in the increase in $F1$ on the \\emph{contract header} (1.8\\%) and \\emph{dispute resolution} (1.6\\%) subsets. In the \\emph{lease details} subset, we also observe an improvement (1.1\\%)."}
{"question": "Consider the paper that introduces the model shown in the first row of the table. What is the difference in performance on the RefCOCOg task between the VL-T5 and VL-BART models within the unified framework proposed by the MVQG-VL-T5, and what is hypothesized as the reason for this divergence?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "explanation_reference": "The paper hypothesizes that the divergence in performance on the RefCOCOg task between VL-T5 and VL-BART is due to the different methods of positional encoding used by T5 and BART. Specifically, BART uses learned absolute positional embeddings, which might lead to the model memorizing the positions of training objects, resulting in high training accuracy but low validation accuracy. This hypothesis is supported by the observation of VL-BART's performance drop in the RefCOCOg task compared to VL-T5.", "evidence_reference": "We also observe that our experiments with \\oursb{} on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in self-attention layers instead. We hypothesize that \\oursb{} found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy)."}
{"question": "Consider the paper that introduces the method which has a lower F1 score than Doc2Graph and a higher F1 score than GNN+MLP. What is the percentage drop in $F_1$ score observed upon the removal of data augmentation during training on the \\cord\\ dataset according to the ablation study of the model proposed in the paper?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having lower F1 score than Doc2Graph and higher F1 score than GNN+MLP?", "answer_anchor": "SPADE", "question_reference": "What is the percentage drop in $F_1$ score observed upon the removal of data augmentation during training on the \\cord\\ dataset according to the ablation study?", "explanation_reference": "The ablation study section of the paper indicates that removing data augmentation during training on the \\cord\\ dataset results in a 2.6% drop in $F_1$ score, highlighting the importance of data augmentation in the model's performance.", "evidence_reference": "~~~~ (-) data augmentation & 81.9 (-2.6) \\\\\\\\"}
{"question": "Consider the paper that introduces the model which has a lower mac-F1 score than Longformer but a higher mac-F1 score than CaselawBERT. What specific aspect of its pre-training process distinguishes the model's adaptation for the legal domain from the adaptation strategies of BERT models in other specialized domains, as discussed in previous studies?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model having lower mac-F1 score than Longformer but higher mac-F1 score than CaselawBERT?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the LEGAL-BERT's pre-training process distinguishes its adaptation for the legal domain from the adaptation strategies of BERT models in other specialized domains, as discussed in previous studies?", "explanation_reference": "The question targets the methodological uniqueness of LEGAL-BERT's adaptation process for the legal domain, specifically focusing on the creation of a new vocabulary. This aspect is crucial for adapting the model to the specialized terminology and syntax of legal texts, which is a significant methodological detail that distinguishes LEGAL-BERT from other domain-specific BERT adaptations.", "evidence_reference": "\\noindent\\\\textbf{\\\\legalbertp} has the same architecture as \\\\bertbase with 12 layers, 768 hidden units and 12 attention heads (110M parameters). We use this architecture in all our experiments unless otherwise stated. We use a newly created vocabulary of equal size to \\\\bert's vocabulary."}
{"question": "Consider the paper that introduces the method that demonstrates the second lowest Acc-7 score on MOSI. What is the computational complexity of the model proposed in the paper compared to the Tensor Fusion Network (TFN) model, and how does this relate to the number of parameters in each model under the specified hyper-parameter setting?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1806.00064", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method demonstrates the lowest Acc-7 score on MOSI?", "answer_anchor": "TFN", "question_reference": "What is the computational complexity of the \\ourl\\ model compared to the Tensor Fusion Network (TFN) model, and how does this relate to the number of parameters in each model under the specified hyper-parameter setting?", "explanation_reference": "The question directly addresses the methodological analysis of the computational complexity and parameter count of the \\ourl\\ model in comparison to the TFN model, highlighting the efficiency of \\ourl\\ in terms of both computational complexity and the number of parameters. This is directly answered by the details provided in the Complexity Analysis section of the paper, which compares the theoretical computational complexity formulas for both models and provides a concrete example of parameter counts under a specific hyper-parameter setting.", "evidence_reference": "Theoretically, the model complexity of our fusion method is $O(d_y \\times r \\times \\sum_{m=1}^M d_m)$ compared to $O(d_y \\prod_{m=1}^M d_m)$ of TFN from Section \\ref{par:stupid_tensor}. In practice, we calculate the total number of parameters used in each model, where we choose $M=3$, $d_1 = 32$, $d_2 = 32$, $d_3 = 64$, $r = 4$, $d_y = 1$. Under this hyper-parameter setting, our model contains about 1.1e6 parameters while TFN contains about 12.5e6 parameters, which is nearly 11 times more."}
{"question": "Consider the paper that introduces the method that demonstrates the lowest score in the CSQA2.0 dev/dev* task. What is the entropy value for the Winogender task using a basic prompt for the model?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method demonstrates the lowest score in CSQA2.0 dev task?", "answer_anchor": "ChatGPT", "question_reference": "What is the entropy value for the Winogender task using a basic prompt for the 175B PPO model?", "explanation_reference": "The entropy value for the Winogender task using a basic prompt for the 175B PPO model directly reflects the model's uncertainty in making choices among the available options for this task, indicating how unbiased the model is in its selections.", "evidence_reference": "Winogender             & entropy     & basic         & 0.750 & 0.721 & 0.735 &  0.583 &  0.535 & 0.503 &  0.698 & 0.587 & 0.618 & \\textbf{0.760} &  0.719 & 0.737 \\\\"}
{"question": "Consider the paper that introduces the model that is on the last line of the Seq2Seq/Tree block of the table. What specific method does the model's solution discrimination module employ to enhance the association between a math word problem and its correct solution?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model on the last line of the Seq2Seq/Tree block?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module employ to enhance the association between a math word problem and its correct solution?", "explanation_reference": "The solution discrimination module enhances the association between a math word problem and its correct solution by employing a method that focuses on gradient-guided token selection. This method identifies the most vulnerable (important) token in the solution equation based on the largest gradient, aiming to manipulate it for generating hard negative samples. This approach is designed to make the solver more accurately associate problems with their correct solutions by distinguishing them from similar but incorrect solutions.", "evidence_reference": "For generating negative solutions, any manipulation of the ground truth solution can lead to a negative one, as implemented in  \\cite{ijcai2021}. However, the  random modification neglects   the importance of tokens   in the solution equation. Although all negative solutions ultimately lead to a wrong answer, the roles they play in minimizing loss functions and serving as contrastive examples to a positive true solution are at different levels of importance.  Our goal is to find  variants of the ground truth solution as hard negative samples, which only manipulate the most vulnerable (important) token.  In fact, this target is  similar to evasion attack \\cite{carlini2017towards} on texts, i.e., maximum effect and minimum manipulation. Therefore, borrowing the idea from white-box evasion attack, we regard the token with the largest gradient as the most important and vulnerable one: \\begin{equation} \\label{grad} y_i = \\mathop{argmax\\;}_{y_i \\in Y}(\\nabla Dis([en_x(X),en_y(Y)]). \\end{equation}"}
{"question": "Consider the paper that introduces the model labeling 'fine-tuned' shown in the table. How does the MVQG-VL-T5 model's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model being fine-tuned shown in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "explanation_reference": "Traditional methods for the RefCOCOg task typically involve classification over a set of visual regions, requiring task-specific architectures and objectives. In contrast, the unified framework proposed in the paper treats RefCOCOg as a text generation task, leveraging the same language modeling architecture and objective used for other vision-and-language tasks. This approach allows for more flexible architecture design and eliminates the need for task-specific modifications.", "evidence_reference": "While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously~\\cite{Yu2018,Chen2020} formulated classification task over a set of visual regions, allowing more flexible architecture design."}
{"question": "Consider the paper that introduces the method that achieves the lowest J_k scores in the WN18RR dataset. What is the primary reason for the performance variance in the model's ablation studies across different datasets?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method shows the lowest J_k score in WN18RR dataset?", "answer_anchor": "EARL", "question_reference": "What is the primary reason for the performance variance in EARL's ablation studies across different datasets?", "explanation_reference": "The variance in performance across different datasets in the ablation studies of EARL is attributed to the data statistics of these datasets, particularly the number of relations they contain. Datasets with more relations provide sufficient distinguishable information for entity embeddings, which influences the impact of removing certain components (e.g., Reserved Entity, ConRel, $k$NResEnt) on the model's performance.", "evidence_reference": "FB15k-237 and CoDEx-L have more relations than WN18RR and YAGO3-10, and diverse relations provide enough distinguishable information for entity embeddings. Thus, even in the 'w/o Reserved Entity' and 'w/o $k$NResEnt', performance is not affected dramatically since ConRel information still exists."}
{"question": "Consider the paper that introduces the model that is represented by the lavender color in the figure. What specific preprocessing steps were applied to the XML files during the data curation process for its base version?", "answer": "", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model is demonstrated in the lavender color?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing steps were applied to the XML files during the data curation process for StarCoderBase?", "explanation_reference": "The question focuses on the specific method used to preprocess XML files during the data curation process for StarCoderBase. The answer directly addresses this by specifying the implementation of a simple XML filter, which is a detailed and specific part of the preprocessing steps mentioned in the paper.", "evidence_reference": "As we inspected the data, we noticed that certain extensions often consisted of XML files. For example, the .sld extension had more than 50% of its files in XML format. To address this, we implemented a simple XML filter that checked for the presence of '<?xml version=' within the first 100 characters of the file."}
{"question": "Consider the paper that introduces the method that achieves an F1 score with a mean of 58.86 in the TAT-QA task. What specific aspect of the policy gradient strategy contributes to the reduction of prediction variance in the selection of in-context examples for few-shot GPT-3?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method demonstrates F1 score with mean 58.86 in TAT-QA task?", "answer_anchor": "PromptPG", "question_reference": "What specific aspect of the policy gradient strategy contributes to the reduction of prediction variance in the selection of in-context examples for few-shot GPT-3?", "explanation_reference": "The policy gradient strategy, by learning to select in-context examples efficiently and stably without human-designed heuristics, directly contributes to reducing the prediction variance compared to random selection. This approach allows for a more systematic and adaptive selection process, which is less prone to the instability associated with random or heuristic-based selections.", "evidence_reference": "Inspired by reinforcement learning's ability to search for an optimal action policy, we propose applying the policy gradient strategy to learn to select in-context examples more efficiently and stably without designing human-designed heuristics."}
{"question": "Consider the paper that introduces the method represented by the lavender color. What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during the model's guided generation?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method represented by the lavender color?", "answer_anchor": "GeDi", "question_reference": "What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during GeDi-guided generation?", "explanation_reference": "The minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) is defined by the condition where \\(n=m\\). This is because \\(\\gV_m\\) is defined as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\).", "evidence_reference": "We define \\(\\gV_m\\) as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\)."}
{"question": "Consider the paper that introduces the model that demonstrates the highest score in the 'T3' column. Which specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the CEAR model?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model demonstrates the highest score in 'T3' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the proposed model?", "explanation_reference": "The answer directly lists the specific hyperparameter values that were determined through grid search for the FewRel dataset, as detailed in the implementation section of the paper.", "evidence_reference": "We find the best hyperparameter values through grid search with a step of 0.1 except 0.05 for \u03c9 and 0.25 for \u03b3. The search spaces for various hyperparameters are \u03b1\u2208[0.2,0.8], \u03b2\u2208[0.1,0.5], \u03bc\u2208[0.1,1.0], \u03c9\u2208[0.05,0.25], \u03b3\u2208[1.0,2.0] and \u03bb1, \u03bb2\u2208[0.5,1.5]. Besides, we fix \u03c41 and \u03c42 to 0.1 and 0.5, respectively. The used hyperparameter values are listed below: For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the model in the table that has 12M updated parameters. What specific component of its architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model in the table has 12M updated parameters?", "answer_anchor": "UniKGQA", "question_reference": "What specific component of UniKGQA's architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "explanation_reference": "The question targets a detailed aspect of the UniKGQA architecture, specifically asking for the component that handles the propagation of semantic matching information across the knowledge graph. The answer, 'Matching information propagation module,' directly addresses this by naming the specific part of the architecture designed for this purpose.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces MetaBINK. What specific improvement in U.Acc. does the model proposed in the paper, MetaBINK, achieve on the Yugioh domain under the few-shot entity linking task when comparing the performance of Syn+Seed data to the Name Matching method?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is on the last row of the upper half of the table?", "answer_anchor": "MetaBINK", "question_reference": "What specific improvement in U.Acc. does MetaBLINK achieve on the Yugioh domain under the few-shot entity linking task when comparing the performance of Syn+Seed data to the Name Matching method?", "explanation_reference": "U.Acc. for MetaBLINK on the Yugioh domain with Syn+seed is 22.82, with name matching is 7.88, 22.82-7.88=14.94", "evidence_reference": "TABLE VI"}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE-1 score equal to 44.52. How does the application of normalizing flow in the model proposed in the paper specifically contribute to the improvement of abstractive text summarization performance?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method in the table that demonstrates a ROUGE 1 score equal to 44.52?", "answer_anchor": "PEGASUS+NTM", "question_reference": "How does the application of normalizing flow in the neural topic model specifically contribute to the improvement of abstractive text summarization performance?", "explanation_reference": "The application of normalizing flow in the neural topic model allows for a more complex and expressive approximation of the document's global semantics, which in turn enhances the summarization model's ability to capture and integrate these semantics into the generated summaries. This leads to summaries that are more informative and coherent.", "evidence_reference": "In this work, we adapt the normalizing flow to the neural topic model to better grasp the global semantic patterns of the document."}
{"question": "Consider the paper that introduces the method that uses dropout as a regularizer for solution-level verifiers compared to token-level verifiers on the GSM8K dataset. What specific performance improvement does this approach provide?", "answer": "", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset represented on the leftmost of the figure?", "answer_anchor": "GSM8K", "question_reference": "What specific performance improvement does using dropout as a regularizer provide for solution-level verifiers compared to token-level verifiers?", "explanation_reference": "The paper mentions that using dropout significantly improves solution-level verifiers, mitigating the overfitting that occurs in the unregularized baseline, and notably reaches a similar level of performance as token-level verifiers. This indicates that while token-level verifiers are less susceptible to overfitting and thus might not see as significant an improvement from dropout, solution-level verifiers benefit substantially, enough to match the performance of the more robust token-level verifiers.", "evidence_reference": "In \\Cref{fig:single_token_dropout}, we see that dropout significantly improves solution-level verifiers, mitigating the overfitting that occurs in the unregularized baseline. Notably, using dropout with solution-level verifiers reaches a similar level of performance as token-level verifiers."}
{"question": "Consider the paper that introduces the model that achieves the best P_k score among the models in the first part of the table. What is the erroneous output fraction for structured summarization models when tested on the QMSum dataset?", "answer": "", "figure": "locality/2310.11772/comparison_2_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets the best P_k score on the upper part of the table??", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What is the erroneous output fraction for structured summarization models when tested on the QMSum dataset?", "explanation_reference": "The erroneous output fraction indicates how frequently the model produces an invalid sentence boundary position. For the QMSum dataset, the structured summarization models did not produce any erroneous segment boundary positions, indicating a high level of accuracy in generating valid sentence indices.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics. \\begin{table}[h] \\small \\centering \\s\\t \\renewcommand{\\arraystretch}{1.4} \\begin{tabular}{cccc} \\toprule Wiki-727K & en\\_city & en\\_disease & QMSum \\\\ \\hline 0.0001 & 0.0025 & 0 & 0 \\\\ \\bottomrule \\end{tabular} \\caption{Fraction of examples with at least one erroneous segment boundary position. This is for the structured summarization models, when tested on the respective test set.} \\label{table:sentpos_nonnumeric} \\end{table}"}
{"question": "Consider the paper that introduces the model that demonstrates the lowest accuracy in the SLOG-all dataset. What specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis, as provided in the attention visualizations section?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model demonstrates the lowest accuracy in SLGO-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the evidence provided in the attention visualizations section, what specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis?", "explanation_reference": "The authors present visual evidence showing that attention head 5 in layer 5 of 6 is involved in anaphora resolution, as demonstrated by the focused attention on the word 'its' and its relation to other parts of the sentence.", "evidence_reference": "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the benchmark that achieves a 'Generation Token Length' near 400 when the 'Compression Ratio' is 1. What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to the model proposed in the paper and answer-only prompting?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which benchmark gets 'Generation Token Length' near 400 when 'Compression Ratio' is 1?", "answer_anchor": "BBH", "question_reference": "What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to answer-only prompting?", "explanation_reference": "The paper suggests that the few-shot evaluation of PaLM 540B with answer-only prompting, which includes both a task instruction and answer options, demonstrates the effect of including these elements in the prompt. This approach outperforms the average human-rater on 6 out of 23 BBH tasks and is overall 1.4% better than the BIG-Bench reported result, indicating that the original setup underestimated language model performance.", "evidence_reference": "The few-shot evaluation of PaLM 540B  with answer-only prompting in this paper, however, outperforms the average human-rater on 6 out of 23 \\bbh{} tasks and is overall 1.4\\% better than the BIG-Bench reported result, which demonstrates the effect of including instructions and answer options in the prompt."}
{"question": "Consider the paper that introduces the method that has 638K tunable parameters. What is the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) used in the experiments?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has 638K tunable parameters?", "answer_anchor": "Hyperformer", "question_reference": "What is the dimension of the task feature embedding (\\bm{z_{\\tau}}) used in the experiments?", "explanation_reference": "The dimension of the task feature embedding (\\bm{z_{\\tau}}) is specified in the Experimental Details section under Hyperparameters, indicating the specific size used for the experiments.", "evidence_reference": "We set the dimension of the task feature embedding ($\\bm{z_{\\tau}}$) to $t'=512$"}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the green color. What specific method does it utilize to ensure that only tokens with high probability are generated, based on their likelihood as determined by both experts and anti-experts?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is method in the figure represented by teh green color?", "answer_anchor": "DExperts", "question_reference": "What specific method does DExperts utilize to ensure that only tokens with high probability are generated, based on their likelihood as determined by both experts and anti-experts?", "explanation_reference": "The method described for controlling the attributes of generated text involves combining pretrained language models with expert and anti-expert LMs in a product of experts. This ensures that tokens only receive high probability if they are deemed likely by the experts and unlikely by the anti-experts, thus directly answering the question.", "evidence_reference": "Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts."}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 63.8% in the StrategyQA dataset. What specific modification to the few-shot prompts used in the generation process is highlighted as a key factor for improving the quality of generated data?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets 63.8 accuracy in StrategyQA dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as a key factor for improving the quality of generated data?", "explanation_reference": "The paper specifies that making a key modification to the few-shot prompts by providing the target answer after the question and before the example CoTs allows LLMs to correct small mistakes in the CoT, which is crucial for generating high-quality data for knowledge distillation.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the dataset that corresponds to the second chart from the left. What specific adjustment was made to the Question Entity Linking task in the model proposed by the paper to address the challenge of achieving agreement on question entities among MTurk workers?", "answer": "", "figure": "locality/2310.12836/ratio_figure.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which dataset lies on the second left in the figure?", "answer_anchor": "Mintaka", "question_reference": "What specific adjustment was made to the Question Entity Linking task to address the challenge of achieving agreement on question entities among MTurk workers?", "explanation_reference": "The adjustment was made due to the initial difficulty in achieving agreement among MTurk workers on the entities within the questions. By modifying the task to focus on verifying a span and then linking it to Wikidata, the process was streamlined and made more manageable, leading to improved agreement rates.", "evidence_reference": "Since early test runs showed it would be difficult to get agreement on question entities, we modified the task so workers only verified a span and linked the entity in Wikidata."}
{"question": "Consider the paper that introduces the method that has an F1 score of 41.3. What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having 41.3 F1 score?", "answer_anchor": "SPADE", "question_reference": "What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically mentioned as a method applied by SPADE to prevent loops and token redundancy in parses. It functions by iteratively trimming tail-sharing-edges and generating new edges until the process becomes self-consistent, with a maximum iteration limit set to 20.", "evidence_reference": "Based on this property, we apply the following simple yet powerful tail collision avoidance algorithm: (1) at each tail node having multiple incoming edges, all edges are trimmed except the one with the highest linking probability; (2) at each head node of the trimmed edges, the new tail node is found by drawing the next probable edge whose probability is larger than $p_{th}$ and belongs to the top three; (3) go back to Step 1 and repeat the routine until the process becomes self-consistent or the max iteration limit is reached (set to 20 in this paper). The algorithm prevents loops and token redundancy in parses."}
{"question": "Consider the paper that introduces the last method shown in the Explicit --> Retrieval-enhanced --> Multi-Stage category. What specific technique from the paper demonstrates the effectiveness of the model proposed in the paper, Knowledge Solver, in improving LLMs' performance on knowledge-required tasks without necessitating additional training or modules?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2309.03118", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "Knowledge Solver", "question_reference": "What specific technique from the paper demonstrates the effectiveness of KSL in improving LLMs' performance on knowledge-required tasks without necessitating additional training or modules?", "explanation_reference": "The effectiveness of KSL in enhancing LLMs' performance on knowledge-required tasks without the need for additional training or modules is attributed to the interactive inference method. This method explicitly injects knowledge into LLMs and guides them to solve tasks, showcasing the primary source of performance improvement.", "evidence_reference": "Our interactive inference method can not only explicitly inject knowledge into LLMs but also guide LLMs to solve tasks. We also demonstrate that performance improvement majorly comes from our specially designed inference method (for zero-shot) and task (for finetuning) instead of instruction tuning."}
{"question": "Consider the paper that introduces the supervised method that results in the lowest score in 10-shot prompting. What specific advantage does the model's dynamic masking offer over static masking in the context of BERT pretraining according to the paper?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which supervised method demonstrates lowest scores in 10-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific advantage does dynamic masking offer over static masking in the context of BERT pretraining according to the paper?", "explanation_reference": "The paper mentions that given the results of comparing static and dynamic masking, and the additional efficiency benefits of dynamic masking, they use dynamic masking in the remainder of the experiments. This implies that beyond performance metrics, dynamic masking offers efficiency improvements which are considered advantageous in the pretraining process.", "evidence_reference": "Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments."}
{"question": "Consider the paper that introduces the supervised method that achieves the highest score in 100-shot prompting. What specific advantage does the model proposed in the paper offer through dynamic masking over static masking in the context of BERT pretraining according to the paper?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which supervised method demonstrates highest scores in 100-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific advantage does dynamic masking offer over static masking in the context of BERT pretraining according to the paper?", "explanation_reference": "The paper mentions that given the results of comparing static and dynamic masking, and the additional efficiency benefits of dynamic masking, they use dynamic masking in the remainder of the experiments. This implies that beyond performance metrics, dynamic masking offers efficiency improvements which are considered advantageous in the pretraining process.", "evidence_reference": "Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments."}
{"question": "Consider the paper that introduces the method that corresponds to the brown color label in the figure. What specific strategy does the paper propose for ensuring the appearance of guide words in generated text without requiring pre-defined ordering of constraints?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method demonstrated in the figure with brown color label?", "answer_anchor": "PPLM", "question_reference": "What specific strategy does the paper propose for ensuring the appearance of guide words in generated text without requiring pre-defined ordering of constraints?", "explanation_reference": "The paper proposes two strategies for controlling generation towards a list of guide words, one of which is 'Guide Closest'. This strategy does not require the guide words to be ordered and works by shifting the score function by the highest cosine similarity across all words in the set of guide words that have not appeared before the current step. This approach allows for the flexible inclusion of guide words in the generated text, aligning with the paper's goal of imposing hard constraints on language generation without the need for pre-defined ordering.", "evidence_reference": "Given a list of guide words $W$, we propose two approaches for both the case where we need words $w_n$ to appear in a fixed order as well as when any order suffices. [...] At any given decoding step, we shift the score function by the highest cosine similarity across all words $w\\in W_{t}$. [...] Explicitly, we score $y_{t}$ as \\begin{align} \\score'(y_{t}, W_{t}\\mid \\yy_{<t}) = & \\,\\score(y_{t} \\mid \\yy_{<t}) \\ + \\\\ \\lambda \\cdot \\max &\\Big(0, \\underset{{w\\in W_t}}{\\max}\\, \\semdist(\\gamma(y_t), \\gamma(w)\\Big) \\nonumber \\end{align}"}
{"question": "Consider the paper that introduces the LLM model that has a test accuracy of 50.7. How does the model proposed in the paper compare to beam search in terms of performance on the AQuA task when using the UL2-20B model with 40 reasoning paths?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which LLM model shows test accuracy by 50.7?", "answer_anchor": "Self-Consistency", "question_reference": "How does self-consistency compare to beam search in terms of performance on the AQuA task when using the UL2-20B model with 40 reasoning paths?", "explanation_reference": "The comparison between self-consistency and beam search on the AQuA task using the UL2-20B model with 40 reasoning paths shows that self-consistency using sampling achieves higher accuracy than beam search, indicating that self-consistency's approach to generating diverse reasoning paths and aggregating the most consistent answer leads to better performance.", "evidence_reference": "Self-consistency using sampling & 19.7 \\scriptsize{$\\pm$ 2.5} & \\textbf{24.9 \\scriptsize{$\\pm$ 2.6}} & \\textbf{25.3 \\scriptsize{$\\pm$ 1.8}} & \\textbf{26.7 \\scriptsize{$\\pm$ 1.0}} & \\textbf{26.9 \\scriptsize{$\\pm$ 0.5}} \\\\ \\midrule \\multirow{3}{*}{AQuA} & Beam search decoding (top beam) & 23.6 & 19.3 & 16.1 & 15.0 &10.2"}
{"question": "Consider the paper that introduces the dataset in the Movies domain only that has a PF field. What specific methodological approach does the generation-based model proposed in the paper employ to mimic human knowledge selection in dialogue response generation?", "answer": "", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which dataset is in Movies domain only and has PF field?", "answer_anchor": "DuConv", "question_reference": "What specific methodological approach does the generation-based model employ to mimic human knowledge selection in dialogue response generation?", "explanation_reference": "The generation-based model uses a methodological approach of minimizing the Kullback-Leibler Divergence (KLDivLoss) between two distributions: the prior distribution (knowledge reasoned by machines) and the posterior distribution (knowledge reasoned by humans). This approach is employed to force the model to mimic human knowledge selection during dialogue response generation.", "evidence_reference": "we introduce two different distributions: 1) the \\emph{prior distribution} $p(k_i | x)$ and the \\emph{posterior distribution} $p(k_i | x, y)$. We take the prior distribution $p(k_i | x)$ as the knowledge reasoned by machines and the posterior distribution $p(k_i | x, y)$ as the knowledge reasoned by humans, and then force the machine to mimic human by minimizing the KLDivLoss between those two distributions"}
{"question": "Consider the paper that introduces the model shown in the figure that corresponds to the green line. What is the Pearson's r correlation coefficient between word overlap and its performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/accuracy_figure.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model is shown in green line?", "answer_anchor": "DPT", "question_reference": "Based on the findings, what is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data indicates the strength of the linear relationship between the vocabulary overlap across time periods and the performance of the model on this specific task. A value close to 1 suggests a strong positive correlation.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the model shown in the first row of the table. What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for tasks involving this model?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "GWLAN", "question_reference": "What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "explanation_reference": "The advantage is highlighted by the fact that even though there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment. This suggests that the joint model's ability to handle multiple related tasks with a single model architecture simplifies the deployment process compared to having separate models for each context type.", "evidence_reference": "Compared with \\textsc{WPM-Sep}, \\textsc{WPM-Joint} shows two advantages. On one hand, even there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment."}
{"question": "Consider the paper that introduces the method in the table that is listed right above One-Round Distillation and right below Specialization. What key modification to the few-shot prompts did the authors make to improve the quality of chain of thought reasoning generated by the model proposed in the paper?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method in the table is listed right above One-Round Distillation and right below Specializing?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What key modification to the few-shot prompts did the authors make to improve the quality of chain of thought reasoning generated by the teacher model?", "explanation_reference": "The key modification mentioned in the paper aimed to improve the quality of the generated chain of thought by guiding the teacher model with the target answer, which helps in correcting small mistakes in the CoT reasoning.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the method that consistently achieves a higher MRR score than NodePiece. How is the initial representation for non-reserved entities generated before being input into the GNN in the model's entity-agnostic encoding process proposed by the paper?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method consistently shows higher MRR than NodePiece?", "answer_anchor": "EARL", "question_reference": "In the EARL model's entity-agnostic encoding process, how is the initial representation for non-reserved entities generated before being input into the GNN?", "explanation_reference": "The initial representation for non-reserved entities in the EARL model is generated by first encoding the ConRel and $k$NResEnt information separately and then combining these two encoded pieces of information. This combination is achieved through a concatenation followed by a transformation using a 2-layer MLP, as described in the Entity-Agnostic Encoding section.", "evidence_reference": "In our GNN framework, similar to previous works \\cite{CompGCN, MaKEr}, we use a linear transformation on the concatenation of entity and relation representations to aggregate the neighbor information. Specifically, the message aggregation for the entity $e$ is: \\begin{equation} \\begin{aligned} \\mathbf{m}_{e}^{l} = \\sum_{(r, t) \\in \\mathcal{O}(e)} \\mathbf{W}_{\\text{out}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_t] + \\sum_{(r,h) \\in \\mathcal{I}(e)} \\mathbf{W}_{\\text{in}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_h], \\label{eq:gnn-agg} \\end{aligned} \\end{equation} where $\\mathcal{O}(e)$ denotes the out-going relation-entity pair set of $e$ and $\\mathcal{I}(e)$ denotes the in-going relation-entity pair set. $\\mathbf{W}_{\\text{out}}^l$ and $\\mathbf{W}_{\\text{in}}^l$ are transformation matrices for out-going and in-going pairs. $l \\in [0, \\dots, L]$ denotes the layer of GNN and $L$ is the total number of GNN layers. The input entity representations are calculated in Equation (\\ref{eq:info-combine}), and the input relation representations (e.g., $\\mathbf{h}_{r}^{0}$) are looked up in a trainable relation embedding matrix $\\mathbf{R} \\in \\mathbb{R}^{|\\mathcal{R}|\\times d}$.  The entity representation of $e$ in the GNN is updated as follows: \\begin{equation} \\mathbf{h}_{e}^{l+1} = \\sigma \\left( \\frac{1}{c}\\mathbf{m}_{e}^{l} + \\mathbf{W}_{\\text{self}}^{l} \\mathbf{h}_{e}^{l} \\right), \\label{eq:gnn-update} \\end{equation} where $c=|\\mathcal{I}(e)+\\mathcal{O}(e)|$ is a normalization constant. $\\mathbf{W}_{\\rm self}^{l}$ is a matrix for self representation update, and $\\sigma$ is an activation function. Furthermore, relation representations will also be updated in each layer: $\\mathbf{h}_{r}^{l+1} = \\sigma \\left( \\mathbf{W}_{\\text{rel}}^{l} \\mathbf{h}_{r}^{l} \\right)$. We use the output representations in the $L$-th layer for entities and relations as their embeddings to calculate scores next."}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that shows the highest Test Accuracy. What specific method does its solution discrimination module employ to generate negative solutions that are variants of the ground truth solution?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which Seq2Seq model shows the higest Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module employ to generate negative solutions that are variants of the ground truth solution?", "explanation_reference": "The solution discrimination module uses a gradient-based manipulation method to identify the most vulnerable (important) token in the ground truth solution and then finds all possible alternatives for this token to generate negative solutions. This method is chosen to ensure that the negative solutions are hard for the model to distinguish from the correct solution, thereby enhancing the model's ability to correctly associate problems with their ground truth solutions.", "evidence_reference": "Our goal is to find variants of the ground truth solution as hard negative samples, which only manipulate the most vulnerable (important) token. Therefore, borrowing the idea from white-box evasion attack, we regard the token with the largest gradient as the most important and vulnerable one: \\begin{equation} \\label{grad} y_i = \\mathop{argmax\\;}_{y_i \\in Y}(\\nabla Dis([en_x(X),en_y(Y)]). \\end{equation}"}
{"question": "Consider the paper that introduces the method represented by the blue line in the figure. Based on the ablation studies, which dataset showed a significant performance degradation when the model proposed in the paper removed multi-hop neighbor information?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method represented in the blue line from the figure?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which dataset showed a significant performance degradation when multi-hop neighbor information was removed?", "explanation_reference": "The ablation study results indicate that removing multi-hop neighbor information ('w/o MulHop') dramatically affected the performance on the WN18RR dataset, as evidenced by the significant drop in performance metrics compared to other ablation settings.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE L score equal to 41.39. What specific activation function is used in the non-linear transformation within the BoW Encoder of the neural topic model proposed in the paper?", "answer": "", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method in the table that demonstrates a ROUGE L score equal to 41.39?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific activation function is used in the non-linear transformation within the BoW Encoder of the neural topic model?", "explanation_reference": "The paper specifies that the non-linear transformation within the BoW Encoder uses a tanh activation function. This detail is crucial for understanding the specific architecture and functionality of the neural topic model described.", "evidence_reference": "the input $\\textbf{x}_{bow}$ is first encoded into a latent variable $\\mathbf{z}$ by a topic encoder. Each input is passed to obtain the prior mean $\\mu$ and prior standard deviation $\\sigma$ \\vspace{-2mm} \\begin{equation} \\pi = f_{MLP}(\\textbf{x}_{bow}), \\mu = f_1(\\pi), \\log \\sigma = f_2(\\pi) \\end{equation} where $f_{MLP}$ is a non-linear transformation with a $\\tanh$ activation function; $f_1$ and $f_2$ are two linear transformations with bias."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 84.70% WInToRe. What specific architectural feature allows this model to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model shows 84.70% WInToRe?", "answer_anchor": "GRIT", "question_reference": "What specific architectural feature of GRIT allows it to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "explanation_reference": "GRIT employs a Deformable DETR-based detector for extracting region features, which inherently does not require class-aware NMS (Non-Maximum Suppression) and RoI (Region of Interest) Align operations that are typically necessary in CNN-based detectors like Faster R-CNN. This architectural choice significantly reduces the computational time for feature extraction.", "evidence_reference": "On the other hand, we employ a Deformable DETR-based detector to extract region features without using all such operations. Table \\ref{tab:extraction} shows the comparison on feature extraction."}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in few-shot prompting. How does the temperature rescaling phenomenon observed in RLHF differ when responding to creative versus factual prompts?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shows the lowest execuation accuracy in few-shot prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "How does the temperature rescaling phenomenon observed in RLHF differ when responding to creative versus factual prompts?", "explanation_reference": "The temperature rescaling phenomenon observed in RLHF shows that for creative prompts, an increase in temperature continues to generate diversity across various RLHF iterations, as indicated by the Self-BLEU slope pattern comparable to that of the SFT model. However, for factual prompts, despite the rising temperature, the model learns to consistently provide the same response, as shown by the diminishing Self-BLEU slope over time. This indicates that RLHF dynamically adjusts the temperature based on the type of prompt, maintaining diversity for creative prompts while reducing it for factual ones.", "evidence_reference": "For instance, when it comes to prompts associated with creativity, such as ``Write a poem,'' an increase in temperature continues to generate diversity across our various RLHF iterations. This can be observed in the Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model. On the other hand, for prompts based on factual information, such as ``What is the capital of ?'' the Self-BLEU slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to consistently provide the same response to factual prompts."}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. Which specific example demonstrates a scenario where the spoken words are ambiguous, but the acoustic and visual modalities provide complementary evidence leading to a correct positive sentiment prediction by the model proposed in the paper?", "answer": "", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method on the first row of the table?", "answer_anchor": "TFN", "question_reference": "In the qualitative analysis section, which specific example demonstrates a scenario where the spoken words are ambiguous, but the acoustic and visual modalities provide complementary evidence leading to a correct positive sentiment prediction by the TFN model?", "explanation_reference": "The question focuses on a detailed part of the qualitative analysis where the TFN model's ability to integrate complementary evidence from acoustic and visual modalities, despite ambiguous spoken words, leads to a correct positive sentiment prediction. This is explicitly mentioned in the description of the second example in the qualitative analysis section.", "evidence_reference": "In the second example, the spoken words are ambiguous since the model has no clue what a B is except a token, but the acoustic and visual modalities are bringing complementary evidences. Our TFN approach correctly identify this trimodal interaction and predicts a positive sentiment."}
{"question": "Consider the paper that introduces the method that has the highest score in the WQ-R task. What specific aspect of the paraphrasing-based approach does the model proposed in the paper identify as potentially more effective for instances with simple expressions compared to their method?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the highest score in WQ R task?", "answer_anchor": "DSM", "question_reference": "What specific aspect of the paraphrasing-based approach does the paper identify as potentially more effective for instances with simple expressions compared to their method?", "explanation_reference": "The paper identifies that for instances with simple expressions, the paraphrasing-based approach may achieve better performance because it focuses on words, implying a more effective word-level diversity compared to their method which focuses more on the structure of the sentences.", "evidence_reference": "However, for instances with simple expressions, the paraphrasing-based method may achieve better performance...the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the method for which the BLEU-1 score is missing in the table. How does the model's performance, proposed by the paper, with concepts extracted from captions compare to using concepts derived from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method does not provide BLEU-1 score?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP with concepts extracted from captions compare to using concepts derived from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "explanation_reference": "The question focuses on comparing the effectiveness of different sources of semantic concepts (caption-extracted vs. object detector-derived) on the performance of the ViTCAP model, specifically in terms of CIDEr scores on the COCO-caption Karpathy split. The answer directly addresses this by providing the specific CIDEr scores achieved using each method, indicating that caption-extracted concepts result in better performance.", "evidence_reference": "FOCAL$_\\text{Tag}$ & $10$ & $35.2$ & $28.0$ & $57.0$ & $117.1$ & $21.4$\\\\ FOCAL$_\\text{Tag+Init}$ & $10$ & $36.0$ & $28.4$ & $57.5$ & $120.5$ & $22.0$\\\\ FOCAL$_\\text{Init}$ & $10$ & $35.0$ & $28.2$ & $57.1$ & $118.0$ & $21.6$\\\\ FOCAL$_\\text{Tag+Init}$ & $40$ & $35.9$ & $28.4$ & $57.6$ & $121.1$ & $22.1$ \\\\ PRED. Concepts & $36.1$ & $28.6$ & $57.6$ & $120.6$ & $21.7$"}
{"question": "Consider the paper that introduces the dataset in the last row of the 'Inconsistency Detection' category. Which model, according to the study's findings, demonstrated the highest percentage of factual hallucinations among the evaluated systems?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset in the last row of `Inconsistency Detection` category?", "answer_anchor": "XSumFaith", "question_reference": "Based on the study's findings, which model demonstrated the highest percentage of factual hallucinations among the evaluated systems?", "explanation_reference": "The question targets a specific detail regarding the performance of different models in producing factual hallucinations, which is a nuanced aspect of the paper's analysis on hallucinations in summarization. The \\bencdec model is identified as having the highest percentage of factual hallucinations, indicating its relative strength in integrating background knowledge to generate summaries that, while may contain hallucinated content, are factually accurate.", "evidence_reference": "In total, 34.7\\% of the \\bencdec abstracts were faithful (26.9\\%) and/or factual (+7.8\\%). This is 7.4\\% absolute better than the next-best model (\\ptgen). The number of unfaithful yet factual summaries for \\bencdec, 7.8\\%, was also the highest."}
{"question": "Consider the paper that introduces the large language model that achieves a lower HVI score than OPT but a higher HVI score than Alpaca. What specific methodological difference in the evaluation setup for the model's performance on the USABO and SAT reading/writing exams, as proposed in the paper, might have impacted its reported performance?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the large language model that demonstrates lower HVI score than OPT but higher HVI score than Alpaca?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "explanation_reference": "This methodological difference is significant because sampling at temperature 0 can lead to more deterministic outcomes based on the generated explanation, potentially affecting the model's performance on these exams in a way that differs from how choices were determined in other exams.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method in the figure that has a perplexity of approximately 30 and an average max toxicity of 0.2. What specific approach does the model proposed in the paper use to ensure that only tokens with high probability are selected during text generation, according to the logical coherence and evidential backing provided?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method in the figure has around 30 perplexity and 0.2 average max toxicity?", "answer_anchor": "DExperts", "question_reference": "What specific method does the DExperts model use to ensure that only tokens with high probability are selected during text generation, according to the logical coherence and evidential backing provided in the paper?", "explanation_reference": "The DExperts model uses a 'product of experts' approach to ensure that during text generation, tokens only receive high probability if they are considered likely by the experts and unlikely by the anti-experts. This method is critical for achieving controlled text generation by filtering out undesirable attributes while promoting desired ones.", "evidence_reference": "Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts."}
{"question": "Consider the paper that introduces the method, with an average max toxicity of more than 0.3, is represented by a circle. What is the success rate (SR) percentage for the Guide Context strategy with a lambda value of 20?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is method with average max toxicity more than 0.3 but with circle label?", "answer_anchor": "PPLM", "question_reference": "What is the success rate (SR) percentage for the Guide Context strategy with a lambda value of 20?", "explanation_reference": "The success rate (SR) for the Guide Context strategy with a lambda value of 20 is directly reported in the paper's results, indicating the effectiveness of this strategy at ensuring the appearance of guide words in the generated text.", "evidence_reference": "Section name: Hyperparameter Analysis_10\\nparagraphs: \\emph{C.} $\\lambda=20$ & \\textbf{95.1} \\rpm 2.3 & 99.3 \\rpm 20.1 & 13.4 \\rpm 2.1"}
{"question": "Consider the paper that introduces the dataset in which KALMV achieves a score of 70.83 for the XL model. What is the percentage of its questions that can be answered using a boolean response, and how does this compare to the percentage of questions answerable using a numerical response?", "answer": "", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset being tested that KALMV gets 70.83 score for XL model?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka's questions that can be answered using a boolean response, and how does this compare to the percentage of questions answerable using a numerical response?", "explanation_reference": "The paper provides specific statistics on the types of answers in the Mintaka dataset, including the percentages of questions that can be answered with different types of responses. The comparison between boolean and numerical response types directly answers the question.", "evidence_reference": "A majority (72\\%) of the questions in Mintaka can be answered using an entity. 14\\% can be answered using a boolean, in yes/no or comparative questions. 7\\% can be answered using a number, such as someone\u2019s age."}
{"question": "Consider the paper that introduces the method that achieves the highest Hits@1 score in the MQA-1H dataset. What is the core component of the model proposed in the paper that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs)?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets the highest Hits@1 score in MQA-1H dataset?", "answer_anchor": "UniKGQA", "question_reference": "What is the core component of UniKGQA that enables the propagation of matching information along the directed edges on KGs?", "explanation_reference": "The core component that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs) in UniKGQA is the matching information propagation module. This module is crucial for the model's ability to effectively navigate and reason over the KG by leveraging the semantic relationships and structure within the graph.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method that scores a 69.56 in the Forgotten Realms category. How does the model proposed in the paper leverage the generated synthetic data to improve the quality of few-shot entity linking?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method got 69.56 score in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model leverage the generated synthetic data to improve the quality of few-shot entity linking?", "explanation_reference": "The MetaBLINK model improves the quality of few-shot entity linking by adopting a meta-learning mechanism that automatically assigns different weights to each synthetic data instance. This approach allows the model to differentiate the quality of synthetic data for more effective training.", "evidence_reference": "To further differentiate the quality of each synthetic data instance for model training, we design a meta-learning mechanism that can automatically assign different weights to the synthetic data."}
{"question": "Consider the paper that introduces the method that is represented by the square marker. What is the relationship between the average attention span and the variance loss $L_{var}$ for the method, as proposed in the paper?", "answer": "", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is demonstrated by square marker?", "answer_anchor": "MMA", "question_reference": "What is the relationship between the average attention span and the variance loss $L_{var}$ as demonstrated in the paper?", "explanation_reference": "The paper explicitly states that the average attention span decreases with an increase in the variance loss $L_{var}$, indicating a direct relationship between these two variables.", "evidence_reference": "We show the relation between the average attention span (averaged over the IWSLT and WMT test sets) versus $L_{var}$ in \\autoref{fig:attention-span}. As expected, the average attention span is reduced as we increase $L_{var}$."}
{"question": "Consider the paper that introduces the model in the figure that has a more negative Spearman's Correlation than 0.60 when the alternative set size is set to 100 in the Mean Cosine setting. What specific methodological adjustment did the authors make to the initialization scheme of the model proposed in the paper to account for its depth?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model in the figure exceed 0.60 Spearman's Correlation as the alternative set size to 100 in Mean cosine setting?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological adjustment did the authors make to the initialization scheme of the DialoGPT model to account for its model depth?", "explanation_reference": "The authors mention modifying the initialization scheme to account for model depth as part of their methodological adjustments to the GPT-2 architecture for the DialoGPT model. This detail is a specific methodological choice aimed at improving the model's performance, likely by ensuring stability in deeper network layers.", "evidence_reference": "Our model inherits from GPT-2 \\cite{gpt2}, a 12-to-48 layer transformer with layer normalization, a initialization scheme that accounts for model depth that we modified, and byte pair encodings \\cite{bpe} for the tokenizer."}
{"question": "Consider the paper that introduces the model that scores an 81.5 in the SRL task. What specific aspect of the unified pre-training framework of AMRBART contributes to its effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR parsing and generation tasks?", "answer": "", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets score of 81.5 in SRL task?", "answer_anchor": "AMRBART", "question_reference": "What specific aspect of the unified pre-training framework contributes to its effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR parsing and generation tasks?", "explanation_reference": "The unified pre-training framework's effectiveness in reducing the gap between pre-training and fine-tuning phases is attributed to the implementation of a dynamic masking rate, which adjusts the masking probability as training progresses. This approach makes the pre-training tasks gradually closer to the fine-tuning tasks, facilitating a smoother transition and better knowledge transfer from pre-training to fine-tuning.", "evidence_reference": "Different from standard masking that uses a static masking rate, we adopt a dynamic masking rate $p$ for task $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g}. Formally, at step $t$, we calculate the masking probability $p$ as:  $p = 0.1 + 0.75 * t/T$, where $0.1$ is the initial masking rate, $T$ denotes the total training step. $p$ increases as $t$ grows, as $t$ approaches to $T$, the pre-training tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} are closer to fine-tuning tasks."}
{"question": "Consider the paper that introduces the model that corresponds to an F1 score of 65.76 on PDTB-Top. What is the Dice similarity score between Instances (1) and (3) in Figure 1 as proposed by the paper, and how is it calculated?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method demonstrates 65.76 F1 score on PDTB-Top?", "answer_anchor": "GOLF", "question_reference": "What is the Dice similarity score between Instances (1) and (3) in Figure 1, and how is it calculated?", "explanation_reference": "The Dice similarity score between Instances (1) and (3) is calculated based on their sense label sequences across three hierarchical levels (Top, Second, and Connective), considering six sub-paths in the hierarchies. The calculation involves the Dice coefficient for each sub-path among the hierarchical levels and takes the average as the similarity score. The paper provides a detailed example calculation leading to a score of approximately 0.7.", "evidence_reference": "Taking Instances (1) and (3) in Figure \\ref{fig: example1} as examples, their label sequences are \\emph{Top: Comparison, Sec: Contrast, Conn: but} and \\emph{Top: Comparison, Sec: Contrast, Conn: however}, respectively. Then the similarity score would be $\\frac{1}{6}(\\frac{2\\times 1}{1+1} + \\frac{2\\times 1}{1+1} + \\frac{2\\times 0}{1+1} + \\frac{2\\times 2}{2+2} + \\frac{2\\times 1}{2+2} + \\frac{2\\times 2}{3+3})\\approx0.7$."}
{"question": "Consider the paper that introduces the model that achieves a P_k score of 24.8 in the en_disease category. What is the erroneous output fraction for structured summarization models, specifically the model proposed in the paper, when tested on the QMSum dataset?", "answer": "", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets 24.8 P_k score in en_disease category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What is the erroneous output fraction for structured summarization models when tested on the QMSum dataset?", "explanation_reference": "The erroneous output fraction indicates how frequently the model produces an invalid sentence boundary position. For the QMSum dataset, the structured summarization models did not produce any erroneous segment boundary positions, indicating a high level of accuracy in generating valid sentence indices.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics. \\begin{table}[h] \\small \\centering \\s\\t \\renewcommand{\\arraystretch}{1.4} \\begin{tabular}{cccc} \\toprule Wiki-727K & en\\_city & en\\_disease & QMSum \\\\ \\hline 0.0001 & 0.0025 & 0 & 0 \\\\ \\bottomrule \\end{tabular} \\caption{Fraction of examples with at least one erroneous segment boundary position. This is for the structured summarization models, when tested on the respective test set.} \\label{table:sentpos_nonnumeric} \\end{table}"}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in few-shot prompting. What specific preprocessing steps were applied to the XML files during the data curation process for its base version?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shows the second best execuation accuracy in few-shot prompting?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing steps were applied to the XML files during the data curation process for StarCoderBase?", "explanation_reference": "The question focuses on the specific method used to preprocess XML files during the data curation process for StarCoderBase. The answer directly addresses this by specifying the implementation of a simple XML filter, which is a detailed and specific part of the preprocessing steps mentioned in the paper.", "evidence_reference": "As we inspected the data, we noticed that certain extensions often consisted of XML files. For example, the .sld extension had more than 50% of its files in XML format. To address this, we implemented a simple XML filter that checked for the presence of '<?xml version=' within the first 100 characters of the file."}
{"question": "Consider the paper that introduces the method that corresponds to the fifth row of the table. What is the recommended selection range of \\(k\\) and \\(\\alpha\\) for contrastive search based on the ablation study conducted in the paper?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the fifth row of the table?", "answer_anchor": "SimCTG", "question_reference": "What is the recommended selection range of \\(k\\) and \\(\\alpha\\) for contrastive search based on the ablation study?", "explanation_reference": "The recommended selection range for \\(k\\) and \\(\\alpha\\) is based on the ablation study's findings that these settings produce results more similar to human-written texts as judged by generation diversity and generation perplexity.", "evidence_reference": "In practice, our recommended selection range of \\(k\\) and \\(\\alpha\\) are \\(k\\in [5,10]\\) and \\(\\alpha\\in[0.5,0.8]\\), as these settings produce results that are more similar to human-written texts as judged by generation diversity and generation perplexity."}
{"question": "Consider the paper that introduces the dataset which has the fewest number of languages but the most number of SM tasks. What specific aspect of its benchmark design, known as SentiEval, aims to address the challenge of prompt sensitivity in evaluating LLMs' sentiment analysis capabilities?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset has the fewest number of languages but the most number of SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific aspect of the \\textsc{SentiEval} benchmark design aims to address the challenge of prompt sensitivity in evaluating LLMs' sentiment analysis capabilities?", "explanation_reference": "The \\textsc{SentiEval} benchmark is designed to address the challenge of prompt sensitivity by incorporating diverse but fixed instructions for evaluating LLMs. This approach aims to make performance comparisons more stable and reliable across different LLMs and studies, by reducing the variability and bias associated with prompt design.", "evidence_reference": "The main idea of \\textsc{SentiEval} is to: 1) break the boundary between individual sentiment analysis tasks to establish a unified testing benchmark, providing a more comprehensive assessment of a model's sentiment analysis proficiency, rather than emphasizing on specific aspects; 2) test the model using natural language instructions presented in various styles. This mimics the real use case when humans interact with the model with natural languages for solving SA tasks, instead of purely learning text-label mapping; 3) equip the benchmark with diverse but fixed instructions, making performance comparisons more stable and reliable across different LLMs and studies."}
{"question": "Consider the paper that introduces the model that has a Recall@7 score of 92.97 for the MWOZ task. What is the Entity F1 score improvement of the model proposed in the paper, specifically the Q-TOD (T5-3B) version, with oracle knowledge on the SMD dataset compared to its performance with a fine-tuned retriever?", "answer": "", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has the 92.97 score in Recall@7 for MWOZ task?", "answer_anchor": "Q-TOD", "question_reference": "What is the Entity F1 score improvement of Q-TOD (T5-3B) with oracle knowledge on the SMD dataset compared to its performance with a fine-tuned retriever?", "explanation_reference": "The improvement can be calculated from the Entity F1 scores provided for Q-TOD (T5-3B) with a fine-tuned retriever and with oracle knowledge. The Entity F1 score with a fine-tuned retriever is 74.96%, and with oracle knowledge, it is 76.20%. The improvement is the difference between these two scores.", "evidence_reference": "~~Q-TOD (T5-3B) & ~~~~~73.44~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~74.96 (+1.52)~~ \\\\ \\quad ~~w/ oracle knowledge & ~~~~~76.20 (\\textbf{+2.76})~~ \\\\"}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 76.3 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What is the specific range of search space for \\(\\Delta_{\\text{R1}}^s\\) during post-softmax quantization as mentioned in the experiment settings of the model proposed in the paper?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the quant method show 76.3 score on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific range of search space for \\(\\Delta_{\\text{R1}}^s\\) during post-softmax quantization as mentioned in the experiment settings?", "explanation_reference": "The question focuses on a detailed aspect of the experiment settings related to the quantization process, specifically the search space for the scaling factor \\(\\Delta_{\\text{R1}}^s\\) used in post-softmax quantization. This detail is crucial for understanding how the quantization parameters are optimized.", "evidence_reference": "For post-softmax quantization, the search space of \\(\\Delta_{\\text{R1}}^s\\) is \\([\\frac{1}{2^{k}},\\frac{1}{2^{k+1}},...,\\frac{1}{2^{k+10}}]\\)."}
{"question": "Consider the paper that introduces the LLM shown in the figure with a model size of 1.7T. What specific methodological difference in the evaluation setup for its performance on the USABO and SAT reading/writing exams, compared to other exams, might have impacted its reported performance?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the large language model that has 1.7T model size?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "explanation_reference": "This methodological difference is significant because sampling at temperature 0 can lead to more deterministic outcomes based on the generated explanation, potentially affecting the model's performance on these exams in a way that differs from how choices were determined in other exams.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method that achieves an average EA score of 67.07 in the FinQA task. How does the order of in-context examples affect the model's performance on the NQ dataset, and what is the observed trend regarding the default and reverse orders?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets 67.07 EA score in FinQA task", "answer_anchor": "KATE", "question_reference": "How does the order of in-context examples affect the performance of KATE on the NQ dataset, and what is the observed trend regarding the default and reverse orders?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results on the NQ dataset showed that the reverse order, where the most similar sentences are placed closer to the test example, performed the best. This suggests that the proximity of similar sentences to the test example in the input sequence may help GPT-3 leverage the corresponding information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the method that results in a score of 14.9 in the Unseen, Test, GC dataset. What specific method does the model proposed in the paper use to decouple the understanding of visual appearance from the variations in natural language instructions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the score of 14.9 in Unseen, Test, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer use to decouple the understanding of visual appearance from the variations in natural language instructions?", "explanation_reference": "The question focuses on a detailed aspect of the methodology used in the paper, specifically how the Episodic Transformer addresses the challenge of understanding complex human instructions in relation to visual appearances. The answer directly addresses this by mentioning the use of synthetic instructions as an intermediate representation, which is a methodological choice made to improve training by decoupling the understanding of visual appearance from the natural language instruction variations.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the model that achieves the highest score in the 'T2' column. What hyperparameter values were used for the FewRel dataset in its experiments?", "answer": "", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model demonstrates the highest score in 'T2' column?", "answer_anchor": "CEAR", "question_reference": "What hyperparameter values were used for the FewRel dataset in the experiments?", "explanation_reference": "The answer directly lists the specific hyperparameter values used for the FewRel dataset as mentioned in the Implementation Details section of the paper, providing precise and concise information.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the method that corresponds to a higher F1 score than that of LDSGM but a lower F1 score than 65.76 for PDTB-Top. What is the model's, proposed by the paper, Macro-F1 score improvement without the segment token </s> over the LDSGM model for the second-level sense 'Exp.List' in PDTB 2.0?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which is the method has a higher F1 score than LDSGM but lower F1 score than 65.76", "answer_anchor": "PCP", "question_reference": "What is the Macro-F1 score improvement of PCP-large without the segment token </s> over the LDSGM model for the second-level sense 'Exp.List' in PDTB 2.0?", "explanation_reference": "The improvement can be calculated from the Macro-F1 scores provided for the second-level sense 'Exp.List' in PDTB 2.0. LDSGM has a Macro-F1 score of 8.98, and PCP-large without the segment token </s> achieved a Macro-F1 score of 37.50. The improvement is the difference between these two scores. 44.04-40.49=3.55", "evidence_reference": "Exp.List            & 0.0           & \\underline{8.98}  & 29.63  & \\textbf{37.50}"}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What is the effect of the size of re-ranked candidate tokens on the model's control performance and text fluency according to the methodology?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "Discup", "question_reference": "What is the effect of the size of re-ranked candidate tokens on the model's control performance and text fluency according to the DisCup methodology?", "explanation_reference": "The paper discusses how the control performance of the DisCup method is proportional to the size of re-ranked candidate tokens, indicating that a deeper sampling scope (larger size of candidate tokens) results in better control performance. However, it also mentions that this comes at the cost of decreased text fluency, as more selected tokens in low-probability regions lead to a decrease in fluency.", "evidence_reference": "The control performance of our method is proportional to the size of candidate tokens $\\mathcal{C}$. As shown in Figure~\\ref{top_k}, the deeper the sampling scope is, the better the control performance will be, but meanwhile the PPL deteriorates."}
{"question": "Consider the paper that introduces the method that is in the first block and has an F1 score of 50.4. Why might the use of Locally Linear Embedding (LLE) for regularizing the space of label prototypes be considered less appropriate compared to the model's, proposed by the paper, clustering-based strategy?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is in the first block while having 50.4 F1 score?", "answer_anchor": "ConCN clusters", "question_reference": "In the context of the paper's methodological approach, why might the use of Locally Linear Embedding (LLE) for regularizing the space of label prototypes be considered less appropriate compared to the proposed clustering-based strategy?", "explanation_reference": "The paper critiques the use of LLE for regularizing the space of label prototypes because it enforces a similarity condition on prototypes based on label similarity, which does not align with the paper's observation that similar labels do not always imply positive correlation or similarity in the context of ultra-fine entity typing. This critique highlights the limitation of LLE in handling the nuanced relationships between labels in UFET, making the proposed clustering-based strategy more suitable as it does not require prototypes of similar labels to be similar.", "evidence_reference": "While adding LLE to the base UFET model has a small positive effect, we can see that the performance is much worse than that of our proposed clustering based strategy (DL). The two post-processing techniques (missing and CN) both have a positive effect on the overall performance, although they have a smaller impact than the DL strategy."}
{"question": "Consider the paper that introduces the model in the figure that has a more negative Spearman's Correlation than 0.60 when the alternative set size is set to 100 in the Mean Cosine setting. Which variant of DialoGPT was statistically indistinguishable from human responses in terms of relevance based on the human evaluation results?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model in the figure exceed 0.60 Spearman's Correlation as the alternative set size to 100 in Mean cosine setting?", "answer_anchor": "DialoGPT Large", "question_reference": "Based on the human evaluation results, which variant of DialoGPT was statistically indistinguishable from human responses in terms of relevance?", "explanation_reference": "The statistical significance testing for human evaluation showed that the differences between the 345M model and human responses in terms of relevance were not statistically significant, indicating that the 345M variant of DialoGPT was indistinguishable from human responses in this aspect.", "evidence_reference": "The differences between 345M model (2) and human response (1) are not statistically significant."}
{"question": "Consider the paper that introduces the method that has a perplexity of approximately 30 and an average max toxicity of around 0.4. What is the success rate (SR) percentage of the model proposed in the paper when using the Guide Context strategy with a lambda value of 20?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is method with around 30 perplexity and around 0.4 average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What is the success rate (SR) percentage for the Guide Context strategy with a lambda value of 20?", "explanation_reference": "The success rate (SR) for the Guide Context strategy with a lambda value of 20 is directly reported in the paper's results, indicating the effectiveness of this strategy at ensuring the appearance of guide words in the generated text.", "evidence_reference": "Section name: Hyperparameter Analysis_10\\nparagraphs: \\emph{C.} $\\lambda=20$ & \\textbf{95.1} \\rpm 2.3 & 99.3 \\rpm 20.1 & 13.4 \\rpm 2.1"}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in direct prompting. How does the temperature rescaling phenomenon observed in RLHF differ when responding to creative versus factual prompts?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shows the lowest execuation accuracy in direct prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "How does the temperature rescaling phenomenon observed in RLHF differ when responding to creative versus factual prompts?", "explanation_reference": "The temperature rescaling phenomenon observed in RLHF shows that for creative prompts, an increase in temperature continues to generate diversity across various RLHF iterations, as indicated by the Self-BLEU slope pattern comparable to that of the SFT model. However, for factual prompts, despite the rising temperature, the model learns to consistently provide the same response, as shown by the diminishing Self-BLEU slope over time. This indicates that RLHF dynamically adjusts the temperature based on the type of prompt, maintaining diversity for creative prompts while reducing it for factual ones.", "evidence_reference": "For instance, when it comes to prompts associated with creativity, such as ``Write a poem,'' an increase in temperature continues to generate diversity across our various RLHF iterations. This can be observed in the Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model. On the other hand, for prompts based on factual information, such as ``What is the capital of ?'' the Self-BLEU slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to consistently provide the same response to factual prompts."}
{"question": "Consider the paper that introduces the model in the figure that has the lowest diversity score for each of p1, p2, p3, and p4. What specific architectural change in its Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "answer": "", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model demonstrates the lowest diversity score in p1, p2, p3, and p4?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change in the Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "explanation_reference": "The paper found that sharing parameters across the encoder and decoder performed nearly as well as not sharing them, without a substantial drop in performance. This approach effectively halves the total number of parameters in the model.", "evidence_reference": "We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than Doc2Graph and a higher F1 score than GNN+MLP. What specific algorithm does the model proposed in the paper apply to prevent loops and token redundancy in parses, and how does it function?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having lower F1 score than Doc2Graph and higher F1 score than GNN+MLP?", "answer_anchor": "SPADE", "question_reference": "What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically mentioned as a method applied by SPADE to prevent loops and token redundancy in parses. It functions by iteratively trimming tail-sharing-edges and generating new edges until the process becomes self-consistent, with a maximum iteration limit set to 20.", "evidence_reference": "Based on this property, we apply the following simple yet powerful tail collision avoidance algorithm: (1) at each tail node having multiple incoming edges, all edges are trimmed except the one with the highest linking probability; (2) at each head node of the trimmed edges, the new tail node is found by drawing the next probable edge whose probability is larger than $p_{th}$ and belongs to the top three; (3) go back to Step 1 and repeat the routine until the process becomes self-consistent or the max iteration limit is reached (set to 20 in this paper). The algorithm prevents loops and token redundancy in parses."}
{"question": "Consider the paper that introduces the method that has a lower J_k score than Random Entity Quantization but a higher J_k score than NodePiece in the FB15k-237 dataset for all values of k between 400 and 1000. What is the relative increase in MRR achieved by the model proposed in the paper compared to RotatE on the same dataset using a similar parameter budget?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method shows the lower J_k score than Random Entity Quantization but higher J_k score than NodePiece for k in the range of [400, 1000] in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "What is the relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "explanation_reference": "The relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset is mentioned in the section summarizing the main results, where it states that EARL uses only 62% parameters and obtains a relative increase of 4.7% on MRR in comparison with RotatE.", "evidence_reference": "Specifically, on FB15k-237, \\model~uses only 62\\% parameters and obtains a relative increase of 4.7\\% on MRR in comparison with RotatE."}
{"question": "Consider the paper that introduces the method in the figure that has a perplexity of approximately 30 and an average max toxicity of 0.2. What specific methodological approach does the model proposed in the paper utilize to ensure that generated text aligns with desired attributes while leveraging the capabilities of both expert and anti-expert language models?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method in the figure has around 30 perplexity and 0.2 average max toxicity?", "answer_anchor": "DExperts", "question_reference": "What specific methodological approach does DExperts utilize to ensure that generated text aligns with desired attributes while leveraging the capabilities of both expert and anti-expert language models?", "explanation_reference": "The methodological approach utilized by DExperts to align generated text with desired attributes involves combining a pretrained language model with expert and anti-expert language models in a product of experts. This ensures that tokens only get high probability if they are considered likely by the experts and unlikely by the anti-experts, effectively steering the generation process.", "evidence_reference": "Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage category. How does the success rate of targeted in-context knowledge updating on FEVER change when incorporating a mixture of original examples, edited relevant examples, and edited irrelevant examples in the prompt compared to using only original examples?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2210.09150", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage?", "answer_anchor": "IC-Retrieval", "question_reference": "How does the success rate of targeted in-context knowledge updating on FEVER change when incorporating a mixture of original examples, edited relevant examples, and edited irrelevant examples in the prompt compared to using only original examples?", "explanation_reference": "The success rate of targeted in-context knowledge updating on FEVER significantly increases from 44.2 when using only original examples in the prompt to 99.9 when incorporating a mixture of original examples, edited relevant examples, and edited irrelevant examples in the prompt. This shows that the inclusion of a diverse set of examples, including those with edited information, greatly improves GPT-3's ability to update its knowledge in a targeted manner while minimizing the impact on unrelated questions.", "evidence_reference": "From Table~\\ref{tab:knowledge_edit_full}, we see that different prompts give vastly different results. Specifically, using only the original examples in the prompt leads to relatively poor success rate (especially on FEVER), while adding edited relevant examples in the prompt leads to better success rate, it leads the model to over-rely on the knowledge updates even on irrelevant questions. However, when incorporating all cases of original examples, edited relevant and irrelevant examples in the prompt, GPT-3 is able to achieve high editing success rate and low drawdown on irrelevant questions."}
{"question": "Consider the paper that introduces the dataset which has fewer validation set samples than GoGenSum but more samples than FactCC. Based on human evaluation results, which model demonstrated the highest percentage of factual hallucinations among its extrinsically hallucinated summaries?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset having less val set samples than GoGenSum but more samples than FacCC?", "answer_anchor": "XSumFaith", "question_reference": "Based on the human evaluation results, which model demonstrated the highest percentage of factual hallucinations among its extrinsically hallucinated summaries?", "explanation_reference": "The \\bencdec model showed the highest percentage of factual hallucinations among its extrinsically hallucinated summaries, indicating that while it did produce hallucinations, a significant portion of these were still factually correct.", "evidence_reference": "The numbers in ``Hallucinated'' columns show the percentage of summaries out of 500 where at least one word was annotated by all three annotators as an intrinsic (I) or extrinsic (E) hallucination. When a summary is not marked with any hallucination, it is ``faithful'' (1- I$\\cup$E). The ``factual'' columns within the ``Hallucinated'' column show for each type (I, E and I$\\cup$E), the percentage of summaries out of 500 annotated by all three annotators as factual. The final ``Factual'' column shows the total percentage of factual summaries (Faithful + I$\\cup$E$_{\\mbox{factual}}$). The highest numbers for faithful and factual, and the lowest numbers for hallucinations are boldfaced."}
{"question": "Consider the paper that introduces the model placed below TransferNet but above UniKGQA in the table. What specific role does the stop-gradient operation play in the end-to-end fine-tuning process of the subgraph retriever and reasoner?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shown in the table is below TransferNet but above UniKGQA?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific role does the stop-gradient operation play in the end-to-end fine-tuning process of the subgraph retriever and reasoner?", "explanation_reference": "The stop-gradient operation is used to prevent the backpropagation of gradients through the reasoner's parameters during the joint training of the retriever and reasoner. This allows for the optimization of the retriever based on the feedback from the reasoner without altering the reasoner's learned parameters during this phase.", "evidence_reference": "In the end-to-end fine-tuning section, it is mentioned that '\\u2026where the stop-gradient operation \\u03b2\\u03b2\\u03a3\\u03a3 is to stop updating the parameters \\u03c6.' This indicates that the stop-gradient operation is applied to ensure that the parameters of the reasoner (\\u03c6) do not get updated while the retriever is being fine-tuned based on the feedback from the reasoner."}
{"question": "Consider the paper that introduces the method that corresponds to the orange line in the figure, specifically UniKGQA. What is the core innovation of the method in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "answer": "", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method in the figure is demonstrated by the orange line?", "answer_anchor": "UniKGQA", "question_reference": "What is the core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the question by summarizing the unique approach of UniKGQA, which integrates the retrieval and reasoning processes into a single framework. This is innovative because it contrasts with previous methods that treated these stages separately, thus enhancing the efficiency and effectiveness of solving multi-hop KGQA tasks.", "evidence_reference": "In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning."}
{"question": "Consider the paper that introduces the model that is on the last line of the Seq2Seq/Tree block of the table. What specific method does the solution discrimination module use to encode the equation representation of a solution in the Ana-CL model?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model on the last line of the Seq2Seq/Tree block?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module use to encode the equation representation of a solution?", "explanation_reference": "The solution discrimination module encodes the equation representation of a solution using a gate-recurrent-unit (GRU), as specified in the description of how the discrimination score is measured.", "evidence_reference": "We define a discrimination score that measures the association of a problem $X$ with its ground truth solution $Y$: \\begin{equation} \\label{eq:s_d} s_d =  Dis([en_x(X),en_y(Y)]) \\end{equation} where $en_y(Y)$ is the representation of equation $Y$ encoded by a gate-recurrent-unit (GRU) \\cite{chung2014empirical}."}
{"question": "Consider the paper that examines the dataset which has fewer validation set samples than GoGenSum but more samples than FactCC. What specific aspect of pretrained models contributes to their superior performance in generating factual summaries compared to non-pretrained models?", "answer": "", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset having less val set samples than GoGenSum but more samples than FacCC?", "answer_anchor": "XSumFaith", "question_reference": "Based on the findings, what specific aspect of pretrained models contributes to their superior performance in generating factual summaries compared to non-pretrained models?", "explanation_reference": "The superior performance of pretrained models in generating factual summaries is attributed to their exposure to vast amounts of text during pretraining. This exposure allows them to better integrate background knowledge with generation, leading to more factual content even in the presence of hallucinations.", "evidence_reference": "The superior performance of \\bencdec is most likely due to its exposure to vast amount of text through pretraining, allowing it to integrate background knowledge with generation. Even so, over 90\\% of \\bencdec hallucinations are  erroneous."}
{"question": "Consider the paper that introduces the method that has an accuracy of 95.20% in automatic evaluation. What is the chosen temperature \\(\\alpha\\) value for positive sentiment control in the model proposed by the paper?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows 95.20 accuracy in automatic evaluation?", "answer_anchor": "Discup", "question_reference": "What is the chosen temperature \\(\\alpha\\) value for positive sentiment control in DisCup?", "explanation_reference": "The chosen temperature \\(\\alpha\\) value for positive sentiment control in DisCup is specified in the experimental details section, indicating the parameter tuning for achieving optimal performance.", "evidence_reference": "For our approach, we search the temperature \\(\\alpha\\) over the value \\(\\{0.1, 0.01, 0.005, 0.001\\}\\), and finally chose \\(\\alpha = 0.005\\) for positive sentiment control,  \\(\\alpha = 0.01\\) for negative sentiment and detoxication."}
{"question": "Consider the paper that discusses the dataset in which KALMV achieves a score of 70.83 for the XL model. What is the percentage of questions from this dataset that can be answered using a boolean response?", "answer": "", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset being tested that KALMV gets 70.83 score for XL model?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka questions that can be answered using a boolean response?", "explanation_reference": "The percentage is directly provided in the dataset statistics, indicating the proportion of questions in Mintaka that can be answered with a boolean (yes/no) response.", "evidence_reference": "A majority (72%) of the questions in Mintaka can be answered using an entity. 14% can be answered using a boolean, in yes/no or comparative questions."}
{"question": "Consider the paper that introduces the model that achieves the best P_k score among the models in the first part of the table. What specific method did the authors employ to encode sentence positions in the encoder input, and how does it differ from using a fixed BOS (Beginning of Sentence) token index?", "answer": "", "figure": "locality/2310.11772/comparison_2_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets the best P_k score on the upper part of the table??", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What specific method did the authors employ to encode sentence positions in the encoder input, and how does it differ from using a fixed BOS token index?", "explanation_reference": "This method directly encodes sentence positions by utilizing the sequence of vocabulary token embeddings as a means to represent the position of each sentence in the input sequence. This approach is distinct from using a fixed Beginning Of Sentence (BOS) token index for every sentence, which does not convey unique position information. The method aims to provide the decoder with unambiguous position information that can be exploited for producing sentence indices at the decoder output, without employing custom schemes like dedicated sentence position embeddings.", "evidence_reference": "At the encoder input for the $i^{th}$ sentence,  we use the $i^{th}$ vocabulary token embedding in place of a fixed BOS token index. Formally, in contrast to \\eqref{eqn:bos}, we set \\begin{equation*} t_{1_1} = 0, \\hspace{2mm}  t_{2_1} = 1, \\hspace{2mm} \\ldots,  t_{|S|_1} = |S|-1. \\end{equation*}"}
{"question": "Consider the paper that introduces the method that results in a better score than MOCA but worse score than LACMA in the Seen, Val, SR dataset. What specific method does the model proposed in the paper use to decouple the understanding of visual appearance from the variations in natural language instructions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the score better than MOCA but worse than LACMA in Seen, Val, SR dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer use to decouple the understanding of visual appearance from the variations in natural language instructions?", "explanation_reference": "The question focuses on a detailed aspect of the methodology used in the paper, specifically how the Episodic Transformer addresses the challenge of understanding complex human instructions in relation to visual appearances. The answer directly addresses this by mentioning the use of synthetic instructions as an intermediate representation, which is a methodological choice made to improve training by decoupling the understanding of visual appearance from the natural language instruction variations.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in direct prompting. How does its performance, as proposed in the paper, on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shows the second best execuation accuracy in direct prompting?", "answer_anchor": "StarCoder", "question_reference": "How does StarCoderBase's performance on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "explanation_reference": "The comparison between completion and insertion formats for StarCoderBase on the Asleep at the Keyboard security benchmark shows that the insertion format leads to a higher percentage of valid code generation and a slightly lower percentage of insecure code generation. This indicates that the insertion format may be more effective for generating secure and valid code.", "evidence_reference": "Completion & StarCoderBase         & 855/1000 (85.50\\%) & 340/855 (39.77\\%) \\\\ Insertion  & StarCoderBase         & 987/1000 (98.70\\%) & 354/987 (35.87\\%)"}
{"question": "Consider the paper that introduces the model that performs the best on the BBBP dataset. What specific tokenization method is used for SMILES sequences in its pre-training corpus, and how does it differ from the tokenization of text?", "answer": "", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model perform the best in the BBBP dataset?", "answer_anchor": "MolXPT", "question_reference": "What specific tokenization method is used for SMILES sequences in the MolXPT pre-training corpus, and how does it differ from the tokenization of text?", "explanation_reference": "The answer directly addresses the question by specifying the distinct tokenization methods used for SMILES sequences and text in the MolXPT pre-training corpus. SMILES sequences are tokenized using a method based on a regular expression, which is different from the byte-pair encoding method used for text, highlighting the tailored approach to handle the unique structure of SMILES compared to natural language text.", "evidence_reference": "Text and SMILES are tokenized separately. For text, we use byte-pair encoding (BPE) to split the words into subwords. For SMILES sequences (including those in wrapped sequences), we tokenize them with the regular expression from Schwaller et al., 2018."}
{"question": "Consider the paper that introduces the model that is in the second-to-last row of the table. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model does not show a decrease in accuracy from the figure?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength and direction of a linear relationship between two variables. In this case, it quantifies the relationship between the word overlap (how vocabularies change over time) and the model performance for the task of political affiliation classification on Twitter data. A value of 0.9817159316285563 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that achieves a score of 28.62 in the WQ-B task. What specific aspect of the paraphrasing-based approach does the paper identify as potentially more effective for instances with simple expressions compared to the model proposed in the paper?", "answer": "", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets 28.62 score in WQ B task?", "answer_anchor": "DSM", "question_reference": "What specific aspect of the paraphrasing-based approach does the paper identify as potentially more effective for instances with simple expressions compared to their method?", "explanation_reference": "The paper identifies that for instances with simple expressions, the paraphrasing-based approach may achieve better performance because it focuses on words, implying a more effective word-level diversity compared to their method which focuses more on the structure of the sentences.", "evidence_reference": "However, for instances with simple expressions, the paraphrasing-based method may achieve better performance...the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the method which exhibits an accuracy of 79.44% in the CJO22 task. What specific methodological limitation does the reliance on the graph construction layer's threshold setting introduce in the model's ability to distinguish confusing law articles?", "answer": "", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shows 79.44 accuracy in CJO22 task?", "answer_anchor": "LADAN", "question_reference": "What specific methodological limitation does the reliance on the graph construction layer's threshold setting introduce in the LADAN model's ability to distinguish confusing law articles?", "explanation_reference": "The reliance on the graph construction layer's threshold setting for dividing law articles into communities introduces a methodological limitation in terms of accuracy. If the threshold is not optimally set, it can lead to inaccurate community detection, which in turn affects the model's ability to effectively distinguish between confusing law articles. This is because the graph distillation operator's performance is contingent on the accuracy of the law article communities obtained by the graph construction layer.", "evidence_reference": "GCL is more critical than GDO because GDO has a limited performance when the law article communities obtained by GCL are not accurate. When removing both GCL and GDO, the accuracy of LADAN decreases to that of HARNN+MTL, which powerfully demonstrates the effectiveness of our method exploiting differences among similar law articles."}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What specific feature of the ALFRED dataset's expert demonstrations makes re-planning during a DAgger-style student-forcing paradigm non-trivial for the SEQ2SEQ method, and can lead to the inability to complete a task?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific feature of the ALFRED dataset's expert demonstrations makes re-planning during a DAgger-style student-forcing paradigm non-trivial, and can lead to the inability to complete a task?", "explanation_reference": "The feature of the ALFRED dataset's expert demonstrations that makes re-planning during a DAgger-style student-forcing paradigm non-trivial, and can lead to the inability to complete a task, is the presence of irreversible actions. This is because if an action taken by the student-forcing model during a task leads to an irreversible state change that deviates from the expert demonstration (e.g., slicing the only apple in the scene when the task is to place an intact apple in the refrigerator), the task cannot be completed as planned.", "evidence_reference": "Obtaining expert demonstration actions on the fly in navigation-only datasets like R2R only requires rerunning $A^*$. In ALFRED, on the fly demonstrations requires re-planning. In some cases re-planning is not possible: if during a task of {Clean & Place, apple, refrigerator, kitchen-3} a student-forcing model slices the only apple in the scene, the action cannot be recovered from and the task cannot be completed."}
{"question": "Consider the paper that introduces the model represented with a dot marker. What specific architectural feature allows DialoGPT Large to efficiently handle the full context in a computationally efficient manner?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model represneted with a dot marker?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific architectural feature of DialoGPT allows it to efficiently handle the full context in a computationally efficient manner?", "explanation_reference": "The question assesses understanding of a core concept in the architecture of DialoGPT that enables it to efficiently process and generate conversational responses by handling the full context. The multi-layer self-attentive mechanism is a fundamental architectural feature that allows for this efficiency.", "evidence_reference": "a transformer-based architecture like GPT-2, which uses a multi-layer self-attentive mechanism to allow fully-connected cross-attention to the full context in a computationally efficient manner"}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 84.70% WInToRe. What specific computational advantage does it have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shows 84.70% WInToRe?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "explanation_reference": "The question focuses on the detailed part of the computational efficiency of GRIT compared to other methods, specifically in the context of feature extraction inference time. This detail highlights GRIT's significant improvement in computational speed.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the dataset in the table that has the second shortest average question length. What is the average improvement in accuracy that the model proposed in the paper achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the longest question average length?", "answer_anchor": "Multialpaca", "question_reference": "What is the average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "explanation_reference": "The average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English is directly stated as 7.6% in the paper.", "evidence_reference": "For languages other than English (the multilingual column), \\textsc{Poly}LM-13B outperforms LLaMA-13B with average improvement up to 7.6%, 5.6%, 3%, and 11% on XCOPA, PAWS-X, XWinagrad, and XNLI, respectively."}
{"question": "Consider the paper that introduces the dataset which has more dev set samples than UIT-VSMEC but fewer dev set samples than ViSpamReviews. What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset having more dev set samples than UIT-VSMEC but less dev set samples than ViSpamReviews?", "answer_anchor": "ViHOS", "question_reference": "What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included in the dataset?", "explanation_reference": "The improvement in F1-score for the PhoBERT_Large model when additional clean comments are included is directly stated in the Experiments and Results section, indicating the performance enhancement due to the inclusion of additional clean comments.", "evidence_reference": "PhoBERT$_{Large}$ considerably outperforms other models in the dataset without additional clean data, achieving 0.6867 in F1-score. In addition, the best model trained on Full data is XLM-R$_{Large}$, which has an F1-score of 0.7770. We find that XLM-R$_{Large}$ increased by 0.1014 and PhoBERT$_{Large}$ increased by 0.0849."}
{"question": "Consider the paper that introduces the method that has the lowest score in the 'Original Persona' column on the original PersonaChat dataset. What is the best setting of the hyperparameter \\(\\gamma\\) for both CSN-sent and this method, and why?", "answer": "", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the lowest score on 'Original Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the best setting of the hyperparameter \\(\\gamma\\) for both CSN-sent and CSN-word on the original PersonaChat dataset, and why?", "explanation_reference": "The best setting of \\(\\gamma\\) is around 0.3 for both CSN-sent and CSN-word because it retains an appropriate amount of relevant document content for response matching, optimizing the balance between filtering out irrelevant content and keeping sufficient information for effective response selection.", "evidence_reference": "The best setting of \\(\\gamma\\) is around 0.3 for both CSN-sent and CSN-word, which retains an appropriate amount of relevant document content for response matching."}
{"question": "Consider the paper that introduces the model that has a macro-F1 score of 27.34. What is the primary reason LegalBERT has faster training and inference times compared to ALBERT and ALBERT-large, despite having more parameters?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model having mac-F1 score of 27.34?", "answer_anchor": "LegalBERT", "question_reference": "What is the primary reason LEGAL-BERTsmall has faster training and inference times compared to ALBERT and ALBERT-large, despite having more parameters?", "explanation_reference": "The efficiency of BERT-based models, including LEGAL-BERTsmall, is not solely determined by the total number of parameters but also by the architecture's dimensions, such as the number of hidden units and attention heads. These dimensions directly impact the computational load during training and inference, particularly in terms of memory usage and gradient calculations. LEGAL-BERTsmall, despite having more parameters than ALBERT and ALBERT-large, is designed with fewer hidden units and attention heads, leading to more efficient gradient accumulation and, consequently, faster training and inference times.", "evidence_reference": "Resource efficiency of the models mostly relies on the number of hidden units ($HU$), attentions heads ($AH$) and Transformer blocks $T$, rather than the number of parameters... \\legalbertsmall despite having $3\\times$ and $2\\times$ the parameters of \\textsc{albert} and \\textsc{albert-large} has faster training and inference times."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 76.3 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What is the specific reason that using twin uniform quantization without the Hessian guided metric significantly decreases the model's proposed in the paper top-1 accuracy on ViT-S/224 at 8-bit quantization compared to using both methods together?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the quant method show 76.3 score on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific reason that using twin uniform quantization without the Hessian guided metric significantly decreases the top-1 accuracy on ViT-S/224 at 8-bit quantization compared to using both methods together?", "explanation_reference": "The significant decrease in top-1 accuracy when using twin uniform quantization without the Hessian guided metric is attributed to the inaccuracy of metrics that only consider local information. The Hessian guided metric, which considers a broader scope of information, including the impact of quantization on the task loss, provides a more accurate determination of the optimal scaling factors. This leads to better quantization performance and higher accuracy.", "evidence_reference": "For instance, the top-1 accuracy on ViT-S/224 achieves 81.00% with both Hessian guided metric and twin uniform quantization at 8-bit quantization, while it decreases to 79.25% without Hessian guided metric, which is even lower than basic PTQ with 80.47% top-1 accuracy. This is also evidence that the metric considering only the local information is inaccurate."}
{"question": "Consider the paper that introduces the method, with an average max toxicity of more than 0.3, represented by a circle. What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the controlled generation experiments conducted by the model proposed in the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method with average max toxicity more than 0.3 but with circle label?", "answer_anchor": "PPLM", "question_reference": "What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the controlled generation experiments?", "explanation_reference": "The selection of \\(\\lambda_0=5\\) is based on empirical results showing that this value resulted in the best combination of average perplexity and repetition score, indicating a balance between fluency and diversity in the generated text.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the method represented by the square marker. What specific mechanism does the model, referred to as MMA, use in its MMA-IL variant to calculate the expected attention for each head, and how does this approach differ from that used by its MMA-H variant?", "answer": "", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is demonstrated by square marker?", "answer_anchor": "MMA", "question_reference": "What specific mechanism does MMA-IL use to calculate the expected attention for each head, and how does it differ from the approach used by MMA-H?", "explanation_reference": "MMA-IL's approach to calculating expected attention involves using a SoftEnergy function, which allows each attention head to attend to all previous encoder states, leveraging more information for translation. This is in contrast to MMA-H, which uses MonotonicEnergy for a hard attention calculation, limiting each head to attend to one encoder state at a time. This distinction highlights the flexibility of MMA-IL in accessing more comprehensive source information compared to the more efficiency-oriented MMA-H.", "evidence_reference": "For MMA-H, we use \\autoref{eq:monotonic_attention} in order to calculate the expected alignment for each layer each head, given $p_{i,j}^{l, h}$. For MMA-IL, we calculate the softmax energy for each head as follows: \\x \\begin{eqnarray} u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j} \\end{eqnarray} and then use \\autoref{eq:milk_recurent} to calculate the expected attention."}
{"question": "Consider the paper that introduces the method represented by the green line in the figure. According to the ablation studies, which dataset showed significant performance degradation when multi-hop neighbor information was removed from the model proposed in the paper?", "answer": "", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method represented in the green line from the figure?", "answer_anchor": "EARL Quantization", "question_reference": "Based on the ablation studies, which dataset showed a significant performance degradation when multi-hop neighbor information was removed?", "explanation_reference": "The ablation study results indicate that removing multi-hop neighbor information ('w/o MulHop') dramatically affected the performance on the WN18RR dataset, as evidenced by the significant drop in performance metrics compared to other ablation settings.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the method that achieves a higher accuracy than DExpert but lower than Air-Decoding. What is the effect of the size of re-ranked candidate tokens on the model proposed in the paper's control performance and text fluency according to its methodology?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows a higher accuracy than DExpert but lower than Air-Decoding?", "answer_anchor": "Discup", "question_reference": "What is the effect of the size of re-ranked candidate tokens on the model's control performance and text fluency according to the DisCup methodology?", "explanation_reference": "The paper discusses how the control performance of the DisCup method is proportional to the size of re-ranked candidate tokens, indicating that a deeper sampling scope (larger size of candidate tokens) results in better control performance. However, it also mentions that this comes at the cost of decreased text fluency, as more selected tokens in low-probability regions lead to a decrease in fluency.", "evidence_reference": "The control performance of our method is proportional to the size of candidate tokens $\\mathcal{C}$. As shown in Figure~\\ref{top_k}, the deeper the sampling scope is, the better the control performance will be, but meanwhile the PPL deteriorates."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 80.3 on Deit-B with a Weight/Activation (W/A) precision of 6/6. What is the specific range of values for R1 when applying twin uniform quantization to post-softmax activations of the model proposed in the paper?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the quant method show 80.3 score on Deit-B?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific range of values for R1 when applying twin uniform quantization to post-softmax activations?", "explanation_reference": "The range for R1 when applying twin uniform quantization to post-softmax activations is defined to well quantify the values using a small scaling factor, Delta_{text{R1}}^{s}, ensuring that R1 covers values very close to zero which are predominant in post-softmax distributions. This range is specifically designed to address the unbalanced distribution of post-softmax values, where most values are close to zero, and a few large values are crucial for the attention mechanism.", "evidence_reference": "For values after softmax, the values in R1 = $[0,2^{k-1}\\Delta_{\\text{R1}}^{s})$ can be well quantified by using a small $\\Delta_{\\text{R1}}^{s}$. To avoid the effect of calibration dataset, we keeps $\\Delta_{\\text{R2}}^{s}$ fixed to $1/2^{k-1}$. Therefore, R2 = $[0,1]$ can cover the whole range, and large values can be well quantified in R2."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 25.9 in the Seen, Val, SR dataset. What specific augmentation methods are adopted for the egocentric observations in the model's training to address the sample insufficiency of imitation learning?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows score of 25.9 in Seen, Val, SR dataset?", "answer_anchor": "MOCA", "question_reference": "What specific augmentation methods are adopted for the egocentric observations in MOCA's training to address the sample insufficiency of imitation learning?", "explanation_reference": "The paper specifies two augmentation methods used to generate perturbed trajectories for each trajectory in training: color swapping, which randomizes the order of the RGB channels, and AutoAugment, which applies predefined image operations such as rotation, shearing, and auto-contrast.", "evidence_reference": "We adopt data augmentation for the egocentric observations, $\\{I_t\\}_{t=1}^T$, to address the sample insufficiency of imitation learning in each trajectory. Specifically, we exploit two augmentation methods; color swapping and AutoAugment."}
{"question": "Consider the paper that introduces the method shown in the fifth row of the table. How does the order of in-context examples affect the performance of this method on the NQ dataset, and what is the observed trend regarding the default and reverse orders?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is shown in the fifth row in the table?", "answer_anchor": "KATE", "question_reference": "How does the order of in-context examples affect the performance of KATE on the NQ dataset, and what is the observed trend regarding the default and reverse orders?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results on the NQ dataset showed that the reverse order, where the most similar sentences are placed closer to the test example, performed the best. This suggests that the proximity of similar sentences to the test example in the input sequence may help GPT-3 leverage the corresponding information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the method that achieves a Hits@1 score of 0.281 in the YAGO43kET dataset. What specific methodological limitation does the model proposed in the paper address in the context of entity typing inference when compared to max-pooling, and how does it impact model training?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets Hits@1 score equal to 0.281 in YAGO43kET datast?", "answer_anchor": "RGCN", "question_reference": "What specific methodological limitation does the exponentially weighted pooling method address in the context of entity typing inference, and how does it compare to max-pooling in terms of model training?", "explanation_reference": "The exponentially weighted pooling method is designed to overcome a specific limitation of max-pooling, where only a small part of the input receives a gradient during backpropagation. This can lead to insufficient training of some embeddings, as they may not represent every attribute of an entity accurately. By ensuring that every input receives a gradient, the exponentially weighted pooling method allows for more comprehensive training of the embeddings, potentially leading to more accurate entity typing results.", "evidence_reference": "Choosing the max value as the final result makes only a small part of the input get the gradient which means some embeddings may not be sufficiently trained. As a result, the model may fail to represent every attribute of an entity accurately. In practice, we adopt an exponentially weighted pooling method similar to softpool~\\citep{DBLP:journals/corr/abs-2101-00440}: \\begin{align} & R_{u, i}=\\mathrm{pool}(\\{R_{u, i}^{Agg2T}, R_{(n_r, n_e), i}^{N2T} \\notag \\\\ & \\vert \\; \\forall (n_e, n_r)\\in \\mathcal{N}(u)\\}),for\\; i in 1,2,\\dots, L, \\end{align} \\begin{equation} \\mathrm{pool}(\\{x_1, x_2,...,x_n\\})=\\sum_{i=1}^{n}w_ix_i, \\end{equation} \\begin{equation} \\label{equation:weight} w_i = \\frac{\\exp \\alpha x_i}{\\sum_{k=1}^{n}\\exp \\alpha x_k}, \\end{equation} where $R_{u, i}\\in \\mathbb{R}$ is the relevance score between entity $u$ and type i. $\\alpha \\in \\mathbb{R}^+$ is a hyperparameter that controls the temperature of the pooling process. The higher $R_{u, i}$ means entity $u$ is more likely to have type i. This pooling method has a similar effect to max-pooling but can generate a gradient for every input which ensures every embedding gets sufficient training."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest number of turns. What specific strategy does the model proposed in the paper employ to enhance its performance on machine translation tasks during curriculum learning?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has the least number of turns from the table?", "answer_anchor": "Multialpaca", "question_reference": "What specific strategy does the model employ to enhance its performance on machine translation tasks during curriculum learning?", "explanation_reference": "The strategy of introducing more multilingual parallel data during curriculum learning is mentioned as significantly boosting the model's performance on translation tasks. This approach likely helps in better aligning the representations across different languages, thereby improving the model's ability to translate between them.", "evidence_reference": "Finally, it is worth noting that introducing more multilingual parallel data during the curriculum learning significantly boost the model performance on translation task."}
{"question": "Consider the paper that introduces the model that has the highest Recall@7 score in the CamRest task. Which model demonstrates better performance in terms of Entity F1 improvement on the SMD dataset when comparing the fine-tuned knowledge retriever's results between the T5-Large and T5-3B models?", "answer": "", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has the highest Recall@7 score in CamRest task?", "answer_anchor": "Q-TOD", "question_reference": "How does the performance of Q-TOD with a fine-tuned knowledge retriever on the SMD dataset compare between the T5-Large and T5-3B models in terms of Entity F1 improvement?", "explanation_reference": "The question focuses on the specific detail of performance improvement when employing a fine-tuned knowledge retriever for Q-TOD on the SMD dataset, comparing the Entity F1 metric improvements between two different model sizes. The answer directly addresses the comparison by quantifying the improvement seen in both models, which is derived from the detailed experimental results provided in the paper.", "evidence_reference": "~~Q-TOD (T5-Large) & ~~~~~71.11~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~71.17 (+0.06)~~ \\\\ \\quad ~~w/ oracle knowledge & ~~~~~71.96 (\\textbf{+0.85})~~ \\\\ \\hline ~~Q-TOD (T5-3B) & ~~~~~73.44~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~74.96 (+1.52)~~ \\\\ \\quad ~~w/ oracle knowledge & ~~~~~76.20 (\\textbf{+2.76})~~ \\\\"}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 63.8% in the StrategyQA dataset. What is the accuracy improvement for the MAWPS dataset when using an external calculator for the model proposed in the paper compared to its baseline accuracy?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets 63.8 accuracy in StrategyQA dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What is the accuracy improvement for the MAWPS dataset when using an external calculator for the CoT finetuned T5 XXL model compared to its baseline accuracy?", "explanation_reference": "The improvement can be calculated from the reported accuracies in the arithmetic reasoning section. The baseline T5 XXL model's accuracy with a calculator is not directly provided, but its baseline accuracy is given as 54.15%. The CoT finetuned T5 XXL model's accuracy with a calculator is reported as 88.22%. Therefore, the improvement is 88.22% - 54.15% = 34.07%.", "evidence_reference": "The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. \\textbf{MAWPS} & 54.15 & \\textbf{70.41} & 88.22 & 93.00 & 93.66"}
{"question": "Consider the paper that introduces the model that results in the second lowest accuracy in the COGS-all dataset. What specific advantage does its use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model demonstrates the second lowest accuracy in COGS-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific advantage does the Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "explanation_reference": "The advantage is highlighted by the experimental variations where changing the number of attention heads and dimensions (keeping the computational cost constant) showed that single-head attention performs worse than the optimal setting, indicating that multi-head attention improves model quality. Additionally, the design of multi-head attention ensures that the total computational cost remains similar to that of single-head attention with full dimensionality, thus not significantly increasing the computational cost.", "evidence_reference": "In Table~\\ref{tab:variations} rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section \\ref{sec:multihead}. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"question": "Consider the paper that introduces the method that exhibits an accuracy score of 71.0 on the VQA-v2 task. What is the improvement in grounding F1 score from the baseline to the model proposed in the paper in the grounded captioning task on the Flickr30k Entities dataset?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows 71.0 accuracy score on VQA-v2 task?", "answer_anchor": "UniTAB", "question_reference": "What is the improvement in grounding F1 score from the baseline to \\modelname~in the grounded captioning task on the Flickr30k Entities dataset?", "explanation_reference": "The improvement in grounding F1 score from the baseline to \\modelname~is calculated by subtracting the baseline F1 score from the \\modelname~F1 score. The baseline F1 score is given as 8.44 (from the Cyclical model), and the \\modelname~F1 score is 12.95. Therefore, the improvement is 12.95 - 8.44 = 4.51.", "evidence_reference": "Cyclical~\\cite{ma2019learning} & 26.8 & 22.4 & 61.1 & 16.8 & 8.44 & 22.78 \\\\ \\hline \\modelname & \\textbf{30.1} & \\textbf{23.7} & \\textbf{69.7} & \\textbf{17.4} & \\textbf{12.95} & \\textbf{34.79}"}
{"question": "Consider the paper that introduces the method that has an F1 score of 65.96. What specific aspect of the biaffine scorer's performance under the single-head constraint in the SERA model significantly outperforms the MLP scorer in the context of entity relation extraction from Visual Relationship Detection (VRDs)?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having 65.96 F1 score?", "answer_anchor": "SERA", "question_reference": "What specific aspect of the biaffine scorer's performance under the single-head constraint significantly outperforms the MLP scorer in the context of entity relation extraction from VRDs?", "explanation_reference": "The biaffine scorer's performance under the single-head constraint is significantly better than the MLP scorer in terms of F1 score, indicating its superior ability to balance precision and recall in the context of entity relation extraction from visually rich documents (VRDs). This is critical for achieving high overall performance in relation extraction tasks.", "evidence_reference": "Using different relation scorer, the trend between these two decoders is contrary. MLP scorer performs poorer than biaffine scorer under single-head constraint. This is because biaffine scorer is more suitable for single-head constraint which has been proved by \\citet{dozat2016deep}."}
{"question": "Consider the paper that introduces the model that demonstrates the lowest accuracy in the SLOG-all dataset. What specific achievement does the Vanilla Transformer demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model demonstrates the lowest accuracy in SLGO-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the Transformer model's performance on English constituency parsing, what specific achievement does it demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "explanation_reference": "The paper highlights that, unlike RNN sequence-to-sequence models, the Transformer model outperforms the BerkeleyParser even when trained only on the WSJ training set of 40K sentences. This indicates the Transformer's superior ability to handle tasks with strong structural constraints and significantly longer outputs than inputs, even with limited training data.", "evidence_reference": "In contrast to RNN sequence-to-sequence models [KVparse15], the Transformer outperforms the BerkeleyParser [petrov-EtAl:2006:ACL] even when training only on the WSJ training set of 40K sentences."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. What is the reported accuracy of the Self-Consistency approach using sampling for the AQuA task on the UL2-20B model with a beam size of 40?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "What is the reported accuracy of self-consistency using sampling for the AQuA task on the UL2-20B model with beam size 40?", "explanation_reference": "The reported accuracy directly answers the question by providing the specific performance metric of self-consistency using sampling for the AQuA task on the UL2-20B model.", "evidence_reference": "Self-consistency using sampling & 19.7 \\scriptsize{$\\pm$ 2.5} & \\textbf{24.9 \\scriptsize{$\\pm$ 2.6}} & \\textbf{25.3 \\scriptsize{$\\pm$ 1.8}} & \\textbf{26.7 \\scriptsize{$\\pm$ 1.0}} & \\textbf{26.9 \\scriptsize{$\\pm$ 0.5}}"}
{"question": "Consider the paper that introduces the quant method that achieves a lower score than APQ-ViT but still scores higher than 76.0 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What is the specific reason that using twin uniform quantization without the Hessian guided metric significantly decreases the model proposed in the paper's top-1 accuracy on ViT-S/224 at 8-bit quantization compared to using both methods together?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the quant method shows lower score than APQ-ViT but higher than 76.0 on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific reason that using twin uniform quantization without the Hessian guided metric significantly decreases the top-1 accuracy on ViT-S/224 at 8-bit quantization compared to using both methods together?", "explanation_reference": "The significant decrease in top-1 accuracy when using twin uniform quantization without the Hessian guided metric is attributed to the inaccuracy of metrics that only consider local information. The Hessian guided metric, which considers a broader scope of information, including the impact of quantization on the task loss, provides a more accurate determination of the optimal scaling factors. This leads to better quantization performance and higher accuracy.", "evidence_reference": "For instance, the top-1 accuracy on ViT-S/224 achieves 81.00% with both Hessian guided metric and twin uniform quantization at 8-bit quantization, while it decreases to 79.25% without Hessian guided metric, which is even lower than basic PTQ with 80.47% top-1 accuracy. This is also evidence that the metric considering only the local information is inaccurate."}
{"question": "Consider the paper that introduces the dataset in the table that has an average answer length of 83.71. What specific advantage does the model proposed in the paper's curriculum learning strategy offer for its performance on low-resource languages?", "answer": "", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method demonstrates 83.71 average answer length from the table?", "answer_anchor": "Multialpaca", "question_reference": "What specific advantage does the curriculum learning strategy offer for \\textsc{Poly}LM's performance on low-resource languages according to the paper?", "explanation_reference": "The curriculum learning strategy is designed to initially focus on high-resource language data (English) and gradually increase the proportion of high-quality, low-resource language data during training. This method facilitates the transfer of learned general knowledge from English to other languages, which is particularly beneficial for improving the model's capabilities in low-resource languages.", "evidence_reference": "The model with curriculum learning has achieved stable progress in mainly all languages in both NLU and MT tasks. First of all, the model performance is enhanced in most low-resource languages, indicating that the general knowledge can be effectively transferred to these languages through raising data proportion."}
{"question": "Consider the paper that introduces the method which is listed in the table below the VL-BART method and above the OFA-base method. What is the impact of introducing the $\\tiny{<}obj\\tiny{>}$ token in the output sequence design of this method, specifically UniTAB, on the model's performance on the referring expression comprehension task?", "answer": "", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the table below VL-BART method and above OFA-base method", "answer_anchor": "UniTAB", "question_reference": "How does the introduction of the $\\tiny{<}obj\\tiny{>}$ token in UniTAB's output sequence design impact the model's performance on the referring expression comprehension task?", "explanation_reference": "The introduction of the $\\tiny{<}obj\\tiny{>}$ token not only naturally represents the word-box alignments but also simplifies the sequence prediction by providing hints of the text-box code-switching, which in turn helps improve the model's performance on tasks like referring expression comprehension by around 1%.", "evidence_reference": "Table~\\ref{table:objtoken} shows the experiments on the Refcocog dataset~\\cite{mao2016generation}. The \\modelname$_\\text{Separate}$ baseline inserts a pair of $\\tiny{<}obj\\tiny{>}$ and ${{\\tiny<}{\\tiny\\backslash}\\text{obj}{\\tiny>}}$ tokens before and after a word-box token segment. We experiment with removing the ${{\\tiny<}{\\tiny\\backslash}\\text{obj}{\\tiny>}}$ token, or both special tokens. We observe an around $1\\%$ accuracy improvement by adding $\\tiny{<}obj\\tiny{>}$ tokens."}
{"question": "Consider the paper that introduces the method that has an accuracy of 95.20% in automatic evaluation. What specific methodological adjustment does the model proposed in the paper make to the control-prompt initialization process to ensure stability during training?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows 95.20 accuracy in automatic evaluation?", "answer_anchor": "Discup", "question_reference": "What specific methodological adjustment does DisCup make to the control-prompt initialization process to ensure stability during training?", "explanation_reference": "The adjustment made to the control-prompt initialization process in DisCup for ensuring stability during training involves re-parameterizing the control-prompts. This is achieved by introducing an external LSTM module that processes the initially randomly initialized control-prompts, making them closer to natural language and thus more stable for training.", "evidence_reference": "Empirically,  we re-parameterize the $P_k$ for stable training. An external $LSTM_{\\theta^\\prime}$ module is introduced to make the control-prompts close to the natural language."}
{"question": "Consider the paper that introduces the model which is shown in the first row of the table. What specific improvement in percentage points did MVQG-VL-T5 achieve over its discriminative counterparts on the out-of-domain subset for the VQA task?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific improvement in percentage points did the generative models achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "explanation_reference": "The improvement is highlighted in the comparison between generative and discriminative models for the VQA task, specifically on the out-of-domain subset. The generative models (\\ourst{} and \\oursb{}) improved upon the discriminative baselines by 6 and 6.2 percentage points respectively, demonstrating the effectiveness of using generative modeling for questions with answers not included in the top-K answer candidates.", "evidence_reference": "This improvement is more significant on the out-of-domain subset, where the generative \\ourst{} and \\oursb{} achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling."}
{"question": "Consider the paper that introduces the model which exhibits the highest score on the BC5CDR (Big Dict) and BC5CDR (Small Dict) datasets. What specific strategy does the model use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "answer": "", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model demonstrates the highest score on BC5CDR (Big Dict) and BC5CDR (Small Dict) Dataset?", "answer_anchor": "BERT", "question_reference": "What specific strategy does BERT use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "explanation_reference": "The question assesses understanding of BERT's pre-training strategy designed to mitigate the mismatch between the pre-training and fine-tuning stages, specifically focusing on the detailed probabilities of the mixed masking strategy employed.", "evidence_reference": "In Section~\\ref{sec:pretraining_tasks}, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective... The following is an ablation study to evaluate the effect of different masking strategies... \\begin{table}[ht] \\begin{center} {\\small \\begin{tabular}{@{}rrrccc@{}} \\toprule \\multicolumn{3}{c}{Masking Rates} & \\multicolumn{3}{c}{Dev Set Results}  \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.2cm}){4-6} \\textsc{Mask} &\\textsc{Same}&\\textsc{Rnd}& {MNLI} &\\multicolumn{2}{c}{NER} &  & & {\\footnotesize Fine-tune} &   {\\footnotesize Fine-tune}& {\\footnotesize Feature-based} \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.1cm}r{0.1cm}){4-4} \\cmidrule(l{0.2cm}){5-6}  80\\%&10\\%&10\\%&84.2&95.4&94.9... \\bottomrule \\end{tabular} } \\end{center} \\caption{\\label{tab:mask_ablation} Ablation over different masking strategies.} \\end{table}"}
{"question": "Consider the paper that introduces the method for which the BLEU-1 score is missing in the table. How does its performance, when utilizing concepts extracted from captions, compare to using concepts from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "answer": "", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method does not provide BLEU-1 score?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP with concepts extracted from captions compare to using concepts from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "explanation_reference": "The paper demonstrates that leveraging concepts extracted directly from captions, as opposed to using pre-defined concepts from an object detector, results in improved performance on the COCO-caption Karpathy split. This is evidenced by the higher CIDEr score achieved when using caption-extracted concepts.", "evidence_reference": "FOCAL$_\\text{Tag+Init}$ & $10$ & $35.9$ & $28.4$  & $57.6$ & \\cellcolor{gray!40}$121.3$ & $21.9$"}
{"question": "Consider the paper that introduces the model that has the highest accuracy in the COGS-all dataset. What specific architectural change was made to the Transformer model in its framework to potentially improve computational efficiency during unsupervised pre-training?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model demonstrates the highest accuracy in COGS-all dataset?", "answer_anchor": "T5", "question_reference": "What specific architectural change was made to the Transformer model in the T5 framework to potentially improve computational efficiency during unsupervised pre-training?", "explanation_reference": "The specific architectural change made to improve computational efficiency during unsupervised pre-training in the T5 framework was replacing entire spans of corrupted tokens with a single token. This approach is mentioned as part of the unsupervised objectives exploration, where it is noted that this method produces shorter target sequences, potentially making unsupervised pre-training more computationally efficient.", "evidence_reference": "We found that most ``denoising'' objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to-text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient."}
{"question": "Consider the paper that introduces the method that has an F1 score of 53.36. What specific architectural component in the model proposed by the paper is responsible for mapping the representation of each node into the number of target classes?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having 53.36 F1 score?", "answer_anchor": "Doc2Graph", "question_reference": "What specific architectural component in Doc2Graph is responsible for mapping the representation of each node into the number of target classes?", "explanation_reference": "The Node Predictor is explicitly mentioned as the component responsible for mapping the representation of each node into the number of target classes, indicating its role in the classification process within the Doc2Graph framework.", "evidence_reference": "Node Predictor: this is a FC layer, that maps the representation of each node into the number of target classes;"}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.679 in the FB15kET dataset. What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the model proposed in the paper?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets MRR score equal to 0.679 in FB15kET datast?", "answer_anchor": "RGCN", "question_reference": "What is the relevance score between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan according to the CET model?", "explanation_reference": "The relevance score indicates how strongly a neighbor correlates with a candidate type for an entity according to the CET model's inference. In this case, the relevance score of 6.93 directly quantifies the strength of the relationship between the neighbor (has won prize, Pulitzer Prize) and the candidate type Pulitzer Prize winners for the entity Bob Dylan, as determined by the CET model.", "evidence_reference": "(has won prize, Pulitzer Prize) & 6.93"}
{"question": "Consider the paper that introduces the LLM model that demonstrates the lowest MSE score. What specific performance improvement does the model proposed in the paper exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "answer": "", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the LLM model that demonstrates the lowest MSE score?", "answer_anchor": "GPT-4", "question_reference": "What specific performance improvement does GPT-4 exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "explanation_reference": "The question assesses understanding of GPT-4's significant improvement in performance on a professional benchmark, the Uniform Bar Exam, compared to its predecessor. This detail highlights GPT-4's advanced capabilities in understanding and generating natural language in complex scenarios.", "evidence_reference": "For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers. This contrasts with GPT-3.5, which scores in the bottom 10%."}
{"question": "Consider the paper that introduces the model that exhibits a 64.1 F1 score in the WebQSP dataset. What is the impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in the model proposed in the paper+NSM on the CWQ dataset?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows 64.1 F1 score in WebQSP dataset?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What is the impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in \\model+NSM on the CWQ dataset?", "explanation_reference": "The impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in \\model+NSM on the CWQ dataset is a decrease of 0.1%, indicating a slight reduction in QA performance without the subgraph merging strategy.", "evidence_reference": "Table~\\ref{tb:treemerge} shows that based on \\model+NSM, the average subgraph size increases from 174 to 204, and Hits@1 of QA drops 0.1\\% when removing the subgraph merging strategy (\\model+NSM w/o GM) but directly taking the union of all the subgraphs from different topic entities to induce the subgraph."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category. What specific methodological limitation does the training process of the model proposed in the paper exhibit regarding the handling of sequence lengths?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2002.01808", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category?", "answer_anchor": "K-Adapter", "question_reference": "What specific methodological limitation does the training process of the factual adapter in K-Adapter exhibit regarding the handling of sequence lengths?", "explanation_reference": "The limitation is directly related to the methodological choice of setting a maximum sequence length during the training of the factual adapter. This choice could potentially limit the model's ability to process and learn from longer sequences that might contain additional or more complex factual information relevant to the task at hand.", "evidence_reference": "To accelerate the training process, we set the max sequence length as 64 as the average sequence length of T-REx-rc is only 22.8."}
{"question": "Consider the paper that introduces the dataset in which KALMV achieves a score of 66.48 for the Large model. What percentage of its questions can be answered using a boolean response?", "answer": "", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset being tested that KALMV gets 66.48 score for Large model?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka questions that can be answered using a boolean response?", "explanation_reference": "The percentage of questions in the Mintaka dataset that can be answered using a boolean (yes/no) response is directly provided in the dataset statistics.", "evidence_reference": "A majority (72\\%) of the questions in Mintaka can be answered using an entity. 14\\% can be answered using a boolean, in yes/no or comparative questions."}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What is the temperature parameter value (\\(\\alpha\\)) used for positive sentiment control in the method?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "Discup", "question_reference": "What is the temperature parameter value (\\(\\alpha\\)) used for positive sentiment control in DisCup?", "explanation_reference": "The temperature parameter (\\(\\alpha\\)) controls the sharpness of the probability distribution in DisCup's unlikelihood training. For positive sentiment control, a specific value of \\(\\alpha\\) is chosen to optimize performance.", "evidence_reference": "For our approach, we search the temperature \\(\\alpha\\) over the value \\(\\{0.1, 0.01, 0.005, 0.001\\}\\), and finally chose \\(\\alpha = 0.005\\) for positive sentiment control,  \\(\\alpha = 0.01\\) for negative sentiment and detoxication."}
{"question": "Consider the paper that introduces the method that has approximately 30 perplexity and the highest average max toxicity. What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the model's controlled generation experiments proposed by the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method with around 30 perplexity and the highest average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the controlled generation experiments?", "explanation_reference": "The selection of \\(\\lambda_0=5\\) is based on empirical results showing that this value resulted in the best combination of average perplexity and repetition score, indicating a balance between fluency and diversity in the generated text.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the model depicted in the figure that exhibits the highest fluctuation. What specific transformation rule is applied by the model proposed in the paper to construct disfluent summaries for the fluency dimension in text summarization?", "answer": "", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown in the figure demonstrates the highest fluctuation?", "answer_anchor": "UniEval", "question_reference": "What specific transformation rule is applied to construct disfluent summaries for the fluency dimension in text summarization?", "explanation_reference": "The answer directly addresses the transformation rule used for creating disfluent summaries in the context of evaluating the fluency dimension, as specified in the paper.", "evidence_reference": "Fluency represents the quality of individual sentences. We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries."}
{"question": "Consider the paper that introduces the transformer-based method that achieves the highest MRR score on the FB15kET dataset. What are the three distinct mechanisms that compose its approach, and how does each contribute to the task of knowledge graph entity typing?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which transformer-based method gets the highest MRR score in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What are the three distinct mechanisms that compose the Transformer-based Entity Typing (TET) approach, and how does each contribute to the task of knowledge graph entity typing?", "explanation_reference": "The answer directly addresses the question by listing the three mechanisms of the TET approach as described in the paper and summarizing their contributions to the entity typing task. Each mechanism's role is clearly defined, reflecting a deep understanding of the paper's content.", "evidence_reference": "TET is composed of three different mechanisms: a local transformer allowing to infer missing entity types by independently encoding the information provided by each of its neighbours; a global transformer aggregating the information of all neighbours of an entity into a single long sequence to reason about more complex entity types; and a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
{"question": "Consider the paper that introduces the method that has the third highest Avg score on the GLUE task. What is the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) used in the experiments?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the second highest Avg score on GLUE task?", "answer_anchor": "Hyperformer", "question_reference": "What is the dimension of the task feature embedding (\\bm{z_{\\tau}}) used in the experiments?", "explanation_reference": "The dimension of the task feature embedding (\\bm{z_{\\tau}}) is specified in the Experimental Details section under Hyperparameters, indicating the specific size used for the experiments.", "evidence_reference": "We set the dimension of the task feature embedding ($\\bm{z_{\\tau}}$) to $t'=512$"}
{"question": "Consider the paper that introduces the Seq2Exp model that exhibits the highest test accuracy. How does its performance on the extended FinQA dataset compare to its performance on the original FinQA dataset in terms of Execution Accuracy (Exec Acc) and Program Accuracy (Prog Acc)?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which Seq2Exp model shows the highest test accuracy?", "answer_anchor": "Elastic", "question_reference": "How does the ELASTIC model's performance on the extended FinQA dataset compare to its performance on the original FinQA dataset in terms of Execution Accuracy (Exec Acc) and Program Accuracy (Prog Acc)?", "explanation_reference": "The question directly asks for a comparison of the ELASTIC model's performance metrics on different subsets of the FinQA dataset. The answer is derived from the reported performance metrics, indicating a decrease in performance on the combined dataset but an increase on the extended dataset alone, showcasing the model's adaptability to diverse operators.", "evidence_reference": "For the performance on the combined test data (original FinQA + extended FinQA), ELASTIC (RoBERTa-large) achieves slightly lower scores (64.5 of Exec Acc and 63.8 of Prog Acc), compared to the results of ELASTIC (RoBERTa-large) achieved on original FinQA dataset (68.96 of Exec Acc and 65.21 of Prog Acc). We also report the metric scores of ELASTIC (RoBERTa-large) achieved on test data from the extended FinQA dataset: 90.0 on both Exec Acc and Prog Acc."}
{"question": "Consider the paper that introduces the method that scores higher than 69.0 but lower than 70.0 in the Forgotten Realms category. What specific improvement in U.Acc. does the model proposed in the paper achieve on the Yugioh domain under the few-shot entity linking task when comparing the performance of Syn+Seed data to the Name Matching method?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method got a score high than 69.0 but lower than 70.0 in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "What specific improvement in U.Acc. does MetaBLINK achieve on the Yugioh domain under the few-shot entity linking task when comparing the performance of Syn+Seed data to the Name Matching method?", "explanation_reference": "U.Acc. for MetaBLINK on the Yugioh domain with Syn+seed is 22.82, with name matching is 7.88, 22.82-7.88=14.94", "evidence_reference": "TABLE VI"}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 80.3 on Deit-B with a Weight/Activation (W/A) precision of 6/6. How does the model proposed in the paper address the issue of quantizing post-GELU activation values with a highly asymmetric distribution through its twin uniform quantization method?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the quant method show 80.3 score on Deit-B?", "answer_anchor": "PTQ4ViT", "question_reference": "How does the twin uniform quantization method proposed in PTQ4ViT address the issue of quantizing post-GELU activation values with a highly asymmetric distribution?", "explanation_reference": "The twin uniform quantization method effectively addresses the issue of quantizing post-GELU activation values, which have a highly asymmetric distribution, by employing two separate quantization ranges. This approach allows for the quantization of positive and negative values with different scaling factors, thereby reducing the quantization error and improving the overall performance of the quantized network.", "evidence_reference": "For activation values after GELU, negative values are located in R1 = $[-2^{k-1}\\Delta_{\\text{R1}}^{g},0]$ and positive values are located in R2=$[0, 2^{k-1}\\Delta_{\\text{R2}}^{g}]$. We also keep $\\Delta_{\\text{R1}}^{g}$ fixed to make R1 just cover the entire range of negative numbers. Since different quantization parameters are used for positive and negative values respectively, the quantization error can be effectively reduced."}
{"question": "Consider the paper that introduces the model in the figure that has a more negative Spearman's Correlation than 0.60 when the alternative set size is set to 100 in the Mean Cosine setting. What specific performance improvement does the model proposed in the paper show over the standard version of the same model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "answer": "", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model in the figure exceed 0.60 Spearman's Correlation as the alternative set size to 100 in Mean cosine setting?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific performance improvement does the DialoGPT (345M, w/ MMI) model show over the standard DialoGPT (345M) model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "explanation_reference": "The improvement in METEOR score for the DialoGPT (345M, w/ MMI) model over the standard DialoGPT (345M) model is derived from the comparison of their METEOR scores in the DSTC-7 Dialogue Generation Challenge results. The standard DialoGPT (345M) model achieved a METEOR score of 8.51%, while the DialoGPT (345M, w/ MMI) model achieved a METEOR score of 11.23%, indicating a specific improvement of 3.06%.", "evidence_reference": "DialoGPT (345M) = 8.51% METEOR; DialoGPT (345M, MMI) = 11.23% METEOR"}
{"question": "Consider the paper that introduces the model that demonstrates the lowest accuracy in the SLOG-all dataset. What specific mechanism does it employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model demonstrates the lowest accuracy in SLGO-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific mechanism does the Transformer employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "explanation_reference": "The mechanism described directly addresses the need to prevent future information from influencing the prediction of the current position in the sequence, which is crucial for maintaining the auto-regressive nature of the model. This is achieved by modifying the self-attention sub-layer in the decoder to mask out future positions.", "evidence_reference": "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."}
{"question": "Consider the paper that introduces the method which achieves a higher score than No Graph but a lower score than TOD-Flow using GPT-turbo with SGD in 24 domains. What is the discount factor (\\(\\lambda\\)) used in the model proposed by the paper for recency weighting in subtask graph inference from real-world data?", "answer": "", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method has higher score than No Graph but lower score than TOD-Flow using GPT-turbo with SGD in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "What is the discount factor (\\(\\lambda\\)) used in the recency weighting for subtask graph inference from real-world data?", "explanation_reference": "The discount factor (\\(\\lambda\\)) used in the recency weighting for subtask graph inference from real-world data is mentioned as part of the method to improve graph generation by taking temporal information into account, specifically assigning higher weight if a subtask has been eligible more recently.", "evidence_reference": "we assign higher weight if a subtask has been eligible more recently: $w_{t, n} = \\max(0.1, \\lambda ^ {t_n-t}), where $0<\\lambda<1$ is the discount factor, $t_n$ is the time step when the precondition for subtask $n$ became satisfied. We used $\\alpha=0.2$ and $\\lambda=0.7$ in our experiments."}
{"question": "Consider the paper that introduces the method in the figure that demonstrates the highest Toxicity Probability Score when the number of samples equals 1M. What specific architectural feature of the model proposed in the paper allows it to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "answer": "", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method in the figure demonstrates the highest Toxicity Probability Score when number of samples equal to 1M?", "answer_anchor": "GRIT", "question_reference": "What specific architectural feature of GRIT allows it to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "explanation_reference": "GRIT employs a Deformable DETR-based detector for extracting region features, which inherently does not require class-aware NMS (Non-Maximum Suppression) and RoI (Region of Interest) Align operations that are typically necessary in CNN-based detectors like Faster R-CNN. This architectural choice significantly reduces the computational time for feature extraction.", "evidence_reference": "On the other hand, we employ a Deformable DETR-based detector to extract region features without using all such operations. Table \\ref{tab:extraction} shows the comparison on feature extraction."}
{"question": "Consider the paper that discusses the dataset in which KALMV achieves a score of 66.48 for the Large model. What specific adjustment was made to the Question Entity Linking task to address the challenge of achieving agreement on question entities among MTurk workers?", "answer": "", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset being tested that KALMV gets 66.48 score for Large model?", "answer_anchor": "Mintaka", "question_reference": "What specific adjustment was made to the Question Entity Linking task to address the challenge of achieving agreement on question entities among MTurk workers?", "explanation_reference": "The adjustment was made due to the initial difficulty in achieving agreement among MTurk workers on the entities within the questions. By modifying the task to focus on verifying a span and then linking it to Wikidata, the process was streamlined and made more manageable, leading to improved agreement rates.", "evidence_reference": "Since early test runs showed it would be difficult to get agreement on question entities, we modified the task so workers only verified a span and linked the entity in Wikidata."}
{"question": "Consider the paper that introduces the model that achieves a higher TP score than GIT but a lower TP score than LLaVA. What is the inference time for feature extraction using the model proposed in the paper compared to VinVL and M2 Transformer?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which models shows higher TP score than GIT but lower TP score than LLaVA?", "answer_anchor": "GRIT", "question_reference": "What is the inference time for feature extraction using GRIT compared to VinVL and M2 Transformer?", "explanation_reference": "The inference time for feature extraction using GRIT is significantly lower than that of VinVL and M2 Transformer, demonstrating GRIT's computational efficiency in this aspect.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the transformer-based method that achieves the highest MRR score on the FB15kET dataset. What specific aspect of the method's design allows it to semantically strengthen the representation of an entity through class membership of types?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which transformer-based method gets the highest MRR score in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What specific aspect of the TET's design allows it to semantically strengthen the representation of an entity through class membership of types?", "explanation_reference": "The question focuses on a detailed aspect of the Transformer-based Entity Typing (TET) approach, specifically how it enhances the semantic representation of an entity. The answer directly addresses this by pointing out that TET utilizes information regarding the class membership of types, which is a methodological detail that semantically strengthens the representation of an entity. This detail is crucial for understanding how TET improves upon existing methods in knowledge graph entity typing by leveraging class membership information to enrich entity representations.", "evidence_reference": "Furthermore, TET uses information about class membership of types to semantically strengthen the representation of an entity."}
{"question": "Consider the paper that introduces the method that achieves an average EA score of 67.07 in the FinQA task. How does the order of in-context examples influence the model's performance on the NQ dataset, and what is the observed effect when the examples are arranged in reverse order?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets 67.07 EA score in FinQA task", "answer_anchor": "KATE", "question_reference": "How does the order of in-context examples influence the performance of KATE on the NQ dataset, and what is the observed effect when the examples are arranged in reverse order?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results on the NQ dataset revealed that arranging the examples in reverse order, where the most similar sentences are placed closer to the test example, yielded the best performance. This suggests that the proximity of similar sentences to the test example may help GPT-3 leverage the corresponding information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the method that is in the first block and has an F1 score of 50.4. What specific performance metric is used to evaluate the effectiveness of the model proposed in the paper in the post-processing strategy?", "answer": "", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is in the first block while having 50.4 F1 score?", "answer_anchor": "ConCN clusters", "question_reference": "What specific performance metric is used to evaluate the effectiveness of the conceptual neighbourhood classifier in the post-processing strategy?", "explanation_reference": "The effectiveness of the conceptual neighbourhood classifier in the post-processing strategy is evaluated based on its precision-oriented approach. This is because the strategy focuses on removing labels that are conceptual neighbours, thus aiming to increase the precision of the label predictions by ensuring that mutually exclusive labels are not predicted for the same entity.", "evidence_reference": "we treat the notion of conceptual neighbourhood in a more informal fashion... Note that this is a precision-oriented strategy."}
{"question": "Consider the paper that introduces the model that has a de-en score of 53.87. What specific advantage does it demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "answer": "", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model has 53.87 de-en score?", "answer_anchor": "GWLAN", "question_reference": "What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "explanation_reference": "The \\textsc{WPM-Joint} model, by being a single model trained on multiple related tasks, demonstrates the advantage of simpler deployment compared to the \\textsc{WPM-Sep} model, which requires training and deploying four separate models for different translation contexts.", "evidence_reference": "Compared with \\textsc{WPM-Sep}, \\textsc{WPM-Joint} shows two advantages. On one hand, even there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment."}
{"question": "Consider the paper that introduces the model in the figure corresponds to the grey line with a star marker. What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shown in the figure represented by grey line with star marker?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing step was applied to the code before indexing it using Elasticsearch for the search tools?", "explanation_reference": "The preprocessing step mentioned specifically applies a lowercase filter and Lucene's ASCIIFoldingFilter to the code, followed by tokenization using a 3-gram tokenizer, before indexing it using Elasticsearch. This step is crucial for preparing the code for efficient and effective search functionality.", "evidence_reference": "The code itself is preprocessed using a lowercase filter and Lucene's \\texttt{ASCIIFoldingFilter}, tokenized using a 3-gram tokenizer, and indexed using the default Lucene implementation of BM25 as a similarity function."}
{"question": "Consider the paper that introduces the method that exhibits the lowest BLEU score in the De->En task over Average Lagging from 5 to 11. What is the relationship between the model's average attention span and the variance loss $L_{var}$, as demonstrated in the paper?", "answer": "", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the lowest BLEU score in De-En task over Average Lagging from 5 to 11?", "answer_anchor": "MMA", "question_reference": "What is the relationship between the average attention span and the variance loss $L_{var}$ as demonstrated in the paper?", "explanation_reference": "The paper explicitly states that the average attention span decreases with an increase in the variance loss $L_{var}$, indicating a direct relationship between these two variables.", "evidence_reference": "We show the relation between the average attention span (averaged over the IWSLT and WMT test sets) versus $L_{var}$ in \\autoref{fig:attention-span}. As expected, the average attention span is reduced as we increase $L_{var}$."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.8173 on the Stance dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of AI venue classification?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets mean classification accuracy 0.8173 on Stance dataset?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of AI venue classification?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the AI venue classification task. A value of 0.9303959931770183 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also tends to increase.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that results in a token-level F1 score equal to 37.03. What specific initialization method was used for the new position embeddings in the model proposed in the paper to leverage RoBERTa's pretrained weights for supporting longer documents?", "answer": "", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method show a token-level F1 score equal to 37.03?", "answer_anchor": "longformer", "question_reference": "What specific initialization method was used for the new position embeddings in the Longformer to leverage RoBERTa's pretrained weights for supporting longer documents?", "explanation_reference": "The method used for initializing new position embeddings in the Longformer, to support documents longer than what RoBERTa's pretrained model could handle, involved copying the existing 512 position embeddings from RoBERTa multiple times. This approach was chosen to preserve the local structure learned by RoBERTa's attention heads across the extended sequence lengths.", "evidence_reference": "To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa's pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times."}
{"question": "Consider the paper that introduces the model shown in the table that has an overall score of less than 3.80. How does its approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shown in the table with overall score less than 3.80?", "answer_anchor": "MVQG-VL-T5", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "explanation_reference": "The paper hypothesizes that the divergence in performance on the RefCOCOg task between VL-T5 and VL-BART is due to the different methods of positional encoding used by T5 and BART. Specifically, BART uses learned absolute positional embeddings, which might lead to the model memorizing the positions of training objects, resulting in high training accuracy but low validation accuracy. This hypothesis is supported by the observation of VL-BART's performance drop in the RefCOCOg task compared to VL-T5.", "evidence_reference": "We also observe that our experiments with \\oursb{} on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in self-attention layers instead. We hypothesize that \\oursb{} found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy)."}
{"question": "Consider the paper that introduces the method in the table that is listed right above One-Round Distillation and right below Specialization. What specific modification to the few-shot prompts used in the model's, proposed by the paper, Chain of Thought (CoT) generation is highlighted as key for improving the quality of generated data?", "answer": "", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method in the table is listed right above One-Round Distillation and right below Specializing?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as key for improving the quality of generated data?", "explanation_reference": "The specific modification mentioned is crucial because it allows the large language models (LLMs) to correct small mistakes in the chain of thought (CoT), thereby improving the quality of the generated data for finetuning smaller models.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the text-davinci-002 GPT-3.5 model using the dataset for the 'dialogue response' task. What hyperparameter settings were employed for generating conversations grounded in narrative?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "2212.10465", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset used for Dialogue response task?", "answer_anchor": "SODA", "question_reference": "What hyperparameter settings were used for generating conversations grounded in narrative using the text-davinci-002 GPT-3.5 model?", "explanation_reference": "The answer directly specifies the hyperparameter settings used for the conversation generation step in the CO3 framework, which is a detail specific to the methodology of generating dialogues grounded in narratives.", "evidence_reference": "We again leverage the \\texttt{text-davinci-002} GPT-3.5 model for generating conversations. An example prompt is ``\\texttt{[narrative] The following is a long in-depth conversation happening in the scene between Madeleine and her coach with multiple turns.\\textbackslash nMadeleine:}''. We use the same hyperparameter setting as the narrative generation."}
{"question": "Consider the paper that introduces the LLM model that has a test accuracy of 50.7. What specific aspect of the model proposed in the paper allows it to act as a 'self-ensemble' rather than requiring multiple models for aggregation?", "answer": "", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which LLM model shows test accuracy by 50.7?", "answer_anchor": "Self-Consistency", "question_reference": "What specific aspect of the self-consistency method allows it to act as a 'self-ensemble' rather than requiring multiple models for aggregation?", "explanation_reference": "The question targets the unique feature of self-consistency that distinguishes it from typical ensemble approaches, which usually involve multiple models. The self-consistency method, by generating diverse reasoning paths from a single model and selecting the most consistent answer, effectively creates a 'self-ensemble' without the need for multiple models.", "evidence_reference": "Self-consistency also differs from a typical ensemble approach where multiple models are trained and the outputs from each model are aggregated, it acts more like a ``self-ensemble'' that works on top of a \\textit{single} language model."}
{"question": "Consider the paper that introduces the dataset that corresponds to the largest blue circle label. What specific challenge did annotators face when annotating tweets in Mozambican Portuguese and Xitsonga, and how did it affect the final dataset?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What dataset is with the largest blue circle label?", "answer_anchor": "AfriSenti", "question_reference": "What specific challenge did annotators face when annotating tweets in Mozambican Portuguese and Xitsonga, and how did it affect the final dataset?", "explanation_reference": "The challenge of code-mixing and sarcasm in tweets made it difficult for annotators to determine the intended meaning, leading to the exclusion of many tweets from the final dataset due to disagreements among annotators on the presence of sarcasm.", "evidence_reference": "One of the significant challenges for the Mozambican Portuguese and Xitsonga data annotators was the presence of code-mixed and sarcastic tweets. Code-mixing in tweets made it challenging for the annotators to determine the intended meaning of the tweet as it involved multiple languages spoken in Mozambique that some annotators were unfamiliar with. Similarly, the presence of two variants of Xitsonga spoken in Mozambique (Changana and Ronga) added to the complexity of the annotation task. Additionally. we excluded many tweets from the final dataset as sarcasm present in tweets was another source of disagreement among the annotators."}
{"question": "Consider the paper that introduces the dataset associated with the task 'Hate Speech Spans Detection (HSSD)'. What specific error type in the error analysis indicates a failure due to the model proposed in the paper's inability to understand comments with indirect and disrespectful references?", "answer": "", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset with task `Hate Speech Spans Detection (HSSD)`?", "answer_anchor": "ViHOS", "question_reference": "What specific error type in the error analysis indicates a failure due to the model's inability to understand comments with indirect and disrespectful references?", "explanation_reference": "The error type 'Allusion' directly addresses the model's failure to correctly interpret comments that refer to another person or subject in an indirect and disrespectful manner, indicating a specific kind of error where the model's understanding of context and indirect references is inadequate.", "evidence_reference": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 84.70% WInToRe. What specific activation function is employed in its parallel cross-attention mechanism for gated activation?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shows 84.70% WInToRe?", "answer_anchor": "GRIT", "question_reference": "What specific activation function is employed in the parallel cross-attention mechanism of GRIT for gated activation?", "explanation_reference": "The parallel cross-attention mechanism in GRIT uses the sigmoid activation function for gated activation, as specified in the description of the parallel cross-attention design.", "evidence_reference": "c_i^g &= \\mathrm{sigmoid}(W^g[{a^{g}_{i}}; x^{\\prime}_{i}] + b^g), \\\\ c_i^r &= \\mathrm{sigmoid}(W^r[{a^{r}_{i}}; x^{\\prime}_{i}] + b^r)."}
{"question": "Consider the paper that introduces the method that has a score of 71.4 in the CB dataset with 4-shot prompting. What specific aspect of the task embeddings derived from the model's soft prompts contributes to their effectiveness in capturing task relationships, as opposed to domain similarity?", "answer": "", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having score of 71.4 in CB dataset with 4-shot prompting?", "answer_anchor": "SPoT", "question_reference": "What specific aspect of the task embeddings derived from SPoT's soft prompts contributes to their effectiveness in capturing task relationships, as opposed to domain similarity?", "explanation_reference": "The effectiveness of task embeddings in capturing task relationships, as opposed to domain similarity, is attributed to their sensitivity to the type of task being performed. This is evidenced by the observation that QNLI, an NLI task derived from the SQuAD dataset, is not closely linked to SQuAD in the task embeddings, indicating that the embeddings are more sensitive to the task type rather than the domain similarity.", "evidence_reference": "We note that QNLI, which is an NLI task built from the SQuAD dataset, is not closely linked to SQuAD; this suggests that our task embeddings are more sensitive to the type of task than domain similarity."}
{"question": "Consider the paper that introduces the model that achieves the best P_k score among the models in the first part of the table. What is the erroneous output fraction for the model proposed in the paper when tested on the Wiki-727k dataset?", "answer": "", "figure": "locality/2310.11772/comparison_2_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets the best P_k score on the upper part of the table??", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What is the erroneous output fraction for structured summarization models when tested on the Wiki-727k dataset?", "explanation_reference": "The erroneous output fraction indicates how frequently the model produces an invalid sentence boundary position. For the QMSum dataset, the structured summarization models did not produce any erroneous segment boundary positions, indicating a high level of accuracy in generating valid sentence indices.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics. \\begin{table}[h] \\small \\centering \\s\\t \\renewcommand{\\arraystretch}{1.4} \\begin{tabular}{cccc} \\toprule Wiki-727K & en\\_city & en\\_disease & QMSum \\\\ \\hline 0.0001 & 0.0025 & 0 & 0 \\\\ \\bottomrule \\end{tabular} \\caption{Fraction of examples with at least one erroneous segment boundary position. This is for the structured summarization models, when tested on the respective test set.} \\label{table:sentpos_nonnumeric} \\end{table}"}
{"question": "Consider the paper that introduces the method that corresponds to a higher F1 score than that of LDSGM but a lower F1 score than 65.76 for PDTB-Top. What is the primary reason it performs better than the PIDRP method across all four top-level senses of the PDTB, especially on the Temporal sense?", "answer": "", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which is the method has a higher F1 score than LDSGM but lower F1 score than 65.76", "answer_anchor": "PCP", "question_reference": "What is the primary reason the PIDRP method performs worse than the PCP method across all four top-level senses of the PDTB, especially on the Temporal sense?", "explanation_reference": "The primary reason for the PIDRP method's inferior performance compared to the PCP method is attributed to the nature of connective prediction being more aligned with the natural language patterns that the model was exposed to during its pre-training phase, as opposed to the direct prediction of implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the method that is in the second row of the table. What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method in the second row of the table?", "answer_anchor": "SPADE", "question_reference": "What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically mentioned as a method applied by SPADE to prevent loops and token redundancy in parses. It functions by iteratively trimming tail-sharing-edges and generating new edges until the process becomes self-consistent, with a maximum iteration limit set to 20.", "evidence_reference": "Based on this property, we apply the following simple yet powerful tail collision avoidance algorithm: (1) at each tail node having multiple incoming edges, all edges are trimmed except the one with the highest linking probability; (2) at each head node of the trimmed edges, the new tail node is found by drawing the next probable edge whose probability is larger than $p_{th}$ and belongs to the top three; (3) go back to Step 1 and repeat the routine until the process becomes self-consistent or the max iteration limit is reached (set to 20 in this paper). The algorithm prevents loops and token redundancy in parses."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.8173 on the Stance dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets mean classification accuracy 0.8173 on Stance dataset?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength and direction of a linear relationship between two variables. In this case, it quantifies the relationship between the word overlap (how vocabularies change over time) and the model performance for the task of political affiliation classification on Twitter data. A value of 0.9817159316285563 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the model that exhibits a 64.1 F1 score in the WebQSP dataset. What specific strategy does the model proposed in the paper employ to update the question embedding during the path expansion process?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows 64.1 F1 score in WebQSP dataset?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific strategy does the subgraph retriever (\\model) employ to update the question embedding during the path expansion process?", "explanation_reference": "The strategy for updating the question embedding during the path expansion process involves concatenating the original question with the historical expanded relations as the input of RoBERTa, which is a specific detail of how the model updates the question representation to reflect the context of the path being expanded.", "evidence_reference": "we update the embedding of the question by simply concatenating the original question with the historical expanded relations in $p^{(t)}$ as the input of RoBERTa, \\emph{i.e.}, \\beq{ \\label{eq:updatequestion} f(q^{(t)}) = \\mbox{RoBERTa}([q;r_{1};\\cdots;r_{t}]), }"}
{"question": "Consider the paper that introduces the model that achieves a score of 21.073 in 10-shot prompting. Based on the critical analysis of its paper, what specific aspect of the NSP (Next Sentence Prediction) loss's implementation in the original BERT model might have led to the observed discrepancy in performance when it was removed?", "answer": "", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model demonstrates score of 21.073 in 10-shot prompting", "answer_anchor": "RoBERTa", "question_reference": "Based on the critical analysis of the RoBERTa paper, what specific aspect of the NSP (Next Sentence Prediction) loss's implementation in the original BERT model might have led to the observed discrepancy in performance when it was removed?", "explanation_reference": "The question focuses on a detailed aspect of the paper's findings regarding the NSP loss's role in BERT's performance. It critically evaluates the logical coherence of the paper's argument and evidence regarding the NSP loss's impact. The answer directly addresses the question by pinpointing a specific implementation detail that could explain the discrepancy in performance observed when the NSP loss was removed.", "evidence_reference": "It is possible that the original BERT implementation may only have removed the loss term while still retaining the segment-pair input format."}
{"question": "Consider the paper that introduces the method that has a perplexity of approximately 30 and an average max toxicity of around 0.4. What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the controlled generation experiments conducted by the model proposed in the paper?", "answer": "", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is method with around 30 perplexity and around 0.4 average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What is the empirical basis for selecting the initial shift strength \\(\\lambda_0=5\\) in the controlled generation experiments?", "explanation_reference": "The selection of \\(\\lambda_0=5\\) is based on empirical results showing that this value resulted in the best combination of average perplexity and repetition score, indicating a balance between fluency and diversity in the generated text.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category. What is the primary reason for introducing random noises into the newly copied parameters during model expansion in this method?", "answer": "", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.06311", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category?", "answer_anchor": "ELLE", "question_reference": "What is the primary reason for introducing random noises into the newly copied parameters during model expansion in ELLE?", "explanation_reference": "The introduction of random noises into the newly copied parameters during model expansion is aimed at breaking the symmetry that results from parameter replication, thereby providing a better initialization for further optimization and accelerating the pre-training process.", "evidence_reference": "Different from \\citet{chen2021bert2bert}, we additionally introduce random noises $\\bm{\\delta}_i$ into the newly copied parameters of $\\bm{W}'$ during initialization. These slight noises would break the symmetry after the replication and accelerate later pre-training."}
{"question": "Consider the paper that introduces the model that has a 6-layer encoder and a 6-layer decoder architecture. What specific method was adopted for selecting high-quality, in-domain sentences from the Commoncrawl corpus for back-translation in the English to Russian translation task?", "answer": "", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "1907.06616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model with 6-layer encoder and 6-layer decoder architecture?", "answer_anchor": "FSMT", "question_reference": "What specific method was adopted for selecting high quality, in-domain sentences from the Commoncrawl corpus for back-translation in the English to Russian translation task?", "explanation_reference": "The answer directly addresses the question by specifying the method used for filtering high quality, in-domain sentences from the Commoncrawl corpus, which is relevant for enhancing the back-translation process in the English to Russian translation task. This method is explicitly mentioned in the paper, indicating its application for improving the quality of monolingual data used in back-translation.", "evidence_reference": "In order to select a limited amount of high quality, in-domain sentences from the larger corpus, we adopt the method of~\\citet{moore2010intelligent} for selecting in-domain data (\\textsection\\ref{subsection:btcc})."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 76.3 on Deit-S with a Weight/Activation (W/A) precision of 6/6. How does the model proposed in the paper address the issue of quantizing post-GELU activation values with a highly asymmetric distribution through its twin uniform quantization method?", "answer": "", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the quant method show 76.3 score on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "How does the twin uniform quantization method proposed in PTQ4ViT address the issue of quantizing post-GELU activation values with a highly asymmetric distribution?", "explanation_reference": "The twin uniform quantization method effectively addresses the issue of quantizing post-GELU activation values, which have a highly asymmetric distribution, by employing two separate quantization ranges. This approach allows for the quantization of positive and negative values with different scaling factors, thereby reducing the quantization error and improving the overall performance of the quantized network.", "evidence_reference": "For activation values after GELU, negative values are located in R1 = $[-2^{k-1}\\Delta_{\\text{R1}}^{g},0]$ and positive values are located in R2=$[0, 2^{k-1}\\Delta_{\\text{R2}}^{g}]$. We also keep $\\Delta_{\\text{R1}}^{g}$ fixed to make R1 just cover the entire range of negative numbers. Since different quantization parameters are used for positive and negative values respectively, the quantization error can be effectively reduced."}
{"question": "Consider the paper that introduces the method that has a CoLA score equal to 55.9 on the GLUE task. What does the clustering of empty string answers in the PCA visualisation for NewsQA instances suggest about the model proposed in the paper's ability to handle specific types of outputs?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has CoLA score equal to 55.9 on GLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "Based on the analysis of hypernetwork embeddings, what does the clustering of empty string answers in the PCA visualisation for NewsQA instances suggest about the model's ability to handle specific types of outputs?", "explanation_reference": "The clustering of empty string answers in the PCA visualisation for NewsQA instances suggests that the hypernetwork has learned to map from embedding space to specific output types, including the differentiation between non-empty and empty string answers. This indicates the model's effective control over the decoder to generate desired outputs based on the input, showcasing its flexibility and adaptability in handling various output types.", "evidence_reference": "A visualisation of the hypernetwork embeddings generated for NewsQA in Figure \\ref{fig:newsqa_embed} further shows that empty string answers are generally clustered together."}
{"question": "Consider the paper that introduces the dataset which has the largest number of instances. What specific methodological approach did the authors use to ensure the aspect-based summaries in the model proposed in the paper had sufficient content overlap with their corresponding sections?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset with the most number of instances?", "answer_anchor": "QASUM", "question_reference": "What specific methodological approach did the authors use to ensure the aspect-based summaries in OASum had sufficient content overlap with their corresponding sections?", "explanation_reference": "The authors employed a greedy mapping algorithm to map each abstract sentence to a list of sentences in the later sections, followed by assigning a matching score based on the ROUGE-1-recall between the abstract sentence and the intersection of its mapped sentences and the sentences in the aspect section. This approach ensured that the aspect-based summaries had sufficient content overlap with their corresponding sections.", "evidence_reference": "Shown in \\cref{alg: greedy mapping}, we first use a greedy method to map each abstract sentence to a list of sentences in the later sections. Then, we assign a matching score $\\mathcal{S}(x, \\alpha)$ for each abstract sentence $x$ and a potential aspect $\\alpha$. We use the \\textit{ROUGE-1-recall} between the abstract sentence $x$ and the intersection of its mapped sentences $\\mathcal{M}(x)$ and the sentences in the aspect section $Y_a$."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 12.79% TP. What specific activation function is employed in its parallel cross-attention mechanism for gated activation?", "answer": "", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shows 12.79% TP?", "answer_anchor": "GRIT", "question_reference": "What specific activation function is employed in the parallel cross-attention mechanism of GRIT for gated activation?", "explanation_reference": "The parallel cross-attention mechanism in GRIT uses the sigmoid activation function for gated activation, as specified in the description of the parallel cross-attention design.", "evidence_reference": "c_i^g &= \\mathrm{sigmoid}(W^g[{a^{g}_{i}}; x^{\\prime}_{i}] + b^g), \\\\ c_i^r &= \\mathrm{sigmoid}(W^r[{a^{r}_{i}}; x^{\\prime}_{i}] + b^r)."}
{"question": "Consider the paper that introduces the model shown in the figure that performs most similarly to GPT-3.5-Turbo for the 'Plausible' and 'Foreign' scenarios. What is the trimming sequence value chosen for its application?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shown in the figure has a similar performance to GPT-3.5-Turbo?", "answer_anchor": "LLaMA-30B", "question_reference": "What is the trimming sequence value chosen for the application?", "explanation_reference": "The trimming sequence value chosen for the application is explicitly mentioned in the section titled \\textsc{Application_8}, indicating the specific value used for the trimming sequence in the application.", "evidence_reference": "For the application, we chose a trimming  sequence $e_{n} = \\frac{1}{10 \\times n}$"}
{"question": "Consider the paper that introduces the model that exhibits a 64.1 F1 score in the WebQSP dataset. What specific performance improvement does the model proposed in the paper, when combined with NSM, achieve on the CWQ dataset in terms of Hits@1 and F1 scores compared to the original NSM model?", "answer": "", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows 64.1 F1 score in WebQSP dataset?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific performance improvement does the \\model+NSM model achieve on the CWQ dataset in terms of Hits@1 and F1 scores compared to the original NSM model?", "explanation_reference": "The question asks for the specific performance improvement of the \\model+NSM model over the original NSM model on the CWQ dataset, focusing on the Hits@1 and F1 metrics. The answer directly provides these specific improvements, indicating the effectiveness of the \\model when combined with NSM.", "evidence_reference": "NSM injected by \\smodel (\\model+NSM) improves 0.4% Hits@1 and 1.3% F1 on WebQSP, 3.9% Hits@1 and 4.7% F1 on CWQ compared with the original NSM."}
{"question": "Consider the paper that introduces the model that is in the second-to-last row of the table. What is the Pearson's correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model does not show a decrease in accuracy from the figure?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the task of political affiliation classification on Twitter data. A value of 0.9817 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "Twitter, \\poliaff{} ($F_1$), 0.9817159316285563"}
{"question": "Consider the paper that introduces the method that achieves a higher accuracy than DExpert but lower than Air-Decoding. What is the chosen temperature \\(\\alpha\\) value for positive sentiment control in the model proposed by the paper?", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows a higher accuracy than DExpert but lower than Air-Decoding?", "answer_anchor": "Discup", "question_reference": "What is the chosen temperature \\(\\alpha\\) value for positive sentiment control in DisCup?", "explanation_reference": "The chosen temperature \\(\\alpha\\) value for positive sentiment control in DisCup is specified in the experimental details section, indicating the parameter tuning for achieving optimal performance.", "evidence_reference": "For our approach, we search the temperature \\(\\alpha\\) over the value \\(\\{0.1, 0.01, 0.005, 0.001\\}\\), and finally chose \\(\\alpha = 0.005\\) for positive sentiment control,  \\(\\alpha = 0.01\\) for negative sentiment and detoxication."}
{"question": "Consider the paper that introduces the method that does not have results in the CSQA2.0 dev and StrategyQA test tasks shown in the table. What is the impact of incorporating pretraining data mix during PPO training on the performance regressions observed on public NLP datasets?", "answer": "", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method does not show result in CSQA2.0 dev and StrategyQA test task?", "answer_anchor": "ChatGPT", "question_reference": "Based on the paper's findings, what is the impact of incorporating pretraining data mix during PPO training on the performance regressions observed on public NLP datasets?", "explanation_reference": "The paper discusses that incorporating pretraining data mix during PPO training mitigates the performance regressions observed on public NLP datasets, suggesting that this approach helps maintain the capabilities of the pretrained model while aligning it with human preferences.", "evidence_reference": "We sweep a range of pretraining loss coefficient ($\\gamma$ in Equation~\\ref{eq2}) to see its effects on the performance of public NLP datasets and validation reward. The results are shown in Figure~\\ref{fig:public-nlp-evals-v-pretrain}. By setting pretraining loss coefficient to greater or equal ~20, the regression on these tasks can be recovered, on the 1.3B model. We also noticed that the sensitivity to pretraining loss coefficient varies across tasks. Although increasing the pretraining loss coefficient causes the validation reward to drop, a single value of 27.8 seems to work well across model sizes, from 1.3B to 175B parameter count. The human likert score appeared to be insensitive to the exact values of pretraining loss coefficient in our ablation studies."}
{"question": "Consider the paper that discusses the Vanilla Transformer, which shows 0 accuracy in the COGS-structural dataset. What specific advantage does the use of multi-head attention in the Vanilla Transformer provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model shows 0 accuracy in COGS-structural dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific advantage does the Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "explanation_reference": "The advantage is highlighted by the experimental variations where changing the number of attention heads and dimensions (keeping the computational cost constant) showed that single-head attention performs worse than the optimal setting, indicating that multi-head attention improves model quality. Additionally, the design of multi-head attention ensures that the total computational cost remains similar to that of single-head attention with full dimensionality, thus not significantly increasing the computational cost.", "evidence_reference": "In Table~\\ref{tab:variations} rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section \\ref{sec:multihead}. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 42.0 in the Seen, Val, GC dataset. What specific approach does the model proposed in the paper use to decouple the understanding of visual appearance from the variations in natural language instructions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the score of 42.0 in Seen, Val, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer use to decouple the understanding of visual appearance from the variations in natural language instructions?", "explanation_reference": "The question focuses on a detailed aspect of the methodology used in the paper, specifically how the Episodic Transformer addresses the challenge of understanding complex human instructions in relation to visual appearances. The answer directly addresses this by mentioning the use of synthetic instructions as an intermediate representation, which is a methodological choice made to improve training by decoupling the understanding of visual appearance from the natural language instruction variations.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the method represented by the purple line. What specific mechanism does its variant, referred to as MMA-IL, use to calculate the expected attention for each head?", "answer": "", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is demonstrated by the purple line?", "answer_anchor": "MMA", "question_reference": "What specific mechanism does the MMA-IL variant use to calculate the expected attention for each head?", "explanation_reference": "The MMA-IL variant calculates the expected attention for each head using a mechanism referred to as 'SoftEnergy'. This is distinct from the MMA-H variant, which uses a hard selection process. The 'SoftEnergy' mechanism allows MMA-IL to attend to all previous encoder states, leveraging more information for translation.", "evidence_reference": "For MMA-IL, we calculate the softmax energy for each head as follows: \\x \\begin{eqnarray} u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j} \\end{eqnarray}"}
{"question": "Consider the paper that introduces the dataset that contains 3,747,569 instances. What specific methodological approach was used to determine the threshold value for including sentences in aspect-based summaries within the model proposed in the paper?", "answer": "", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset with 3,747,569 instances?", "answer_anchor": "QASUM", "question_reference": "What specific methodological approach was used to determine the threshold value for including sentences in aspect-based summaries within the OASum dataset?", "explanation_reference": "The methodological approach for determining the threshold value for including sentences in aspect-based summaries involved manually evaluating randomly selected Wikipedia pages. This process included assigning these pages to expert annotators for quality assessment, thereby ensuring the summaries' relevance and quality.", "evidence_reference": "To determine the exact value of the threshold, we try $\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]$ and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality."}
{"question": "Consider the paper that introduces the benchmark that has the highest 'Generation Token Length' in the figure. What specific improvement does the model proposed in the paper, CoT prompting, provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which benchmark has the highest 'Generation Token Length' in the figure?", "answer_anchor": "BBH", "question_reference": "What specific improvement does CoT prompting provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "explanation_reference": "The improvement is calculated by comparing the performance increase from answer-only prompting to CoT prompting for the Codex model on the 'Tracking Shuffled Objects' task. This is derived from the performance metrics provided, showing a significant gain when using CoT prompting.", "evidence_reference": ""}
{"question": "Consider the paper that introduces the method that results in a score of 14.9 in the Unseen, Test, GC dataset. What specific method does the model proposed in the paper leverage to decouple the understanding of visual appearance from the variations in natural language instructions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows the score of 14.9 in Unseen, Test, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer leverage to decouple the understanding of visual appearance from the variations in natural language instructions?", "explanation_reference": "The question focuses on a detailed aspect of the methodology employed by the Episodic Transformer to address the challenge of understanding complex human instructions in dynamic environments. The answer directly addresses this by specifying the use of synthetic instructions as an intermediate representation, which is a strategy to separate the process of understanding the visual aspects of an environment from the variations inherent in natural language instructions.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the large language model that corresponds to an HVI score of 47. What specific methodological difference in the evaluation setup for the model's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "answer": "", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the large language model that demonstrates 47 HVI scores?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard approach of directly extracting the model's letter choice from the explanation for most exam runs. Instead, for these specific exams, the approach involved sampling a letter choice at temperature 0 using the explanation already sampled, indicating a unique handling of these exams compared to others.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the model that achieves the highest score on the SST-2 dataset. How does its performance on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model that demonstrates the highest score on SST-2 dataset?", "answer_anchor": "BERT", "question_reference": "How does the performance of BERT on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "explanation_reference": "The performance comparison between fine-tuning and feature-based approaches for the NER task, under various masking strategies during pre-training, shows that the fine-tuning approach consistently outperforms the feature-based approach. This is evident from the Dev set results for NER, where the fine-tuning approach yields higher F1 scores compared to the feature-based approach across all masking strategies.", "evidence_reference": "In the table presented in the Ablation for Different Masking Procedures section, the Dev set results for NER under different masking strategies show that the fine-tuning approach (95.4, 94.9, 95.2, 95.2 for different strategies) consistently achieves higher F1 scores than the feature-based approach (94.9, 94.0, 94.6, 94.7 for the same strategies), indicating better performance."}
{"question": "Consider the paper that introduces the model that has a lower mac-F1 score than Longformer but a higher mac-F1 score than CaselawBERT. What specific aspect of its architecture contributes most significantly to its efficiency in terms of training and inference speed compared to other BERT-based models?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model having lower mac-F1 score than Longformer but higher mac-F1 score than CaselawBERT?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the \\legalbertsmall model's architecture contributes most significantly to its efficiency in terms of training and inference speed compared to other BERT-based models?", "explanation_reference": "The efficiency of the \\legalbertsmall model in terms of training and inference speed is primarily attributed to its architecture, which includes fewer layers, fewer hidden units, and fewer attention heads compared to other BERT-based models. This streamlined architecture reduces computational requirements and memory usage, making it faster and more resource-efficient.", "evidence_reference": "Our hypothesis is that such a specialised \\bert model can perform well against generic \\bert models, despite its fewer parameters. [...] This light-weight model, trains approx.\\ 4 times faster, while also requiring fewer hardware resources. [...] \\textsc{legal-bert-small} & 35M    & 6  & 512   & 8    & 26       & $2.43\\times$ & $4.00\\times$     & $1.70\\times$."}
{"question": "Consider the paper that introduces the method that demonstrates the lowest EA score on the FinQA task. What specific aspect of the policy gradient strategy, as proposed in the paper, contributes to the reduction of prediction variance in the selection of in-context examples for few-shot GPT-3?", "answer": "", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method demonstrates the lowest EA score in FinQA task?", "answer_anchor": "PromptPG", "question_reference": "What specific aspect of the policy gradient strategy contributes to the reduction of prediction variance in the selection of in-context examples for few-shot GPT-3?", "explanation_reference": "The policy gradient strategy, by learning to select in-context examples efficiently and stably without human-designed heuristics, directly contributes to reducing the prediction variance compared to random selection. This approach allows for a more systematic and adaptive selection process, which is less prone to the instability associated with random or heuristic-based selections.", "evidence_reference": "Inspired by reinforcement learning's ability to search for an optimal action policy, we propose applying the policy gradient strategy to learn to select in-context examples more efficiently and stably without designing human-designed heuristics."}
{"question": "Consider the paper that introduces the model that has the highest accuracy in the COGS-all dataset. What specific architectural change in its Transformer architecture was found to not result in a substantial performance drop while halving the total parameter count?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model demonstrates the highest accuracy in COGS-all dataset?", "answer_anchor": "T5", "question_reference": "What specific architectural change in the Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "explanation_reference": "The paper found that sharing parameters across the encoder and decoder performed nearly as well as not sharing them, without a substantial drop in performance. This approach effectively halves the total number of parameters in the model.", "evidence_reference": "We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count."}
{"question": "Consider the paper that introduces the benchmark that has a higher 'Generation Token Length' than ShareGPT and GSM8k. What specific criteria were used to exclude tasks from its subset due to their reliance on specialized knowledge or being outside the scope of the work?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which benchmark has the higher 'Generation Token Length' than ShareGPT and GSM8k?", "answer_anchor": "BBH", "question_reference": "What specific criteria were used to exclude tasks from the BIG-Bench Hard (BBH) subset due to their reliance on specialized knowledge or being outside the scope of the work?", "explanation_reference": "The criteria for excluding tasks from the BBH subset due to their reliance on specialized knowledge or being outside the scope of the work were explicitly listed in the appendix under the heading 'Criteria: Task is outside the scope of this work.'", "evidence_reference": "Criteria: Task is outside the scope of this work: not solvable by authors within 60 minutes, requires specialized knowledge, or not even worth attempting with chain-of-thought."}
{"question": "Consider the paper that introduces the method that achieves a higher accuracy than DExpert but lower than Air-Decoding. What is the temperature parameter value (\\(\\alpha\\)) used for positive sentiment control in the model proposed by the paper?\n\nA) 0.5\nB) 0.7\nC) 0.9\nD) 1.2", "answer": "", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows a higher accuracy than DExpert but lower than Air-Decoding?", "answer_anchor": "Discup", "question_reference": "What is the temperature parameter value (\\(\\alpha\\)) used for positive sentiment control in DisCup?", "explanation_reference": "The temperature parameter (\\(\\alpha\\)) controls the sharpness of the probability distribution in DisCup's unlikelihood training. For positive sentiment control, a specific value of \\(\\alpha\\) is chosen to optimize performance.", "evidence_reference": "For our approach, we search the temperature \\(\\alpha\\) over the value \\(\\{0.1, 0.01, 0.005, 0.001\\}\\), and finally chose \\(\\alpha = 0.005\\) for positive sentiment control,  \\(\\alpha = 0.01\\) for negative sentiment and detoxication."}
{"question": "Consider the paper that introduces the method that is in the last row of the upper half of the table. What specific method does the model proposed in the paper use to generate high-quality synthetic data for few-shot entity linking?", "answer": "", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is on the last row of the upper half of the table?", "answer_anchor": "MetaBINK", "question_reference": "What specific method does MetaBLINK use to generate high-quality synthetic data for few-shot entity linking?", "explanation_reference": "The paper specifies that to improve the quality of synthetic data, they adopt a mention rewriting strategy using the T5 model. This approach is chosen to alleviate the bias introduced by exact matching and generate more effective synthetic samples by rewriting mentions to be semantically similar to entity descriptions.", "evidence_reference": "To further improve the quality of generated data, we adopt the T5 model to generate more semantic-like data samples... In our model, we suppose the mention contains part of semantic information of the corresponding entities, so we add the prefix 'summarize': to the entity's description to force the model to summarize the entity in a few words."}
{"question": "Consider the paper that examines the dataset that includes 1 SM task and 4 languages. What specific method did the authors find to be competitive or even superior to Language Adaptive Fine-Tuning (LAFT) for Nigerian languages, particularly for Nigerian Pidgin (pcm)?", "answer": "", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset with 1 SM task and 4 languages?", "answer_anchor": "NaijaSenti", "question_reference": "What specific method did the authors find to be competitive or even superior to Language Adaptive Fine-Tuning (LAFT) for Nigerian languages, particularly for Nigerian Pidgin (pcm)?", "explanation_reference": "The authors found AfriBERTa to be competitive or better than Language Adaptive Fine-Tuning (LAFT) for the Nigerian languages, particularly for Nigerian Pidgin (pcm), due to AfriBERTa's focus on African languages and its smaller model size, which facilitates easier deployment.", "evidence_reference": "Overall, we found AfriBERTa to be the best baseline model for all languages because the model is more African language-centric. The main advantage of AfriBERTa is its smaller model size, which makes it easier to deploy especially on the African continent where most research labs cannot afford powerful GPUs."}
{"question": "Consider the paper that introduces the method that has an F1 score of 54.83. What is the F1 score for semantic entity recognition (SER) task in the multitask fine-tuning setting for the Italian language using the model proposed in the paper?", "answer": "", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having 54.84 F1 score?", "answer_anchor": "LayoutXLM", "question_reference": "What is the F1 score for semantic entity recognition (SER) task in the multitask fine-tuning setting for the Italian language using the LayoutXLM LARGE model?", "explanation_reference": "The F1 score for the semantic entity recognition (SER) task in the multitask fine-tuning setting for the Italian language using the LayoutXLM LARGE model is directly reported in the multitask fine-tuning results table.", "evidence_reference": "In the multitask fine-tuning accuracy (F1) on the \\task dataset section, the table shows that the F1 score for the SER task for the Italian language using the LayoutXLM LARGE model is 0.8372."}
{"question": "Consider the paper that introduces the optimization method which exhibits the second-highest reward accuracy. How does the model's performance, proposed by the paper, in terms of win rate against reference completions in the TL;DR summarization task compare to PPO at its optimal sampling temperature?", "answer": "", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What optimization method demonstrates the second highest reward accuracy?", "answer_anchor": "DPO", "question_reference": "How does the Direct Preference Optimization (DPO) algorithm's performance in terms of win rate against reference completions in the TL;DR summarization task compare to PPO at its optimal sampling temperature?", "explanation_reference": "The question focuses on comparing the performance of DPO and PPO in a specific task (TL;DR summarization) and requires understanding of the experimental results presented in the paper. The answer directly compares the win rates of DPO and PPO, providing a clear measure of DPO's superior performance in this context.", "evidence_reference": "DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model. We find that DPO has a win rate of approximately 61% at a temperature of 0.0, exceeding the performance of PPO at ~57% at its optimal sampling temperature of 0.0."}
{"question": "Consider the paper that introduces the method that has a CoLA score equal to 55.9 on the GLUE task. What specific performance improvement does the model proposed in the paper achieve on the GLUE benchmark when using T5 v1.1 + LM as the underlying model compared to the Task Hypernet method?", "answer": "", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has CoLA score equal to 55.9 on GLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "What specific performance improvement does the Hyperdecoder approach achieve on the GLUE benchmark when using T5 v1.1 + LM as the underlying model compared to the Task Hypernet method?", "explanation_reference": "The Hyperdecoder approach's performance improvement over the Task Hypernet method is calculated based on their average GLUE benchmark scores when using T5 v1.1 + LM as the underlying model. The Task Hypernet method has an average score of 54.3, while the Hyperdecoder approach achieves an average score of 86.5. The improvement is calculated as ((86.5 - 54.3) / 54.3) * 100% = 32.2%.", "evidence_reference": "Task Hypernet & 2.7% & 0.0 & 82.1 & 16.4 / 16.4 & 70.4 / 81.4 & 89.8 / 86.5 & 56.6 & 64.8 & 50.7 & 54.3 \\\\ \\textbf{Hyperdecoder (ours)} & 2.9% & 58.7$_{2.3}$ & \\textbf{95.9}$_{0.4}$* & 91.8$_{0.7}$ / \\textbf{92.0}$_{0.4}$* & 89.2$_{1.5}$ / 92.0$_{0.9}$ & 91.1$_{0.2}$ / 88.3$_{0.4}$ & \\textbf{90.0}$_{0.2}$* & \\textbf{94.2}$_{0.4}$ & \\textbf{80.8}$_{2.2}$ & \\textbf{86.5}${_{0.5}}\\\\dagger$ \\\\"}
{"question": "Consider the paper that introduces the method shown in the table that achieves a PPL score of 24.466 for the Test Seen task. What is the recommended selection range of \\(k\\) and \\(\\alpha\\) for contrastive search based on the ablation study conducted in the paper?", "answer": "", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown in the table gets 24.466 PPL for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the recommended selection range of \\(k\\) and \\(\\alpha\\) for contrastive search based on the ablation study?", "explanation_reference": "The recommended selection range for \\(k\\) and \\(\\alpha\\) is based on the ablation study's findings that these settings produce results more similar to human-written texts as judged by generation diversity and generation perplexity.", "evidence_reference": "In practice, our recommended selection range of \\(k\\) and \\(\\alpha\\) are \\(k\\in [5,10]\\) and \\(\\alpha\\in[0.5,0.8]\\), as these settings produce results that are more similar to human-written texts as judged by generation diversity and generation perplexity."}
{"question": "Consider the paper that introduces the method which has an average score of 82.8 with zero-shot prompting. How does the model's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "answer": "", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method has average score of 82.8 with zero-shot prompting?", "answer_anchor": "SALAM", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task diverge in performance between the VL-T5 and VL-BART models, and what is hypothesized as the reason for this divergence?", "explanation_reference": "The paper hypothesizes that the divergence in performance on the RefCOCOg task between VL-T5 and VL-BART is due to the different methods of positional encoding used by T5 and BART. Specifically, BART uses learned absolute positional embeddings, which might lead to the model memorizing the positions of training objects, resulting in high training accuracy but low validation accuracy. This hypothesis is supported by the observation of VL-BART's performance drop in the RefCOCOg task compared to VL-T5.", "evidence_reference": "We also observe that our experiments with \\oursb{} on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in self-attention layers instead. We hypothesize that \\oursb{} found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy)."}
{"question": "Consider the paper that introduces the model represented by the blue bar. What specific architectural change in the Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "answer": "", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model is demonstrated by the blue bar?", "answer_anchor": "T5", "question_reference": "What specific architectural change in the Transformer model was found to not result in a substantial performance drop while halving the total parameter count?", "explanation_reference": "The paper found that sharing parameters across the encoder and decoder performed nearly as well as not sharing them, without a substantial drop in performance. This approach effectively halves the total number of parameters in the model.", "evidence_reference": "We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count."}
{"question": "Consider the paper that introduces the model shown in the figure that is represented by the pink line. What are the specific conditions (C') and (C'') assumed for the convergence of the cumulative distribution function resulting from the DA-WR algorithm as applied in the LLaMA-30B model?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shown in the figure is represented by the pink line?", "answer_anchor": "LLaMA-30B", "question_reference": "What are the specific conditions (C') and (C'') assumed for the convergence of the cdf resulting from the DA-WR algorithm?", "explanation_reference": "The conditions (C') and (C'') are necessary assumptions for ensuring the convergence of the cumulative distribution function (cdf) resulting from the Data Augmentation - Weighted Resampling (DA-WR) algorithm to the target distribution as the sample size n approaches infinity. These conditions are related to the differences in indicators for whether a value is less than or equal to x and the differences in weights between the initial and augmented datasets, respectively, and they must tend to zero faster than 1/n.", "evidence_reference": "\\item {\\bf (C')} : $\\max_{i=1,\\cdots,n} |\\mathbb{I}_{X_i\\leq x} - \\mathbb I_{X_i'\\leq x}| = o(1/n)$ \\item {\\bf (C'')} : $\\max_{i=1,\\cdots,n} |q_i -q_i' | =o(1/n)$"}
{"question": "Consider the paper that introduces the optimization method that has the lowest BLEU score among all models in the table. What specific mechanism does the model proposed in the paper employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What optimization method shows the lowest BLEU score across all models?", "answer_anchor": "Transformer base", "question_reference": "What specific mechanism does the Transformer employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "explanation_reference": "The mechanism described directly addresses the need to prevent future information from influencing the prediction of the current position in the sequence, which is crucial for maintaining the auto-regressive nature of the model. This is achieved by modifying the self-attention sub-layer in the decoder to mask out future positions.", "evidence_reference": "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."}
{"question": "Consider the paper that introduces the optimization method that has the lowest BLEU score among all models in the table. According to the authors' analysis, what specific linguistic function is attention head 5 in layer 5 of 6 involved in, based on the evidence provided in the attention visualizations section?", "answer": "", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What optimization method shows the lowest BLEU score across all models?", "answer_anchor": "Transformer base", "question_reference": "Based on the evidence provided in the attention visualizations section, what specific linguistic function does attention head 5 in layer 5 of 6 appear to be involved in, according to the authors' analysis?", "explanation_reference": "The authors present visual evidence showing that attention head 5 in layer 5 of 6 is involved in anaphora resolution, as demonstrated by the focused attention on the word 'its' and its relation to other parts of the sentence.", "evidence_reference": "Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the benchmark that has the highest 'Generation Token Length' in the figure. What is the average human-rater performance for the 'Temporal Sequences' task in the BIG-Bench Hard (BBH) suite?", "answer": "", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which benchmark has the highest 'Generation Token Length' in the figure?", "answer_anchor": "BBH", "question_reference": "What is the average human-rater performance for the 'Temporal Sequences' task in the BIG-Bench Hard (BBH) suite?", "explanation_reference": "The average human-rater performance for the 'Temporal Sequences' task is directly provided in the detailed results table for the BBH tasks.", "evidence_reference": "Temporal Sequences & 25.0 & 52.2 & 90.8 & 100 & 33.6 & 67.2 & 77.6 & 96.8 & 39.6 & 78.8"}
{"question": "Consider the paper that introduces the model placed fourth in the table. What is the primary reason LegalBERT has faster training and inference times compared to ALBERT and ALBERT-large, despite having more parameters?", "answer": "", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shown in the fourth row of the table?", "answer_anchor": "LegalBERT", "question_reference": "What is the primary reason LEGAL-BERTsmall has faster training and inference times compared to ALBERT and ALBERT-large, despite having more parameters?", "explanation_reference": "The efficiency of BERT-based models, including LEGAL-BERTsmall, is not solely determined by the total number of parameters but also by the architecture's dimensions, such as the number of hidden units and attention heads. These dimensions directly impact the computational load during training and inference, particularly in terms of memory usage and gradient calculations. LEGAL-BERTsmall, despite having more parameters than ALBERT and ALBERT-large, is designed with fewer hidden units and attention heads, leading to more efficient gradient accumulation and, consequently, faster training and inference times.", "evidence_reference": "Resource efficiency of the models mostly relies on the number of hidden units ($HU$), attentions heads ($AH$) and Transformer blocks $T$, rather than the number of parameters... \\legalbertsmall despite having $3\\times$ and $2\\times$ the parameters of \\textsc{albert} and \\textsc{albert-large} has faster training and inference times."}
{"question": "Consider the paper that introduces the model that achieves the second highest score in the Stance column. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of AI venue classification?", "answer": "", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shows consistently low accuracy than VIBE model?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of AI venue classification?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the AI venue classification task. A value of 0.9303959931770183 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also tends to increase.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that has fewer 'rounds to completion' than GPT-4 + Belief but more 'rounds to completion' than the CBS planner. How does the inclusion of agent ID in the agent-specific global state influence the model's performance in the SMAC domain, particularly in maps where agents may assume different roles?", "answer": "", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having lower `rounds to completion` than GPT-4 + Belief but higher `rounds to completion` than CBS planner?", "answer_anchor": "MAPPO", "question_reference": "How does the inclusion of agent ID in the agent-specific global state influence MAPPO's performance in the SMAC domain, particularly in maps where agents may assume different roles?", "explanation_reference": "The inclusion of agent ID in the agent-specific global state allows for an agent-specific value function depending on an agent's type or role, which is particularly helpful when the environment contains heterogeneous agents. This is supported by the superior performance of the variant with agent ID compared to the variant without it in the 3s5z vs. 3s6z map, demonstrating the importance of agent-specific features in forming an effective global state.", "evidence_reference": "Including the agent id in the death mask, as is done in variant (1), is particularly important in maps which agents may take on different roles, as demonstrated by the superior performance of variant (1) compared to variant (4), which does not contain the agent ID in the death-mask zero-state, in the 3s5z vs. 3s6z map."}
{"question": "Consider the paper that introduces the model that demonstrates the highest Oracle score. Why does it achieve a higher \\ednascore compared to other diverse decoding strategies on both summarization datasets in the context of the paper?", "answer": "", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model that demonstrates the highest Oracle score?", "answer_anchor": "Composition", "question_reference": "In the context of the paper, why does Composition Sampling with \\frost$_{\\hspace*{-.1cm}++}$ achieve a higher \\ednascore compared to other diverse decoding strategies on both summarization datasets?", "explanation_reference": "The paper states that Composition Sampling with \\frost$_{\\hspace*{-.1cm}++}$ is most effective in generating faithful summaries, as demonstrated automatically (with best entailment scores on XSum and CNN/DailyMail) and by humans (with highest ratings on XSum and CNN/DailyMail); these summaries are also diverse, achieving the highest \\ednascore scores on both summarization datasets. This indicates that the method's ability to generate summaries that are both faithful to the input and diverse in content leads to its higher \\ednascore.", "evidence_reference": "Composition(\\frost$_{\\hspace*{-.1cm}++}$) is most effective in generating faithful summaries, as demonstrated automatically (with best entailment scores on XSum and CNN/DailyMail) and by humans (with highest ratings on XSum and CNN/DailyMail); these summaries are also diverse achieving highest \\ednascore scores on both summarization datasets."}
{"question": "Consider the paper that introduces the model in the figure corresponds to the grey line with a star marker. How does its performance on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "answer": "", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shown in the figure represented by grey line with star marker?", "answer_anchor": "StarCoder", "question_reference": "How does StarCoderBase's performance on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "explanation_reference": "The comparison between completion and insertion formats for StarCoderBase on the Asleep at the Keyboard security benchmark shows that the insertion format leads to a higher percentage of valid code generation and a slightly lower percentage of insecure code generation. This indicates that the insertion format may be more effective for generating secure and valid code.", "evidence_reference": "Completion & StarCoderBase         & 855/1000 (85.50\\%) & 340/855 (39.77\\%) \\\\ Insertion  & StarCoderBase         & 987/1000 (98.70\\%) & 354/987 (35.87\\%)"}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What specific feature of the ALFRED dataset's expert demonstrations, as illustrated by this method, differentiates it from using a simple object class prediction approach for interactions?", "answer": "", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific feature of the ALFRED dataset's expert demonstrations differentiates it from using a simple object class prediction approach for interactions?", "explanation_reference": "The ALFRED dataset's expert demonstrations involve specifying a pixelwise interaction mask of the target object for interactions, which is more realistic and detailed compared to a simple object class prediction approach where localization is treated as a solved problem. This feature allows for precise interaction with objects in the environment, highlighting the dataset's complexity and its focus on realistic simulation for training models.", "evidence_reference": "Motivated by work in robotics on segmentation-based grasping~\\cite{mousavian2019graspnet}, agents in \\dataset{} interact with objects visually, specifying a pixelwise interaction mask of the target object."}
{"question": "Consider the paper that introduces the model labeling 'fine-tuned' shown in the table, MVQG-VL-T5. What specific feature of the visual embeddings allows it to discriminate regions from different images when multiple images are given to it?", "answer": "", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model being fine-tuned shown in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific feature of the visual embeddings allows the model to discriminate regions from different images when multiple images are given to it?", "explanation_reference": "The feature that allows the model to discriminate regions from different images when multiple images are given to it is the use of image ids. This is explicitly mentioned as a function of the image ids encoded within the visual embeddings.", "evidence_reference": "Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in \\NLVR{}~\\cite{Suhr2019}, models take two input images)."}
{"question": "Consider the paper that introduces the method that achieves a relatively constant MRR score in the FB15k-237 dataset as entity code entropy increases. Which component's removal, according to ablation studies, resulted in the most dramatic decrease in performance on the WN18RR dataset?", "answer": "", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method shows a huge increase as entity code entropy increases in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which component's removal resulted in the most dramatic performance decrease on the WN18RR dataset?", "explanation_reference": "The ablation study results for the WN18RR dataset show that the removal of the MulHop component resulted in the most significant performance decrease. This indicates that multi-hop neighbor information is crucial for the model's performance on this dataset.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the model that achieves the highest score on the MNLI dataset. In its ablation study for different masking procedures, what was the MNLI Dev set result when utilizing a 100% 'Mask' strategy without incorporating any 'Same' or 'Rnd' strategies?", "answer": "", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model that demonstrates the highest score on MNLI dataset?", "answer_anchor": "BERT", "question_reference": "In the ablation study for different masking procedures, what was the MNLI Dev set result when using a 100% 'Mask' strategy with no 'Same' or 'Rnd' strategies?", "explanation_reference": "The question specifically targets the results of an ablation study focused on different masking strategies used during the pre-training of BERT. The answer, 84.3, directly corresponds to the MNLI Dev set result for the scenario where 100% of the target tokens were replaced with the [MASK] symbol, with no tokens kept the same or replaced with a random token. This detail is explicitly provided in the table under the ablation study for different masking procedures, making it a precise answer derived from the paper's content.", "evidence_reference": "100%&0%&0%&84.3&94.9&94.0"}
{"question": "Consider the paper that introduces the optimization method that exhibits the second-highest reward accuracy. What specific mathematical derivation allows the model proposed in the paper, specifically the DPO algorithm, to bypass the explicit reward modeling step while optimizing under existing models of human preferences?", "answer": "", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What optimization method demonstrates the second highest reward accuracy?", "answer_anchor": "DPO", "question_reference": "What specific mathematical derivation allows the DPO algorithm to bypass the explicit reward modeling step while optimizing under existing models of human preferences?", "explanation_reference": "The DPO algorithm leverages an analytical mapping from reward functions to optimal policies, which allows for the transformation of a loss function over reward functions into a loss function over policies. This change-of-variables approach enables the DPO algorithm to bypass the explicit reward modeling step while still optimizing under existing models of human preferences, such as the Bradley-Terry model.", "evidence_reference": "Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop."}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in few-shot prompting. What specific performance improvement does the model proposed in the paper, LLaMA-7B, demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "answer": "", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shows the lowest execuation accuracy in few-shot prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "What specific performance improvement does the \\modelname 70B model demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "explanation_reference": "The question focuses on the performance improvement of the \\modelname 70B model over the Falcon 40B model specifically in the Commonsense Reasoning benchmark. The answer, 37.5%, directly reflects the performance metric for the \\modelname 70B model in this category, indicating a significant improvement over the Falcon 40B model's performance.", "evidence_reference": "\\multirow{4}{*}{\\cinnamon} & 7B & 16.8 & 63.9 & 48.9 & 61.3 & 14.6 & 45.3 & 32.6 & 29.3 \\\\ & 13B & 24.5 & 66.9 & 55.4 & 65.8 & 28.7 & 54.8 & 39.4 & 39.1 \\\\ & 34B & 27.8 & 69.9 & 58.7 & 68.0 & 24.2 & 62.6 & 44.1 & 43.4 \\\\ & 70B & \\textbf{37.5} & \\textbf{71.9} & \\textbf{63.6} & \\textbf{69.4} & \\textbf{35.2} & \\textbf{68.9} & \\textbf{51.2} & \\textbf{54.2} \\\\"}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.717 in the FB15kET dataset. How does the model proposed in the paper ensure the preservation of graph structure while integrating neighbour content?", "answer": "", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets MRR score equal to 0.717 in FB15kET datast?", "answer_anchor": "TET", "question_reference": "How does the TET approach ensure the preservation of graph structure while integrating neighbour content?", "explanation_reference": "The context transformer is specifically designed to integrate neighbours' content in a differentiated way through information exchange between neighbour pairs, which inherently preserves the graph structure by maintaining the relational context of each entity.", "evidence_reference": "a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
