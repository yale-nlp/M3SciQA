{"question": "Consider the paper that introduces the method that achieves a score of 28.62 in the WQ-B task. How does the model proposed in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "answer": "The proposed approach addresses the challenge of generating questions with both word-level and structure-level diversity by introducing a dual model framework that leverages external natural questions. This framework consists of a forward model and a backward model, which are interwoven by two selection strategies. These strategies are designed to select diverse natural expressions from external questions and integrate them into the generation model. This method enables the injection of diverse expressions into the generation process, thereby enhancing the diversity of the generated questions at both the word level and the structure level.", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets 28.62 score in WQ B task?", "answer_anchor": "DSM", "question_reference": "How does the proposed approach in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "explanation_reference": "The paper discusses leveraging external natural questions to diversify question generation, indicating that this method can generate questions with different expressions by covering a broader range of semantic patterns and expressions. This approach implicitly addresses both word-level and structure-level diversity by introducing varied semantic patterns from external sources.", "evidence_reference": "In our approach, we introduce external natural questions to diversify question generation, which can generate questions with different expressions, since these natural questions cover a wider range of semantic patterns and expressions."}
{"question": "Consider the paper that introduces the method in the table that corresponds to a ROUGE L score equal to 41.39. How does the application of normalizing flow in the model proposed in the paper specifically contribute to the improvement of abstractive text summarization performance?", "answer": "The application of normalizing flow in the neural topic model specifically contributes to the improvement of abstractive text summarization performance by enabling a better approximation of the true posterior distribution of global semantics. This enriched representation of global semantics allows the summarization model to have a deeper understanding of the overall document content, which in turn helps in generating summaries that are more aligned with the key points and nuances of the original text. Additionally, the integration of a contextualized gating mechanism ensures that the influence of global semantics on the summarization process is optimally balanced, preventing the potential overwhelming of contextualized representations and maintaining the quality of the generated summaries.", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method in the table that demonstrates a ROUGE L score equal to 41.39?", "answer_anchor": "PEGASUS+NTM", "question_reference": "How does the application of normalizing flow in the neural topic model specifically contribute to the improvement of abstractive text summarization performance?", "explanation_reference": "The application of normalizing flow in the neural topic model contributes to the improvement of abstractive text summarization performance by enabling a more accurate approximation of the true distribution of global semantics. This enhanced approximation allows the summarization model to better understand the overall document, leading to summaries that are more informative and capture key points more effectively.", "evidence_reference": "To this end, we propose a method to adapt normalizing flow in the neural topic model to have a better approximation of true distribution and integrate it into the summarization model. Integrating flow mechanism to better approximate the true posterior has been proven to improve performance for variational inference \\cite{rezende2015variational} as well as for downstream tasks such as image synthesis \\cite{kingma2016improved}, etc."}
{"question": "Consider the paper that introduces the method that results in a better score than MOCA but worse score than LACMA in the Seen, Val, SR dataset. What specific performance improvement does the model proposed in the paper provide through pretraining and joint training with synthetic instructions on the ALFRED benchmark's unseen test split?", "answer": "8.5% task success rates on unseen test splits", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shows the score better than MOCA but worse than LACMA in Seen, Val, SR dataset?", "answer_anchor": "E.T.", "question_reference": "What specific performance improvement does pretraining and joint training with synthetic instructions provide on the ALFRED benchmark's unseen test split?", "explanation_reference": "The question focuses on the specific detail of how pretraining and joint training with synthetic instructions impact the performance on the ALFRED benchmark, particularly on the unseen test split. The answer directly reflects the improvement mentioned in the paper, emphasizing the effectiveness of the proposed methods in handling environments not encountered during training.", "evidence_reference": "Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits."}
{"question": "Consider the paper that introduces the method that is in the second row of the table. What is the recall value for the model proposed in the paper using BERT-large (Bl) as the language model encoder in the UFET task before applying the Prior Knowledge about Labels (PKL) strategy?", "answer": "47.5", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is in the second row of the table?", "answer_anchor": "ConCN clusters", "question_reference": "What is the recall value for the DenoiseFET model using BERT-large (Bl) as the language model encoder in the UFET task before applying the Prior Knowledge about Labels (PKL) strategy?", "explanation_reference": "The recall value for the DenoiseFET model using BERT-large (Bl) as the language model encoder in the UFET task before applying the PKL strategy is directly reported in the experimental results section, indicating the model's ability to correctly identify relevant labels before the enhancement with PKL.", "evidence_reference": "DenoiseFET & Bl & 52.6 & 47.5 & 49.8"}
{"question": "Consider the paper that introduces the dataset that contains 3,747,569 instances. What is the average number of aspects per article in the model proposed in the paper, and what percentage of articles have less than 9 aspects?", "answer": "1.82 aspects per article and 99%", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset with 3,747,569 instances?", "answer_anchor": "QASUM", "question_reference": "What is the average number of aspects per article in the OASum dataset, and what percentage of articles have less than 9 aspects?", "explanation_reference": "The question specifically asks for detailed statistics regarding the distribution of aspects per article within the OASum dataset. The answer directly addresses this by providing the average number of aspects per article and the percentage of articles with fewer than 9 aspects, which are key details extracted from the 'Data Statistics and Analysis' section of the paper.", "evidence_reference": "In \\cref{tab:big_table}, we compare \\textbf{\\DATANAME} with other query/aspect-based summarization datasets. \\textbf{\\DATANAME} contains a significantly larger amount of aspect types. On average, there are 1.82 aspects per article and 99% articles have less than 9 aspects per single document."}
{"question": "Consider the paper that introduces the model that performs the best on the BBBP dataset. What specific finetuning strategy did the authors find to yield slightly better results for MoleculeNet tasks, and how is it characterized?", "answer": "finetuning the tags only", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model perform the best in the BBBP dataset?", "answer_anchor": "MolXPT", "question_reference": "What specific finetuning strategy did the authors find to yield slightly better results for MoleculeNet tasks, and how is it characterized?", "explanation_reference": "The authors explored two finetuning strategies for MoleculeNet tasks: finetuning the full prompts and finetuning the tags only. They found that finetuning the tags only yielded slightly better results, characterized by focusing the finetuning process on the classification tags at the end of the prompts rather than the entire prompt sequence.", "evidence_reference": "According to our exploration, Eqn.(\\ref{eq:finetune_label_only}) achieves slightly better results and we use it for all tasks (see Appendix \\ref{sec:moleculenet_detailed_result} for the results)."}
{"question": "Consider the paper that introduces the method in the table that corresponds to the highest ROUGE 2 score. What specific performance improvement, in terms of ROUGE-1 score, does the integration of normalizing flow into the PEGASUS+NTM model achieve over using a VAE-based neural topic model without gating on the XSum test set?", "answer": "0.4", "figure": "locality/2311.00588/comparison_table.png", "anchor_arxiv_id": "2311.00588", "reference_arxiv_id": "2109.10616", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method in the table that demonstrates the highest ROUGE 2 score?", "answer_anchor": "PEGASUS+NTM", "question_reference": "What specific performance improvement (in terms of ROUGE-1 score) does the integration of normalizing flow into the neural topic model achieve over using a VAE-based neural topic model without gating on the XSum test set?", "explanation_reference": "The integration of normalizing flow into the neural topic model achieves a specific performance improvement of 0.4 in terms of ROUGE-1 score over using a VAE-based neural topic model without gating on the XSum test set, as indicated by the comparison of ROUGE-1 scores between the model configurations in the ablation study.", "evidence_reference": "without the normalizing flow, the improvement that the latent vector brings is downgraded, nearly 0.4 of ROUGE-1 for using contextualized gating and 0.53 of ROUGE-1 in non-gating case."}
{"question": "Consider the paper that introduces the method that corresponds to a higher F1 score than that of LDSGM but a lower F1 score than 65.76 for PDTB-Top. What is the main reason the model proposed in the paper performs worse on all four top-level senses of the PDTB, especially on the Temporal sense?", "answer": "The main reason the PIDRP method performs worse than the PCP method on all four top-level senses of the PDTB, especially on the Temporal sense, is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which is the method has a higher F1 score than LDSGM but lower F1 score than 65.76", "answer_anchor": "PCP", "question_reference": "What is the main reason the PIDRP method performs worse than the PCP method on all four top-level senses of the PDTB, especially on the Temporal sense?", "explanation_reference": "The paper suggests that the PIDRP method's poorer performance compared to the PCP method across all top-level senses, particularly in the Temporal sense, is due to the fact that connective prediction aligns more closely with the natural language patterns utilized during the model's pre-training stage, as opposed to direct implicit discourse relation prediction.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the method which has a perplexity of 60. What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during the model's-guided generation?", "answer": "n=m", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having 60 perplexity?", "answer_anchor": "GeDi", "question_reference": "What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during GeDi-guided generation?", "explanation_reference": "The minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) is defined by the condition where \\(n=m\\). This is because \\(\\gV_m\\) is defined as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\).", "evidence_reference": "We define \\(\\gV_m\\) as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\)."}
{"question": "Consider the paper that introduces the transformer-based method that achieves the highest MRR score on the FB15kET dataset. What specific aspect of the model proposed in the paper allows for the differentiated integration of neighbor content while preserving the graph structure?", "answer": "The context transformer integrates neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure.", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which transformer-based method gets the highest MRR score in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What specific aspect of the TET approach allows for the differentiated integration of neighbour content while preserving the graph structure?", "explanation_reference": "The context transformer is specifically designed to integrate neighbours' content in a differentiated way through information exchange between neighbour pairs, which directly addresses the question's focus on preserving the graph structure while integrating neighbour content.", "evidence_reference": "a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
{"question": "Consider the paper that introduces the model that achieves the highest score in the 'T2' column. What specific hyperparameter values were used for the FewRel dataset in its experiments, and how do these values compare to those used for the TACRED dataset?", "answer": "For FewRel, the values were \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, the values were \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model demonstrates the highest score in 'T2' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were used for the FewRel dataset in the experiments, and how do these values compare to those used for the TACRED dataset?", "explanation_reference": "The specific hyperparameter values for the FewRel and TACRED datasets are directly provided in the Implementation Details section of the paper, allowing for a direct comparison between the two sets of values.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7."}
{"question": "Consider the paper that introduces the optimization method that has a BLEU score of 27.3. What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "answer": "The attention heads exhibit behavior that seems related to the structure of the sentence, such as attending to a distant dependency of the verb 'making', completing the phrase 'making...more difficult', and apparently involved in anaphora resolution.", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What optimization method shows BLEU score of 27.3?", "answer_anchor": "Transformer base", "question_reference": "What specific behavior related to the structure of the sentence do the attention heads from the encoder self-attention at layer 5 of 6 exhibit, as shown in the attention visualizations?", "explanation_reference": "The answer is directly supported by the descriptions provided in the Attention Visualizations section, where it is mentioned that many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difficult', and that two attention heads, also in layer 5 of 6, are apparently involved in anaphora resolution. These behaviors indicate that the attention heads have learned to perform tasks related to the structural aspects of sentences.", "evidence_reference": "An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb `making', completing the phrase `making...more difficult'.  Attentions here shown only for the word `making'. Different colors represent different heads. Best viewed in color. Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word `its' for attention heads 5 and 6. Note that the attentions are very sharp for this word."}
{"question": "Consider the paper that introduces the method that has a lower J_k score than Random Entity Quantization but a higher J_k score than NodePiece in the FB15k-237 dataset for all values of k between 400 and 1000. How is the initial representation for non-reserved entities generated in the model's entity-agnostic encoding process before being input into the GNN?", "answer": "For non-reserved entities, their initial representation before being input into the GNN is generated by combining the encoded connected relation information (\\(\\mathbf{h}_{e}^{\\rm c}\\)) and the encoded \\(k\\)-nearest reserved entity information (\\(\\mathbf{h}_{e}^{\\rm k}\\)) through a 2-layer MLP (\\(f_{m}\\)), as described in Equation (\\ref{eq:info-combine}).", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method shows the lower J_k score than Random Entity Quantization but higher J_k score than NodePiece for k in the range of [400, 1000] in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "In the EARL model's entity-agnostic encoding process, how is the initial representation for non-reserved entities generated before being input into the GNN?", "explanation_reference": "The initial representation for non-reserved entities in the EARL model is generated by first encoding the ConRel and $k$NResEnt information separately and then combining these two encoded pieces of information. This combination is achieved through a concatenation followed by a transformation using a 2-layer MLP, as described in the Entity-Agnostic Encoding section.", "evidence_reference": "In our GNN framework, similar to previous works \\cite{CompGCN, MaKEr}, we use a linear transformation on the concatenation of entity and relation representations to aggregate the neighbor information. Specifically, the message aggregation for the entity $e$ is: \\begin{equation} \\begin{aligned} \\mathbf{m}_{e}^{l} = \\sum_{(r, t) \\in \\mathcal{O}(e)} \\mathbf{W}_{\\text{out}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_t] + \\sum_{(r,h) \\in \\mathcal{I}(e)} \\mathbf{W}_{\\text{in}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_h], \\label{eq:gnn-agg} \\end{aligned} \\end{equation} where $\\mathcal{O}(e)$ denotes the out-going relation-entity pair set of $e$ and $\\mathcal{I}(e)$ denotes the in-going relation-entity pair set. $\\mathbf{W}_{\\text{out}}^l$ and $\\mathbf{W}_{\\text{in}}^l$ are transformation matrices for out-going and in-going pairs. $l \\in [0, \\dots, L]$ denotes the layer of GNN and $L$ is the total number of GNN layers. The input entity representations are calculated in Equation (\\ref{eq:info-combine}), and the input relation representations (e.g., $\\mathbf{h}_{r}^{0}$) are looked up in a trainable relation embedding matrix $\\mathbf{R} \\in \\mathbb{R}^{|\\mathcal{R}|\\times d}$.  The entity representation of $e$ in the GNN is updated as follows: \\begin{equation} \\mathbf{h}_{e}^{l+1} = \\sigma \\left( \\frac{1}{c}\\mathbf{m}_{e}^{l} + \\mathbf{W}_{\\text{self}}^{l} \\mathbf{h}_{e}^{l} \\right), \\label{eq:gnn-update} \\end{equation} where $c=|\\mathcal{I}(e)+\\mathcal{O}(e)|$ is a normalization constant. $\\mathbf{W}_{\\rm self}^{l}$ is a matrix for self representation update, and $\\sigma$ is an activation function. Furthermore, relation representations will also be updated in each layer: $\\mathbf{h}_{r}^{l+1} = \\sigma \\left( \\mathbf{W}_{\\text{rel}}^{l} \\mathbf{h}_{r}^{l} \\right)$. We use the output representations in the $L$-th layer for entities and relations as their embeddings to calculate scores next."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than BROS and a higher F1 score than LayoutXLM. What is the F1 score improvement percentage when applying multi-task learning (MTL) with entity labeling and relation extraction tasks on the FUNSD dataset?", "answer": "0.86%", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having lower F1 score than BROS and higher F1 score than LayoutXLM?", "answer_anchor": "SERA", "question_reference": "What is the F1 score improvement percentage when applying multi-task learning (MTL) with entity labeling and relation extraction tasks on the FUNSD dataset?", "explanation_reference": "The improvement percentage is calculated based on the F1 score improvement mentioned in the training strategies section, where it states that relation extraction task can improve by about 0.86% F1 while labeling model performance drops a little.", "evidence_reference": "Relation extraction task can improve by about 0.86\\% F1 while labeling model performance drops a little from Table~\\ref{trainstrategy}."}
{"question": "Consider the paper that introduces the method that has three empty entries in the table for the mathematical reasoning and commonsense reasoning tasks. What specific modification to the few-shot prompts used in the model's, proposed by the paper, CoT generation is highlighted as a key factor for improving the quality of generated data?", "answer": "Providing the model with the target after posing the question and before providing example CoT.", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has three empty entries in mathematical reasoning and commonsense reasoning tasks?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as a key factor for improving the quality of generated data?", "explanation_reference": "The paper specifies that making a key modification to the few-shot prompts by providing the target answer after the question and before the example CoTs allows LLMs to correct small mistakes in the CoT, which is crucial for generating high-quality data for knowledge distillation.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the method that has 638K tunable parameters. What is the dimension of the task embedding (I_\\tau) used in the experiments?", "answer": "64", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has 638K tunable parameters?", "answer_anchor": "Hyperformer", "question_reference": "What is the dimension of the task embedding (I_\\tau)used in the experiments?", "explanation_reference": "The dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) is explicitly stated as part of the experimental setup, indicating the specific size used for encoding task features.", "evidence_reference": "We set the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) to \\(t'=512\\), and the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) to \\(t=64\\)."}
{"question": "Consider the paper that introduces the method which is directly above the dashed line in few-shot prompting. What specific improvement in percentage points did the model proposed in the paper achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "answer": "6 and 6.2 points", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method is shown right above the dashed line in few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific improvement in percentage points did the generative models achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "explanation_reference": "The improvement is highlighted in the comparison between generative and discriminative models for the VQA task, specifically on the out-of-domain subset. The generative models (\\ourst{} and \\oursb{}) improved upon the discriminative baselines by 6 and 6.2 percentage points respectively, demonstrating the effectiveness of using generative modeling for questions with answers not included in the top-K answer candidates.", "evidence_reference": "This improvement is more significant on the out-of-domain subset, where the generative \\ourst{} and \\oursb{} achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling."}
{"question": "Consider the paper that introduces the dataset with the largest number of dialogues. What specific aspect of the conversation goal completion rate significantly demonstrates the effectiveness of the knowledge posterior/prior distribution learning in the model proposed by the paper?", "answer": "more knowledge to achieve the conversation goal  (much higher rate on score '2')", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset has the most number of dialogues?", "answer_anchor": "DuConv", "question_reference": "What specific aspect of the conversation goal completion rate significantly demonstrates the effectiveness of the knowledge posterior/prior distribution learning in the proposed generation model?", "explanation_reference": "The question focuses on a detailed aspect of the conversation goal completion rate that highlights the effectiveness of the knowledge posterior/prior distribution learning in the proposed generation model. The answer directly addresses this by pointing out the significantly higher rate of achieving score '2' in goal completion, which indicates a more engaging and coherent conversation due to effective knowledge exploitation.", "evidence_reference": "From this table, it can be seen that our proposed generation model can exploit more knowledge to achieve the conversation goal  (much higher rate on score '2'), making the conversation more engaging and coherent."}
{"question": "Consider the paper that introduces the method that has 638K tunable parameters. How does the model proposed in the paper ensure parameter efficiency while enabling task-specific adaptation in the context of multi-task learning?", "answer": "The \\methodefficient model ensures parameter efficiency while enabling task-specific adaptation in the context of multi-task learning by employing a compact hypernetwork shared across tasks and layers. This hypernetwork learns to generate task and layer-specific adapter parameters, conditioned on task and layer id embeddings. The shared hypernetwork captures information across tasks, enabling positive transfer between related domains and transferable tasks, while adapters reduce negative interference by encapsulating task-specific information. For each new task, the model only requires learning an additional task embedding, significantly reducing the number of trained parameters.", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has 638K tunable parameters?", "answer_anchor": "Hyperformer", "question_reference": "How does the \\methodefficient model ensure parameter efficiency while enabling task-specific adaptation in the context of multi-task learning?", "explanation_reference": "The \\methodefficient model achieves parameter efficiency through the use of shared hypernetworks across tasks and layers, which generate task-specific adapter parameters. This approach allows for knowledge sharing across tasks while enabling task-specific adaptation through the generation of task, layer id, and adapter position conditioned parameters. This method balances the need for a compact model with the flexibility required for task-specific adjustments.", "evidence_reference": "Our approach learns a task feature embedding per task, consisting of $Tt$ parameters. We additionally employ layer id and adapter position embeddings in the encoder and decoder, which require $2(2+L)t$ parameters, with a fixed embedding size of $t$ for all these feature embeddings. We consider a separate task projector networks $h'_I$ for encoder and decoder, which is in both cases a two-layer MLP, consisting of a total of $2(3te+et)$ parameters, where $e=128$ is the hidden dimension for the task-projector network. Our hypernetwork for adapters in encoder/decoder consists of $2(2thd)$ parameters and our layer normalization hypernetwork consists of $2(2th)$ parameters. In total, this results in $\\underbrace{t(T+4+2L)}_{\\text{Task features}}+\\underbrace{8te+2t(2hd+2h)}_\\text{Hypernetworks}$ parameters."}
{"question": "Consider the paper that introduces the method at the rightmost part of the figure. What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during the generation guided by this method?", "answer": "n=m", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method on the right most coordinates of the figure?", "answer_anchor": "GeDi", "question_reference": "What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during GeDi-guided generation?", "explanation_reference": "The minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) is defined by the condition where \\(n=m\\). This is because \\(\\gV_m\\) is defined as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\).", "evidence_reference": "We define \\(\\gV_m\\) as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\)."}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. What is the primary reason for the performance drop in the model proposed in the paper when the hyperparameter gamma is set to 1?", "answer": "The primary reason for the performance drop when the hyperparameter gamma is set to 1 in the CSN model is that it results in no document content being selected, effectively degenerating the model to non-document-grounded response selection, which sharply decreases performance.", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "CSN-word", "question_reference": "What is the primary reason for the performance drop when the hyperparameter gamma is set to 1 in the CSN model?", "explanation_reference": "Setting gamma to 1 results in no document content being selected for response matching, effectively degenerating the model to non-document-grounded response selection, which significantly reduces its performance.", "evidence_reference": "On the other hand, when  $\\gamma=1$, \\ie, no document content is selected, it degenerates to non document-grounded response selection and the performance also drops sharply."}
{"question": "Consider the paper that introduces the method that has the lowest MAE in the CH-SIMS task. Which specific aspect of spoken language dynamics does the Spoken Language Embedding Subnetwork in the model proposed by the paper focus on to handle the volatile nature of spoken opinions based on the critical analysis of its approach to multimodal sentiment analysis?", "answer": "The Spoken Language Embedding Subnetwork focuses on building models that are capable of operating in the presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech.", "figure": "locality/2310.05804/result_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the highest MAE in CH-SIMS task?", "answer_anchor": "Tensor Fusion", "question_reference": "Based on the critical analysis of the Tensor Fusion Network's approach to multimodal sentiment analysis, what specific aspect of spoken language dynamics does the Spoken Language Embedding Subnetwork focus on to handle the volatile nature of spoken opinions?", "explanation_reference": "The Spoken Language Embedding Subnetwork is designed to handle the volatile nature of spoken language by building models capable of operating in the presence of unreliable and idiosyncratic speech traits. It achieves this by focusing on important parts of speech, which allows the model to maintain the relevance of the utterance's meaning even when encountering unusable information or recovering usable information later in the speech.", "evidence_reference": "The first part conveys the actual message and the rest is speaker thinking out loud eventually agreeing with the first part. The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech."}
{"question": "Consider the paper that introduces the method that consistently achieves a higher MRR score than NodePiece. What is the primary reason for the performance variance in the model proposed by the paper's ablation studies across different datasets?", "answer": "The primary reason for the performance variance in EARL's ablation studies across different datasets is the different characteristics of the datasets, particularly the number of relations. Datasets with more relations (e.g., FB15k-237 and CoDEx-L) provide enough distinguishable information for entity embeddings through connected relation information (ConRel), making the performance less affected by the removal of other components like reserved entities or $k$NResEnt. In contrast, datasets with fewer relations (e.g., WN18RR and YAGO3-10) rely more on the distinguishable information provided by reserved entities and $k$NResEnt, showing more significant performance variance in the ablation studies.", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method consistently shows higher MRR than NodePiece?", "answer_anchor": "EARL", "question_reference": "What is the primary reason for the performance variance in EARL's ablation studies across different datasets?", "explanation_reference": "The variance in performance across different datasets in the ablation studies of EARL is attributed to the data statistics of these datasets, particularly the number of relations they contain. Datasets with more relations provide sufficient distinguishable information for entity embeddings, which influences the impact of removing certain components (e.g., Reserved Entity, ConRel, $k$NResEnt) on the model's performance.", "evidence_reference": "FB15k-237 and CoDEx-L have more relations than WN18RR and YAGO3-10, and diverse relations provide enough distinguishable information for entity embeddings. Thus, even in the 'w/o Reserved Entity' and 'w/o $k$NResEnt', performance is not affected dramatically since ConRel information still exists."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. What specific performance improvement does the model proposed in the paper achieve on the GSM8K task when used with PaLM-540B, compared to chain-of-thought prompting?", "answer": "+17.9%", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "What specific performance improvement does self-consistency achieve on the GSM8K task when used with PaLM-540B, compared to chain-of-thought prompting?", "explanation_reference": "The question targets a detailed comparison of performance improvements achieved by the self-consistency method over the chain-of-thought prompting method on a specific arithmetic reasoning task (GSM8K) using a specific language model (PaLM-540B). This detail requires understanding of the empirical results presented in the paper, specifically focusing on the exact numerical improvement in accuracy.", "evidence_reference": "Self-consistency & 93.7 {\\scriptsize(+1.8)} & 99.3 {\\scriptsize(+4.6)} & 81.9 {\\scriptsize(+7.9)} & 48.3 {\\scriptsize(+12.5)} & 86.6 {\\scriptsize(+7.6)} & 74.4 {\\scriptsize(+17.9)}"}
{"question": "Consider the paper that introduces the model that has the highest accuracy in the COGS-all dataset. What specific architectural change was made to the Transformer's layer normalization in the model proposed in the paper compared to its originally proposed form?", "answer": "a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model demonstrates the highest accuracy in COGS-all dataset?", "answer_anchor": "T5", "question_reference": "What specific architectural change was made to the Transformer's layer normalization in the T5 model compared to its originally proposed form?", "explanation_reference": "The T5 model made specific modifications to the original Transformer architecture's layer normalization. Specifically, it simplified layer normalization by placing it outside the residual path and removing the additive bias. This change is orthogonal to the experimental factors considered in the empirical survey of transfer learning.", "evidence_reference": "Our encoder-decoder Transformer implementation closely follows its originally-proposed form \\citep{vaswani2017attention}. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of ``blocks'', each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization \\citep{ba2016layer} is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection \\citep{he2016deep} adds each subcomponent's input to its output."}
{"question": "Consider the paper that introduces the model in the figure that has the lowest diversity score for each of p1, p2, p3, and p4. What specific architectural change was made to the Transformer model in its framework to potentially improve computational efficiency during unsupervised pre-training?", "answer": "using objectives that produce short target sequences", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model demonstrates the lowest diversity score in p1, p2, p3, and p4?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change was made to the Transformer model in the T5 framework to potentially improve computational efficiency during unsupervised pre-training?", "explanation_reference": "The specific architectural change made to improve computational efficiency during unsupervised pre-training in the T5 framework was replacing entire spans of corrupted tokens with a single token. This approach is mentioned as part of the unsupervised objectives exploration, where it is noted that this method produces shorter target sequences, potentially making unsupervised pre-training more computationally efficient.", "evidence_reference": "We found that most ``denoising'' objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-to-text setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient."}
{"question": "Consider the paper that introduces the method shown in the first row of the table. Which specific mathematical reasoning capability does the TabMWP dataset aim to assess in machines, as highlighted by the challenges presented in its design?", "answer": "multi-hop mathematical reasoning over heterogeneous information", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is shown in the first row of the table?", "answer_anchor": "PromptPG", "question_reference": "What specific mathematical reasoning capability does the TabMWP dataset aim to assess in machines, as highlighted by the challenges presented in its design?", "explanation_reference": "The TabMWP dataset is designed to assess machines' ability to perform multi-hop mathematical reasoning over heterogeneous information, which involves looking up table cells given textual clues and conducting multi-step operations to predict the final answer. This capability is highlighted as a core challenge presented by the dataset's design, requiring systems to align and reason over both textual and tabular data.", "evidence_reference": "To solve problems in \\data, a system requires multi-hop mathematical reasoning over heterogeneous information by looking up table cells given textual clues and conducting multi-step operations to predict the final answer."}
{"question": "Consider the paper that introduces the dataset with the largest number of dialogues. What specific methodological approach does the generation-based model, proposed by the paper, employ to mimic human knowledge selection in dialogue response generation?", "answer": "The generation-based model employs a methodological approach that includes an extra knowledge selection paradigm enhanced by a knowledge-aware generator. This generator uses a combination of a prior distribution and a posterior distribution to mimic human knowledge selection. The prior distribution is based on the dialogue context and the dialogue goal, while the posterior distribution also considers the response. The model minimizes the Kullback-Leibler divergence between these two distributions to mimic human knowledge selection.", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset has the most number of dialogues?", "answer_anchor": "DuConv", "question_reference": "What specific methodological approach does the generation-based model employ to mimic human knowledge selection in dialogue response generation?", "explanation_reference": "The generation-based model uses a methodological approach of minimizing the Kullback-Leibler Divergence (KLDivLoss) between two distributions: the prior distribution (knowledge reasoned by machines) and the posterior distribution (knowledge reasoned by humans). This approach is employed to force the model to mimic human knowledge selection during dialogue response generation.", "evidence_reference": "we introduce two different distributions: 1) the \\emph{prior distribution} $p(k_i | x)$ and the \\emph{posterior distribution} $p(k_i | x, y)$. We take the prior distribution $p(k_i | x)$ as the knowledge reasoned by machines and the posterior distribution $p(k_i | x, y)$ as the knowledge reasoned by humans, and then force the machine to mimic human by minimizing the KLDivLoss between those two distributions"}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Internet-enhanced category. What specific performance improvement does the model proposed in the paper show over the simpler model across all datasets, and how is this improvement quantitatively described in the paper?", "answer": "Consistent outperformance", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.05115", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the first method shown in Explicit --> Internet-enhanced", "answer_anchor": "Internet-Fewshot", "question_reference": "What specific performance improvement does the \\obsearchpoe\\~ model show over the simpler \\obsearch\\~ model across all datasets, and how is this improvement quantitatively described in the paper?", "explanation_reference": "The paper quantitatively describes the performance improvement of the \\obsearchpoe\\~ model over the simpler \\obsearch\\~ model as a consistent outperformance across all datasets. This indicates that the probabilistic reranking method employed by the \\obsearchpoe\\~ model, which combines multiple probabilities including the notion of paragraph 'goodness', leads to better overall performance in open-domain question answering tasks when compared to the basic approach of conditioning on Google Search results used by the \\obsearch\\~ model.", "evidence_reference": "Finally, our probabilistic reranking further improves performance: across all datasets,  ~\\obsearchpoe\\~ outperforms consistently the simpler \\obsearch\\~."}
{"question": "Consider the paper that introduces the model represented by a blue line in the figure. What specific methodological approach did DialoGPT Large employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "answer": "Maximum Mutual Information (MMI) scoring function", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model represented with a blue line in the figure?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological approach did DialoGPT employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "explanation_reference": "The paper describes the implementation of a maximum mutual information (MMI) scoring function to specifically address the problem of generating bland, uninformative samples in open-domain text generation models. This method penalizes bland hypotheses by maximizing the backward model likelihood, which is a direct response to the challenge mentioned.", "evidence_reference": "To address this problem, we implement a maximum mutual information (MMI) scoring function~\\cite{li2015diversity, zhang2018generating}. MMI employs a pre-trained \\textit{backward} model to predict source sentences from given responses, i.e., $P(\\\\text{Source}|\\\\text{target})$."}
{"question": "Consider the paper that introduces the method that has the third highest Avg score on the GLUE task. How does the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) in the model proposed in the paper compare to the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\))?", "answer": "The dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) is \\(t'=512\\), and the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) is \\(t=64\\).", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the second highest Avg score on GLUE task?", "answer_anchor": "Hyperformer", "question_reference": "How does the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) compare to the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) in the proposed \\methodefficient model?", "explanation_reference": "The dimensions are specified in the Experimental Details section, indicating that the task feature embedding (\\(\\bm{z_{\\tau}}\\)) is larger (512) compared to the task embedding (\\(\\bm{I_{\\tau}}\\)) which is smaller (64).", "evidence_reference": "We set the dimension of the task feature embedding (\\(\\bm{z_{\\tau}}\\)) to \\(t'=512\\), and the dimension of the task embedding (\\(\\bm{I_{\\tau}}\\)) to \\(t=64\\)."}
{"question": "Consider the paper that introduces the model that achieves a higher TP score than GIT but a lower TP score than LLaVA. What specific architectural feature of the model proposed in the paper allows it to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "answer": "Deformable DETR-based detector", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which models shows higher TP score than GIT but lower TP score than LLaVA?", "answer_anchor": "GRIT", "question_reference": "What specific architectural feature of GRIT allows it to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "explanation_reference": "GRIT employs a Deformable DETR-based detector for extracting region features, which inherently does not require class-aware NMS (Non-Maximum Suppression) and RoI (Region of Interest) Align operations that are typically necessary in CNN-based detectors like Faster R-CNN. This architectural choice significantly reduces the computational time for feature extraction.", "evidence_reference": "On the other hand, we employ a Deformable DETR-based detector to extract region features without using all such operations. Table \\ref{tab:extraction} shows the comparison on feature extraction."}
{"question": "Consider the paper that introduces the method that has an accuracy of 78.1% on the VQA-v2 task. How does the model's performance on the RefCOCO+ testA set compare to the previous state-of-the-art model UNICORN in terms of improvement points?", "answer": "6.65", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method shows 78.1 Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "How does OFA's performance on the RefCOCO+ testA set compare to the previous state-of-the-art model UNICORN in terms of improvement points?", "explanation_reference": "The question specifically asks for the improvement in performance points of OFA over the previous state-of-the-art model UNICORN on a particular dataset (RefCOCO+ testA). The answer directly reflects the numerical improvement in performance, which is a detail that can be extracted from the comparison of results between OFA and UNICORN.", "evidence_reference": "Compared with the previous SOTA UNICORN, OFA achieves significant improvement with a gain of 3.61, 6.65 and 4.85 points on the testA sets of RefCOCO and RefCOCO+ as well as the test-u set of RefCOCOg."}
{"question": "Consider the paper that introduces the model shown on the penultimate line of the table. What is the exact improvement in ROC-AUC score for the ClinTox dataset achieved by the model proposed in the paper over GEM?", "answer": "5.2", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shown on the penult line?", "answer_anchor": "MolXPT", "question_reference": "What is the exact improvement in ROC-AUC score for the ClinTox dataset achieved by MolXPT over GEM?", "explanation_reference": "The improvement can be calculated by subtracting the ROC-AUC score of GEM on the ClinTox dataset from that of MolXPT. GEM achieved a score of 90.1, while MolXPT achieved 95.3. Therefore, the improvement is 95.3 - 90.1 = 5.2.", "evidence_reference": "GEM & $72.4\\pm0.4$ & \\textbf{78.1 $\\pm$ 0.1} & $90.1\\pm1.3$ & \\textbf{80.6 $\\pm$ 0.9} & $85.6\\pm1.1$ & $67.2\\pm0.4$ & $79.0$ \\\\ \\hline \\ourM{} & \\textbf{80.0 $\\pm$ 0.5} &  $77.1\\pm0.2$  & \\textbf{95.3 $\\pm$ 0.2} & $78.1\\pm0.4$ & \\textbf{88.4 $\\pm$ 1.0} & \\textbf{71.7 $\\pm$ 0.2} & \\textbf{81.9} \\\\"}
{"question": "Consider the paper that introduces the method which exhibits a score of 34.9 in the Acc-7 metric on MOSI. What specific feature of spoken language does the Spoken Language Embedding Subnetwork in the model focus on to handle the volatile nature of spoken opinions?", "answer": "focusing on important parts of speech", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method that shows 34.7 score in Acc-7 metric on MOSI?", "answer_anchor": "TFN", "question_reference": "What specific feature of spoken language does the Spoken Language Embedding Subnetwork in the Tensor Fusion Network model focus on to handle the volatile nature of spoken opinions?", "explanation_reference": "The Spoken Language Embedding Subnetwork is designed to focus on important parts of speech to effectively deal with the volatile nature of spoken language, which often lacks proper structure and includes idiosyncratic speech traits.", "evidence_reference": "The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech."}
{"question": "Consider the paper that introduces the method that has an accuracy of 82.82% in the CAIL2018 task. What specific methodological weakness does the model proposed in the paper address in the context of learning attention vectors for semantically close law articles, as identified in the comparison with Luo et al.'s (2017) approach?", "answer": "Learning each law article's attention vector independently, which may result in similar attention vectors for semantically close law articles, hence ineffective in distinguishing confusing charges.", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shows 82.82 accuracy in CAIL2018 task?", "answer_anchor": "LADAN", "question_reference": "What specific methodological weakness does the LADAN model address in the context of learning attention vectors for semantically close law articles, as identified in the comparison with Luo et al.'s (2017) approach?", "explanation_reference": "The question targets a specific methodological weakness that the LADAN model aims to overcome, which is the challenge of distinguishing confusing charges when similar attention vectors are learned for semantically close law articles. This issue is directly addressed in the comparison with the approach by Luo et al. (2017), where the paper critiques the independent learning of each law article's attention vector, leading to the ineffectiveness in distinguishing confusing charges.", "evidence_reference": "Nevertheless, the weakness is that they learn each law article's attention vector independently, and this may result in that similar attention vectors are learned for semantically close law articles; hence, it is ineffective in distinguishing confusing charges."}
{"question": "Consider the paper that introduces the dataset represented by the smallest blue circle. What specific method did the authors find to be competitive or even superior to Language Adaptive Fine-Tuning (LAFT) for Nigerian languages, particularly for Nigerian Pidgin (pcm)?", "answer": "AfriBERTa", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset represented by the smallest blue label?", "answer_anchor": "NaijaSenti", "question_reference": "What specific method did the authors find to be competitive or even superior to Language Adaptive Fine-Tuning (LAFT) for Nigerian languages, particularly for Nigerian Pidgin (pcm)?", "explanation_reference": "The authors found AfriBERTa to be competitive or better than Language Adaptive Fine-Tuning (LAFT) for the Nigerian languages, particularly for Nigerian Pidgin (pcm), due to AfriBERTa's focus on African languages and its smaller model size, which facilitates easier deployment.", "evidence_reference": "Overall, we found AfriBERTa to be the best baseline model for all languages because the model is more African language-centric. The main advantage of AfriBERTa is its smaller model size, which makes it easier to deploy especially on the African continent where most research labs cannot afford powerful GPUs."}
{"question": "Consider the paper that introduces the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections category. What specific performance metric does the model proposed in the paper achieve on multi-hop questions in the {\\dscf} dataset when using GPT-3 as the base model, considering instances where all associated edited facts are successfully retrieved from memory?", "answer": "73.1%", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2305.14795", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections?", "answer_anchor": "MeLLo", "question_reference": "What specific performance metric does MeLLo achieve on multi-hop questions in the {\\dscf} dataset when using GPT-3 as the base model, considering instances where all associated edited facts are successfully retrieved from memory?", "explanation_reference": "The performance metric of 73.1% for MeLLo on multi-hop questions in the {\\dscf} dataset when using GPT-3 as the base model, considering instances where all associated edited facts are successfully retrieved from memory, indicates the effectiveness of MeLLo in leveraging the retrieval component to answer multi-hop questions accurately. This metric specifically highlights MeLLo's capability to utilize edited facts for answering complex questions, provided that the retrieval system efficiently fetches all relevant edited facts.", "evidence_reference": "Among those questions where all associated facts are successfully retrieved from memory, MeLLo can answer $73.1\\%$ of them correctly."}
{"question": "Consider the paper that introduces the method which exhibits the highest accuracy on the VQA-v2 task. How does the model's performance on the visual question answering (VQA) task compare when using images of resolution 480x480 versus 640x640 during finetuning?", "answer": "An improvement", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method shows the highest Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "How does the OFA model's performance on the visual question answering (VQA) task compare when using images of resolution 480x480 versus 640x640 during finetuning?", "explanation_reference": "The paper mentions that when finetuning OFA models, particularly the $\\text{OFA}\\rm_{Large}$ and $\\text{OFA}\\rm_{Huge}$, the image resolution is increased from 480x480 to 640x640. This detail implies that using a higher resolution during finetuning is expected to improve the model's performance on tasks like VQA, as higher resolution images provide more detailed visual information for the model to analyze.", "evidence_reference": "When finetuning $\\text{OFA}\\rm_{Large}$ and $\\text{OFA}\\rm_{Huge}$, we increase the image resolution from $480$ to $640$."}
{"question": "Consider the paper that introduces the model represented by the orange bar. What specific achievement does the Vanilla Transformer model demonstrate over RNN sequence-to-sequence models in terms of performance on English constituency parsing when trained solely on the WSJ training set?", "answer": "Outperforms the BerkeleyParser", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model is demonstrated by the yellow bar?", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the Transformer model's performance on English constituency parsing, what specific achievement does it demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "explanation_reference": "The paper highlights that, unlike RNN sequence-to-sequence models, the Transformer model outperforms the BerkeleyParser even when trained only on the WSJ training set of 40K sentences. This indicates the Transformer's superior ability to handle tasks with strong structural constraints and significantly longer outputs than inputs, even with limited training data.", "evidence_reference": "In contrast to RNN sequence-to-sequence models [KVparse15], the Transformer outperforms the BerkeleyParser [petrov-EtAl:2006:ACL] even when training only on the WSJ training set of 40K sentences."}
{"question": "Consider the paper that introduces the dataset located at the bottom left of the figure. What specific method did the authors identify as competitive or even superior to Language Adaptive Fine-Tuning (LAFT) for Nigerian languages, particularly for Nigerian Pidgin (pcm)?", "answer": "AfriBERTa", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset located on the bottom left of the figure?", "answer_anchor": "NaijaSenti", "question_reference": "What specific method did the authors find to be competitive or even superior to Language Adaptive Fine-Tuning (LAFT) for Nigerian languages, particularly for Nigerian Pidgin (pcm)?", "explanation_reference": "The authors found AfriBERTa to be competitive or better than Language Adaptive Fine-Tuning (LAFT) for the Nigerian languages, particularly for Nigerian Pidgin (pcm), due to AfriBERTa's focus on African languages and its smaller model size, which facilitates easier deployment.", "evidence_reference": "Overall, we found AfriBERTa to be the best baseline model for all languages because the model is more African language-centric. The main advantage of AfriBERTa is its smaller model size, which makes it easier to deploy especially on the African continent where most research labs cannot afford powerful GPUs."}
{"question": "Consider the paper that introduces the dataset which is shown in the second row of the table. What specific aspect of the conversation goal completion rate significantly demonstrates the effectiveness of the knowledge posterior/prior distribution learning in the model proposed by the paper?", "answer": "more knowledge to achieve the conversation goal  (much higher rate on score '2')", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset shown in the second row of the table?", "answer_anchor": "DuConv", "question_reference": "What specific aspect of the conversation goal completion rate significantly demonstrates the effectiveness of the knowledge posterior/prior distribution learning in the proposed generation model?", "explanation_reference": "The question focuses on a detailed aspect of the conversation goal completion rate that highlights the effectiveness of the knowledge posterior/prior distribution learning in the proposed generation model. The answer directly addresses this by pointing out the significantly higher rate of achieving score '2' in goal completion, which indicates a more engaging and coherent conversation due to effective knowledge exploitation.", "evidence_reference": "From this table, it can be seen that our proposed generation model can exploit more knowledge to achieve the conversation goal  (much higher rate on score '2'), making the conversation more engaging and coherent."}
{"question": "Consider the paper that introduces the model in the second-to-last row of the table. What mathematical modification is applied in its methodology of quantifying temporal degradation (TD) to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "answer": "-\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t)", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model does not show a decrease in accuracy from the figure?", "answer_anchor": "DPT", "question_reference": "In the methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "explanation_reference": "The modification ensures that the calculated value reflects performance deterioration in a consistent manner, increasing as performance worsens, irrespective of whether the misalignment is due to training data being from the past or future relative to the evaluation data.", "evidence_reference": "Let $S_{t' \\shortto t}$ indicate the performance a model trained on timestamp $t'$ data and evaluated on the timestamp $t$. Let $$ {D} (t' \\shortto{} t) = -\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t), $$  In other words, ${D} (t' \\shortto{} t)$ is a modified difference in performance between a aligned and misaligned models."}
{"question": "Consider the paper that introduces the dataset which has 1 SM task and 14 languages. What specific method was used to address the challenge of collecting tweets in those languages that share geographic locations but lack curated stopword lists, and how was this method validated?", "answer": "To address the challenge of collecting tweets in languages that share geographic locations but have no curated stopword lists, a combination of location-based and vocabulary-based collection strategies was used. This included the creation of stopword lists for some languages by ranking words based on their frequency across different domains and using a word co-occurrence-based approach to extract stopwords. Native speakers verified the generated lists before use. This approach was validated through the successful collection and annotation of tweets in 14 African languages, as demonstrated by the creation of the AfriSenti datasets.", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What dataset has 1 SM task and 14 languages?", "answer_anchor": "AfriSenti", "question_reference": "What specific method was used to address the challenge of collecting tweets in languages that share geographic locations but have no curated stopword lists, and how was it validated?", "explanation_reference": "The question focuses on the detailed methodology used to collect tweets for languages without curated stopword lists, specifically asking about the approach taken to address this challenge and the validation process. The answer directly addresses this by mentioning the specific method (word co-occurrence-based approach) and the validation process (verification by native speakers), which are both detailed in the paper.", "evidence_reference": "We also used a word co-occurrence-based approach to extract stopwords using text sources from different domains. We lower-cased and removed punctuation marks and numbers, constructed a co-occurrence graph, and filtered out the words that occurred most often. Native speakers verified the generated lists before use."}
{"question": "Consider the paper that introduces the model shown in the figure that corresponds to the green line. What mathematical modification is applied to the difference in performance between aligned and misaligned models in its methodology of quantifying temporal degradation (TD) to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "answer": "-\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t)", "figure": "locality/2310.10191/accuracy_figure.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model is shown in green line?", "answer_anchor": "DPT", "question_reference": "In the methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "explanation_reference": "The modification ensures that the calculated value reflects performance deterioration in a consistent manner, increasing as performance worsens, irrespective of whether the misalignment is due to training data being from the past or future relative to the evaluation data.", "evidence_reference": "Let $S_{t' \\shortto t}$ indicate the performance a model trained on timestamp $t'$ data and evaluated on the timestamp $t$. Let $$ {D} (t' \\shortto{} t) = -\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t), $$  In other words, ${D} (t' \\shortto{} t)$ is a modified difference in performance between a aligned and misaligned models."}
{"question": "Consider the paper that introduces the quant method that achieves a lower score than APQ-ViT but still scores higher than 76.0 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What is the specific range of values for R1 when applying twin uniform quantization to post-softmax activations of the model proposed in the paper?", "answer": "$[0,2^{k-1}\\Delta_{\\text{R1}}^{s})$", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the quant method shows lower score than APQ-ViT but higher than 76.0 on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific range of values for R1 when applying twin uniform quantization to post-softmax activations?", "explanation_reference": "The range for R1 when applying twin uniform quantization to post-softmax activations is defined to well quantify the values using a small scaling factor, Delta_{text{R1}}^{s}, ensuring that R1 covers values very close to zero which are predominant in post-softmax distributions. This range is specifically designed to address the unbalanced distribution of post-softmax values, where most values are close to zero, and a few large values are crucial for the attention mechanism.", "evidence_reference": "For values after softmax, the values in R1 = $[0,2^{k-1}\\Delta_{\\text{R1}}^{s})$ can be well quantified by using a small $\\Delta_{\\text{R1}}^{s}$. To avoid the effect of calibration dataset, we keeps $\\Delta_{\\text{R2}}^{s}$ fixed to $1/2^{k-1}$. Therefore, R2 = $[0,1]$ can cover the whole range, and large values can be well quantified in R2."}
{"question": "Consider the paper that introduces the method that corresponds to the orange line in the figure. What specific component of the model's architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "answer": "matching information propagation module", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method in the figure is demonstrated by the orange line?", "answer_anchor": "UniKGQA", "question_reference": "What specific component of UniKGQA's architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "explanation_reference": "The question targets a detailed aspect of the UniKGQA architecture, specifically asking for the component that handles the propagation of semantic matching information across the knowledge graph. The answer, 'Matching information propagation module,' directly addresses this by naming the specific part of the architecture designed for this purpose.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method in the figure represented by the 'x' (cross) marker. What is the core component of the method that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs)?", "answer": "matching information propagation module", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method in the figure is demonstrated by the 'x' (cross) marker?", "answer_anchor": "UniKGQA", "question_reference": "What is the core component of UniKGQA that enables the propagation of matching information along the directed edges on KGs?", "explanation_reference": "The core component that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs) in UniKGQA is the matching information propagation module. This module is crucial for the model's ability to effectively navigate and reason over the KG by leveraging the semantic relationships and structure within the graph.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the model that has an F1 score higher than PCP's but lower than DiscoPrompt's on PDTB-Top. How does the model's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact, and what does this indicate about the scoring function's effectiveness?", "answer": "Replacing the Local Hierarchy-aware Contrastive loss $\\mathcal{L}_{L}$ with the hard-label version $\\mathcal{L}_{L'}$ results in a notable performance drop, indicating the scoring function's effectiveness in considering more subtle semantic structures of the local hierarchy.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method with an F1 score higher than PCP but lower than DiscoPrompt?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF framework's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact, and what does this indicate about the scoring function's effectiveness?", "explanation_reference": "The question focuses on the comparison between the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ and its hard-label version $\\mathcal{L}_{L'}$, specifically asking about the performance impact of replacing $\\mathcal{L}_{L}$ with $\\mathcal{L}_{L'}$. The answer highlights that such a replacement leads to a significant decrease in performance, which underscores the importance of the scoring function used in $\\mathcal{L}_{L}$. This function accounts for the nuanced semantic structures within the local hierarchy, thereby contributing to the model's overall effectiveness.", "evidence_reference": "Secondly, we replace the Local Hierarchy-aware Contrastive loss $\\mathcal{L}_{L}$ (Equation (\\ref{equation: soft local})) with the hard-label version $\\mathcal{L}_{L'}$ (Equation (\\ref{equation: hard local})) and find that the performance drops notably."}
{"question": "Consider the paper that introduces the dataset associated with the task 'Hate Speech Spans Detection (HSSD)'. What is the F1-score improvement for the model proposed in the paper, specifically PhoBERT_Large, when additional clean comments are included in the dataset?", "answer": "0.0849", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset with task `Hate Speech Spans Detection (HSSD)`?", "answer_anchor": "ViHOS", "question_reference": "What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included in the dataset?", "explanation_reference": "The improvement in F1-score for the PhoBERT_Large model when additional clean comments are included is directly stated in the Experiments and Results section, indicating the performance enhancement due to the inclusion of additional clean comments.", "evidence_reference": "PhoBERT$_{Large}$ considerably outperforms other models in the dataset without additional clean data, achieving 0.6867 in F1-score. In addition, the best model trained on Full data is XLM-R$_{Large}$, which has an F1-score of 0.7770. We find that XLM-R$_{Large}$ increased by 0.1014 and PhoBERT$_{Large}$ increased by 0.0849."}
{"question": "Consider the paper that introduces the method that achieves an average EA score of 67.07 in the FinQA task. What is the highest EM score obtained in the ablation study on the effect of in-context example orders for GPT-3 on the NQ dataset using the model proposed in the paper?", "answer": "Reverse order", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets 67.07 EA score in FinQA task", "answer_anchor": "KATE", "question_reference": "In the ablation study on the effect of in-context example orders for GPT-3 on the NQ dataset using KATE$_{\\\\text{nli+sts-b}}$, which specific order of in-context examples yielded the highest EM score?", "explanation_reference": "The question focuses on the detailed part of the paper where the authors conducted an ablation study to understand how the order of in-context examples affects the performance of GPT-3. The highest EM score indicates the most effective order for arranging in-context examples to improve GPT-3's performance on the NQ dataset.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category. What is the specific reason for introducing random noises into the newly copied parameters during the width expansion process in this method?", "answer": "To break the symmetry after the replication and accelerate later pre-training.", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.06311", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Replay-based category?", "answer_anchor": "ELLE", "question_reference": "What is the specific reason for introducing random noises into the newly copied parameters during the width expansion process in ELLE?", "explanation_reference": "The introduction of random noises into the newly copied parameters during the width expansion process is aimed at breaking the symmetry that results from parameter replication. This symmetry breaking is crucial for accelerating the pre-training process, as it provides a better initialization for further optimization. Without this step, the replicated parameters would remain identical, potentially slowing down the learning process due to lack of diversity in the parameter space.", "evidence_reference": "Different from \\citet{chen2021bert2bert}, we additionally introduce random noises $\\bm{\\delta}_i$ into the newly copied parameters of $\\bm{W}'$ during initialization. These slight noises would break the symmetry after the replication and accelerate later pre-training."}
{"question": "Consider the paper that introduces the method that has a score of 73.6 in the CB dataset with 4-shot prompting. How does the model's performance with 12 source tasks compare to its performance with 6 source tasks on the MRQA and Others benchmarks?", "answer": "MPT with 12 source tasks outperforms MPT with 6 source tasks on the MRQA and Others benchmarks, achieving an average F1 of 72.6% and accuracy of 85.6% respectively, compared to 72.2% and 85.5% with 6 source tasks.", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having score of 73.6 in CB dataset with 4-shot prompting?", "answer_anchor": "MPT", "question_reference": "How does the performance of MPT with 12 source tasks compare to MPT with 6 source tasks on the MRQA and Others benchmarks?", "explanation_reference": "The comparison between MPT trained with 6 source tasks and MPT trained with 12 source tasks on the MRQA and Others benchmarks shows that incorporating additional diverse source tasks slightly improves the performance, indicating that MPT can effectively utilize cross-task knowledge from a larger set of source tasks for target adaptation.", "evidence_reference": "MPT (w/ 6 Source Tasks) & 72.0 & 75.8 & 77.2 & 63.7 & 72.2 & 56.5 & 96.4 & 95.5  & 93.5 & 85.5 \\n MPT (w/ 12 Source Tasks) & 72.1 & 76.4 & 77.9 & 64.0 & 72.6 & 56.6 & 96.8 & 95.9   & 92.9 & 85.6"}
{"question": "Consider the paper that introduces the model that has the highest Recall@7 score in the CamRest task. What specific token is used to represent a null query in cases where no query is required, such as greetings or thanks, within the system proposed by the paper?", "answer": "[NOTHING]", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model has the highest Recall@7 score in CamRest task?", "answer_anchor": "Q-TOD", "question_reference": "What specific token is used to represent a null query in cases where no query is required, such as greetings or thanks, within the Q-TOD system?", "explanation_reference": "The specific token used to represent a null query in the Q-TOD system, for instances where the dialogue does not require a query (e.g., greetings or thanks), is explicitly mentioned in the paper.", "evidence_reference": "In cases where no query is required, e.g. greetings or thanks, a special token \\texttt{[NOTHING]} is used to represent the null query at this turn."}
{"question": "Consider the paper that introduces the method that is missing a result for the WQ-M task in the table. How does the paraphrasing-based approach differ from the model proposed in the paper in handling instances with simple expressions, according to the limitations section?", "answer": "The paraphrasing-based approach focuses on words, while the proposed method focuses more on the structure of the sentences.", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method does not show a result for WQ M task?", "answer_anchor": "DSM", "question_reference": "How does the paraphrasing-based approach differ from the proposed method in handling instances with simple expressions, according to the limitations section?", "explanation_reference": "The distinction between the paraphrasing-based approach and the proposed method is highlighted in the limitations section, where it is mentioned that for instances with simple expressions, the paraphrasing-based method may perform better by focusing on words, whereas the proposed method concentrates more on sentence structure.", "evidence_reference": "For example, the ground truth is 'What religion in Australia that influenced Arthur Schopenhauer?', the paraphrasing-based approach generates 'What faith in Australia inspired Arthur Schopenhauer?'. Our method generates  'What is the religion in Australia that influenced  Arthur Schopenhauer? '. We observe that the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the method that is in the third row of the table. What specific approach does the model proposed in the paper employ to minimize the SQL annotation error rate, and how does it ensure the selection of semantically equivalent and efficient SQL as ground truth?", "answer": "The \\textsc{Bird} benchmark employs a double-blind annotation approach to minimize the SQL annotation error rate. This approach involves two independent SQL annotators generating SQLs for the same question without discussion. SQLs yielding identical results are collected, and those with discrepancies are reviewed by experts until a consensus is reached. This method dramatically reduces the SQL annotation error rate by ensuring a low probability for two skilled annotators to generate the same incorrect results when databases have large values. To ensure the selection of semantically equivalent and efficient SQL as ground truth, the more semantic-equivalent and efficient SQL selected by experts for each question is picked as ground truth SQL in \\textsc{Bird}. Additionally, external knowledge evidence sentences are recorded for each SQL if utilized, enhancing the model's comprehension of database values and promoting the generation of efficient SQL queries.", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is in the first row of the table?", "answer_anchor": "BIRD", "question_reference": "What specific approach does the \\textsc{Bird} benchmark employ to minimize the SQL annotation error rate, and how does it ensure the selection of semantically equivalent and efficient SQL as ground truth?", "explanation_reference": "The \\textsc{Bird} benchmark employs a double-blind annotation approach to minimize the SQL annotation error rate. This method involves two independent SQL annotators generating SQLs for the same question without discussion. SQLs yielding identical results are gathered, and any discrepancies are resolved by expert review until a consensus is reached. This process dramatically reduces the SQL annotation error rate by ensuring a very low probability for two skilled annotators to generate the same incorrect results when databases have large values. The more semantically equivalent and efficient SQL selected by experts for each question is picked as the ground truth SQL in \\textsc{Bird}.", "evidence_reference": "As shown in Figure \\ref{wkf} (b), we employ a double-blind approach \\citep{csqa2} for SQL annotation. This approach involves two independent SQL annotators who generate SQLs for the same question without discussion. The annotated SQLs are executed in databases, and those yielding identical results are gathered. Otherwise, the SQLs are checked with experts until a consensus is reached. Double-blind procedures can dramatically reduce the SQL annotation error rate, as there is a small probability for two skillful annotators to generate the same incorrect results when databases have large values. The more semantic-equivalent and efficient SQL selected by experts for each question is picked as ground truth SQL in \\textsc{Bird}, and the external knowledge evidence sentences are recorded for each SQL if utilized."}
{"question": "Consider the paper that discusses the dataset which has more dev set samples than UIT-VSMEC but fewer dev set samples than ViSpamReviews. What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "answer": "Non-diacritical marks comments", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset having more dev set samples than UIT-VSMEC but less dev set samples than ViSpamReviews?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "explanation_reference": "The paper discusses how non-diacritical marks comments can trick annotators into needing to re-read the text multiple times due to the absence of punctuation, diacritics, and the presence of ambiguous text that could be interpreted inappropriately in multiple ways. This specific linguistic phenomenon is highlighted as a challenge in understanding and annotating the dataset.", "evidence_reference": "Non-diacritical marks comments might trick annotators a little bit... there are some problems causing annotators to re-read multiple times as no punctuation, diacritic, and the text 'con ng' could be considered as 'crazy girl' or 'the type (of human)' and both of these meanings is inappropriate."}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 18.4 on the GSM8K dataset. What specific modification to the few-shot prompts used in the model's generation, as proposed in the paper, is highlighted as key for improving the quality of generated data?", "answer": "Providing the model with the target after posing the question and before providing example CoT.", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets 18.4 accuracy in GSM8K dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as key for improving the quality of generated data?", "explanation_reference": "The specific modification mentioned is crucial because it allows the large language models (LLMs) to correct small mistakes in the chain of thought (CoT), thereby improving the quality of the generated data for finetuning smaller models.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the method that achieves the lowest J_k scores in the WN18RR dataset. According to the ablation studies, which dataset experienced a significant performance degradation when multi-hop neighbor information was removed from the model proposed in the paper?", "answer": "WN18RR", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method shows the lowest J_k score in WN18RR dataset?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which dataset showed a significant performance degradation when multi-hop neighbor information was removed?", "explanation_reference": "The ablation study results indicate that removing multi-hop neighbor information ('w/o MulHop') dramatically affected the performance on the WN18RR dataset, as evidenced by the significant drop in performance metrics compared to other ablation settings.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the quant method that achieves a lower score than APQ-ViT but still scores higher than 76.0 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What specific advantage does its twin uniform quantization offer for post-softmax and post-GELU activation values in terms of hardware efficiency?", "answer": "It enables efficient processing on existing hardware devices including CPUs and GPUs by using a shift operation to align the two quantization ranges, avoiding the need for format transformation and extra FP32 multiplication and FP32 addition.", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the quant method shows lower score than APQ-ViT but higher than 76.0 on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What specific advantage does the twin uniform quantization offer for post-softmax and post-GELU activation values in terms of hardware efficiency?", "explanation_reference": "The twin uniform quantization is designed to efficiently process on existing hardware devices like CPUs and GPUs by using the shift operation, which avoids the need for format transformation and additional FP32 operations that are more computationally expensive.", "evidence_reference": "Our method uses the shift operation, avoiding the format transformation and extra FP32 multiplication and FP32 addition."}
{"question": "Consider the paper that introduces the method that achieves a higher EA score than Fixed set but a lower EA score than Diverse KATE in the FinQA task. How does the order of in-context examples influence the model's performance on the NQ dataset, and what is the observed effect when the examples are arranged in reverse order?", "answer": "The reverse order performs the best.", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets higher EA score than Fixed set but lower EA score than Diverse KATE in FinQA task?", "answer_anchor": "KATE", "question_reference": "How does the order of in-context examples influence the performance of KATE on the NQ dataset, and what is the observed effect when the examples are arranged in reverse order?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results on the NQ dataset revealed that arranging the examples in reverse order, where the most similar sentences are placed closer to the test example, yielded the best performance. This suggests that the proximity of similar sentences to the test example may help GPT-3 leverage the corresponding information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the model shown in the figure that is consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B. How does the model proposed in the paper's base version perform on the Asleep at the Keyboard security benchmark in terms of valid and insecure code generation when comparing completion and insertion formats?", "answer": "StarCoderBase has a higher rate of valid code generation in the insertion format (98.70%) compared to the completion format (85.50%), but a slightly lower rate of insecure code generation in the insertion format (35.87%) compared to the completion format (39.77%).", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shown in the figure consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B?", "answer_anchor": "StarCoder", "question_reference": "How does StarCoderBase's performance on the Asleep at the Keyboard security benchmark compare between completion and insertion formats in terms of valid and insecure code generation?", "explanation_reference": "The comparison between completion and insertion formats for StarCoderBase on the Asleep at the Keyboard security benchmark shows that the insertion format leads to a higher percentage of valid code generation and a slightly lower percentage of insecure code generation. This indicates that the insertion format may be more effective for generating secure and valid code.", "evidence_reference": "Completion & StarCoderBase         & 855/1000 (85.50\\%) & 340/855 (39.77\\%) \\\\ Insertion  & StarCoderBase         & 987/1000 (98.70\\%) & 354/987 (35.87\\%)"}
{"question": "Consider the paper that introduces the method represented by the blue line in the figure. Based on the ablation studies, which component's removal from the model proposed in the paper resulted in the most dramatic performance decrease on the WN18RR dataset?", "answer": "w/o MulHop", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method represented in the blue line from the figure?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which component's removal resulted in the most dramatic performance decrease on the WN18RR dataset?", "explanation_reference": "The ablation study results for the WN18RR dataset show that the removal of the MulHop component resulted in the most significant performance decrease. This indicates that multi-hop neighbor information is crucial for the model's performance on this dataset.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the model that corresponds to an F1 score of 65.76 on PDTB-Top. How does the model proposed in the paper's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact?", "answer": "The performance drops notably when replacing $\\mathcal{L}_{L}$ with $\\mathcal{L}_{L'}$.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method demonstrates 65.76 F1 score on PDTB-Top?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF model's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact?", "explanation_reference": "The notable drop in performance when replacing the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ with its hard-label version $\\mathcal{L}_{L'}$ demonstrates the effectiveness of the scoring function in $\\mathcal{L}_{L}$, which considers more subtle semantic structures of the local hierarchy.", "evidence_reference": "Secondly, we replace the Local Hierarchy-aware Contrastive loss $\\mathcal{L}_{L}$ (Equation (\\ref{equation: soft local})) with the hard-label version $\\mathcal{L}_{L'}$ (Equation (\\ref{equation: hard local})) and find that the performance drops notably."}
{"question": "Consider the paper that introduces the dataset which has a training set size of 8,844. What specific linguistic phenomenon can cause annotators to re-read multiple times due to issues such as lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "answer": "Non-diacritical marks comments", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset having training set size 8,844?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon, as detailed in the paper, can cause annotators to re-read multiple times due to issues like lack of punctuation, diacritics, and ambiguous text meaning, particularly when the text could be interpreted as inappropriate in multiple ways?", "explanation_reference": "The paper discusses how non-diacritical marks comments can trick annotators into needing to re-read the text multiple times due to the absence of punctuation, diacritics, and the presence of ambiguous text that could be interpreted inappropriately in multiple ways. This specific linguistic phenomenon is highlighted as a challenge in understanding and annotating the dataset.", "evidence_reference": "Non-diacritical marks comments might trick annotators a little bit... there are some problems causing annotators to re-read multiple times as no punctuation, diacritic, and the text 'con ng' could be considered as 'crazy girl' or 'the type (of human)' and both of these meanings is inappropriate."}
{"question": "Consider the paper that introduces the method that has a CoLA score equal to 55.9 on the GLUE task. What specific advantage does the model proposed in the paper offer for handling long-context out-of-domain datasets in MRQA compared to a simple linear classifier?", "answer": "The Hyperdecoder approach offers the specific advantage of generating unique decoder layers for every input into a model, which allows for more flexible adaptation to long-context out-of-domain datasets in MRQA by leveraging similarities between samples across datasets and avoiding potential interference within the same dataset.", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has CoLA score equal to 55.9 on GLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "What specific advantage does the Hyperdecoder approach offer for handling long-context out-of-domain datasets in MRQA compared to a simple linear classifier?", "explanation_reference": "The Hyperdecoder's ability to control the decoder to output specific labels when needed, and its flexibility to generate arbitrary text output for long-context out-of-domain datasets, distinguishes it from a simple linear classifier. This flexibility is crucial for multi-tasking and handling long-context documents where the model must switch between generating short set labels and arbitrary longer text.", "evidence_reference": "This is especially important for multi-tasking and long-context documents where the model must swap between generating short set labels and arbitrary longer text."}
{"question": "Consider the paper that introduces the method that achieves an F1 score of 87.63 in the Token (I-topo) category. Based on the ablation studies, what specific performance gain does the Span Boundary Objective (SBO) provide over span masking alone in coreference resolution on the development set?", "answer": "2.7% F1", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets F1 score 87.63 in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "Based on the ablation studies, what specific performance gain does the Span Boundary Objective (SBO) provide over span masking alone in coreference resolution on the development set?", "explanation_reference": "The question targets the detailed outcome of applying the Span Boundary Objective (SBO) in addition to span masking, specifically focusing on its impact on coreference resolution. It requires understanding the comparative analysis provided in the ablation studies section, which discusses the incremental benefits of SBO over just span masking.", "evidence_reference": "Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone."}
{"question": "Consider the paper that introduces the model on the last line of the Seq2Seq/Tree block of the table. What specific methodological limitation does the paper acknowledge regarding the model's ability to incorporate commonsense knowledge, and how does it suggest addressing this limitation in future work?", "answer": "The paper acknowledges the methodological limitation of the solver's inability to incorporate commonsense knowledge when such knowledge is not explicitly given in the problem description. It suggests addressing this limitation in future work by injecting commonsense knowledge into MWP solvers.", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model on the last line of the Seq2Seq/Tree block?", "answer_anchor": "Ana-CL", "question_reference": "What specific methodological limitation does the paper acknowledge regarding the solver's ability to incorporate commonsense knowledge, and how does it suggest addressing this limitation in future work?", "explanation_reference": "The paper acknowledges a limitation in the solver's ability to incorporate commonsense knowledge, which is essential for solving real-world MWP scenarios that require understanding of concepts like '1km = 1000m' or 'one day = 24 hours'. The suggested approach to address this limitation in future work is by injecting commonsense knowledge into MWP solvers, indicating a direction for enhancing the solver's capability to handle problems requiring such knowledge.", "evidence_reference": "As mentioned in \\cite{lin2020numersense,DBLP:journals/corr/abs-2107-13435}, MWP solving in the real-word scenario requires many commonsense knowledge, e.g., 1km = 1000m and one day = 24 hours. When these commonsense constants are not explicitly given in the problem description, our MWP solver has no chance to solve problems that require them. A future direction could be injecting commonsense knowledge into MWP solvers."}
{"question": "Consider the paper that introduces the dataset which has 1 language but 13 SM tasks. What specific linguistic phenomenon, as discussed in the paper, necessitates an understanding of social media for accurate interpretation, especially in its expression within the Chinese language?", "answer": "\u60a8\u8bf4\u7684\u90fd\u5bf9", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset has 1 language but 13 SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly, and how is it expressed in Chinese?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity, including irony, which can be difficult for models to interpret. It specifically mentions the Chinese phrase '\\u60a8\\u8bf4\\u7684\\u90fd\\u5bf9' as an example of irony that requires familiarity with social media context to be correctly understood as potentially ironic rather than a straightforward agreement.", "evidence_reference": "For example, on Chinese social media, a comment '\\u60a8\\u8bf4\\u7684\\u90fd\\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically."}
{"question": "Consider the paper that introduces the model that has the highest Recall@7 score in the CamRest task. What is the Entity F1 score improvement of the model proposed in the paper, specifically Q-TOD (T5-3B), with oracle knowledge on the SMD dataset compared to its performance with a fine-tuned retriever?", "answer": "1.24%", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has the highest Recall@7 score in CamRest task?", "answer_anchor": "Q-TOD", "question_reference": "What is the Entity F1 score improvement of Q-TOD (T5-3B) with oracle knowledge on the SMD dataset compared to its performance with a fine-tuned retriever?", "explanation_reference": "The improvement can be calculated from the Entity F1 scores provided for Q-TOD (T5-3B) with a fine-tuned retriever and with oracle knowledge. The Entity F1 score with a fine-tuned retriever is 74.96%, and with oracle knowledge, it is 76.20%. The improvement is the difference between these two scores.", "evidence_reference": "~~Q-TOD (T5-3B) & ~~~~~73.44~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~74.96 (+1.52)~~ \\\\ \\quad ~~w/ oracle knowledge & ~~~~~76.20 (\\textbf{+2.76})~~ \\\\"}
{"question": "Consider the paper that introduces the model that has a macro-F1 score of 27.34. What specific aspect of its pre-training process distinguishes the model's adaptation for the legal domain from the adaptation strategies of BERT models in other specialized domains, as discussed in previous studies?", "answer": "Pre-training BERT from scratch on domain-specific corpora with a new vocabulary of sub-word units.", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model having mac-F1 score of 27.34?", "answer_anchor": "LegalBERT", "question_reference": "What specific aspect of the LEGAL-BERT's pre-training process distinguishes its adaptation for the legal domain from the adaptation strategies of BERT models in other specialized domains, as discussed in previous studies?", "explanation_reference": "The question targets the methodological uniqueness of LEGAL-BERT's adaptation process for the legal domain, specifically focusing on the creation of a new vocabulary. This aspect is crucial for adapting the model to the specialized terminology and syntax of legal texts, which is a significant methodological detail that distinguishes LEGAL-BERT from other domain-specific BERT adaptations.", "evidence_reference": "\\noindent\\\\textbf{\\\\legalbertp} has the same architecture as \\\\bertbase with 12 layers, 768 hidden units and 12 attention heads (110M parameters). We use this architecture in all our experiments unless otherwise stated. We use a newly created vocabulary of equal size to \\\\bert's vocabulary."}
{"question": "Consider the paper that introduces the method in the figure represented by the 'x' (cross) marker. What specific component of the model's architecture proposed in the paper is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "answer": "matching information propagation module", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method in the figure is demonstrated by the 'x' (cross) marker?", "answer_anchor": "UniKGQA", "question_reference": "What specific component of UniKGQA's architecture is directly responsible for propagating the semantic matching information along the directed edges on KGs?", "explanation_reference": "The question targets a detailed aspect of the UniKGQA architecture, specifically asking for the component that handles the propagation of semantic matching information across the knowledge graph. The answer, 'Matching information propagation module,' directly addresses this by naming the specific part of the architecture designed for this purpose.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method that achieves an F1 score with a mean of 58.86 in the TAT-QA task. What specific aspect of the policy gradient strategy allows the model proposed in the paper to outperform heuristic-based example selection strategies in the context of few-shot GPT-3?", "answer": "The specific aspect of the policy gradient strategy that allows PromptPG to outperform heuristic-based example selection strategies in the context of few-shot GPT-3 is its ability to learn to select in-context examples from a small amount of training data dynamically, thereby constructing the optimal prompt for the test example. This approach significantly reduces the prediction variance compared to random selection and improves the selection of in-context examples over existing strategies.", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method demonstrates F1 score with mean 58.86 in TAT-QA task?", "answer_anchor": "PromptPG", "question_reference": "What specific aspect of the policy gradient strategy allows PromptPG to outperform heuristic-based example selection strategies in the context of few-shot GPT-3?", "explanation_reference": "The policy gradient strategy, as applied in PromptPG, enables the model to learn and select the most effective in-context examples for few-shot GPT-3 dynamically. This approach is contrasted with heuristic-based strategies that rely on predefined rules, which may not always align with the nuances of each problem. By learning from the training data, PromptPG can adapt its selection strategy to optimize performance, leading to its superior results.", "evidence_reference": "Compared to random selection, selecting the same question or answer type of examples helps the model to take the task-relevant examples as the prompt, thus improving the accuracy and reducing the variance. \\model shows its effectiveness in selecting optimal in-context examples over other strategies and largely reduces the instability."}
{"question": "Consider the paper that introduces the model which has a lower mac-F1 score than Longformer but a higher mac-F1 score than CaselawBERT. What is the primary reason this model has faster training and inference times compared to ALBERT and ALBERT-large, despite having more parameters?", "answer": "LEGAL-BERTsmall has faster training and inference times due to its smaller size in terms of hidden units' dimensionality and the number of attention heads, which affects gradient accumulation in feed-forward and multi-head attention layers, rather than the total number of parameters.", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model having lower mac-F1 score than Longformer but higher mac-F1 score than CaselawBERT?", "answer_anchor": "LegalBERT", "question_reference": "What is the primary reason LEGAL-BERTsmall has faster training and inference times compared to ALBERT and ALBERT-large, despite having more parameters?", "explanation_reference": "The efficiency of BERT-based models, including LEGAL-BERTsmall, is not solely determined by the total number of parameters but also by the architecture's dimensions, such as the number of hidden units and attention heads. These dimensions directly impact the computational load during training and inference, particularly in terms of memory usage and gradient calculations. LEGAL-BERTsmall, despite having more parameters than ALBERT and ALBERT-large, is designed with fewer hidden units and attention heads, leading to more efficient gradient accumulation and, consequently, faster training and inference times.", "evidence_reference": "Resource efficiency of the models mostly relies on the number of hidden units ($HU$), attentions heads ($AH$) and Transformer blocks $T$, rather than the number of parameters... \\legalbertsmall despite having $3\\times$ and $2\\times$ the parameters of \\textsc{albert} and \\textsc{albert-large} has faster training and inference times."}
{"question": "Consider the paper that introduces the model that results in the highest Self-BLEU score on the TellMeWhy dataset. What is the K-L divergence between the prediction results of the model proposed in the paper and ground-truth for question type distribution learning on the test set?", "answer": "0.0089", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model shows the highest Self-BLEU score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the prediction results of the BERT-based model and ground-truth for question type distribution learning on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how well the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the method that is represented by the square marker. What specific mechanism does its variant, referred to as the model proposed in the paper, use to calculate the expected attention for each head?", "answer": "The MMA-IL variant calculates the expected attention for each head by first calculating the softmax energy for each head as follows: \\(u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j}\\) and then using Equation \\(\\beta_{i,j} = \\sum_{k=j}^{|\\vx|} \\left( \\frac{\\alpha_{i, k} \\exp(u_{i,j})}{\\sum_{l=1}^k  \\exp(u_{i,l})} \\right)\\) to calculate the expected attention.", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is demonstrated by square marker?", "answer_anchor": "MMA", "question_reference": "What specific mechanism does the MMA-IL variant use to calculate the expected attention for each head?", "explanation_reference": "The MMA-IL variant calculates the expected attention for each head using a mechanism referred to as 'SoftEnergy'. This is distinct from the MMA-H variant, which uses a hard selection process. The 'SoftEnergy' mechanism allows MMA-IL to attend to all previous encoder states, leveraging more information for translation.", "evidence_reference": "For MMA-IL, we calculate the softmax energy for each head as follows: \\x \\begin{eqnarray} u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j} \\end{eqnarray}"}
{"question": "Consider the paper that introduces the model depicted in the figure that exhibits the highest fluctuation. How does the model's sequential training approach, as proposed in the paper, specifically address the negative transfer problem observed in certain evaluation dimensions?", "answer": "The sequential training approach of UniEval specifically addresses the negative transfer problem observed in certain evaluation dimensions by employing a method from continual learning: whenever a new dimension is introduced, a small portion of data from all previous dimensions is added to replay. This strategy allows for easy extension of the evaluator to new dimensions without training from scratch and enables explicit learning of dimensions related to basic linguistic features before moving on to dimensions that require a better understanding of the text.", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown in the figure demonstrates the highest fluctuation?", "answer_anchor": "UniEval", "question_reference": "How does the sequential training approach of UniEval specifically address the negative transfer problem observed in certain evaluation dimensions?", "explanation_reference": "The sequential training approach with data replay from previous dimensions is designed to mitigate the negative transfer problem by ensuring that the model retains knowledge from previously learned dimensions while learning new ones. This method allows for a smooth extension of the evaluator to new dimensions without starting from scratch, and it is particularly effective in maintaining performance across related linguistic features before moving on to dimensions that require a deeper understanding of the text.", "evidence_reference": "To tackle this issue, we employ a simple and effective method from continual learning: whenever a new dimension is introduced, we add small portion of data from all previous dimensions to replay."}
{"question": "Consider the paper that introduces the dataset that corresponds to the second chart from the left. What specific operational limitation does the Rigel model have when predicting answers from it?", "answer": "Rigel cannot perform sorting or filtering.", "figure": "locality/2310.12836/ratio_figure.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which dataset lies on the second left in the figure?", "answer_anchor": "Mintaka", "question_reference": "What specific operational limitation does the Rigel model have when predicting answers from the Mintaka dataset?", "explanation_reference": "The Rigel model's limitation is explicitly mentioned in the analysis of its performance on complex questions. It is capable of predicting a path from an entity to all related entities but lacks the functionality to sort or filter these entities, which is crucial for answering questions that require these operations.", "evidence_reference": "Given the Marvel question again, Rigel predicts a path from the Marvel Cinematic Universe to all the Marvel films. Rigel can't perform sorting or filtering, but it's possible to see what the next steps should be: identifying the chronological order, ordering by chronological order, and finding the second in the ordered list."}
{"question": "Consider the paper that introduces the Seq2Exp model that exhibits the highest test accuracy. What specific advantage does the model proposed in the paper demonstrate over FinQANet in terms of handling operands in the extended FinQA dataset?", "answer": "ELASTIC is adaptable to the number of operands following an operator, making it domain agnostic to support diverse operators.", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which Seq2Exp model shows the highest test accuracy?", "answer_anchor": "Elastic", "question_reference": "What specific advantage does ELASTIC demonstrate over FinQANet in terms of handling operands in the extended FinQA dataset?", "explanation_reference": "The advantage is highlighted by ELASTIC's ability to solve questions from the extended FinQA dataset that require generating operators with more than two operands, showcasing its adaptability and flexibility in handling diverse numerical reasoning tasks.", "evidence_reference": "These questions are proposed based on the original passages in the FinQA dataset. In addition, they are about superlative questions, which require to be solved by using superlative operators (i.e., \\(\\textit{smallest}\\) and \\(\\textit{biggest}\\)). As a result, unlike questions from the original FinQA dataset, the numbers of operands used to solve these extended questions are not limited to two."}
{"question": "Consider the paper that introduces the model that achieves a score of 3.84 in the Grounding task. What specific improvement in percentage points did the model proposed in the paper achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "answer": "6 and 6.2 points", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the base model tested in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific improvement in percentage points did the generative models achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "explanation_reference": "The improvement is highlighted in the comparison between generative and discriminative models for the VQA task, specifically on the out-of-domain subset. The generative models (\\ourst{} and \\oursb{}) improved upon the discriminative baselines by 6 and 6.2 percentage points respectively, demonstrating the effectiveness of using generative modeling for questions with answers not included in the top-K answer candidates.", "evidence_reference": "This improvement is more significant on the out-of-domain subset, where the generative \\ourst{} and \\oursb{} achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling."}
{"question": "Consider the paper that introduces the model depicted in the figure that exhibits the highest fluctuation. What specific transformation rule is applied by the model proposed in the paper to construct disfluent summaries for the fluency dimension in text summarization?", "answer": "We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries.", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the figure demonstrates the highest fluctuation?", "answer_anchor": "UniEval", "question_reference": "What specific transformation rule is applied to construct disfluent summaries for the fluency dimension in text summarization?", "explanation_reference": "The answer directly addresses the transformation rule used for creating disfluent summaries in the context of evaluating the fluency dimension. This detail is part of the pseudo data construction process for the fluency dimension, where specific actions are taken to manipulate a text span to generate disfluent summaries.", "evidence_reference": "Fluency represents the quality of individual sentences. We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries."}
{"question": "Consider the paper that introduces the method shown in the fifth row of the table. What is the impact of the order of in-context examples on the model's results for the NQ dataset using the method, specifically when applying KATE$_{\\text{nli+sts-b}}$?", "answer": "The reverse order performs the best.", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is shown in the fifth row in the table?", "answer_anchor": "KATE", "question_reference": "What is the impact of the order of in-context examples on KATE's results for the NQ dataset using KATE$_{\\text{nli+sts-b}}$ method?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results reveals that, for the NQ dataset, arranging the examples in reverse order (where the example closer to the test prompt in the embedding space is placed closer to the test prompt in the input sequence) yields the best performance. This suggests that the proximity of semantically similar sentences to the test example may help GPT-3 leverage the relevant information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the method that has approximately 30 perplexity and the highest average max toxicity. What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments after analyzing its effect on perplexity and repetition scores?", "answer": "5", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method with around 30 perplexity and the highest average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments after analyzing its effect on perplexity and repetition scores?", "explanation_reference": "The initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments is determined based on its effect on perplexity and repetition scores. The paper mentions that for \\(\\lambda_0=5\\), the average perplexity (58.4) and repetition score (3.5%) are the best among the considered values, indicating this value was chosen for subsequent experiments.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the score described as a \"fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits) by the model\". What is the Pearson correlation range for its cross-model estimates in CoLA?", "answer": "0.40 < r < 0.65", "figure": "locality/2311.16298/description_table.png", "anchor_arxiv_id": "2311.16298", "reference_arxiv_id": "2110.08420", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the name of the score with description 'Fine-grained information-theoretic quantity whose expectation value is the amount of usable information (in bits by the model.'?", "answer_anchor": "PVI", "question_reference": "What is the Pearson correlation range for cross-model \\pvi estimates in CoLA?", "explanation_reference": "The Pearson correlation range for cross-model \\pvi estimates in CoLA indicates the degree of agreement between the difficulty estimates of individual instances across different models. This range is specifically mentioned to highlight the variability and noisiness in difficulty estimates for CoLA due to its lower amount of usable information compared to datasets like SNLI.", "evidence_reference": "The cross-model Pearson correlation between \\pvi estimates of SNLI instances is very high ($r >$ 0.80). However, the cross-model Pearson correlation is lower for CoLA (0.40 $< r <$ 0.65); see Fig.~\\ref{fig:correlations_heatmap} in Appendix~\\ref{appendix:inter-epoch}."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. Why does the model proposed in the paper using beam search to decode each reasoning path result in worse performance compared to sampling?", "answer": "Beam search tends to produce a lower diversity in the outputs, which is less effective for tasks that benefit from exploring a diverse set of reasoning paths.", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "In the context of the self-consistency method, why does using beam search to decode each reasoning path result in worse performance compared to sampling?", "explanation_reference": "The paper explains that the key to better performance in self-consistency is the diversity of the reasoning paths. Beam search, by its nature, tends to produce less diverse outputs compared to sampling methods, which is why its performance is worse when used within the self-consistency framework.", "evidence_reference": "Note self-consistency can also adopt beam search to decode each reasoning path (results are shown as ``Self-consistency using beam search''), but its performance is worse compared to self-consistency with sampling. The reason is that beam search yields a lower diversity in the outputs."}
{"question": "Consider the paper that introduces the model represented by a blue line in the figure. What specific performance improvement does it show over the standard version of the same model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "answer": "3.03%", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model represented with a blue line in the figure?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific performance improvement does the DialoGPT (345M, w/ MMI) model show over the standard DialoGPT (345M) model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "explanation_reference": "The improvement in METEOR score for the DialoGPT (345M, w/ MMI) model over the standard DialoGPT (345M) model is derived from the comparison of their METEOR scores in the DSTC-7 Dialogue Generation Challenge results. The standard DialoGPT (345M) model achieved a METEOR score of 8.51%, while the DialoGPT (345M, w/ MMI) model achieved a METEOR score of 11.23%, indicating a specific improvement of 3.06%.", "evidence_reference": "DialoGPT (345M) = 8.51% METEOR; DialoGPT (345M, MMI) = 11.23% METEOR"}
{"question": "Consider the paper that introduces the large language model which has the second lowest HVI score among those in the figure corresponding to a purple bar. What specific performance improvement does the model proposed in the paper exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "answer": "GPT-4 achieves a score that falls in the top 10% of test takers, contrasting with GPT-3.5, which scores in the bottom 10%.", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the large language model that demonstrates the second lowest HVI score shown in purple bar?", "answer_anchor": "GPT-4", "question_reference": "What specific performance improvement does GPT-4 exhibit over GPT-3.5 in the context of the Uniform Bar Exam?", "explanation_reference": "The question assesses understanding of GPT-4's significant improvement in performance on a professional benchmark, the Uniform Bar Exam, compared to its predecessor. This detail highlights GPT-4's advanced capabilities in understanding and generating natural language in complex scenarios.", "evidence_reference": "For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers. This contrasts with GPT-3.5, which scores in the bottom 10%."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 84.70% WInToRe. What is its inference time for feature extraction compared to VinVL and M2 Transformer?", "answer": "31 ms", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shows 84.70% WInToRe?", "answer_anchor": "GRIT", "question_reference": "What is the inference time for feature extraction using GRIT compared to VinVL and M2 Transformer?", "explanation_reference": "The inference time for feature extraction using GRIT is significantly lower than that of VinVL and M2 Transformer, demonstrating GRIT's computational efficiency in this aspect.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the method for which a certain benchmark results in the highest execution accuracy. What specific approach does the model proposed in the paper employ to minimize the SQL annotation error rate, and how does it ensure the selection of semantically equivalent and efficient SQL as ground truth?", "answer": "The \\textsc{Bird} benchmark employs a double-blind annotation approach to minimize the SQL annotation error rate. This approach involves two independent SQL annotators generating SQLs for the same question without discussion. SQLs yielding identical results are collected, and those with discrepancies are reviewed by experts until a consensus is reached. This method dramatically reduces the SQL annotation error rate by ensuring a low probability for two skilled annotators to generate the same incorrect results when databases have large values. To ensure the selection of semantically equivalent and efficient SQL as ground truth, the more semantic-equivalent and efficient SQL selected by experts for each question is picked as ground truth SQL in \\textsc{Bird}. Additionally, external knowledge evidence sentences are recorded for each SQL if utilized, enhancing the model's comprehension of database values and promoting the generation of efficient SQL queries.", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Under this method, which benchmark demonstrates the highest execution accuracy?", "answer_anchor": "BIRD", "question_reference": "What specific approach does the \\textsc{Bird} benchmark employ to minimize the SQL annotation error rate, and how does it ensure the selection of semantically equivalent and efficient SQL as ground truth?", "explanation_reference": "The \\textsc{Bird} benchmark employs a double-blind annotation approach to minimize the SQL annotation error rate. This method involves two independent SQL annotators generating SQLs for the same question without discussion. SQLs yielding identical results are gathered, and any discrepancies are resolved by expert review until a consensus is reached. This process dramatically reduces the SQL annotation error rate by ensuring a very low probability for two skilled annotators to generate the same incorrect results when databases have large values. The more semantically equivalent and efficient SQL selected by experts for each question is picked as the ground truth SQL in \\textsc{Bird}.", "evidence_reference": "As shown in Figure \\ref{wkf} (b), we employ a double-blind approach \\citep{csqa2} for SQL annotation. This approach involves two independent SQL annotators who generate SQLs for the same question without discussion. The annotated SQLs are executed in databases, and those yielding identical results are gathered. Otherwise, the SQLs are checked with experts until a consensus is reached. Double-blind procedures can dramatically reduce the SQL annotation error rate, as there is a small probability for two skillful annotators to generate the same incorrect results when databases have large values. The more semantic-equivalent and efficient SQL selected by experts for each question is picked as ground truth SQL in \\textsc{Bird}, and the external knowledge evidence sentences are recorded for each SQL if utilized."}
{"question": "Consider the paper that introduces the method that corresponds to a higher F1 score than that of LDSGM but a lower F1 score than 65.76 for PDTB-Top. What is the primary reason for its better performance compared to the PIDRP method across all four top-level senses of the PDTB?", "answer": "The main reason for the poor performance of the PIDRP method compared to the PCP method across all four top-level senses of the PDTB is that connective prediction is closer to the natural language patterns when the model is in the pre-training stage than direct implicit discourse relation prediction.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which is the method has a higher F1 score than LDSGM but lower F1 score than 65.76", "answer_anchor": "PCP", "question_reference": "What is the primary reason for the poor performance of the PIDRP method compared to the PCP method across all four top-level senses of the PDTB?", "explanation_reference": "The paper suggests that the main reason for the PIDRP method's inferior performance is that predicting connectives aligns more closely with the natural language patterns encountered during the pre-training stage of the model, as opposed to directly predicting implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the quantization method that achieves a score of 80.3 on Deit-B with a Weight/Activation (W/A) precision of 6/6. What is the specific range of search space for \\(\\Delta_{\\text{R1}}^s\\) during post-softmax quantization as mentioned in the experiment settings?", "answer": "\\([\\frac{1}{2^{k}},\\frac{1}{2^{k+1}},...,\\frac{1}{2^{k+10}}]\\)", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the quant method show 80.3 score on Deit-B?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific range of search space for \\(\\Delta_{\\text{R1}}^s\\) during post-softmax quantization as mentioned in the experiment settings?", "explanation_reference": "The question focuses on a detailed aspect of the experiment settings related to the quantization process, specifically the search space for the scaling factor \\(\\Delta_{\\text{R1}}^s\\) used in post-softmax quantization. This detail is crucial for understanding how the quantization parameters are optimized.", "evidence_reference": "For post-softmax quantization, the search space of \\(\\Delta_{\\text{R1}}^s\\) is \\([\\frac{1}{2^{k}},\\frac{1}{2^{k+1}},...,\\frac{1}{2^{k+10}}]\\)."}
{"question": "Consider the paper that introduces the method that exhibits the highest accuracy on the VQA-v2 task. What specific strategy does the model proposed in the paper employ during finetuning and inference to address the inefficiency and potential for generating invalid labels in classification tasks?", "answer": "Trie-based search", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method shows the highest Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "What specific strategy does OFA employ during finetuning and inference to address the inefficiency and potential for generating invalid labels in classification tasks?", "explanation_reference": "The paper mentions the use of a Trie-based search strategy to overcome the inefficiencies and potential for generating invalid labels during classification tasks in the finetuning and inference stages. This strategy enhances performance by ensuring that the model's output is constrained within the set of valid labels, addressing the mentioned problems directly.", "evidence_reference": "For inference, we apply the decoding strategies, e.g., beam search, to enhance the quality of generation. However, this paradigm has several problems in classification tasks: 1. optimizing on the entire vocabulary is unnecessary and inefficient; 2. the model may generate invalid labels out of the closed label set during inference. To overcome these issues, we introduce a search strategy based on prefix tree (Trie, \\cite{trie})."}
{"question": "Consider the paper that introduces the dataset shown in the second row of the table. Based on the experimental results on this dataset, which model demonstrated the highest improvement in performance when normalized topics were introduced, and what specific metric was most significantly affected?", "answer": "norm generation, F1/BLEU1/BLEU2", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset shown in the second row of the table?", "answer_anchor": "DuConv", "question_reference": "Based on the experimental results, which model demonstrated the highest improvement in performance when normalized topics were introduced, and what specific metric was most significantly affected?", "explanation_reference": "The norm generation model showed the most significant improvement in performance when normalized topics were introduced, particularly in the F1/BLEU1/BLEU2 metrics, indicating a substantial enhancement in the quality and relevance of the generated responses.", "evidence_reference": "norm generation & 32.50\\% & 58.50\\% & 24.3 & \\textbf{41.84} / \\textbf{0.347} / \\textbf{0.198} & 0.057 / 0.155 & \\textbf{9.78} / 38.02 / \\textbf{15.27}"}
{"question": "Consider the paper that introduces the dataset which exhibits the highest accuracy for Method 2. What specific architectural feature allows the model proposed in the paper to simultaneously perform language modeling and correctness prediction tasks?", "answer": "A small scalar head that outputs predictions on a per-token basis.", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What dataset demonstrates the highest accuracy with method 2?", "answer_anchor": "GSM8K", "question_reference": "What specific architectural feature allows the verifier models to simultaneously perform language modeling and correctness prediction tasks?", "explanation_reference": "The verifier models are designed with a unique architectural feature that enables them to handle both language modeling and correctness prediction tasks. This feature is a \u2248 that operates on the logits outputted by the language model's final unembedding layer, specifically shifting and scaling the logit corresponding to a special token in the vocabulary reserved for the verifier\u2019s predictions. This design allows the rest of the tokens to continue representing the language modeling objective, while the special token is used for correctness predictions.", "evidence_reference": "We implement this scalar head as a single bias parameter and single gain parameter that operate on the logits outputted by the language model\u2019s final unembedding layer. Specifically, the bias and gain shift and scale the logit corresponding to a special token in the vocabulary."}
{"question": "Consider the paper that introduces the dataset that corresponds to the last row of the table. Which specific error type in the error analysis indicates a failure due to the model proposed in the paper's inability to understand comments with indirect and disrespectful references?", "answer": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way.", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset shown in the last row of the table?", "answer_anchor": "ViHOS", "question_reference": "What specific error type in the error analysis indicates a failure due to the model's inability to understand comments with indirect and disrespectful references?", "explanation_reference": "The error type 'Allusion' directly addresses the model's failure to correctly interpret comments that refer to another person or subject in an indirect and disrespectful manner, indicating a specific kind of error where the model's understanding of context and indirect references is inadequate.", "evidence_reference": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way."}
{"question": "Consider the paper that introduces the method that has the highest score in the WQ-R task. How does the paraphrasing-based approach differ from the model proposed in the paper in handling instances with simple expressions, according to the limitations section?", "answer": "The paraphrasing-based approach focuses on words, while the proposed method focuses more on the structure of the sentences.", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the highest score in WQ R task?", "answer_anchor": "DSM", "question_reference": "How does the paraphrasing-based approach differ from the proposed method in handling instances with simple expressions, according to the limitations section?", "explanation_reference": "The distinction between the paraphrasing-based approach and the proposed method is highlighted in the limitations section, where it is mentioned that for instances with simple expressions, the paraphrasing-based method may perform better by focusing on words, whereas the proposed method concentrates more on sentence structure.", "evidence_reference": "For example, the ground truth is 'What religion in Australia that influenced Arthur Schopenhauer?', the paraphrasing-based approach generates 'What faith in Australia inspired Arthur Schopenhauer?'. Our method generates  'What is the religion in Australia that influenced  Arthur Schopenhauer? '. We observe that the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the dataset which has the largest number of Queries|Aspects in the ABS category. What specific methodological limitation is highlighted by the authors regarding the model's aspect discovery stage's performance in the Software domain?", "answer": "The aspect discovery stage has a low precision.", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset with the most number of Queries|Aspects in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What specific methodological limitation is highlighted by the authors regarding the aspect discovery stage's performance in the Software domain?", "explanation_reference": "The authors highlight a methodological limitation in the aspect discovery stage, specifically mentioning that despite the model's ability to extract aspects, it struggles with extracting relevant aspects for the article, as evidenced by the low precision observed in the Software domain. This indicates a limitation in the model's aspect classification accuracy, affecting its overall performance in aspect-based summarization tasks.", "evidence_reference": "We observed a general trend of low precision for aspect discovery. We hypothesize that this is due to limited target aspects for each article; correctly extracted aspects affect negatively to precision if they do not exist in the target article."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than LayoutXLM and a higher F1 score than SPADE. What specific methodological adjustment does the model proposed in the paper make to the GraphSAGE aggregation strategy to accommodate the unique challenges of document graph structures?", "answer": "The Doc2Graph model adjusts the GraphSAGE aggregation strategy by redefining the neighborhood aggregation to consider only a subset of neighbors $\\Upsilon(i) = \\{j \\in N(i): |i - j| < threshold\\}$, where $|i - j|$ is the Euclidean distance between nodes $i$ and $j$, normalized between 0 and 1, and stored on their connecting edge. This adjustment, along with a constant scale factor $c$, aims to maintain the locality property during the message passing algorithm, accommodating the unique challenges of document graph structures.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having lower F1 score than LayoutXLM and high F1 score than SPADE?", "answer_anchor": "Doc2Graph", "question_reference": "What specific methodological adjustment does the Doc2Graph model make to the GraphSAGE aggregation strategy to accommodate the unique challenges of document graph structures?", "explanation_reference": "The adjustment to the GraphSAGE aggregation strategy is made to address the challenge of naturally defining graph structures in documents, which is not straightforward. By redefining the neighborhood aggregation to consider only neighbors within a certain Euclidean distance threshold, the model aims to maintain locality during the message passing algorithm, which is crucial for document graphs where the spatial arrangement of elements is significant.", "evidence_reference": "Then, given a document, we redefine the above equation as: \\begin{equation} \\label{eq:graph-sage} h_{N(i)}^{l+1} = \\frac{c}{|\\Upsilon(i)|} \\sum_{j \\in \\Upsilon(i)} h^{l}_{j} \\end{equation} where $\\Upsilon(i) = \\{j \\in N(i): |i - j| < threshold\\}$, $|i - j|$ is the Euclidean distance of nodes $i$ and $j$ saved (normalized between 0 and 1) on their connecting edge, and $c$ is a constant scale factor."}
{"question": "Consider the paper that introduces the dataset which has a training set size of 8,844. What specific error type in the error analysis indicates a failure due to the model proposed in the paper's inability to understand comments with indirect and disrespectful references?", "answer": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way.", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset having training set size 8,844?", "answer_anchor": "ViHOS", "question_reference": "What specific error type in the error analysis indicates a failure due to the model's inability to understand comments with indirect and disrespectful references?", "explanation_reference": "The error type 'Allusion' directly addresses the model's failure to correctly interpret comments that refer to another person or subject in an indirect and disrespectful manner, indicating a specific kind of error where the model's understanding of context and indirect references is inadequate.", "evidence_reference": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way."}
{"question": "Consider the paper that introduces the model that performs the second best in the ClinTox dataset. What is the peak learning rate used during its pre-training?\n\nA) 1e-3\nB) 5e-4\nC) 2e-5\nD) 1e-6", "answer": "0.0005", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model perform the second best in the ClinTox dataset?", "answer_anchor": "MolXPT", "question_reference": "What is the peak learning rate used during the pre-training of MolXPT?", "explanation_reference": "The peak learning rate is a specific hyperparameter value used during the pre-training phase of the MolXPT model. It is mentioned directly in the section detailing the pre-training hyper-parameters.", "evidence_reference": "The peak learning rate is $0.0005$ and the warm-up steps are 20000."}
{"question": "Consider the paper that introduces the method which is shown in the table above the 'Magister et al' row but below the 'UL2' row. How does the performance of a GPT-2 Large model fine-tuned with ground truth step-by-step annotation compare to one fine-tuned with the model proposed in the paper-generated Chain of Thought (CoT) on the GSM8K dataset?", "answer": "The GPT-2 Large model fine-tuned with ground truth step-by-step annotation outperforms the one fine-tuned with LLM-generated Chain of Thought (CoT) on the GSM8K dataset.", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown in the table above method proposed by Magister et al but below UL2 method?", "answer_anchor": "DecomDistill", "question_reference": "In the context of distilling reasoning capabilities into smaller models, how does the performance of a GPT-2 Large model fine-tuned with ground truth step-by-step annotation compare to one fine-tuned with LLM-generated Chain of Thought (CoT) on the GSM8K dataset?", "explanation_reference": "The comparison between the performance of models fine-tuned with different types of annotations indicates the effectiveness of the distillation approach. The specific performance metrics for the GPT-2 Large model, when fine-tuned with ground truth step-by-step annotation and LLM-generated CoT, directly answer the question by showing the relative effectiveness of these training strategies on the GSM8K dataset.", "evidence_reference": "GSM8K & Large (774M) &  4.62 & 14.10 & -& 12.85 & 17.89 & \\bf 21.08 ($\\uparrow 33\\%$) & 13.25"}
{"question": "Consider the paper that introduces the method that corresponds to the third row of the table. How does the gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) in the simplified theoretical analysis of the model's loss function indicate the direction of model optimization?", "answer": "-2*p_{\\text {unlike}}", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is shown in the third row of the table?", "answer_anchor": "Discup", "question_reference": "How does the gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) in the simplified theoretical analysis of DisCup's loss function indicate the direction of model optimization?", "explanation_reference": "The gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) being a negative value indicates that model optimization is always in the direction of decreasing the probability of the unlikely token. This is because a negative gradient value for $h_{\\text{unlike}}$ means that the optimization process will work to reduce the logit value of the unlikely token, thereby decreasing its probability in the model's output distribution.", "evidence_reference": "The gradient of logit $h_{unlike}$ could be represented as:  \\begin{equation} \\begin{aligned} \\frac{\\partial \\mathcal{L}_t}{\\partial h_{\\text{unlike}}} & =\\left(0-p_{\\text {unlike}}\\right)- \\frac{p_{\\text {unlike}}}{1-p_{\\text {unlike}}}\\left(1-p_{\\text {unlike}}\\right) \\\\ &=-2*p_{\\text {unlike}} \\end{aligned} \\end{equation}"}
{"question": "Consider the paper that introduces the dataset which has the fewest number of languages but the most number of SM tasks. What specific linguistic phenomenon, as mentioned in the paper, requires familiarity with social media to interpret correctly, and how is it expressed in Chinese?", "answer": "\u60a8\u8bf4\u7684\u90fd\u5bf9", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset has the fewest number of languages but the most number of SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly, and how is it expressed in Chinese?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity, including irony, which can be difficult for models to interpret. It specifically mentions the Chinese phrase '\\u60a8\\u8bf4\\u7684\\u90fd\\u5bf9' as an example of irony that requires familiarity with social media context to be correctly understood as potentially ironic rather than a straightforward agreement.", "evidence_reference": "For example, on Chinese social media, a comment '\\u60a8\\u8bf4\\u7684\\u90fd\\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically."}
{"question": "Consider the paper that introduces the method which demonstrates the highest BLEU-1 score for the Test Seen task according to the table. What is the effect of the contrastive loss margin \\(\\rho\\) on the model's perplexity when it is set to either too small or too large values?", "answer": "When the contrastive loss margin \\(\\rho\\) is set to either too small or too large values, it leads to a sub-optimal perplexity.", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown in the table demonstrates the highest BLEU-1 score for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the effect of the contrastive loss margin \\(\\rho\\) on the model perplexity when it is set to either too small or too large values?", "explanation_reference": "The paper discusses how setting the contrastive loss margin \\(\\rho\\) to either too small (e.g., \\(0.1\\)) or too large (e.g., \\(1.0\\)) values leads to a representation space that is either less or too isotropic, resulting in sub-optimal perplexity. This indicates that there is an optimal range for \\(\\rho\\) that balances the isotropy of the representation space to achieve better model perplexity.", "evidence_reference": "However, when \\(\\rho\\) is either too small (e.g., \\(0.1\\)) or large (e.g., \\(1.0\\)), the learned representation space of the model would be either less or too isotropic, leading to a sub-optimal perplexity."}
{"question": "Consider the paper that introduces the method that demonstrates the lowest score in the CSQA2.0 dev/dev* task. What specific methodological approach does the paper employ to align the model proposed in the paper with human intentions, particularly in terms of handling sensitive content?", "answer": "Reinforcement learning from human feedback (RLHF)", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method demonstrates the lowest score in CSQA2.0 dev task?", "answer_anchor": "ChatGPT", "question_reference": "What specific methodological approach does the paper employ to align language models with human intentions, particularly in terms of handling sensitive content?", "explanation_reference": "The paper employs Reinforcement Learning from Human Feedback (RLHF) as the specific methodological approach to align language models with human intentions, especially for handling sensitive content. This approach involves training models to act in accordance with user intentions, which includes being helpful, truthful, and harmless, particularly in contexts that may involve sensitive content.", "evidence_reference": "Our labelers consist of contractors hired either through Upwork, or sourced from Scale AI. Unlike previous work on RLHF that focused mostly on the summarization domain~\\cite{ziegler2019fine,stiennon2020learning,wu2021recursively}, in this work we want humans to label a broad set of natural language prompts submitted to language models, some of which may be sensitive in nature. Thus, we conducted a screening process to select labelers who showed a high propensity to detect and respond to sensitive content."}
{"question": "Consider the paper that introduces the dataset which has a test set size of 1,106. What specific linguistic phenomenon is used in the model proposed in the paper as an example to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "answer": "Puns", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset having 1,106 test set size?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon is used as an example to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "explanation_reference": "The paper discusses various linguistic phenomena that pose challenges to hate speech detection, including the use of puns, where phrases only make sense when read backwards. This is highlighted in the example of 'B\u1ed3n K\u1ef3 L\u1eafc', which, when read backwards, becomes 'B\u1eafc K\u1ef3 L\u1ed3n', illustrating how puns can be used to convey hate speech in a concealed manner.", "evidence_reference": "Some comments use phrases that only read them backwards, they make sense. As in the example, 'B\u1ed3n K\u1ef3 L\u1eafc', if this phrase is read backwards, it is 'B\u1eafc K\u1ef3 L\u1ed3n' (pussy north)."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Internet-enhanced category. What specific method was used in the experimental setup to condition the model proposed in the paper on shorter excerpts extracted from the retrieved documents, and how was the relevance of these excerpts to the question determined?", "answer": "TF-IDF embeddings and cosine similarity", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.05115", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the first method shown in Explicit --> Internet-enhanced", "answer_anchor": "Internet-Fewshot", "question_reference": "In the experimental setup, what specific method was used to condition the model on shorter excerpts extracted from the retrieved documents, and how was the relevance of these excerpts to the question determined?", "explanation_reference": "The method used to condition the model on shorter excerpts from the retrieved documents involved chunking all documents into paragraphs of 6 sentences, then embedding the question and these paragraphs using TF-IDF. Cosine similarity was used to produce a ranked list of evidence paragraphs based on their relevance to the question, thus selecting smaller, more relevant parts of the full documents for the prompt.", "evidence_reference": "As documents in \\emph{D} can originate from news or Wikipedia articles to whole books, they tend to be long, with an average length of 2,056 words. As this exceeds the input sequence length of models, we condition the model on shorter excerpts extracted from the documents in \\emph{D}. Specifically, we first chunk all documents into paragraphs of 6 sentences. We then embed \\emph{q} and the paragraphs using TF-IDF and using cosine similarity we produce a (ranked) list of evidence paragraphs \\emph{P}, thus only using in the prompt smaller, more relevant parts of the full documents."}
{"question": "Consider the paper that analyzes the dataset located in the top left of the figure. What specific linguistic phenomenon mentioned in the paper, requiring familiarity with social media to interpret correctly, is expressed in Chinese?", "answer": "\u60a8\u8bf4\u7684\u90fd\u5bf9", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset located on the top left of the figure?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly, and how is it expressed in Chinese?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity, including irony, which can be difficult for models to interpret. It specifically mentions the Chinese phrase '\\u60a8\\u8bf4\\u7684\\u90fd\\u5bf9' as an example of irony that requires familiarity with social media context to be correctly understood as potentially ironic rather than a straightforward agreement.", "evidence_reference": "For example, on Chinese social media, a comment '\\u60a8\\u8bf4\\u7684\\u90fd\\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically."}
{"question": "Consider the paper that discusses the dataset located at the bottom left of the figure. What is the Fleiss kappa inter-annotator agreement score for the Yoruba language when considering the 3-class sentiment classification?", "answer": "0.600", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset located on the bottom left of the figure?", "answer_anchor": "NaijaSenti", "question_reference": "What is the Fleiss kappa inter-annotator agreement score for the Yoruba language when considering the 3-class sentiment classification?", "explanation_reference": "The Fleiss kappa score for the Yoruba language in the 3-class sentiment classification setup indicates the level of agreement among annotators on the sentiment classification of tweets in Yoruba. This score is a measure of the reliability of agreement between annotators beyond chance.", "evidence_reference": "IAA ($\\kappa$) for Yoruba in the 3-class setup is $0.600$."}
{"question": "Consider the paper that introduces the benchmark that corresponds to the light green color in the figure. What is the average human-rater performance for the 'Temporal Sequences' task in the suite it belongs to?", "answer": "90.8%", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which benchmark is represented using the light green color from the figure?", "answer_anchor": "BBH", "question_reference": "What is the average human-rater performance for the 'Temporal Sequences' task in the BIG-Bench Hard (BBH) suite?", "explanation_reference": "The average human-rater performance for the 'Temporal Sequences' task is directly provided in the detailed results table for the BBH tasks.", "evidence_reference": "Temporal Sequences & 25.0 & 52.2 & 90.8 & 100 & 33.6 & 67.2 & 77.6 & 96.8 & 39.6 & 78.8"}
{"question": "Consider the paper that introduces the model represented by a blue line in the figure. Which specific architectural feature allows it to efficiently handle the full context in a computationally efficient manner?", "answer": "multi-layer self-attentive mechanism", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model represented with a blue line in the figure?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific architectural feature of DialoGPT allows it to efficiently handle the full context in a computationally efficient manner?", "explanation_reference": "The question assesses understanding of a core concept in the architecture of DialoGPT that enables it to efficiently process and generate conversational responses by handling the full context. The multi-layer self-attentive mechanism is a fundamental architectural feature that allows for this efficiency.", "evidence_reference": "a transformer-based architecture like GPT-2, which uses a multi-layer self-attentive mechanism to allow fully-connected cross-attention to the full context in a computationally efficient manner"}
{"question": "Consider the paper that introduces the method that scores a 70.1 in the 'Revised Persona' column. What is the primary reason for the performance drop when the hyperparameter gamma is set to 1 in the model proposed in the paper?", "answer": "The primary reason for the performance drop when the hyperparameter gamma is set to 1 in the CSN model is that it results in no document content being selected, effectively degenerating the model to non-document-grounded response selection, which sharply decreases performance.", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method gets score of 70.1 in 'Revised Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the primary reason for the performance drop when the hyperparameter gamma is set to 1 in the CSN model?", "explanation_reference": "Setting gamma to 1 results in no document content being selected for response matching, effectively degenerating the model to non-document-grounded response selection, which significantly reduces its performance.", "evidence_reference": "On the other hand, when  $\\gamma=1$, \\ie, no document content is selected, it degenerates to non document-grounded response selection and the performance also drops sharply."}
{"question": "Consider the paper that examines the dataset with the largest number of dialogues. What specific distribution statistics were analyzed in the paper to assess goal completion and knowledge exploitation?", "answer": "goal completion: 0, 1, 2\n\nknowledge used: # triplets, # properties", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset has the most number of dialogues?", "answer_anchor": "DuConv", "question_reference": "What specific distribution statistics were analyzed to assess goal completion and knowledge exploitation in the paper?", "explanation_reference": "The question assesses understanding of the detailed analysis conducted in the paper regarding how well the conversation goals were achieved and how the knowledge was exploited in the dialogue process. It focuses on the specific aspects of 'goal completion' and 'knowledge used', which are key to evaluating the effectiveness of the proposed models in utilizing the knowledge graph for proactive conversation.", "evidence_reference": "Analysis on goal completion and knowledge exploitation."}
{"question": "Consider the paper that introduces the method which has the highest perplexity. What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during the generation guided by this method?", "answer": "n=m", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having the highest perplexity?", "answer_anchor": "GeDi", "question_reference": "What is the minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) during GeDi-guided generation?", "explanation_reference": "The minimum number of tokens in the set \\(\\gV_m\\) that maintains at least \\(\\rho\\) in cumulative probability mass in \\(P_w(x_t|x_{<t},c)\\) is defined by the condition where \\(n=m\\). This is because \\(\\gV_m\\) is defined as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\).", "evidence_reference": "We define \\(\\gV_m\\) as \\(\\gV_n\\) for \\(n=m\\), meaning that \\(\\gV_m\\) will contain the minimum number of tokens possible at the head of the distribution for \\(P_{\\theta}(c|x_t,x_{<t})\\) to maintain a minimum cumulative probability of \\(\\rho\\) in \\(P_w(x_t|x_{<t},c)\\)."}
{"question": "Consider the paper that introduces the model that corresponds to the lowest BERTScore F1 score on the TellMeWhy dataset. What is the primary limitation of using the model proposed in the paper for event-centric summary generation in educational question generation, as identified in the paper?", "answer": "Factuality error", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What model shows the lowest BERTScore F1 score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What is the primary limitation of using the BART model for event-centric summary generation in educational question generation, as identified in the paper?", "explanation_reference": "The paper identifies the primary limitation of using the BART model for event-centric summary generation as the factuality error problem, indicating that sometimes the system may generate non-factual facts in terms of the original context.", "evidence_reference": "Owing to the factuality error problem of our system, we suggest to further investigate constructing structured knowledge of fairy tales and knowledge-grounded question generation for real-world applications."}
{"question": "Consider the paper that introduces the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. What specific aspect of the zero-shot reasoning process of this method directly addresses the challenge of LLMs' hallucination during inference?", "answer": "The transformation of the retrieval process into a multi-hop decision sequence.", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2309.03118", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "Knowledge Solver", "question_reference": "What specific aspect of the Knowledge Solver's zero-shot reasoning process directly addresses the challenge of LLMs' hallucination during inference?", "explanation_reference": "The question targets the core mechanism by which the Knowledge Solver (KSL) paradigm mitigates the issue of hallucination in LLMs, which is a critical analysis of how the method improves upon the limitations of LLMs by leveraging their generalizability in a structured manner. The answer is derived from the description of how KSL simplifies the process of searching for necessary knowledge from Knowledge Graphs into a multi-hop decision sequence, which is a direct strategy to counteract hallucination by providing a structured path of reasoning rather than relying on the LLMs' unguided generation capabilities.", "evidence_reference": "In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to solve these shortcomings, which teaches LLMs themselves to search for knowledge from external knowledge bases. To be specific, we simplify the process of searching for necessary knowledge from KGs into a multi-hop decision sequence."}
{"question": "Consider the paper that introduces the method demonstrated by the solid lavender line. What specific computational advantage does the model proposed in the paper have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "answer": "GRIT reduces the feature extraction inference time to 31 ms, compared to 304 ms for VinVL and 736 ms for ${\\cal M}^2$ Transformer.", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method demonstrated in the lavendar solid line?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time?", "explanation_reference": "The question focuses on the detailed part of the computational efficiency of GRIT compared to other methods, specifically in the context of feature extraction inference time. This detail highlights GRIT's significant improvement in computational speed.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the model that demonstrates the highest score in the 'T3' column. What is the observed accuracy drop percentage for the model proposed in the paper on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "answer": "9.7", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates the highest score in 'T3' column?", "answer_anchor": "CEAR", "question_reference": "What is the observed accuracy drop percentage for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "explanation_reference": "The accuracy drop for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00) is directly reported in the empirical study results table.", "evidence_reference": "CRL & [0.85, 1.00) & 71.1 & 9.7 & 64.8 & 11.4"}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.679 in the FB15kET dataset. How does the model proposed in the paper's CET method's N2T mechanism specifically handle the embeddings of neighbor relations and entities to infer an entity's type?", "answer": "It conducts non-linear activation on the difference between neighbor entity and neighbor relation embeddings, then applies a linear layer.", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2109.07990", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets MRR score equal to 0.679 in FB15kET datast?", "answer_anchor": "RGCN", "question_reference": "How does the proposed CET method's N2T mechanism specifically handle the embeddings of neighbor relations and entities to infer an entity's type?", "explanation_reference": "The N2T mechanism of the CET method focuses on using each neighbor independently to infer the missing types of central entities. Specifically, it calculates the difference between the embeddings of the neighbor entity and the neighbor relation, applies a non-linear activation function (ReLU in this context), and then sends this result through a linear layer to produce the relevance score for type inference. This process is designed to reduce the interference of irrelevant information on entity typing by focusing on individual attributes represented by neighbors.", "evidence_reference": "In practice, CET follows the translating assumption in TransE to obtain the neighbor embedding, then conducts non-linear activation on neighbor embedding and sent it to a linear layer: \\(\\vec{R}_{(n_r, n_e)}^{N2T} = \\matrix{W}\\mathrm{Relu}(\\vec{n}_e - \\vec{n}_r) + \\vec{b}\\), where \\(\\matrix{W}\\in\\mathbb{R}^{L\\times k}, \\vec{b}\\in\\mathbb{R}^{L}\\) are the learning parameters and \\(\\vec{R}_{(n_r, n_e)}^{N2T} \\in \\mathbb{R}^{L}\\) is the relevance score calculated by the N2T mechanism, where the i-th entry represents the relevance score between neighbor \\((n_e, n_r)\\) and type i."}
{"question": "Consider the paper that introduces the dataset which has the largest number of instances in the ABS category. What is the precision achieved for aspect discovery in the Software domain with a threshold \\(\\lambda\\) set to 0.9?", "answer": "45.1", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset with the most number of instances in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What is the precision achieved for aspect discovery in the Software domain with a threshold \\(\\lambda\\) set to 0.9?", "explanation_reference": "The precision value directly answers the question by indicating the performance of aspect discovery in the Software domain with the specified threshold.", "evidence_reference": "With the threshold \\(\\lambda\\) set to 0.9, we achieved the precision of 45.1, which shows that the aspect discovery has the ability to extract aspects, but not as good at extracting \\textit{relevant} aspects for the article."}
{"question": "Consider the paper that introduces the model that corresponds to the first row of the table. What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "answer": "The paper proposes a novel framework that combines question type prediction and event-centric summarization to generate educational questions for storybooks.", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model is in the first row of the table?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach introduced in the paper, which is a combination of predicting the distribution of question types and generating summaries focused on salient events to facilitate the generation of educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the method in the figure represented by the 'x' (cross) marker. What is the core innovation of the model proposed in the paper in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "answer": "The core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks is the unification of retrieval and reasoning in both model architecture and parameter learning.", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method in the figure is demonstrated by the 'x' (cross) marker?", "answer_anchor": "UniKGQA", "question_reference": "What is the core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the question by summarizing the unique approach of UniKGQA, which integrates the retrieval and reasoning processes into a single framework. This is innovative because it contrasts with previous methods that treated these stages separately, thus enhancing the efficiency and effectiveness of solving multi-hop KGQA tasks.", "evidence_reference": "In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning."}
{"question": "Consider the paper that introduces the method that results in a score of 22.4 in the GSM8K dataset. What specific performance tradeoff is observed when selecting the best model, as proposed by the paper, based on the GSM8K validation set versus the M-A-S validation performance?", "answer": "The specific performance tradeoff observed when selecting the best model based on the GSM8K validation set versus the M-A-S validation performance is that choosing the best model based on the GSM8K validation set does not necessarily lead to the best validation performance on the M-A-S OOD setting. However, choosing the best model based on the M-A-S validation performance leads to a smaller performance drop in GSM8K.", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2301.12726", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method demonstrates score of 22.4 in GSM8K dataset?", "answer_anchor": "SpecialFT", "question_reference": "What specific performance tradeoff is observed when selecting the best model based on the GSM8K validation set versus the M-A-S validation performance?", "explanation_reference": "The paper discusses the tradeoffs between in-distribution (GSM8K) and out-of-distribution (M-A-S) performance when selecting models based on different validation sets. Selecting the best model based on the GSM8K validation set does not necessarily lead to the best out-of-distribution performance on the M-A-S setting. Conversely, choosing the best model based on the M-A-S validation performance results in a smaller performance drop in GSM8K but achieves better validation performance on the M-A-S OOD setting, indicating a specific tradeoff between optimizing for in-distribution versus out-of-distribution performance.", "evidence_reference": "Because in Fig.~\\ref{fig:exp:dynamics} A, both in-distribution and out-of-distribution fluctuates, choosing the best in-distribution checkpoint does not necessarily lead  to the best out-of-distribution checkpoint. This observation is shown in Table~\\ref{tab:exp:model_selection} where if we select the best model based on the GSM8K validation set, it does cannot achieve the best validation performance on the M-A-S OOD setting. Yet choosing the best model based on the M-A-S validation performance leads to a smaller performance drop in GSM8K."}
{"question": "Consider the paper that introduces the dataset which has the largest number of instances, QASUM. What specific threshold value was chosen for the matching score to include an abstract sentence in an aspect-based summary in the model proposed in the paper, and how was this value determined?", "answer": "0.5", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset with the most number of instances?", "answer_anchor": "QASUM", "question_reference": "What specific threshold value was chosen for the matching score to include an abstract sentence in an aspect-based summary in OASum, and how was this value determined?", "explanation_reference": "The threshold value of 0.5 for the matching score was chosen based on manual evaluation of dataset quality. Specifically, different threshold values were tested, and for each, a set of Wikipedia pages was evaluated by experts. The chosen value reflects the balance between content overlap and summary quality, as indicated by the manual evaluation results.", "evidence_reference": "To filter out sentences with limited content overlap, an aspect-based summary includes only abstract sentences with a matching score $\\mathcal{S}(x, a)$ greater or equal to a pre-defined threshold $\\lambda$. To determine the exact value of the threshold, we try $\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]$ and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality. The Cohen's kappa between annotators is calculated to be 0.43, showing moderate agreement. The results are shown in \\cref{tab:quality}. We then choose to use $\\lambda=0.5$."}
{"question": "Consider the paper that introduces the dataset which has the largest number of Queries|Aspects in the OABS category. What specific threshold value was chosen for the matching score to include an abstract sentence in an aspect-based summary in the model proposed by the paper, and how was this value determined?", "answer": "0.5", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset with the most number of Queries|Aspects in OABS category?", "answer_anchor": "QASUM", "question_reference": "What specific threshold value was chosen for the matching score to include an abstract sentence in an aspect-based summary in OASum, and how was this value determined?", "explanation_reference": "The threshold value of 0.5 for the matching score was chosen based on manual evaluation of dataset quality. Specifically, different threshold values were tested, and for each, a set of Wikipedia pages was evaluated by experts. The chosen value reflects the balance between content overlap and summary quality, as indicated by the manual evaluation results.", "evidence_reference": "To filter out sentences with limited content overlap, an aspect-based summary includes only abstract sentences with a matching score $\\mathcal{S}(x, a)$ greater or equal to a pre-defined threshold $\\lambda$. To determine the exact value of the threshold, we try $\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]$ and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality. The Cohen's kappa between annotators is calculated to be 0.43, showing moderate agreement. The results are shown in \\cref{tab:quality}. We then choose to use $\\lambda=0.5$."}
{"question": "Consider the paper that introduces the method that achieves a score of 31.8 in the Seen, Test, SR dataset. How does its performance with OSCAR initialization and without predicting the parent object or visual region classification compare on the Seen and Unseen validation folds for the Task and GC metrics?", "answer": "Seen Task: 37.44%, Seen GC: 44.62%, Unseen Task: 5.73%, Unseen GC: 15.91%", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the score of 31.8 in Seen, Test, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "How does the performance of EmBERT with OSCAR initialization and without predicting the parent object or visual region classification compare on the Seen and Unseen validation folds for the Task and GC metrics?", "explanation_reference": "The question specifically targets the performance metrics of EmBERT under a particular configuration\u2014using OSCAR initialization but without predicting the parent object ('P(O)') or utilizing the visual region classification (VRC) loss. The answer directly corresponds to the performance metrics provided in the validation fold performance table for this specific configuration.", "evidence_reference": "OSCAR & 18 & 200 & \\cblkmark & & & \\B{37.44} (\\B{28.81}) & \\B{44.62} (\\B{36.41}) & \\B{\\phantom{0}5.73} (\\B{\\phantom{0}3.09}) & \\B{15.91} (\\B{\\phantom{0}9.33})"}
{"question": "Consider the paper that introduces the method that corresponds to the second highest Acc-5 score on MOSI. Based on the ablation study results, what combination of tasks achieved the highest F1-Score for the model proposed in the paper in 'negative/positive' right calculation method?", "answer": "M, T, V", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "2102.04830", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method demonstrates the highest Acc-5 score on MOSI", "answer_anchor": "Self-MM", "question_reference": "Based on the ablation study results, which combination of tasks achieved the highest F1-Score for the right calculation method in 'negative/positive'?", "explanation_reference": "The combination of tasks 'M, T, V' (multimodal, text, and vision) achieved the highest F1-Score for the right calculation method in 'negative/positive', which is indicated as 86.00 in the ablation study results table.", "evidence_reference": "M, T, V    & 0.714          & 0.797          & 84.26/85.91 & 84.33/86.00"}
{"question": "Consider the paper that introduces the method that exhibits a score of 34.9 in the Acc-7 metric on MOSI. Which specific example in the qualitative analysis section demonstrates a scenario where the spoken words are ambiguous, but the acoustic and visual modalities provide complementary evidence leading to a correct positive sentiment prediction by the model proposed in the paper?", "answer": "The second example", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method that shows 34.7 score in Acc-7 metric on MOSI?", "answer_anchor": "TFN", "question_reference": "In the qualitative analysis section, which specific example demonstrates a scenario where the spoken words are ambiguous, but the acoustic and visual modalities provide complementary evidence leading to a correct positive sentiment prediction by the TFN model?", "explanation_reference": "The question focuses on a detailed part of the qualitative analysis where the TFN model's ability to integrate complementary evidence from acoustic and visual modalities, despite ambiguous spoken words, leads to a correct positive sentiment prediction. This is explicitly mentioned in the description of the second example in the qualitative analysis section.", "evidence_reference": "In the second example, the spoken words are ambiguous since the model has no clue what a B is except a token, but the acoustic and visual modalities are bringing complementary evidences. Our TFN approach correctly identify this trimodal interaction and predicts a positive sentiment."}
{"question": "Consider the paper that introduces the dataset which exhibits the highest accuracy for Method 2. What specific performance improvement does using dropout as a regularizer provide for solution-level verifiers compared to token-level verifiers?", "answer": "Similar level of performance as token-level verifiers", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What dataset demonstrates the highest accuracy with method 2?", "answer_anchor": "GSM8K", "question_reference": "What specific performance improvement does using dropout as a regularizer provide for solution-level verifiers compared to token-level verifiers?", "explanation_reference": "The paper mentions that using dropout significantly improves solution-level verifiers, mitigating the overfitting that occurs in the unregularized baseline, and notably reaches a similar level of performance as token-level verifiers. This indicates that while token-level verifiers are less susceptible to overfitting and thus might not see as significant an improvement from dropout, solution-level verifiers benefit substantially, enough to match the performance of the more robust token-level verifiers.", "evidence_reference": "In \\Cref{fig:single_token_dropout}, we see that dropout significantly improves solution-level verifiers, mitigating the overfitting that occurs in the unregularized baseline. Notably, using dropout with solution-level verifiers reaches a similar level of performance as token-level verifiers."}
{"question": "Consider the paper that introduces the model shown in the figure represented by the pink line. What specific condition must hold for the kernel estimator in the WR algorithm to ensure the convergence of the cdf $F^*(x)$ of $X^*$ to the target $F_0(x)$ as $n$ approaches infinity?", "answer": "classical assumption \\begin{description} \\item {\\bf (C)} : $h_n^k + \\frac{\\log(n)}{nh_n} =O( e_n^2)$ holds and  $f \\in {\\cal C}^k$ ($k$ times derivable) for some $k\\in \\N^*$. \\end{description}$", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shown in the figure is represented by the pink line?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition must hold for the kernel estimator in the WR algorithm to ensure the convergence of the cdf $F^*(x)$ of $X^*$ to the target $F_0(x)$ as $n$ approaches infinity?", "explanation_reference": "The condition specified ensures that the kernel estimator used in the WR algorithm has the appropriate convergence properties. It combines requirements on the bandwidth $h_n$, the sample size $n$, and the smoothness of the density function $f$, which must be $k$ times derivable. This condition is necessary to guarantee that the cumulative distribution function (cdf) of the resampled $X^*$ converges to the target distribution $F_0(x)$ as the sample size $n$ grows to infinity.", "evidence_reference": "We introduce a classical assumption \\begin{description} \\item {\\bf (C)} : $h_n^k + \\frac{\\log(n)}{nh_n} =O( e_n^2)$ holds and  $f \\in {\\cal C}^k$ ($k$ times derivable) for some $k\\in \\N^*$. \\end{description}"}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-De task in the Test2016 dataset in Previous Image-must Systems. What is the average BLEU score improvement of this model over the text-only baseline on the EN$\\rightarrow$DE task for the Transformer-Tiny model on the Multi30K dataset?", "answer": "2.1", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model demonstrates the highest performance in En-De task in Test2016 dataset in Previous Image-must Systems?", "answer_anchor": "VALHALLA", "question_reference": "What is the average BLEU score improvement of the model proposed in the paper over the text-only baseline on the EN$\\rightarrow$DE task for the Transformer-Tiny model on the Multi30K dataset?", "explanation_reference": "The question focuses on the specific detail of the average BLEU score improvement achieved by VALHALLA over the text-only baseline for the EN$\\rightarrow$DE task using the Transformer-Tiny model on the Multi30K dataset. The answer is directly provided in the Results on Multi30K section, where it states that using Transformer-Tiny as the backbone, VALHALLA obtains an average 35.4 BLEU in EN$\\rightarrow$DE, which is about 2.1 BLEU improvements over the text-only baseline.", "evidence_reference": "Using Transformer-Tiny as the backbone, \\ours obtains an average $35.4$ BLEU in EN$\\rightarrow$DE and $54.4$ BLEU in EN$\\rightarrow$FR, which is about $2.1$ and $1.4$ BLEU improvements over the text-only baseline."}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that has a Test Accuracy of 79.6. What specific method does the model's solution discrimination module employ to enhance the association between a math word problem and its correct solution?", "answer": "Gradient-guided token selection", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which Seq2Seq model shows 79.6 Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module employ to enhance the association between a math word problem and its correct solution?", "explanation_reference": "The solution discrimination module enhances the association between a math word problem and its correct solution by employing a method that focuses on gradient-guided token selection. This method identifies the most vulnerable (important) token in the solution equation based on the largest gradient, aiming to manipulate it for generating hard negative samples. This approach is designed to make the solver more accurately associate problems with their correct solutions by distinguishing them from similar but incorrect solutions.", "evidence_reference": "For generating negative solutions, any manipulation of the ground truth solution can lead to a negative one, as implemented in  \\cite{ijcai2021}. However, the  random modification neglects   the importance of tokens   in the solution equation. Although all negative solutions ultimately lead to a wrong answer, the roles they play in minimizing loss functions and serving as contrastive examples to a positive true solution are at different levels of importance.  Our goal is to find  variants of the ground truth solution as hard negative samples, which only manipulate the most vulnerable (important) token.  In fact, this target is  similar to evasion attack \\cite{carlini2017towards} on texts, i.e., maximum effect and minimum manipulation. Therefore, borrowing the idea from white-box evasion attack, we regard the token with the largest gradient as the most important and vulnerable one: \\begin{equation} \\label{grad} y_i = \\mathop{argmax\\;}_{y_i \\in Y}(\\nabla Dis([en_x(X),en_y(Y)]). \\end{equation}"}
{"question": "Consider the paper that introduces the method that exhibits the highest score in the Seen, Val, SR dataset. What specific architectural feature allows the model proposed in the paper, EmBERT, to handle long-horizon planning effectively?", "answer": "Segment-Level Recurrent Action Decoder where involves 1. Object-centric navigation 2. Decoupled Multimodal Transformers", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2108.04927", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the best score in Seen, Val, SR dataset?", "answer_anchor": "EmBERT", "question_reference": "What specific architectural feature allows EmBERT to handle long-horizon planning effectively?", "explanation_reference": "The Segment-Level Recurrent Action Decoder is designed to model long trajectories with recurrent segment-level state reuse, effectively handling the quadratic complexity of the self-attention mechanism in transformers for long sequences. This architectural feature specifically addresses the challenge of long-horizon planning in the ALFRED benchmark.", "evidence_reference": "Inspired by the TransformerXL model, we design the Segment-Level Recurrent Action Decoder architecture that models long trajectories with recurrent segment-level state reuse. At training time we divide trajectories into temporal segments of size $s$. Given two consecutive segments, $\\mathbf{s}_i$ and $\\mathbf{s}_{i+1}$, \\modelac\\ caches the representations generated for segment $\\mathbf{s}_i$. The computed gradient does not flow from $\\mathbf{s}_{i+1}$ to $\\mathbf{s}_i$, but cached representations are used as extended context. When predicting the next action, the model can still perform self-attention over the previous segment representations, effectively incorporating additional contextual information that spans a high number of previous timesteps."}
{"question": "Consider the paper that introduces the LLM shown in the figure with a model size of 1.7T. What specific methodological difference in the evaluation of its performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "answer": "The specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams that contributed to potential minimal impact on results was the direct sampling of a letter choice at temperature 0 using the already-sampled explanation for these exams, as opposed to extracting the model's letter choice directly from the explanation for most other exam runs.", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the large language model that has 1.7T model size?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard procedure used for most exam runs, where the model's letter choice is extracted directly from the explanation. This approach for the USABO and SAT reading/writing runs indicates a unique handling of these exams, which could contribute to the minimal impact on the overall results, as it relies on the model's generated explanation to determine the final answer choice.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the large language model that corresponds to an HVI score of 47. What specific methodological difference in the evaluation setup might have impacted the reported performance of this model, specifically on the USABO and SAT reading/writing exams compared to other exams?", "answer": "The specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams compared to other exams that might have impacted its reported performance is the sampling of a letter choice at temperature 0 using the already-sampled explanation for these exams, rather than extracting the model's letter choice directly from the explanation as done for most other exam runs.", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the large language model that demonstrates 47 HVI scores?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams compared to other exams might have impacted its reported performance?", "explanation_reference": "This methodological difference is significant because sampling at temperature 0 can lead to more deterministic outcomes based on the generated explanation, potentially affecting the model's performance on these exams in a way that differs from how choices were determined in other exams.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the model that exhibits the most negative Spearman's Correlation Coefficient. Which variant of DialoGPT was statistically indistinguishable from human responses in terms of relevance based on the human evaluation results?", "answer": "DialoGPT (345M, w/ MMI)", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model in the figure has the highest Spearman's Correlation?", "answer_anchor": "DialoGPT Large", "question_reference": "Based on the human evaluation results, which variant of DialoGPT was statistically indistinguishable from human responses in terms of relevance?", "explanation_reference": "The statistical significance testing for human evaluation showed that the differences between the 345M model and human responses in terms of relevance were not statistically significant, indicating that the 345M variant of DialoGPT was indistinguishable from human responses in this aspect.", "evidence_reference": "The differences between 345M model (2) and human response (1) are not statistically significant."}
{"question": "Consider the paper that introduces the method that has the highest score in the WQ-R task. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions by the model proposed in the paper?", "answer": "0.949", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has the highest score in WQ R task?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the dataset which includes 1 SM task and 4 languages. What is the primary reason that applying Language Adaptive Fine-tuning (LAFT) on its Twitter domain did not improve performance?", "answer": "The primary reason applying Language Adaptive Fine-tuning (LAFT) on the Twitter domain did not improve performance is the small size of the Twitter data.", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset with 1 SM task and 4 languages?", "answer_anchor": "NaijaSenti", "question_reference": "What is the primary reason applying Language Adaptive Fine-tuning (LAFT) on the Twitter domain did not improve performance?", "explanation_reference": "The paper explains that applying LAFT on the Twitter domain did not improve performance primarily due to the small size of the Twitter data available for pre-training, which is often short and less comprehensive compared to the general domain data.", "evidence_reference": "Interestingly, applying LAFT on the Twitter domain did not improve performance. The main reason for this is the small size of the Twitter data."}
{"question": "Consider the paper that introduces the method that results in a score of 22.4 in the GSM8K dataset. What specific dynamic programming algorithm does the paper tweak for aligning the tokenizers between GPT and T5 models?", "answer": "Needleman\u2013Wunsch algorithm", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2301.12726", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method demonstrates score of 22.4 in GSM8K dataset?", "answer_anchor": "SpecialFT", "question_reference": "What specific dynamic programming algorithm does the paper tweak for aligning the tokenizers between GPT and T5 models?", "explanation_reference": "The paper mentions tweaking a textbook dynamic programming algorithm used in bioinformatics for sequence alignment, specifically citing the Needleman\u2013Wunsch algorithm as an example of such an algorithm that they adapted for their purpose of aligning token sequences between different models.", "evidence_reference": "Our dynamic program is a slight tweak of the textbook dynamic programming algorithms used in bioinformatics for sequence alignment (such as the Needleman\u2013Wunsch algorithm~\\cite{Needleman1970AGM}) and in signal processing (such as dynamic time wrapping~\\cite{Senin2008DynamicTW})."}
{"question": "Consider the paper that introduces the model that corresponds to the penultimate row of the table. What is the observed accuracy drop percentage for the model proposed in the paper on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "answer": "9.7", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model is shown in the penultimate row in the table?", "answer_anchor": "CEAR", "question_reference": "What is the observed accuracy drop percentage for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "explanation_reference": "The accuracy drop for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00) is directly reported in the empirical study results table.", "evidence_reference": "CRL & [0.85, 1.00) & 71.1 & 9.7 & 64.8 & 11.4"}
{"question": "Consider the paper that introduces the method which has 11.0 rounds to completion. What is the impact of using a death mask with agent-specific global state (AS) on the model's performance in the SMAC domain?", "answer": "Using a death mask with agent-specific global state (AS) significantly improves MAPPO's performance in the SMAC domain.", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having 11.0 rounds to Completion?", "answer_anchor": "MAPPO", "question_reference": "What is the impact of using a death mask with agent-specific global state (AS) on MAPPO's performance in the SMAC domain?", "explanation_reference": "The death mask effectively handles the drastic distribution shift in the critic input when an agent dies, by replacing the state for a dead agent with a zero state containing the agent's ID. This approach leads to superior performance compared to other methods of handling agent deaths, as it allows the critic to more accurately fit the average post-death reward for agent $a$ to the input $\\boldsymbol{0_a}$.", "evidence_reference": "Fig.~\\ref{fig:app-Ablation-death-new} demonstrates the effect of death mask on MAPPO(AS)'s performance in the SMAC domain, showing significant improvement."}
{"question": "Consider the paper that introduces the model in the figure corresponds to the grey line with a star marker. What specific preprocessing steps were applied to the XML files during the data curation process for StarCoderBase?", "answer": "Implemented a simple XML filter that checked for the presence of '<?xml version=' within the first 100 characters of the file.", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shown in the figure represented by grey line with star marker?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing steps were applied to the XML files during the data curation process for StarCoderBase?", "explanation_reference": "The question focuses on the specific method used to preprocess XML files during the data curation process for StarCoderBase. The answer directly addresses this by specifying the implementation of a simple XML filter, which is a detailed and specific part of the preprocessing steps mentioned in the paper.", "evidence_reference": "As we inspected the data, we noticed that certain extensions often consisted of XML files. For example, the .sld extension had more than 50% of its files in XML format. To address this, we implemented a simple XML filter that checked for the presence of '<?xml version=' within the first 100 characters of the file."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. How does the performance of the model proposed in the paper, named ViTCAP, with concepts extracted from captions compare to using concepts derived from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "answer": "Using concepts extracted from captions leads to better performance (121.8 CIDEr) compared to using concepts derived from an object detector (117.4 CIDEr for BUTD and 119.7 CIDEr for VinVL).", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method is shown in the penult row?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP with concepts extracted from captions compare to using concepts derived from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "explanation_reference": "The question focuses on comparing the effectiveness of different sources of semantic concepts (caption-extracted vs. object detector-derived) on the performance of the ViTCAP model, specifically in terms of CIDEr scores on the COCO-caption Karpathy split. The answer directly addresses this by providing the specific CIDEr scores achieved using each method, indicating that caption-extracted concepts result in better performance.", "evidence_reference": "FOCAL$_\\text{Tag}$ & $10$ & $35.2$ & $28.0$ & $57.0$ & $117.1$ & $21.4$\\\\ FOCAL$_\\text{Tag+Init}$ & $10$ & $36.0$ & $28.4$ & $57.5$ & $120.5$ & $22.0$\\\\ FOCAL$_\\text{Init}$ & $10$ & $35.0$ & $28.2$ & $57.1$ & $118.0$ & $21.6$\\\\ FOCAL$_\\text{Tag+Init}$ & $40$ & $35.9$ & $28.4$ & $57.6$ & $121.1$ & $22.1$ \\\\ PRED. Concepts & $36.1$ & $28.6$ & $57.6$ & $120.6$ & $21.7$"}
{"question": "Consider the paper that introduces the method that has a lower Hits@1 score than ReasoningLM but a higher Hits@1 score than NSM across all fine-tuning samples. What is the core innovation of the model proposed in the paper in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "answer": "The core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks is the unification of retrieval and reasoning in both model architecture and parameter learning.", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has lower Hit@1 score than ReasoningLM but higher Hit@1 score than NSM as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What is the core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the question by summarizing the unique approach of UniKGQA, which integrates the retrieval and reasoning processes into a single framework. This is innovative because it contrasts with previous methods that treated these stages separately, thus enhancing the efficiency and effectiveness of solving multi-hop KGQA tasks.", "evidence_reference": "In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning."}
{"question": "Consider the paper that introduces the method that has an average score of 82.8 with zero-shot prompting. What specific adaptation in the text embeddings allows the model proposed in the paper to build correspondence among query text, label text, and objects in grounding tasks?", "answer": "Embedding sharing", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method has average score of 82.8 with zero-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific adaptation in the text embeddings allows the model to build correspondence among query text, label text, and objects in grounding tasks?", "explanation_reference": "The specific adaptation that allows the model to build correspondence among query text, label text, and objects in grounding tasks is the use of embedding sharing. This is achieved by reusing the text embeddings of visual sentinel tokens as region id embeddings, which enables the model to establish a connection between the text and visual elements, particularly useful in grounding tasks.", "evidence_reference": "In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens \\{\\texttt{<vis\\_1>}, $\\dots$, \\texttt{<vis\\_n>}\\}, which corresponds to image regions. As illustrated in Fig.~\\ref{fig:architecture}, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec.~\\ref{sec:visual_embeddings}. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec.~\\ref{sec:pretraining}, referring expression comprehension in Sec.~\\ref{sec:refcoco})."}
{"question": "Consider the paper that introduces the method which has approximately 30 perplexity and the highest average max toxicity. What is the model's success rate (SR) percentage for the Guide Context strategy with a lambda value of 20?", "answer": "95.1%", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is method with around 30 perplexity and the highest average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What is the success rate (SR) percentage for the Guide Context strategy with a lambda value of 20?", "explanation_reference": "The success rate (SR) for the Guide Context strategy with a lambda value of 20 is directly reported in the paper's results, indicating the effectiveness of this strategy at ensuring the appearance of guide words in the generated text.", "evidence_reference": "Section name: Hyperparameter Analysis_10\\nparagraphs: \\emph{C.} $\\lambda=20$ & \\textbf{95.1} \\rpm 2.3 & 99.3 \\rpm 20.1 & 13.4 \\rpm 2.1"}
{"question": "Consider the paper that introduces the model that results in the highest Self-BLEU score on the TellMeWhy dataset. What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "answer": "The paper proposes a novel framework that combines question type prediction and event-centric summarization to generate educational questions for storybooks.", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model shows the highest Self-BLEU score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach of combining question type prediction with event-centric summarization, which is the core methodological innovation of the study for generating educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the model that scores an 84.2 in the 'T10' column. What hyperparameter values were used for the FewRel dataset in the experiments?", "answer": "$\\alpha=0.5$, $\\beta=0.5$, $\\tau_1=0.1$, $\\mu=0.5$, $\\omega=0.1$, $\\tau_2=0.5$, $\\gamma=1.25$, $\\lambda_1=0.5$, $\\lambda_2=1.1$.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets score of 84.2 in 'T10' column?", "answer_anchor": "CEAR", "question_reference": "What hyperparameter values were used for the FewRel dataset in the experiments?", "explanation_reference": "The answer directly lists the specific hyperparameter values used for the FewRel dataset as mentioned in the Implementation Details section of the paper, providing precise and concise information.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the method that has a perplexity of 60. What specific computational advantage does the model proposed in the paper's online classification trick offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "answer": "GeDi's online classification trick can compute $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier.", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having 60 perplexity?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's online classification trick offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "explanation_reference": "The computational advantage is highlighted by comparing the efficiency of GeDi's method to compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token using only two parallel forward passes (one for each control code) against the requirement of a unidirectional classifier needing $|\\\\gV|$ forward passes for a vocabulary set $\\\\gV$. This comparison underlines GeDi's significant reduction in computational demand.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the method that is represented by the purple line. What specific methodological adjustment is made to its variant, MMA-H, to address the potential issue of outlier attention heads affecting latency or attention span?", "answer": "attention variance loss to MMA-H", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is demonstrated by the purple line?", "answer_anchor": "MMA", "question_reference": "What specific methodological adjustment is made to the MMA-H variant to address the potential issue of outlier attention heads affecting latency or attention span?", "explanation_reference": "The attention variance loss (L_var) is introduced specifically for the MMA-H variant to control the variance in attention spans across different heads, aiming to prevent outlier heads from significantly affecting the model's latency or the uniformity of attention spans. This methodological adjustment is designed to mitigate the potential issue of having some heads that either read too fast or too slow, which could lead to inefficiencies in simultaneous translation.", "evidence_reference": "In \\autoref{sec:latency-control}, we introduced the attention variance loss to MMA-H in order to prevent outlier attention heads from increasing the latency or increasing the attention span."}
{"question": "Consider the paper that introduces the text-davinci-002 GPT-3.5 model within the CO\\textsubscript{3} framework using the dataset for the 'dialogue response' task. What specific hyperparameter settings were employed for generating conversations grounded in narrative?", "answer": "Temperature: 0.9, top-p: 0.95, frequency penalty: 1.0, presence penalty: 0.6, max tokens: 1024.", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "2212.10465", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset used for Dialogue response task?", "answer_anchor": "SODA", "question_reference": "What specific hyperparameter settings were used for generating conversations grounded in narrative using the text-davinci-002 GPT-3.5 model within the CO\\textsubscript{3} framework?", "explanation_reference": "The question asks for the detailed hyperparameter settings specific to the process of generating conversations from narratives within the CO\\textsubscript{3} framework, which is a detail that directly relates to the methodology of the paper. The answer provides these settings, which are crucial for replicating the conversation generation process described in the paper.", "evidence_reference": "We leverage \\texttt{text-davinci-002} GPT-3.5 for generating narratives. We set temperature to 0.9, top-p to 0.95, frequency penalty to 1.0, presence penalty to 0.6, and max tokens to 1024."}
{"question": "Consider the paper that introduces the method that achieves a higher score than No Graph but a lower score than TOD-Flow using GPT-turbo with SGD in 24 domains. How does the recency weighting in the complexity-regularized ILP method for graph generation from subtask state labels, as proposed in the paper, influence the precondition inference?", "answer": "It assigns higher weight to data samples where a subtask has become eligible more recently, influencing the precondition inference by prioritizing recent eligibility over older instances.", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method has higher score than No Graph but lower score than TOD-Flow using GPT-turbo with SGD in 24 domains?", "answer_anchor": "MSG^2", "question_reference": "How does the recency weighting in the complexity-regularized ILP method for graph generation from subtask state labels influence the precondition inference?", "explanation_reference": "The recency weighting modifies the precondition inference process by incorporating temporal information, specifically by giving more importance to recent subtask eligibility. This approach helps in dealing with the sparse and noisy data by emphasizing the most relevant information for determining subtask dependencies.", "evidence_reference": "w_{t, n} = \\max(0.1, \\lambda ^ {t_n-t}), where $0<\\lambda<1$ is the discount factor, $t_n$ is the time step when the precondition for subtask $n$ became satisfied."}
{"question": "Consider the paper that introduces the method which corresponds to the penultimate row in the figure. How does the MSG^2 method ensure the prevention of cycles in the resulting subtask graph proposed by the paper, which could lead to a causality paradox?", "answer": "By performing precondition inference in a layer-wise fashion.", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown in the figure in the penultimate row?", "answer_anchor": "MSG^2", "question_reference": "How does the proposed Multimodal Subtask Graph Generation (MSG2) approach ensure the prevention of cycles in the resulting subtask graph, which could lead to a causality paradox?", "explanation_reference": "The paper addresses the potential issue of forming cycles in the resulting subtask graph, which could lead to a causality paradox, by performing precondition inference in a layer-wise fashion. This method ensures that the edge in the subtask graph is formed from the lower depth to the higher depth, preventing the formation of cycles.", "evidence_reference": "One major problem of inferring the precondition independently for each subtask is the possibility of forming a cycle in the resulting subtask graph, which leads to a causality paradox (\\ie, subtask A is a precondition of subtask B and subtask B is a precondition of subtask A). To avoid this problem, we perform precondition inference in a layer-wise fashion similar to~\\citet{sohn-iclr20}."}
{"question": "Consider the paper that introduces the model that scores higher than ACA but lower than RationaleCL in the 'T5' column. What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the model proposed in the paper?", "answer": "$\\alpha=0.5$, $\\beta=0.5$, $\\tau_1=0.1$, $\\mu=0.5$, $\\omega=0.1$, $\\tau_2=0.5$, $\\gamma=1.25$, $\\lambda_1=0.5$, $\\lambda_2=1.1$.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets score higher than ACA but lower than RationalCL in 'T5' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the proposed model?", "explanation_reference": "The answer directly lists the specific hyperparameter values that were determined through grid search for the FewRel dataset, as detailed in the implementation section of the paper.", "evidence_reference": "We find the best hyperparameter values through grid search with a step of 0.1 except 0.05 for \u03c9 and 0.25 for \u03b3. The search spaces for various hyperparameters are \u03b1\u2208[0.2,0.8], \u03b2\u2208[0.1,0.5], \u03bc\u2208[0.1,1.0], \u03c9\u2208[0.05,0.25], \u03b3\u2208[1.0,2.0] and \u03bb1, \u03bb2\u2208[0.5,1.5]. Besides, we fix \u03c41 and \u03c42 to 0.1 and 0.5, respectively. The used hyperparameter values are listed below: For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the dataset represented by the smallest blue circle. What is the Fleiss kappa inter-annotator agreement score for the Yoruba language when considering the 3-class sentiment classification?", "answer": "0.600", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset represented by the smallest blue label?", "answer_anchor": "NaijaSenti", "question_reference": "What is the Fleiss kappa inter-annotator agreement score for the Yoruba language when considering the 3-class sentiment classification?", "explanation_reference": "The Fleiss kappa score for the Yoruba language in the 3-class sentiment classification setup indicates the level of agreement among annotators on the sentiment classification of tweets in Yoruba. This score is a measure of the reliability of agreement between annotators beyond chance.", "evidence_reference": "IAA ($\\kappa$) for Yoruba in the 3-class setup is $0.600$."}
{"question": "Consider the paper that introduces the large language model that achieves a lower HVI score than OPT but a higher HVI score than Alpaca. What specific methodological difference in the evaluation setup for the model's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "answer": "The use of sampling a letter choice at temperature 0 using the already-sampled explanation for certain exams, rather than extracting the model's letter choice directly from the explanation.", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the large language model that demonstrates lower HVI score than OPT but higher HVI score than Alpaca?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard approach of directly extracting the model's letter choice from the explanation for most exam runs. Instead, for these specific exams, the approach involved sampling a letter choice at temperature 0 using the explanation already sampled, indicating a unique handling of these exams compared to others.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the dataset with the largest number of dialogues. What is the percentage of dialogues in that dataset that achieved a goal completion score of 2 in the norm generation model?", "answer": "43%", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset has the most number of dialogues?", "answer_anchor": "DuConv", "question_reference": "What is the percentage of dialogues in the norm generation model that achieved a goal completion score of 2?", "explanation_reference": "The percentage of dialogues that achieved a goal completion score of 2 in the norm generation model is directly reported in the analysis of goal completion and knowledge exploitation.", "evidence_reference": "goal completion & 2 & 43%"}
{"question": "Consider the paper that introduces the model that has a score lower than 0.82 but higher than 0.815 in the Stance column. In its methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "answer": "-\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t)", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model has accuracy consistently lower than 0.82 across all volumne of adaptive data?", "answer_anchor": "DPT", "question_reference": "In the methodology of quantifying temporal degradation (TD), what mathematical modification is applied to the difference in performance between aligned and misaligned models to ensure that, as performance deteriorates, the calculated value increases regardless of the direction of time between the training and evaluation timestamps?", "explanation_reference": "The modification ensures that the calculated value reflects performance deterioration in a consistent manner, increasing as performance worsens, irrespective of whether the misalignment is due to training data being from the past or future relative to the evaluation data.", "evidence_reference": "Let $S_{t' \\shortto t}$ indicate the performance a model trained on timestamp $t'$ data and evaluated on the timestamp $t$. Let $$ {D} (t' \\shortto{} t) = -\\left(S_{t' \\shortto t} - S_{t \\shortto t} \\right) \\times \\text{sign}(t' - t), $$  In other words, ${D} (t' \\shortto{} t)$ is a modified difference in performance between a aligned and misaligned models."}
{"question": "Consider the paper that introduces the method which is in the second row of the table. What specific algorithmic enhancement does the model proposed in the paper, identified as SPADE, apply to handle the issue of tail-sharing edges in the spatial dependency graphs, and what is its impact on the F1 score for the \\cord\\ dataset?", "answer": "SPADE applies a tail collision avoidance algorithm to handle the issue of tail-sharing edges in the spatial dependency graphs. This algorithm iteratively trims the tail-sharing edges, keeping only the one with the highest linking probability, and generates new edges until the process becomes self-consistent. This enhancement leads to an increase in the F1 score by +1.0% for the \\cord\\ dataset with oracle (ground truth OCR results), and by +0.8% without the oracle.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method in the second row of the table?", "answer_anchor": "SPADE", "question_reference": "What specific algorithmic enhancement does SPADE apply to handle the issue of tail-sharing edges in the spatial dependency graphs, and what is its impact on the F1 score for the \\cord\\ dataset?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically designed to address the issue of tail-sharing edges in spatial dependency graphs by iteratively trimming these edges and generating new ones until the process becomes self-consistent. This algorithmic enhancement is critical for improving the model's ability to accurately parse and extract information from documents, as evidenced by the reported improvements in the F1 score for the \\cord\\ dataset.", "evidence_reference": "Using this property, we integrate Tail Collision Avoidance algorithm (\\caabb) that iteratively trims the tail-sharing-edges and generate new edges until the process becomes self-consistent (Section \\ref{sec:graph_gen}). $F_1$ increases by +1.0\\% and +0.8\\% with and without the oracle upon the integration (2nd row, \\cordabb)."}
{"question": "Consider the paper that introduces the method that achieves a higher accuracy than DExpert but lower than Air-Decoding. How does the gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) in the simplified theoretical analysis of the model's, proposed by the paper, loss function indicate the direction of model optimization?", "answer": "-2*p_{\\text {unlike}}", "figure": "locality/2310.14892/result_table.png", "anchor_arxiv_id": "2310.14892", "reference_arxiv_id": "2210.09551", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows a higher accuracy than DExpert but lower than Air-Decoding?", "answer_anchor": "Discup", "question_reference": "How does the gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) in the simplified theoretical analysis of DisCup's loss function indicate the direction of model optimization?", "explanation_reference": "The gradient expression for the logit corresponding to the unlikely token ($h_{\\text{unlike}}$) being a negative value indicates that model optimization is always in the direction of decreasing the probability of the unlikely token. This is because a negative gradient value for $h_{\\text{unlike}}$ means that the optimization process will work to reduce the logit value of the unlikely token, thereby decreasing its probability in the model's output distribution.", "evidence_reference": "The gradient of logit $h_{unlike}$ could be represented as:  \\begin{equation} \\begin{aligned} \\frac{\\partial \\mathcal{L}_t}{\\partial h_{\\text{unlike}}} & =\\left(0-p_{\\text {unlike}}\\right)- \\frac{p_{\\text {unlike}}}{1-p_{\\text {unlike}}}\\left(1-p_{\\text {unlike}}\\right) \\\\ &=-2*p_{\\text {unlike}} \\end{aligned} \\end{equation}"}
{"question": "Consider the paper that introduces the method that has a score of 71.4 in the CB dataset with 4-shot prompting. What is the specific performance improvement (in percentage points) observed on the SuperGLUE benchmark when using the T5 XXL model compared to the baseline Prompt Tuning?", "answer": "+2.4", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having score of 71.4 in CB dataset with 4-shot prompting?", "answer_anchor": "SPoT", "question_reference": "Based on the SPoT approach, what is the specific performance improvement (in percentage points) observed on the SuperGLUE benchmark when using the T5 XXL model compared to the baseline Prompt Tuning?", "explanation_reference": "The specific performance improvement observed on the SuperGLUE benchmark when using the T5 XXL model with the SPoT approach, compared to the baseline Prompt Tuning, is mentioned as a +2.4 point average accuracy improvement. This detail directly answers the question by specifying the improvement in performance due to the application of SPoT.", "evidence_reference": "For instance, on the SuperGLUE benchmark, we obtain +10.1 and +2.4 point average accuracy improvements using the T5 Base (220M parameter) and T5 XXL (11B parameter) models, respectively."}
{"question": "Consider the paper that introduces the optimization method that exhibits an R2 score of 0.191. How does its reparameterization of the reward function in terms of the corresponding optimal policy and the reference policy address the under-specification issue inherent in the Plackett-Luce family of models?", "answer": "By canceling the partition function in the preference model equation, allowing the human preference probability to be expressed solely in terms of the optimal policy and reference policy.", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What optimization method show 0.191 R2 score?", "answer_anchor": "DPO", "question_reference": "In the context of Direct Preference Optimization (DPO), how does the reparameterization of the reward function in terms of its corresponding optimal policy and the reference policy address the under-specification issue inherent in the Plackett-Luce family of models?", "explanation_reference": "The reparameterization of the reward function in terms of its corresponding optimal policy and the reference policy directly addresses the under-specification issue by canceling out the partition function in the preference model equation. This simplification allows the human preference probability to be expressed solely in terms of the optimal policy and reference policy, thus providing a clear and direct method to optimize the language model policy based on human preferences without the need for an explicit reward model.", "evidence_reference": "Substituting the reparameterization in Eq.~\\ref{eq:main_eq} for $r^*(x,y)$ into the preference model Eq.~\\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\\pi^*$ and reference policy $\\piref$."}
{"question": "Consider the paper that introduces the method which exhibits a score of 34.9 in the Acc-7 metric on MOSI. What specific aspect of spoken language dynamics does the Spoken Language Embedding Subnetwork in the model proposed by the paper focus on to handle the volatile nature of spoken opinions?", "answer": "The Spoken Language Embedding Subnetwork focuses on building models that are capable of operating in the presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech.", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method that shows 34.7 score in Acc-7 metric on MOSI?", "answer_anchor": "TFN", "question_reference": "Based on the critical analysis of the Tensor Fusion Network's approach to multimodal sentiment analysis, what specific aspect of spoken language dynamics does the Spoken Language Embedding Subnetwork focus on to handle the volatile nature of spoken opinions?", "explanation_reference": "The Spoken Language Embedding Subnetwork is designed to handle the volatile nature of spoken language by building models capable of operating in the presence of unreliable and idiosyncratic speech traits. It achieves this by focusing on important parts of speech, which allows the model to maintain the relevance of the utterance's meaning even when encountering unusable information or recovering usable information later in the speech.", "evidence_reference": "The first part conveys the actual message and the rest is speaker thinking out loud eventually agreeing with the first part. The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech."}
{"question": "Consider the paper that introduces the model that achieves a P_k score of 24.8 in the en_disease category. What specific approach does the paper employ to improve the model's ability to generate accurate sentence positions for generative segmentation?", "answer": "Using the $i^\\text{th}$ vocabulary token embedding in place of a fixed BOS token index for the $i^\\text{th}$ sentence.", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets 24.8 P_k score in en_disease category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What specific approach does the paper employ to improve the model's ability to generate accurate sentence positions for generative segmentation?", "explanation_reference": "The paper describes a method to enhance the model's implicit ability to convey position information from input to its decoder, crucial for producing accurate sentence positions in generative segmentation. This is achieved by substituting the fixed BOS token index with the $i^\\text{th}$ vocabulary token embedding for the $i^\\text{th}$ sentence, thereby providing unambiguous position information to the decoder without employing custom schemes like dedicated sentence position embeddings.", "evidence_reference": "At the encoder input for the $i^\\text{th}$ sentence,  we use the $i^\\text{th}$ vocabulary token embedding in place of a fixed BOS token index. Formally, in contrast to \\eqref{eqn:bos}, we set \\begin{equation*} t_{1_1} = 0, \\hspace{2mm}  t_{2_1} = 1, \\hspace{2mm} \\ldots,  t_{|S|_1} = |S|-1. \\end{equation*}"}
{"question": "Consider the paper that introduces the method that demonstrates the lowest EA score on the FinQA task. What specific feature of the BERT model's output is utilized in the model's policy network for dynamic prompting via policy gradient in PromptPG?", "answer": "\\texttt{[CLS]} token representation", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method demonstrates the lowest EA score in FinQA task?", "answer_anchor": "PromptPG", "question_reference": "What specific feature of the BERT model's output is utilized in the policy network for dynamic prompting via policy gradient in PromptPG?", "explanation_reference": "The policy network in PromptPG uses the BERT model's output for generating contextualized representations of the given problem and candidate examples. Specifically, it utilizes the [CLS] token representation from BERT's output as the problem encoding, which is then used to learn both the semantic similarity provided by the pre-trained BERT model and the hidden logical similarity among the math problems.", "evidence_reference": "To get the contextualized representation of the given problem and candidate examples, we use the BERT~[CLS] token representation as the problem encoding. We add a small linear layer on top of the BERT final pooling layer."}
{"question": "Consider the paper that introduces the method whose results are displayed in a lighter grey color in the table. How does the model's performance on the visual question answering (VQA) task compare when using images of resolution 480x480 versus 640x640 during finetuning?", "answer": "An improvement", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is using grey-ish color for showing results in the table?", "answer_anchor": "OFA-base", "question_reference": "How does the OFA model's performance on the visual question answering (VQA) task compare when using images of resolution 480x480 versus 640x640 during finetuning?", "explanation_reference": "The paper mentions that when finetuning OFA models, particularly the $\\text{OFA}\\rm_{Large}$ and $\\text{OFA}\\rm_{Huge}$, the image resolution is increased from 480x480 to 640x640. This detail implies that using a higher resolution during finetuning is expected to improve the model's performance on tasks like VQA, as higher resolution images provide more detailed visual information for the model to analyze.", "evidence_reference": "When finetuning $\\text{OFA}\\rm_{Large}$ and $\\text{OFA}\\rm_{Huge}$, we increase the image resolution from $480$ to $640$."}
{"question": "Consider the paper that introduces the method that achieves an average EA score of 67.07 in the FinQA task. What is the model's, proposed by the paper, EM score using the reverse order of in-context examples on the NQ dataset?", "answer": "42.8", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets 67.07 EA score in FinQA task", "answer_anchor": "KATE", "question_reference": "What is the EM score for KATE using the reverse order of in-context examples on the NQ dataset?", "explanation_reference": "The EM score for KATE using the reverse order of in-context examples on the NQ dataset directly reflects the model's performance under this specific condition, which is detailed in the ablation study on the effect of in-context example orders.", "evidence_reference": "For the default order, the example $A$ is to the left of example $B$ if $A$ is closer to the test prompt $x$ than $B$ in the embedding space. For the reverse order, the example A is to the right of example B... The reverse order performs the best... 42.8"}
{"question": "Consider the paper that introduces the model that has a score lower than 0.82 but higher than 0.815 in the Stance column. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model has accuracy consistently lower than 0.82 across all volumne of adaptive data?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength and direction of a linear relationship between two variables. In this case, it quantifies the relationship between the word overlap (how vocabularies change over time) and the model performance for the task of political affiliation classification on Twitter data. A value of 0.9817159316285563 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that is placed directly above the PHA method in the table. What specific advantage does the HyperDecoder approach offer for handling long-context out-of-domain datasets in MRQA compared to a simple linear classifier?", "answer": "The Hyperdecoder approach offers the specific advantage of generating unique decoder layers for every input into a model, which allows for more flexible adaptation to long-context out-of-domain datasets in MRQA by leveraging similarities between samples across datasets and avoiding potential interference within the same dataset.", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shown in the table is right above PHA method?", "answer_anchor": "HyperDecoder", "question_reference": "What specific advantage does the Hyperdecoder approach offer for handling long-context out-of-domain datasets in MRQA compared to a simple linear classifier?", "explanation_reference": "The Hyperdecoder's ability to control the decoder to output specific labels when needed, and its flexibility to generate arbitrary text output for long-context out-of-domain datasets, distinguishes it from a simple linear classifier. This flexibility is crucial for multi-tasking and handling long-context documents where the model must switch between generating short set labels and arbitrary longer text.", "evidence_reference": "This is especially important for multi-tasking and long-context documents where the model must swap between generating short set labels and arbitrary longer text."}
{"question": "Consider the paper that introduces the method which has a score of 73.6 in the CB dataset with 4-shot prompting. What is the relative improvement percentage of this method over vanilla PT on the SuperGLUE benchmark?", "answer": "16.3\\%", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having score of 73.6 in CB dataset with 4-shot prompting?", "answer_anchor": "MPT", "question_reference": "What is the relative improvement percentage of \\ours over vanilla PT on the SuperGLUE benchmark?", "explanation_reference": "The relative improvement percentage is calculated based on the performance improvement of \\ours over vanilla PT on the SuperGLUE benchmark, as mentioned in the Results and Analysis section.", "evidence_reference": "When compared to vanilla PT~\\citep{lester2021power}, \\ours obtains a relative improvement of $13\\%$ on GLUE and  $16\\%$ on SuperGLUE with the same number of task-specific parameters, highlighting the benefits of transferring knowledge from multiple source tasks."}
{"question": "Consider the paper that introduces the dataset that has the largest number of Queries|Aspects in the OABS category. What specific methodological approach was used to determine the threshold value for including sentences in aspect-based summaries within this dataset?", "answer": "Manual evaluation with randomly selected Wikipedia pages and expert annotators.", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset with the most number of Queries|Aspects in OABS category?", "answer_anchor": "QASUM", "question_reference": "What specific methodological approach was used to determine the threshold value for including sentences in aspect-based summaries within the OASum dataset?", "explanation_reference": "The methodological approach for determining the threshold value for including sentences in aspect-based summaries involved manually evaluating randomly selected Wikipedia pages. This process included assigning these pages to expert annotators for quality assessment, thereby ensuring the summaries' relevance and quality.", "evidence_reference": "To determine the exact value of the threshold, we try $\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]$ and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality."}
{"question": "Consider the paper that introduces the method represented by the purple line. What is the relationship between the average attention span and the variance loss $L_{var}$, as demonstrated in the model proposed in the paper?", "answer": "The relationship between the average attention span and the variance loss \\(L_{var}\\) is that the average attention span is reduced as \\(L_{var}\\) is increased.", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is demonstrated by the purple line?", "answer_anchor": "MMA", "question_reference": "What is the relationship between the average attention span and the variance loss $L_{var}$ as demonstrated in the paper?", "explanation_reference": "The paper explicitly states that the average attention span decreases with an increase in the variance loss $L_{var}$, indicating a direct relationship between these two variables.", "evidence_reference": "We show the relation between the average attention span (averaged over the IWSLT and WMT test sets) versus $L_{var}$ in \\autoref{fig:attention-span}. As expected, the average attention span is reduced as we increase $L_{var}$."}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 18.4 on the GSM8K dataset. What key modification did the authors make to the few-shot prompts to improve the quality of chain of thought reasoning generated by the model proposed in the paper?", "answer": "The key modification was adapting the few-shot prompts to provide the model with the target after posing the question and before providing example CoT.", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets 18.4 accuracy in GSM8K dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What key modification to the few-shot prompts did the authors make to improve the quality of chain of thought reasoning generated by the teacher model?", "explanation_reference": "The key modification mentioned in the paper aimed to improve the quality of the generated chain of thought by guiding the teacher model with the target answer, which helps in correcting small mistakes in the CoT reasoning.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the method which is represented by the lavender color. What is the primary goal of discriminatively training CC-LMs with this method?", "answer": "The primary goal of discriminatively training CC-LMs with GeDi training is to make them better discriminators for GeDi-guided generation.", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method represented by the lavender color?", "answer_anchor": "GeDi", "question_reference": "What is the primary goal of discriminatively training CC-LMs with GeDi training?", "explanation_reference": "The primary goal of discriminatively training CC-LMs with GeDi training is to enhance their capabilities as discriminators, specifically for the purpose of guiding generation in a more controlled manner. This is aimed at improving the efficiency and effectiveness of the GeDi-guided generation process.", "evidence_reference": "For this reason, we propose training CC-LMs discriminatively as classifiers with GeDi training, with the primary goal of making them better discriminators for GeDi-guided generation."}
{"question": "Consider the paper that introduces the method that has a score of 87.3 in the SciTail dataset with 16-shot prompting. How does the performance of the model proposed in the paper change when the prompt length is increased from 300 to 400?", "answer": "absolute 1.8% drop in accuracy", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having score of 78.6 in SciTail dataset with 16-shot prompting?", "answer_anchor": "MPT", "question_reference": "In the multitask prompt tuning (MPT) approach, how does the performance change when increasing the prompt length from 300 to 400?", "explanation_reference": "The performance on SuperGLUE decreases by an absolute 1.8% when the prompt length is increased from 300 to 400, indicating possible overfitting at longer prompt lengths.", "evidence_reference": "However, further increasing the prompt length from 300 to 400 leads to an absolute 1.8% drop in accuracy, possibly due to overfitting."}
{"question": "Consider the paper that introduces the model in the table that has 12M updated parameters. What is the core component of the model that enables the propagation of matching information along the directed edges on KGs?", "answer": "matching information propagation module", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model in the table has 12M updated parameters?", "answer_anchor": "UniKGQA", "question_reference": "What is the core component of UniKGQA that enables the propagation of matching information along the directed edges on KGs?", "explanation_reference": "The core component that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs) in UniKGQA is the matching information propagation module. This module is crucial for the model's ability to effectively navigate and reason over the KG by leveraging the semantic relationships and structure within the graph.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method that corresponds to the leftmost bar in the figure. What is the improvement in True Negative Rate (TNR) for detecting LSUN samples using DenseNet, when comparing the model proposed in the paper to ODIN, given that 95% of CIFAR-100 samples are correctly detected?", "answer": "50.2%", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown on the leftmost bar in the figure?", "answer_anchor": "MSP", "question_reference": "What is the improvement in TNR for detecting LSUN samples using DenseNet when comparing the proposed method to ODIN, given that 95% of CIFAR-100 samples are correctly detected?", "explanation_reference": "The improvement in TNR for detecting LSUN samples using DenseNet when comparing the proposed method to ODIN, given that 95% of CIFAR-100 samples are correctly detected, is calculated based on the improvement from ODIN's performance to the proposed method's performance. The TNR for ODIN is 41.2%, and for the proposed method, it is 91.4%. Therefore, the improvement is 91.4% - 41.2% = 50.2%.", "evidence_reference": "Our method improves the TNR, i.e., the fraction of detected LSUN samples, compared to ODIN: $41.2\\\\%\\\\rightarrow 91.4\\\\%$ using DenseNet, when 95\\\\% of CIFAR-100 samples are correctly detected."}
{"question": "Consider the paper that introduces the method that has the second lowest overall performance for the Seen condition. How does the model proposed in the paper handle the scenario when the agent's path is blocked by immovable objects during inference?", "answer": "To handle the scenario when the agent's path is blocked by immovable objects during inference, the model proposes an 'obstruction evasion' mechanism in the Action Policy Module (APM) to avoid obstacles at inference time.", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the second lowest over performance for Seen condition?", "answer_anchor": "MOCA", "question_reference": "How does the model handle the scenario when the agent's path is blocked by immovable objects during inference?", "explanation_reference": "The question focuses on a specific methodological aspect of the model, particularly how it addresses the challenge of immovable obstacles during inference. The answer directly points to the solution proposed in the paper, which is the introduction of an 'obstruction evasion' mechanism within the Action Policy Module (APM).", "evidence_reference": "To address such unanticipated situations, we propose an 'obstruction evasion' mechanism in the APM to avoid obstacles at inference time."}
{"question": "Consider the paper that introduces the method that demonstrates the second lowest Acc-7 score on MOSI. Which model has a lower computational complexity compared to the Tensor Fusion Network (TFN) model, in terms of the number of parameters when $M=3$, $d_1 = 32$, $d_2 = 32$, $d_3 = 64$, $r = 4$, and $d_y = 1$?", "answer": "The \\ourl\\ model contains about 1.1e6 parameters while TFN contains about 12.5e6 parameters.", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1806.00064", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method demonstrates the lowest Acc-7 score on MOSI?", "answer_anchor": "TFN", "question_reference": "What is the computational complexity of the \\ourl\\ model compared to the Tensor Fusion Network (TFN) model in terms of the number of parameters when $M=3$, $d_1 = 32$, $d_2 = 32$, $d_3 = 64$, $r = 4$, and $d_y = 1$?", "explanation_reference": "The question specifically asks for a comparison in the number of parameters between the \\ourl\\ model and the TFN model under a given set of hyper-parameters. The answer directly addresses this by providing the exact number of parameters for both models under the specified conditions, which is detailed in the Complexity Analysis section of the paper.", "evidence_reference": "Under this hyper-parameter setting, our model contains about 1.1e6 parameters while TFN contains about 12.5e6 parameters, which is nearly 11 times more."}
{"question": "Consider the paper that introduces the model shown on the penultimate line of the table. What specific tokenization method is used for SMILES sequences in its pre-training corpus, MolXPT, and how does this method differ from the tokenization of text?", "answer": "For SMILES sequences, the tokenization method used involves tokenizing them with a regular expression from Schwaller et al., 2018. Additionally, for each SMILES sequence, a start-of-molecule token is added at the beginning, and an end-of-molecule token is appended at the end. This method differs from the tokenization of text, where byte-pair encoding (BPE) is used to split words into subwords, with the number of BPE merge operations being 40k.", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shown on the penult line?", "answer_anchor": "MolXPT", "question_reference": "What specific tokenization method is used for SMILES sequences in the MolXPT pre-training corpus, and how does it differ from the tokenization of text?", "explanation_reference": "The answer directly addresses the question by specifying the distinct tokenization methods used for SMILES sequences and text in the MolXPT pre-training corpus. SMILES sequences are tokenized using a method based on a regular expression, which is different from the byte-pair encoding method used for text, highlighting the tailored approach to handle the unique structure of SMILES compared to natural language text.", "evidence_reference": "Text and SMILES are tokenized separately. For text, we use byte-pair encoding (BPE) to split the words into subwords. For SMILES sequences (including those in wrapped sequences), we tokenize them with the regular expression from Schwaller et al., 2018."}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 18.4 on the GSM8K dataset. What specific modification to the few-shot prompts used in the model's, proposed by the paper, CoT generation is highlighted as a key factor for improving the quality of generated data?", "answer": "Providing the model with the target after posing the question and before providing example CoT.", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets 18.4 accuracy in GSM8K dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as a key factor for improving the quality of generated data?", "explanation_reference": "The paper specifies that making a key modification to the few-shot prompts by providing the target answer after the question and before the example CoTs allows LLMs to correct small mistakes in the CoT, which is crucial for generating high-quality data for knowledge distillation.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 12.79% TP. What specific computational advantage does it have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "answer": "GRIT achieves a computational advantage over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time by reducing it by a factor of 10 compared to the others. This is achieved by employing a DETR-based detector for extracting region features without using computationally expensive regional operations such as class-aware Non-Maximum Suppression (NMS) and Region of Interest (RoI) Align, which are used by the other methods.", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shows 12.79% TP?", "answer_anchor": "GRIT", "question_reference": "What specific computational advantage does GRIT have over VinVL and ${\\cal M}^2$ Transformer in terms of feature extraction inference time, and how is this achieved?", "explanation_reference": "GRIT's computational advantage in feature extraction time over VinVL and ${\\cal M}^2$ Transformer is quantitatively significant, reducing the time to 31 ms compared to 304 ms and 736 ms, respectively. This efficiency is achieved by utilizing a Deformable DETR-based detector that eliminates the need for computationally expensive regional operations such as NMS and RoI Align, which are used in the other methods.", "evidence_reference": "Table \\ref{tab:extraction} shows the comparison on feature extraction. VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms. ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms. \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms."}
{"question": "Consider the paper that introduces the model that achieves an F1 score of 73.1 in the en_city category. What specific method did the authors employ to encode sentence positions in the encoder input of the model proposed in the paper, and how does it differ from using a fixed BOS token index?", "answer": "At the encoder input for the \\(i^\\text{th}\\) sentence, the authors used the \\(i^\\text{th}\\) vocabulary token embedding in place of a fixed BOS token index.", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets 73.1 F1 score in en_city category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What specific method did the authors employ to encode sentence positions in the encoder input, and how does it differ from using a fixed BOS token index?", "explanation_reference": "This method directly encodes sentence positions by utilizing the sequence of vocabulary token embeddings as a means to represent the position of each sentence in the input sequence. This approach is distinct from using a fixed Beginning Of Sentence (BOS) token index for every sentence, which does not convey unique position information. The method aims to provide the decoder with unambiguous position information that can be exploited for producing sentence indices at the decoder output, without employing custom schemes like dedicated sentence position embeddings.", "evidence_reference": "At the encoder input for the $i^{th}$ sentence,  we use the $i^{th}$ vocabulary token embedding in place of a fixed BOS token index. Formally, in contrast to \\eqref{eqn:bos}, we set \\begin{equation*} t_{1_1} = 0, \\hspace{2mm}  t_{2_1} = 1, \\hspace{2mm} \\ldots,  t_{|S|_1} = |S|-1. \\end{equation*}"}
{"question": "Consider the paper that introduces the method that achieves an average EA score of 67.07 in the FinQA task. What is the impact of the order of in-context examples on the model's results for the NQ dataset using the method proposed in the paper?", "answer": "The reverse order performs the best.", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets 67.07 EA score in FinQA task", "answer_anchor": "KATE", "question_reference": "What is the impact of the order of in-context examples on KATE's results for the NQ dataset using KATE$_{\\text{nli+sts-b}}$ method?", "explanation_reference": "The exploration of how the order of in-context examples affects KATE's results reveals that, for the NQ dataset, arranging the examples in reverse order (where the example closer to the test prompt in the embedding space is placed closer to the test prompt in the input sequence) yields the best performance. This suggests that the proximity of semantically similar sentences to the test example may help GPT-3 leverage the relevant information more effectively.", "evidence_reference": "On this particular NQ dataset, the reverse order performs the best. One possible explanation is that since tokens next to each other have similar positional embeddings, putting the most similar sentences close to the test example may be helpful for GPT-3 to leverage the corresponding information."}
{"question": "Consider the paper that introduces the model that achieves a 16.5 Top-1 score on the SQuAD dataset. How does the \\ednascore metric, utilized by the model, balance the evaluation of faithfulness and diversity in generated summaries?", "answer": "The \\ednascore metric balances the evaluation of faithfulness and diversity in generated summaries by being the harmonic mean of Entailment and (1-Self-Entailment). Higher values of \\ednascore imply more faithful and diverse summaries.", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model gets 16.5 Top-1 score in SQuAD dataset?", "answer_anchor": "Composition", "question_reference": "How does the \\ednascore metric balance the evaluation of faithfulness and diversity in generated summaries?", "explanation_reference": "The \\ednascore metric is designed to jointly measure faithfulness and diversity in summaries. It uses Self-Entailment to assess diversity, which is effective because both components of \\ednascore (faithfulness and diversity) yield values in a similar output space through the same trained model (Entailment), facilitating a balanced evaluation.", "evidence_reference": "The reason \\ednascore relies on Self-Entailment to measure diversity is because the faithfulness metric is also based on Entailment. This means that both components will be mapped to a score in a similar output space (i.e., they both yield values between 0 and 1 obtained through the same trained model), making it more likely to be properly balanced when mixed."}
{"question": "Consider the paper that introduces the dataset that has a test set size of 1,106. What specific error type in the error analysis indicates a failure due to the model proposed in the paper's inability to understand comments with indirect and disrespectful references?", "answer": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way.", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset having 1,106 test set size?", "answer_anchor": "ViHOS", "question_reference": "What specific error type in the error analysis indicates a failure due to the model's inability to understand comments with indirect and disrespectful references?", "explanation_reference": "The error type 'Allusion' directly addresses the model's failure to correctly interpret comments that refer to another person or subject in an indirect and disrespectful manner, indicating a specific kind of error where the model's understanding of context and indirect references is inadequate.", "evidence_reference": "Allusion: The comment refers to another person or subject in an indirect and disrespectful way."}
{"question": "Consider the paper that introduces the LLM shown in the figure with a model size of 1.7T. What specific methodological difference in the evaluation setup for the model's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "answer": "The use of sampling a letter choice at temperature 0 using the already-sampled explanation for certain exams, rather than extracting the model's letter choice directly from the explanation.", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the large language model that has 1.7T model size?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard approach of directly extracting the model's letter choice from the explanation for most exam runs. Instead, for these specific exams, the approach involved sampling a letter choice at temperature 0 using the explanation already sampled, indicating a unique handling of these exams compared to others.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the model shown on the first line of the table. What is the Dev F1 score for its feature-based approach using the model's base version with embeddings only on the CoNLL-2003 NER task?", "answer": "91.0", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shown on the first line?", "answer_anchor": "BERT", "question_reference": "What is the Dev F1 score for the feature-based approach using BERTbase with embeddings only on the CoNLL-2003 NER task?", "explanation_reference": "The Dev F1 score directly measures the performance of the feature-based approach using BERTbase with only embeddings on the CoNLL-2003 Named Entity Recognition task, indicating how well the model performed without fine-tuning or additional context.", "evidence_reference": "Feature-based approach (\\bertbase) &  &  \\\\ \\;\\;\\;Embeddings & 91.0 &- \\\\"}
{"question": "Consider the paper that introduces the method that has a lower Hits@1 score than ReasoningLM but a higher Hits@1 score than NSM across all fine-tuning samples. What is the core component of the model proposed in the paper that enables the propagation of matching information along the directed edges on KGs?", "answer": "matching information propagation module", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method has lower Hit@1 score than ReasoningLM but higher Hit@1 score than NSM as the number of fine-tuning samples increases?", "answer_anchor": "UniKGQA", "question_reference": "What is the core component of UniKGQA that enables the propagation of matching information along the directed edges on KGs?", "explanation_reference": "The core component that enables the propagation of matching information along the directed edges on Knowledge Graphs (KGs) in UniKGQA is the matching information propagation module. This module is crucial for the model's ability to effectively navigate and reason over the KG by leveraging the semantic relationships and structure within the graph.", "evidence_reference": "UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Knowledge Editing category. What specific hyperparameters were searched for and what were their final selected values for the OnlineEWC method, as it relates to CMR?", "answer": "$\\lambda_{\\text{EWC}}=5$ and $\\gamma_{\\text{EWC}}=0.9$", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2205.02014", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "CMR", "question_reference": "What specific hyperparameters were searched for the OnlineEWC method, and what were their final selected values?", "explanation_reference": "The question focuses on the methodological details of implementing the OnlineEWC method, specifically asking about the hyperparameters involved and their optimized values. This is a detail that directly pertains to the experimental setup and the effectiveness of the method in the context of the CMR problem.", "evidence_reference": "Therefore, we also have two hps $\\lambda_{\\text{EWC}}$ and $\\gamma_{\\text{EWC}}$, which we searched over $\\{1, 5, 10\\}$ and $\\{1.0, 0.95, 0.9, 0.8\\}$. We finally use $5$ and $0.9$ for their best performance."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest number of turns. What specific advantage does the curriculum learning strategy offer for the model's performance on low-resource languages according to the paper?", "answer": "The curriculum learning strategy enables the transfer of general knowledge from English to other languages, leading to significant improvements in overall performance on low-resource languages.", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the least number of turns from the table?", "answer_anchor": "Multialpaca", "question_reference": "What specific advantage does the curriculum learning strategy offer for \\textsc{Poly}LM's performance on low-resource languages according to the paper?", "explanation_reference": "The curriculum learning strategy is designed to initially focus on high-resource language data (English) and gradually increase the proportion of high-quality, low-resource language data during training. This method facilitates the transfer of learned general knowledge from English to other languages, which is particularly beneficial for improving the model's capabilities in low-resource languages.", "evidence_reference": "The model with curriculum learning has achieved stable progress in mainly all languages in both NLU and MT tasks. First of all, the model performance is enhanced in most low-resource languages, indicating that the general knowledge can be effectively transferred to these languages through raising data proportion."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than SERA and a higher F1 score than Doc2Graph. What specific strategy does the model proposed in the paper employ to unify multilingual multimodal inputs efficiently?", "answer": "LayoutXLM employs a strategy of obtaining character-level bounding boxes after tokenization using SentencePiece with a unigram language model, and then calculating the bounding box of each token by merging the bounding boxes of all characters it contains. This approach efficiently unifies the multilingual multimodal inputs.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having lower F1 score than SERA and higher F1 score than Doc2Graph?", "answer_anchor": "LayoutXLM", "question_reference": "What specific strategy does LayoutXLM employ to unify multilingual multimodal inputs efficiently?", "explanation_reference": "LayoutXLM employs a strategy of obtaining character-level bounding boxes for unifying multilingual multimodal inputs efficiently. This approach allows the model to handle the diversity in the definition of linguistic units across different languages without language-specific preprocessing.", "evidence_reference": "However, for LayoutXLM, this strategy is not applicable because the definition of the linguistic unit is different from language to language. To prevent the language-specific pre-processing, we decide to obtain the character-level bounding boxes."}
{"question": "Consider the paper that introduces the model that corresponds to the green dashed line. What specific architectural change was made to the Transformer's layer normalization in this model compared to its originally proposed form?", "answer": "a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model demonstrates in the gree dashed line?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change was made to the Transformer's layer normalization in the T5 model compared to its originally proposed form?", "explanation_reference": "The T5 model made specific modifications to the original Transformer architecture's layer normalization. Specifically, it simplified layer normalization by placing it outside the residual path and removing the additive bias. This change is orthogonal to the experimental factors considered in the empirical survey of transfer learning.", "evidence_reference": "Our encoder-decoder Transformer implementation closely follows its originally-proposed form \\citep{vaswani2017attention}. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of ``blocks'', each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization \\citep{ba2016layer} is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection \\citep{he2016deep} adds each subcomponent's input to its output."}
{"question": "Consider the paper that introduces the method that corresponds to the leftmost bar in the figure. Which theoretical connection underlies the model proposed in the paper for detecting abnormal samples?", "answer": "The theoretical connection underlies the proposed method for detecting abnormal samples in the paper is the equivalence of the posterior distribution defined by the generative classifier under Gaussian Discriminant Analysis (GDA) with tied covariance assumption to the softmax classifier.", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown on the leftmost bar in the figure?", "answer_anchor": "MSP", "question_reference": "What theoretical connection underlies the proposed method for detecting abnormal samples in the paper?", "explanation_reference": "The paper establishes a theoretical connection between Gaussian Discriminant Analysis (GDA) and the softmax classifier, which underlies the proposed method for detecting abnormal samples. This connection is utilized to fit class-conditional Gaussian distributions to the features of a pre-trained softmax neural classifier, thereby enabling the detection of out-of-distribution and adversarial samples.", "evidence_reference": "Specifically, we assume that pre-trained features can be fitted well by a class-conditional Gaussian distribution since its posterior distribution can be shown to be equivalent to the softmax classifier under Gaussian discriminant analysis (see Section \\ref{sec:main_results_score} for our justification)."}
{"question": "Consider the paper that introduces the model that scores higher than ACA but lower than RationaleCL in the 'T5' column. What hyperparameter values were used for the FewRel dataset in its experiments?", "answer": "$\\alpha=0.5$, $\\beta=0.5$, $\\tau_1=0.1$, $\\mu=0.5$, $\\omega=0.1$, $\\tau_2=0.5$, $\\gamma=1.25$, $\\lambda_1=0.5$, $\\lambda_2=1.1$.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets score higher than ACA but lower than RationalCL in 'T5' column?", "answer_anchor": "CEAR", "question_reference": "What hyperparameter values were used for the FewRel dataset in the experiments?", "explanation_reference": "The answer directly lists the specific hyperparameter values used for the FewRel dataset as mentioned in the Implementation Details section of the paper, providing precise and concise information.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-Fr task in the MSCOCO dataset. What specific loss function does the paper introduce to reduce the mismatch between training and inference in its framework?", "answer": "Consistency loss", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model demonstrates the highest performance in En-Fr task in MSCOCO dataset?", "answer_anchor": "VALHALLA", "question_reference": "What specific loss function is introduced to reduce the mismatch between training and inference in the VALHALLA framework?", "explanation_reference": "The consistency loss is introduced to reduce the mismatch between training and inference by encouraging consistency between predictions using either ground-truth or hallucinated visual representations.", "evidence_reference": "To reduce this mismatch, we define a \\emph{consistency loss}...where $y_i^M = p(y_i | x, z, y_{<i}; \\mathbf{f}_{\\mathbf{T}})$ and $y_i^H = p(y_i | x, \\hat{z}, y_{<i}; \\mathbf{f}_{\\mathbf{T}})$ are the next word distributions from ground-truth visual tokens and hallucinated features respectively, and $\\text{KL}[y^M_i \\Vert y^H_i]$ is the Kullback-Leibler divergence between the two conditional distributions."}
{"question": "Consider the paper that introduces the model that scores a 91.5 in the NER task. What specific dynamic masking rate formula is adopted for the tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} during its pre-training phase?", "answer": "$p = 0.1 + 0.75 * t/T$", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets score of 91.5 in NER task?", "answer_anchor": "AMRBART", "question_reference": "What specific dynamic masking rate formula is adopted for the tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} during pre-training?", "explanation_reference": "The formula for the dynamic masking rate is directly provided in the section discussing the unified pre-training framework, indicating how the masking probability increases over time during pre-training for specific tasks.", "evidence_reference": "Formally, at step $t$, we calculate the masking probability $p$ as:  \\begin{equation} \\label{eq:maskrate} p = 0.1 + 0.75 * t/T, \\end{equation} where $0.1$ is the initial masking rate, $T$ denotes the total training step. $p$ increases as $t$ grows, as $t$ approaches to $T$, the pre-training tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} are closer to fine-tuning tasks."}
{"question": "Consider the paper that introduces the method whose results are displayed in a lighter grey color in the table. What specific improvement in CIDEr score does the model proposed in the paper contribute to image captioning through the text infilling pretraining task?", "answer": "+0.8", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is using grey-ish color for showing results in the table?", "answer_anchor": "OFA-base", "question_reference": "What specific improvement in CIDEr score does text infilling pretraining task contribute to image captioning?", "explanation_reference": "The improvement in CIDEr score due to the text infilling pretraining task for image captioning is directly stated in the Ablation on Multitask Pretraining section, indicating the specific impact of this task on enhancing the model's performance in generating image captions.", "evidence_reference": "Text infilling brings improvement on image caption ($+0.8$ CIDEr)"}
{"question": "Consider the paper that introduces the model that has a score lower than 0.82 but higher than 0.815 in the Stance column. What is the Pearson's correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model has accuracy consistently lower than 0.82 across all volumne of adaptive data?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the task of political affiliation classification on Twitter data. A value of 0.9817 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "Twitter, \\poliaff{} ($F_1$), 0.9817159316285563"}
{"question": "Consider the paper that introduces the model that scores a 91.5 in the NER task. What specific aspect of its unified pre-training framework contributes to its effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR tasks?", "answer": "Dynamic masking rate", "figure": "locality/2310.11964/comparison_table.png", "anchor_arxiv_id": "2310.11964", "reference_arxiv_id": "2203.07836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets score of 91.5 in NER task?", "answer_anchor": "AMRBART", "question_reference": "What specific aspect of the unified pre-training framework contributes to its effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR tasks?", "explanation_reference": "The unified pre-training framework's effectiveness in reducing the gap between pre-training and fine-tuning phases for AMR tasks is attributed to the introduction of a dynamic masking rate. This approach adjusts the masking probability over time, making the pre-training tasks gradually more similar to the fine-tuning tasks, thus facilitating knowledge transfer and reducing the gap between these phases.", "evidence_reference": "Different from standard masking that uses a static masking rate, we adopt a dynamic masking rate $p$ for task $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g}. Formally, at step $t$, we calculate the masking probability $p$ as:  $p = 0.1 + 0.75 * t/T$, where $0.1$ is the initial masking rate, $T$ denotes the total training step. $p$ increases as $t$ grows, as $t$ approaches to $T$, the pre-training tasks $\\hat{\\texttt{t}}$\\texttt{g2t} and \\texttt{t}$\\hat{\\texttt{g}}$\\texttt{2g} are closer to fine-tuning tasks."}
{"question": "Consider the paper that introduces the model which is placed fourth in the table. What specific improvement in F1 score was observed for the 'lease details' subset when comparing LegalBERT to the tuned BERT base?", "answer": "1.1%", "figure": "locality/2310.11368/comparison_table.png", "anchor_arxiv_id": "2310.11368", "reference_arxiv_id": "2010.02559", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model shown in the fourth row of the table?", "answer_anchor": "LegalBERT", "question_reference": "What specific improvement in F1 score was observed for the 'lease details' subset when comparing LEGAL-BERT variants to the tuned BERT base?", "explanation_reference": "The question focuses on extracting a precise statistical improvement from the experimental results section, specifically targeting the performance comparison between LEGAL-BERT variants and the tuned BERT base model on a particular subset of a dataset. The answer directly reflects the observed improvement in F1 score for the 'lease details' subset, showcasing the effectiveness of LEGAL-BERT in this specific task.", "evidence_reference": "On the contrary, we observe a more substantial improvement in the more difficult multi-label task (2.5%) indicating that the \\legalbert variations benefit from in-domain knowledge. On \\contractsdata, the drop in perplexity is larger (5.6), which is reflected in the increase in $F1$ on the \\emph{contract header} (1.8\\%) and \\emph{dispute resolution} (1.6\\%) subsets. In the \\emph{lease details} subset, we also observe an improvement (1.1\\%)."}
{"question": "Consider the paper that introduces the method which has 11.0 rounds to completion. What is the effect of using a death mask on the model's performance in the SMAC domain compared to ignoring states in which an agent is dead when computing GAE?", "answer": "Using a death mask on MAPPO's performance in the SMAC domain compared to ignoring states in which an agent is dead when computing GAE significantly improves the final performance and stability of MAPPO.", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having 11.0 rounds to Completion?", "answer_anchor": "MAPPO", "question_reference": "What is the effect of using a death mask on MAPPO's performance in the SMAC domain compared to ignoring states in which an agent is dead when computing GAE?", "explanation_reference": "The ablation studies demonstrate that using a death mask, which involves replacing the value state for a dead agent with a zero state containing the agent's ID, results in superior performance compared to other options, including ignoring states in which an agent is dead when computing GAE. This suggests that handling the non-stationarity introduced by agent deaths appropriately is crucial for MAPPO's performance in the SMAC domain.", "evidence_reference": "Fig.~\\ref{fig:app-Ablation-death} and Fig. \\ref{fig:app-Ablation-ignore} illustrates the influence of the death mask on MAPPO's performance in the SMAC domain."}
{"question": "Consider the paper that introduces the model which has a lower mean classification accuracy than VIBE but higher mean classification accuracy than UDALM on the Stance dataset. What is the Pearson's r correlation coefficient between word overlap and the performance of DPT for the task of AI venue classification?", "answer": "0.9303959931770183", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model has lower accuracy than VIBE but higher accuracy than UDALM?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of AI venue classification?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the AI venue classification task. A value of 0.9303959931770183 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also tends to increase.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that corresponds to the fifth row of the table. What is the primary reason for the degeneration of neural language models, specifically focusing on this method, as proposed in the paper?", "answer": "The anisotropic distribution of token representations", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the fifth row of the table?", "answer_anchor": "SimCTG", "question_reference": "What is the primary reason for the degeneration of neural language models as identified in the paper?", "explanation_reference": "The paper identifies the anisotropic distribution of token representations as the primary reason for model degeneration, where representations reside in a narrow subset of the entire space, leading to unnatural generated text and undesirable repetitions.", "evidence_reference": "In this work, we argue that the degeneration of neural language models stems from the anisotropic distribution of token representations, i.e., their representations reside in a narrow subset of the entire space."}
{"question": "Consider the paper that introduces the method that demonstrates the second highest score in the TweetEval Irony dataset for both zero-shot and few-shot prompting. How does the model's, proposed by the paper, approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "answer": "The unified framework's approach to handling the RefCOCOg task differs from traditional methods in that it uses a single architecture with the same language modeling objective for all tasks, including RefCOCOg. Traditional methods typically require designing task-specific architectures and objectives for each task, such as a multi-label classifier for visual question answering or a softmax classifier for referring expression comprehension. In contrast, the unified framework formulates tasks as multimodal conditional text generation, where models learn to generate labels in text based on visual and textual inputs, without the need for task-specific architectures or objectives.", "figure": "locality/2310.15746/comparison_2_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method demonstrates the second highest score in TweetEval Irony dataset in both zero-shot and few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "How does the unified framework's approach to handling the RefCOCOg task differ from traditional methods in terms of model architecture and learning objectives?", "explanation_reference": "Traditional methods for the RefCOCOg task typically involve classification over a set of visual regions, requiring task-specific architectures and objectives. In contrast, the unified framework proposed in the paper treats RefCOCOg as a text generation task, leveraging the same language modeling architecture and objective used for other vision-and-language tasks. This approach allows for more flexible architecture design and eliminates the need for task-specific modifications.", "evidence_reference": "While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously~\\cite{Yu2018,Chen2020} formulated classification task over a set of visual regions, allowing more flexible architecture design."}
{"question": "Consider the paper that introduces the method shown in the table that demonstrates the highest BLEU-1 score for the Test Seen task. What is the primary reason for the degeneration of neural language models as identified in the paper?", "answer": "The anisotropic distribution of token representations", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the table demonstrates the highest BLEU-1 score for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What is the primary reason for the degeneration of neural language models as identified in the paper?", "explanation_reference": "The paper identifies the anisotropic distribution of token representations as the primary reason for model degeneration, where representations reside in a narrow subset of the entire space, leading to unnatural generated text and undesirable repetitions.", "evidence_reference": "In this work, we argue that the degeneration of neural language models stems from the anisotropic distribution of token representations, i.e., their representations reside in a narrow subset of the entire space."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than SERA and a higher F1 score than Doc2Graph. What is the F1 score for the semantic entity recognition (SER) task in the multitask fine-tuning setting for the Italian language using the model proposed in the paper's LARGE version?\n\nA) 92.5%\nB) 89.3%\nC) 94.7%\nD) 87.2%", "answer": "0.8372", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having lower F1 score than SERA and higher F1 score than Doc2Graph?", "answer_anchor": "LayoutXLM", "question_reference": "What is the F1 score for semantic entity recognition (SER) task in the multitask fine-tuning setting for the Italian language using the LayoutXLM LARGE model?", "explanation_reference": "The F1 score for the semantic entity recognition (SER) task in the multitask fine-tuning setting for the Italian language using the LayoutXLM LARGE model is directly reported in the multitask fine-tuning results table.", "evidence_reference": "In the multitask fine-tuning accuracy (F1) on the \\task dataset section, the table shows that the F1 score for the SER task for the Italian language using the LayoutXLM LARGE model is 0.8372."}
{"question": "Consider the paper that introduces the model that results in the second lowest accuracy in the COGS-all dataset. What specific mechanism does it employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "answer": "This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What model demonstrates the second lowest accuracy in COGS-all dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "What specific mechanism does the Transformer employ to prevent positions in the decoder from attending to subsequent positions, thereby preserving its auto-regressive property?", "explanation_reference": "The mechanism described directly addresses the need to prevent future information from influencing the prediction of the current position in the sequence, which is crucial for maintaining the auto-regressive nature of the model. This is achieved by modifying the self-attention sub-layer in the decoder to mask out future positions.", "evidence_reference": "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."}
{"question": "Consider the paper that introduces the model that corresponds to the brown bars in the figure. What specific performance improvement does it demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "answer": "37.5%", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model is demonstrated in the brown color?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific performance improvement does the \\modelname 70B model demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "explanation_reference": "The question focuses on the performance improvement of the \\modelname 70B model over the Falcon 40B model specifically in the Commonsense Reasoning benchmark. The answer, 37.5%, directly reflects the performance metric for the \\modelname 70B model in this category, indicating a significant improvement over the Falcon 40B model's performance.", "evidence_reference": "\\multirow{4}{*}{\\cinnamon} & 7B & 16.8 & 63.9 & 48.9 & 61.3 & 14.6 & 45.3 & 32.6 & 29.3 \\\\ & 13B & 24.5 & 66.9 & 55.4 & 65.8 & 28.7 & 54.8 & 39.4 & 39.1 \\\\ & 34B & 27.8 & 69.9 & 58.7 & 68.0 & 24.2 & 62.6 & 44.1 & 43.4 \\\\ & 70B & \\textbf{37.5} & \\textbf{71.9} & \\textbf{63.6} & \\textbf{69.4} & \\textbf{35.2} & \\textbf{68.9} & \\textbf{51.2} & \\textbf{54.2} \\\\"}
{"question": "Consider the paper that introduces the Twitter dataset that has the most number of languages compared to all other Twitter datasets. What specific sentiment analysis sub-tasks do the authors of AfriSenti plan to extend their dataset to in the future?", "answer": "Additional African languages and other sentiment analysis sub-tasks.", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset (Twitter) has the most number of languages compared to all Twitter datasets?", "answer_anchor": "AfriSenti", "question_reference": "What specific sentiment analysis sub-tasks do the authors of AfriSenti plan to extend their dataset to in the future?", "explanation_reference": "The authors explicitly mention their future plans to extend AfriSenti to additional African languages and other sentiment analysis sub-tasks, indicating a broader scope beyond the current dataset's focus.", "evidence_reference": "In the future, we plan to extend \\textit{AfriSenti} to additional African languages and other sentiment analysis sub-tasks."}
{"question": "Consider the paper that introduces the method at the rightmost part of the figure. What specific computational advantage does the model proposed in the paper offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "answer": "GeDi's online classification trick can compute $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier.", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method on the right most coordinates of the figure?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's online classification trick offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "explanation_reference": "The computational advantage is highlighted by comparing the efficiency of GeDi's method to compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token using only two parallel forward passes (one for each control code) against the requirement of a unidirectional classifier needing $|\\\\gV|$ forward passes for a vocabulary set $\\\\gV$. This comparison underlines GeDi's significant reduction in computational demand.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the method that results in a token-level F1 score equal to 37.03. What specific configuration led to the best performance in the WikiHop development set ablation study for the model proposed in the paper?", "answer": "seqlen: 4,096, 15 epochs", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method show a token-level F1 score equal to 37.03?", "answer_anchor": "longformer", "question_reference": "What specific configuration led to the best performance in the WikiHop development set ablation study for \\model?", "explanation_reference": "The best performance configuration for \\model in the WikiHop development set ablation study is indicated by the highest positive change in accuracy compared to the base \\model configuration. This configuration is detailed as having a sequence length of 4,096 and being trained for 15 epochs, which resulted in a +1.2 change in accuracy.", "evidence_reference": "\\model (seqlen: 4,096, 15 epochs) & 75.0 / +1.2"}
{"question": "Consider the paper that introduces the dataset represented by the leftmost bar in the figure, GSM8K. What specific aspect of the token-level verifiers' training procedure is likely responsible for their initial uncertainty in solution correctness, as observed in the verifier visualization section?", "answer": "The token-level verifiers' training procedure likely causes initial uncertainty in solution correctness due to the large fraction of incorrect model-generated samples they are trained on, which makes them initially unsure about the correctness of a solution and gradually gain certainty as the solution progresses.", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset represented on the leftmost of the figure?", "answer_anchor": "GSM8K", "question_reference": "What specific aspect of the token-level verifiers' training procedure is likely responsible for their initial uncertainty in solution correctness, as observed in the verifier visualization section?", "explanation_reference": "The observed initial uncertainty of the token-level verifiers in assessing solution correctness is attributed to the training procedure, where a significant portion of the training data consists of incorrect model-generated samples. This exposure to a high volume of incorrect samples during training likely conditions the verifier to start with a baseline of uncertainty, gradually adjusting its confidence as more of the solution is revealed and it can better assess correctness.", "evidence_reference": "Note that the model is initially unsure about whether the solution is correct and gradually gains certainty as the solution progresses: this is likely a property of the verifier training procedure, where it trains on a large fraction of incorrect model-generated samples."}
{"question": "Consider the paper that introduces the model shown in the figure that is consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B. What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for the model's base version?", "answer": "0.42", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shown in the figure consistently better than MPT-7B-Instruct but consistently worse than LLaMA-30B?", "answer_anchor": "StarCoder", "question_reference": "What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for StarCoderBase?", "explanation_reference": "The percentage of responses flagged as toxic using a toxicity classifier for StarCoderBase in the RealToxicityPrompts evaluation is provided directly in the results table for the toxicity evaluation.", "evidence_reference": "StarCoderBase & 0.42 & 1.12"}
{"question": "Consider the paper that introduces MeLLo, the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections category. What specific performance improvement does the model proposed in the paper demonstrate over MEMIT in terms of multi-hop accuracy on the {\\dscf} dataset when using GPT-J as the base model?", "answer": "60.8%", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2305.14795", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the last method shown in Explicit --> Memory-enhanced --> Feedback or Corrections?", "answer_anchor": "MeLLo", "question_reference": "What specific performance improvement does MeLLo demonstrate over MEMIT in terms of multi-hop accuracy on the {\\dscf} dataset when using GPT-J as the base model?", "explanation_reference": "The question focuses on comparing the multi-hop accuracy performance between MEMIT and MeLLo on the {\\dscf} dataset using GPT-J as the base model. The answer directly reflects the significant improvement MeLLo achieves over MEMIT, highlighting MeLLo's effectiveness in handling multi-hop questions post knowledge editing.", "evidence_reference": "MEMIT's multi-hop performance changes from $43.4\\% \\to 8.1\\%$ with GPT-J... Among those questions where all associated facts are successfully retrieved from memory, MeLLo can answer $73.1\\%$ of them correctly."}
{"question": "Consider the paper that introduces the method which is in the second row of the table. What specific algorithm does the model proposed in the paper apply to prevent loops and token redundancy in parses, and how does it function?", "answer": "Tail collision avoidance algorithm", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method in the second row of the table?", "answer_anchor": "SPADE", "question_reference": "What specific algorithm does SPADE apply to prevent loops and token redundancy in parses, and how does it function?", "explanation_reference": "The Tail Collision Avoidance algorithm is specifically mentioned as a method applied by SPADE to prevent loops and token redundancy in parses. It functions by iteratively trimming tail-sharing-edges and generating new edges until the process becomes self-consistent, with a maximum iteration limit set to 20.", "evidence_reference": "Based on this property, we apply the following simple yet powerful tail collision avoidance algorithm: (1) at each tail node having multiple incoming edges, all edges are trimmed except the one with the highest linking probability; (2) at each head node of the trimmed edges, the new tail node is found by drawing the next probable edge whose probability is larger than $p_{th}$ and belongs to the top three; (3) go back to Step 1 and repeat the routine until the process becomes self-consistent or the max iteration limit is reached (set to 20 in this paper). The algorithm prevents loops and token redundancy in parses."}
{"question": "Consider the paper that introduces the model that is placed below TransferNet but above UniKGQA in the table. What is the impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in the model proposed in the paper+NSM on the CWQ dataset?", "answer": "drop by 0.1%", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method shown in the table is below TransferNet but above UniKGQA?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What is the impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in \\model+NSM on the CWQ dataset?", "explanation_reference": "The impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in \\model+NSM on the CWQ dataset is a decrease of 0.1%, indicating a slight reduction in QA performance without the subgraph merging strategy.", "evidence_reference": "Table~\\ref{tb:treemerge} shows that based on \\model+NSM, the average subgraph size increases from 174 to 204, and Hits@1 of QA drops 0.1\\% when removing the subgraph merging strategy (\\model+NSM w/o GM) but directly taking the union of all the subgraphs from different topic entities to induce the subgraph."}
{"question": "Consider the paper that introduces the benchmark that results in the highest execution accuracy for this method. What specific mathematical operations are included in the Numeric Reasoning Knowledge category of external knowledge evidence?", "answer": "MINUS, ADDITION, DIVISION, MULTIPLY", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Under this method, which benchmark demonstrates the highest execution accuracy?", "answer_anchor": "BIRD", "question_reference": "What specific mathematical operations are included in the Numeric Reasoning Knowledge category of external knowledge evidence in the BIRD benchmark?", "explanation_reference": "The question assesses understanding of the specific types of mathematical operations that are considered as part of the Numeric Reasoning Knowledge category for external knowledge evidence in the BIRD benchmark. This detail is crucial for understanding the complexity and requirements of the benchmark in terms of the mathematical reasoning capabilities expected from text-to-SQL models.", "evidence_reference": "In our benchmark, we present 8 basic math operations, including 4 complex operations as \\citep{finqa}: \\texttt{MINUS}, \\texttt{ADDITION}, \\texttt{DIVISION}, \\texttt{MULTIPLY}."}
{"question": "Consider the paper that introduces the model that is in the second-to-last row of the table. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model does not show a decrease in accuracy from the figure?", "answer_anchor": "DPT", "question_reference": "Based on the findings, what is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data indicates the strength of the linear relationship between the vocabulary overlap across time periods and the performance of the model on this specific task. A value close to 1 suggests a strong positive correlation.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the model that corresponds to the penultimate row of the table. What hyperparameter values were used for the FewRel dataset in the experiments?", "answer": "$\\alpha=0.5$, $\\beta=0.5$, $\\tau_1=0.1$, $\\mu=0.5$, $\\omega=0.1$, $\\tau_2=0.5$, $\\gamma=1.25$, $\\lambda_1=0.5$, $\\lambda_2=1.1$.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model is shown in the penultimate row in the table?", "answer_anchor": "CEAR", "question_reference": "What hyperparameter values were used for the FewRel dataset in the experiments?", "explanation_reference": "The answer directly lists the specific hyperparameter values used for the FewRel dataset as mentioned in the Implementation Details section of the paper, providing precise and concise information.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the method which has a score of 61.2 in the BoolQ dataset with 32-shot prompting. What specific aspect of the task embeddings derived from the model proposed in the paper contributes to their effectiveness in capturing task relationships, as opposed to domain similarity?", "answer": "Task embeddings capture task relationships more sensitively to the type of task than to domain similarity.", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having score of 61.2 in BoolQ dataset with 32-shot prompting?", "answer_anchor": "SPoT", "question_reference": "What specific aspect of the task embeddings derived from SPoT's soft prompts contributes to their effectiveness in capturing task relationships, as opposed to domain similarity?", "explanation_reference": "The effectiveness of task embeddings in capturing task relationships, as opposed to domain similarity, is attributed to their sensitivity to the type of task being performed. This is evidenced by the observation that QNLI, an NLI task derived from the SQuAD dataset, is not closely linked to SQuAD in the task embeddings, indicating that the embeddings are more sensitive to the task type rather than the domain similarity.", "evidence_reference": "We note that QNLI, which is an NLI task built from the SQuAD dataset, is not closely linked to SQuAD; this suggests that our task embeddings are more sensitive to the type of task than domain similarity."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest dialogues. What specific advantage does the curriculum learning strategy offer for the model's performance on low-resource languages according to the paper?", "answer": "The curriculum learning strategy enables the transfer of general knowledge from English to other languages, leading to significant improvements in overall performance on low-resource languages.", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the least number of dialogues from the table?", "answer_anchor": "Multialpaca", "question_reference": "What specific advantage does the curriculum learning strategy offer for \\textsc{Poly}LM's performance on low-resource languages according to the paper?", "explanation_reference": "The curriculum learning strategy is designed to initially focus on high-resource language data (English) and gradually increase the proportion of high-quality, low-resource language data during training. This method facilitates the transfer of learned general knowledge from English to other languages, which is particularly beneficial for improving the model's capabilities in low-resource languages.", "evidence_reference": "The model with curriculum learning has achieved stable progress in mainly all languages in both NLU and MT tasks. First of all, the model performance is enhanced in most low-resource languages, indicating that the general knowledge can be effectively transferred to these languages through raising data proportion."}
{"question": "Consider the paper that introduces the method that achieves a relatively constant MRR score in the FB15k-237 dataset as entity code entropy increases. How is the initial representation for non-reserved entities generated in the model's entity-agnostic encoding process before being input into the Graph Neural Network (GNN)?", "answer": "For non-reserved entities, their initial representation before being input into the GNN is generated by combining the encoded connected relation information (\\(\\mathbf{h}_{e}^{\\rm c}\\)) and the encoded \\(k\\)-nearest reserved entity information (\\(\\mathbf{h}_{e}^{\\rm k}\\)) through a 2-layer MLP (\\(f_{m}\\)), as described in Equation (\\ref{eq:info-combine}).", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method shows a huge increase as entity code entropy increases in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "In the EARL model's entity-agnostic encoding process, how is the initial representation for non-reserved entities generated before being input into the GNN?", "explanation_reference": "The initial representation for non-reserved entities in the EARL model is generated by first encoding the ConRel and $k$NResEnt information separately and then combining these two encoded pieces of information. This combination is achieved through a concatenation followed by a transformation using a 2-layer MLP, as described in the Entity-Agnostic Encoding section.", "evidence_reference": "In our GNN framework, similar to previous works \\cite{CompGCN, MaKEr}, we use a linear transformation on the concatenation of entity and relation representations to aggregate the neighbor information. Specifically, the message aggregation for the entity $e$ is: \\begin{equation} \\begin{aligned} \\mathbf{m}_{e}^{l} = \\sum_{(r, t) \\in \\mathcal{O}(e)} \\mathbf{W}_{\\text{out}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_t] + \\sum_{(r,h) \\in \\mathcal{I}(e)} \\mathbf{W}_{\\text{in}}^l [\\mathbf{h}^l_r; \\mathbf{h}^l_h], \\label{eq:gnn-agg} \\end{aligned} \\end{equation} where $\\mathcal{O}(e)$ denotes the out-going relation-entity pair set of $e$ and $\\mathcal{I}(e)$ denotes the in-going relation-entity pair set. $\\mathbf{W}_{\\text{out}}^l$ and $\\mathbf{W}_{\\text{in}}^l$ are transformation matrices for out-going and in-going pairs. $l \\in [0, \\dots, L]$ denotes the layer of GNN and $L$ is the total number of GNN layers. The input entity representations are calculated in Equation (\\ref{eq:info-combine}), and the input relation representations (e.g., $\\mathbf{h}_{r}^{0}$) are looked up in a trainable relation embedding matrix $\\mathbf{R} \\in \\mathbb{R}^{|\\mathcal{R}|\\times d}$.  The entity representation of $e$ in the GNN is updated as follows: \\begin{equation} \\mathbf{h}_{e}^{l+1} = \\sigma \\left( \\frac{1}{c}\\mathbf{m}_{e}^{l} + \\mathbf{W}_{\\text{self}}^{l} \\mathbf{h}_{e}^{l} \\right), \\label{eq:gnn-update} \\end{equation} where $c=|\\mathcal{I}(e)+\\mathcal{O}(e)|$ is a normalization constant. $\\mathbf{W}_{\\rm self}^{l}$ is a matrix for self representation update, and $\\sigma$ is an activation function. Furthermore, relation representations will also be updated in each layer: $\\mathbf{h}_{r}^{l+1} = \\sigma \\left( \\mathbf{W}_{\\text{rel}}^{l} \\mathbf{h}_{r}^{l} \\right)$. We use the output representations in the $L$-th layer for entities and relations as their embeddings to calculate scores next."}
{"question": "Consider the paper that introduces the model that exhibits the most negative Spearman's Correlation Coefficient. What specific methodological approach did this model employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "answer": "Maximum Mutual Information (MMI) scoring function", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model in the figure has the highest Spearman's Correlation?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological approach did DialoGPT employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "explanation_reference": "The paper describes the implementation of a maximum mutual information (MMI) scoring function to specifically address the problem of generating bland, uninformative samples in open-domain text generation models. This method penalizes bland hypotheses by maximizing the backward model likelihood, which is a direct response to the challenge mentioned.", "evidence_reference": "To address this problem, we implement a maximum mutual information (MMI) scoring function~\\cite{li2015diversity, zhang2018generating}. MMI employs a pre-trained \\textit{backward} model to predict source sentences from given responses, i.e., $P(\\\\text{Source}|\\\\text{target})$."}
{"question": "Consider the paper that introduces the method which does not have results in the CSQA2.0 dev and StrategyQA test tasks shown in the table. What is the entropy value for the Winogender task using a basic prompt for the 175B PPO model proposed by this method?", "answer": "0.618", "figure": "locality/2311.18397/result_table.png", "anchor_arxiv_id": "2311.18397", "reference_arxiv_id": "2203.02155", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method does not show result in CSQA2.0 dev and StrategyQA test task?", "answer_anchor": "ChatGPT", "question_reference": "What is the entropy value for the Winogender task using a basic prompt for the 175B PPO model?", "explanation_reference": "The entropy value for the Winogender task using a basic prompt for the 175B PPO model directly reflects the model's uncertainty in making choices among the available options for this task, indicating how unbiased the model is in its selections.", "evidence_reference": "Winogender             & entropy     & basic         & 0.750 & 0.721 & 0.735 &  0.583 &  0.535 & 0.503 &  0.698 & 0.587 & 0.618 & \\textbf{0.760} &  0.719 & 0.737 \\\\"}
{"question": "Consider the paper that introduces the method that has a score of 71.4 in the CB dataset with 4-shot prompting. What specific metric is used by the model proposed in the paper to compute the similarity between the average pooled representations of the prompt tokens when defining task similarity through prompts?", "answer": "Cosine Similarity of Average Tokens", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having score of 71.4 in CB dataset with 4-shot prompting?", "answer_anchor": "SPoT", "question_reference": "What specific metric is used to compute the similarity between the average pooled representations of the prompt tokens when defining task similarity through prompts?", "explanation_reference": "The metric used to compute the similarity between the average pooled representations of the prompt tokens when defining task similarity through prompts is explicitly mentioned as cosine similarity.", "evidence_reference": "We compute the cosine similarity between the average pooled representations of the prompt tokens: \\[sim(t^1, t^2) = cos(\\dfrac{1}{\\bmmc{L}}\\sum_{i} \\bm{e}_i^{1}, \\dfrac{1}{\\bmmc{L}}\\sum_{j} \\bm{e}_j^{2}),\\] where $\\bm{e}_i^{1}, \\bm{e}_j^{2}$ denote the respective prompt tokens of $\\bm{e}^{1}, \\bm{e}^{2}$, and $cos$ denotes the cosine similarity."}
{"question": "Consider the paper that identifies the dataset with the largest number of Queries|Aspects in the OABS category. What specific threshold value was chosen for the matching score to ensure the inclusion of abstract sentences in the aspect-based summaries during its construction, and how was this value determined?", "answer": "0.5", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset with the most number of Queries|Aspects in OABS category?", "answer_anchor": "QASUM", "question_reference": "What specific threshold value was chosen for the matching score to ensure the inclusion of abstract sentences in the aspect-based summaries during the dataset construction, and how was this value determined?", "explanation_reference": "The chosen threshold value for the matching score, which determines the inclusion of abstract sentences in the aspect-based summaries, was 0.5. This value was determined through manual evaluation, where 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold were randomly picked and assigned to 5 experts for evaluating the dataset quality. The threshold of 0.5 was selected based on these evaluations.", "evidence_reference": "To filter out sentences with limited content overlap, an aspect-based summary includes only abstract sentences with a matching score \\(\\mathcal{S}(x, a)\\) greater or equal to a pre-defined threshold \\(\\lambda\\). To determine the exact value of the threshold, we try \\(\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]\\) and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality. We then choose to use \\(\\lambda=0.5\\)."}
{"question": "Consider the paper that introduces the model that is placed below TransferNet but above UniKGQA in the table. What specific strategy does the model proposed in the paper employ to update the question embedding during the path expansion process?", "answer": "The specific strategy employed by the subgraph retriever (\\model) to update the question embedding during the path expansion process involves concatenating the original question with the historical expanded relations in the path as the input of RoBERTa, i.e., \\(f(q^{(t)}) = \\mbox{RoBERTa}([q;r_{1};\\cdots;r_{t}])\\).", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shown in the table is below TransferNet but above UniKGQA?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific strategy does the subgraph retriever (\\model) employ to update the question embedding during the path expansion process?", "explanation_reference": "The strategy for updating the question embedding during the path expansion process involves concatenating the original question with the historical expanded relations as the input of RoBERTa, which is a specific detail of how the model updates the question representation to reflect the context of the path being expanded.", "evidence_reference": "we update the embedding of the question by simply concatenating the original question with the historical expanded relations in $p^{(t)}$ as the input of RoBERTa, \\emph{i.e.}, \\beq{ \\label{eq:updatequestion} f(q^{(t)}) = \\mbox{RoBERTa}([q;r_{1};\\cdots;r_{t}]), }"}
{"question": "Consider the paper that introduces the method that scores a 70.1 in the 'Revised Persona' column. What is the implication of the model's performance on the CMUDoG dataset for future research directions in document-grounded conversation systems?", "answer": "Exploring selection at the topic level.", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets score of 70.1 in 'Revised Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the implication of the CSN model's performance on the CMUDoG dataset for future research directions in document-grounded conversation systems?", "explanation_reference": "The performance of the CSN model on the CMUDoG dataset, which showed that both sentence-level and word-level content selection strategies work equally well, implies that future research could benefit from exploring document content selection at the topic level. This is suggested as a potential future work direction in the conclusion, indicating that the findings from the CMUDoG dataset performance could lead to the exploration of more granular or higher-level content selection mechanisms, such as topic-level selection, to further improve the effectiveness of document-grounded conversation systems.", "evidence_reference": "As a future work, it would be interesting to study if the selection can be done at topic level, in addition to sentence and word levels."}
{"question": "Consider the paper that introduces the method that has a METEOR cross entropy score lower than ours but higher than X-Transformer. How does the performance of the model proposed in the paper, when using concepts extracted from captions, compare to using concepts derived from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "answer": "Using concepts extracted from captions leads to better performance (121.8 CIDEr) compared to using concepts derived from an object detector (117.4 CIDEr for BUTD and 119.7 CIDEr for VinVL).", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method has a METEOR score lower than (Ours) but higher than X-Transformer?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP with concepts extracted from captions compare to using concepts derived from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "explanation_reference": "The question focuses on comparing the effectiveness of different sources of semantic concepts (caption-extracted vs. object detector-derived) on the performance of the ViTCAP model, specifically in terms of CIDEr scores on the COCO-caption Karpathy split. The answer directly addresses this by providing the specific CIDEr scores achieved using each method, indicating that caption-extracted concepts result in better performance.", "evidence_reference": "FOCAL$_\\text{Tag}$ & $10$ & $35.2$ & $28.0$ & $57.0$ & $117.1$ & $21.4$\\\\ FOCAL$_\\text{Tag+Init}$ & $10$ & $36.0$ & $28.4$ & $57.5$ & $120.5$ & $22.0$\\\\ FOCAL$_\\text{Init}$ & $10$ & $35.0$ & $28.2$ & $57.1$ & $118.0$ & $21.6$\\\\ FOCAL$_\\text{Tag+Init}$ & $40$ & $35.9$ & $28.4$ & $57.6$ & $121.1$ & $22.1$ \\\\ PRED. Concepts & $36.1$ & $28.6$ & $57.6$ & $120.6$ & $21.7$"}
{"question": "Consider the paper that introduces the model that achieves the second highest score in the Stance column. What is the Pearson's correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model shows consistently low accuracy than VIBE model?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the task of political affiliation classification on Twitter data. A value of 0.9817 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "Twitter, \\poliaff{} ($F_1$), 0.9817159316285563"}
{"question": "Consider the paper that introduces the LLM model that corresponds to an r score of 0.813. What specific methodological difference in the evaluation setup for the model's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "answer": "The use of sampling a letter choice at temperature 0 using the already-sampled explanation for certain exams, rather than extracting the model's letter choice directly from the explanation.", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the LLM model that demonstrates the r score equal to 0.813?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard approach of directly extracting the model's letter choice from the explanation for most exam runs. Instead, for these specific exams, the approach involved sampling a letter choice at temperature 0 using the explanation already sampled, indicating a unique handling of these exams compared to others.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method that scores higher than 69.0 but lower than 70.0 in the Forgotten Realms category. How does the model proposed in the paper leverage the generated synthetic data to improve the quality of few-shot entity linking?", "answer": "The MetaBLINK model leverages the generated synthetic data to improve the quality of few-shot entity linking by using a meta-learning mechanism to automatically assign different weights to each synthetic entity-mention pair. This approach allows the model to exploit rich semantic information from the synthetic data more effectively, leading to improved training and performance on few-shot entity linking tasks.", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method got a score high than 69.0 but lower than 70.0 in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model leverage the generated synthetic data to improve the quality of few-shot entity linking?", "explanation_reference": "The MetaBLINK model improves the quality of few-shot entity linking by adopting a meta-learning mechanism that automatically assigns different weights to each synthetic data instance. This approach allows the model to differentiate the quality of synthetic data for more effective training.", "evidence_reference": "To further differentiate the quality of each synthetic data instance for model training, we design a meta-learning mechanism that can automatically assign different weights to the synthetic data."}
{"question": "Consider the paper that introduces the specific hyperparameter settings for generating narratives from sentence-form commonsense knowledge using GPT-3.5 in the framework of the dataset used for the 'dialogue response' task. What specific hyperparameter settings were used?", "answer": "We leverage \\texttt{text-davinci-002} GPT-3.5 for generating narratives. We set temperature to 0.9, top-p to 0.95, frequency penalty to 1.0, presence penalty to 0.6, and max tokens to 1024.", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07188", "reference_arxiv_id": "2212.10465", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset used for Dialogue response task?", "answer_anchor": "SODA", "question_reference": "What specific hyperparameter settings were used for generating narratives from sentence-form commonsense knowledge using GPT-3.5 in the SODA framework?", "explanation_reference": "These hyperparameter settings are directly related to the process of transforming sentence-form commonsense knowledge into narratives within the SODA framework, ensuring the narratives are detailed yet closely aligned with the original commonsense knowledge.", "evidence_reference": "We prompt GPT-3.5 with [sentence-form commonsense] Rewrite this story with more specific details in two or three sentences:. We find long narratives tend to be driven far away from the original commonsense knowledge. Therefore, we set the length of the narrative to two or three sentences. We leverage text-davinci-002 GPT-3.5 for generating narratives. We set temperature to 0.9, top-p to 0.95, frequency penalty to 1.0, presence penalty to 0.6, and max tokens to 1024."}
{"question": "Consider the paper that introduces the method that achieves the highest Precision score in the Token (I-topo) category. What specific distribution is used for sampling span lengths in its span masking scheme, and what is the mean span length resulting from this distribution?", "answer": "Geometric distribution with $p=0.2$, mean span length = $3.8$", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets the highest Precision score in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific distribution is used for sampling span lengths in SpanBERT's span masking scheme, and what is the mean span length resulting from this distribution?", "explanation_reference": "The paper specifies that span lengths are sampled from a geometric distribution with parameter \\(p=0.2\\), and this sampling strategy results in a mean span length of 3.8. This detail directly addresses the conceptual understanding of SpanBERT's approach to span masking, highlighting the statistical method used for determining the lengths of spans to be masked during pre-training.", "evidence_reference": "Given a sequence of tokens \\(X = ( x_1 , x_2, \\ldots , x_n )\\), we select a subset of tokens \\(Y \\subseteq X\\) by iteratively sampling spans of text until the masking budget (e.g. 15% of \\(X\\)) has been spent. At each iteration, we first sample a span length (number of words) from a geometric distribution \\(\\ell \\sim \\mathrm{Geo}(p)\\), which is skewed towards shorter spans... Following preliminary trials, we set \\(p=0.2\\), and also clip \\(\\ell\\) at \\(\\ell_{max}=10\\). This yields a mean span length of \\(\\mathrm{mean}(\\ell)=3.8\\)."}
{"question": "Consider the paper that introduces the method that exhibits the lowest BLEU score in the De->En task over Average Lagging from 5 to 11. What specific mechanism does the model proposed in the paper use to calculate the expected attention for each head?", "answer": "The MMA-IL variant calculates the expected attention for each head by first calculating the softmax energy for each head as follows: \\(u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j}\\) and then using Equation \\(\\beta_{i,j} = \\sum_{k=j}^{|\\vx|} \\left( \\frac{\\alpha_{i, k} \\exp(u_{i,j})}{\\sum_{l=1}^k  \\exp(u_{i,l})} \\right)\\) to calculate the expected attention.", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shows the lowest BLEU score in De-En task over Average Lagging from 5 to 11?", "answer_anchor": "MMA", "question_reference": "What specific mechanism does the MMA-IL variant use to calculate the expected attention for each head?", "explanation_reference": "The MMA-IL variant calculates the expected attention for each head using a mechanism referred to as 'SoftEnergy'. This is distinct from the MMA-H variant, which uses a hard selection process. The 'SoftEnergy' mechanism allows MMA-IL to attend to all previous encoder states, leveraging more information for translation.", "evidence_reference": "For MMA-IL, we calculate the softmax energy for each head as follows: \\x \\begin{eqnarray} u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j} \\end{eqnarray}"}
{"question": "Consider the paper that introduces the method that achieves an accuracy of 18.4 on the GSM8K dataset. Based on the ablation study findings, what is the accuracy of the model proposed in the paper when fine-tuned on 20% of the GSM8K dataset data and utilizing an external calculator?", "answer": "20.47%", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method gets 18.4 accuracy in GSM8K dataset?", "answer_anchor": "CoT Fine-tuned", "question_reference": "Based on the ablation study findings, what is the accuracy of T5 XXL finetuned on 20% of the GSM8K dataset data when using an external calculator?", "explanation_reference": "The ablation study on dataset size reveals that finetuning T5 XXL on only 20% of the GSM8K dataset data results in an accuracy of 20.47% when an external calculator is used. This detail directly answers the question by providing the specific performance metric for a subset of the data under specified conditions.", "evidence_reference": "20\\% (1067 examples) & 11.22 & 20.47"}
{"question": "Consider the paper that introduces the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category. What is the specific combination of methods within that method that yielded state-of-the-art performance in OECT scores based on the integration of regularization and replay methods?", "answer": "MIR with OnlineL2Reg", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2205.02014", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "CMR", "question_reference": "Based on the findings related to the integration of regularization and replay methods, what specific combination of CMR methods yielded state-of-the-art performance in OEC\\T~scores?", "explanation_reference": "The combination of MIR (a replay method) and OnlineL2Reg (a regularization method) is mentioned to produce a noticeable improvement over both individual methods, achieving the state-of-the-art performance in OEC\\T~scores. This suggests that integrating regularization terms for replay methods is a promising direction for future research in CMR.", "evidence_reference": "Inspired by Fig.~\\ref{fig:heatmap} and findings in \\texttt{(Q3)}, we add an initial experiment by combining the \\textit{MIR} and \\textit{OnlineL2Reg} and show its performance in Table~\\ref{tab:main}. Interestingly, we indeed observe this combination produces a noticeable improvement over both \\textit{MIR} and \\textit{OnlineL2Reg}, yielding the state-of-the-art performance in OEC\\T~scores."}
{"question": "Consider the paper that introduces the method in the figure that has a perplexity of approximately 30 and an average max toxicity of 0.2. What specific formatting guideline is provided by the model proposed in the paper for the width of the abstract text compared to the columns for the text in the body of the paper?", "answer": "The width of the abstract text should be smaller than the width of the columns for the text in the body of the paper by 0.6 cm on each side.", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2105.03023", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is method in the figure has around 30 perplexity and 0.2 average max toxicity?", "answer_anchor": "DExperts", "question_reference": "What specific formatting guideline is provided for the width of the abstract text compared to the columns for the text in the body of the paper?", "explanation_reference": "The guideline for the width of the abstract text specifies that it should be smaller than the width of the columns for the text in the body of the paper by 0.6 cm on each side, ensuring a clear distinction and visual hierarchy between the abstract and the main text.", "evidence_reference": "Type the abstract at the beginning of the first column. The width of the abstract text should be smaller than the width of the columns for the text in the body of the paper by 0.6 cm on each side."}
{"question": "Consider the paper that introduces the method which is in the second row of the table. What is the percentage drop in $F_1$ score observed upon the removal of data augmentation during training on the \\cord\\ dataset according to the ablation study proposed in the paper?", "answer": "2.6%", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2005.00642", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method in the second row of the table?", "answer_anchor": "SPADE", "question_reference": "What is the percentage drop in $F_1$ score observed upon the removal of data augmentation during training on the \\cord\\ dataset according to the ablation study?", "explanation_reference": "The ablation study section of the paper indicates that removing data augmentation during training on the \\cord\\ dataset results in a 2.6% drop in $F_1$ score, highlighting the importance of data augmentation in the model's performance.", "evidence_reference": "~~~~ (-) data augmentation & 81.9 (-2.6) \\\\\\\\"}
{"question": "Consider the paper that introduces the model that performs the best on the BBBP dataset. What specific advantage does it demonstrate over Galactica in terms of pre-training data utilization?", "answer": "MolXPT leverages the information from surrounding text by wrapping SMILES sequences with text, which is not explicitly done by Galactica.", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model perform the best in the BBBP dataset?", "answer_anchor": "MolXPT", "question_reference": "What specific advantage does MolXPT demonstrate over Galactica in terms of pre-training data utilization?", "explanation_reference": "The advantage is highlighted by the explicit use of 'wrapped' sequences in MolXPT's pre-training, which incorporates both molecular SMILES and surrounding text, allowing for a more effective representation by leveraging the complementary information from both modalities. This approach is contrasted with Galactica, which, despite also using SMILES and text, does not build and train on these 'wrapped' sequences, missing out on the potential benefits of such integration.", "evidence_reference": "A possible explanation of the superior performance is that the SMILES describes the component and structural information of molecules, while the text describes the general properties. They are complementary to each other, and joint training on them brings more effective representations."}
{"question": "Consider the paper that introduces the model that results in the highest Self-BLEU score on the TellMeWhy dataset. What is the K-L divergence between the model's predicted and ground-truth question type distributions on the test set?", "answer": "0.0089", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What model shows the highest Self-BLEU score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What is the K-L divergence between the predicted and ground-truth question type distributions on the test set?", "explanation_reference": "The K-L divergence value directly measures the performance of the question type distribution learning module, indicating how closely the predicted question type distribution aligns with the actual distribution in the test set.", "evidence_reference": "On the test set, the K-L divergence between the prediction results of our BERT-based model and ground-truth is $0.0089$"}
{"question": "Consider the paper that introduces the benchmark that has the highest 'Generation Token Length' in the figure. What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to the model proposed in the paper and answer-only prompting?", "answer": "The underestimation is primarily due to the lack of inclusion of instructions and answer options in the prompt.", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which benchmark has the highest 'Generation Token Length' in the figure?", "answer_anchor": "BBH", "question_reference": "What is the primary reason for the underestimation of language model capabilities in the BIG-Bench paper according to the findings related to answer-only prompting?", "explanation_reference": "The paper suggests that the few-shot evaluation of PaLM 540B with answer-only prompting, which includes both a task instruction and answer options, demonstrates the effect of including these elements in the prompt. This approach outperforms the average human-rater on 6 out of 23 BBH tasks and is overall 1.4% better than the BIG-Bench reported result, indicating that the original setup underestimated language model performance.", "evidence_reference": "The few-shot evaluation of PaLM 540B  with answer-only prompting in this paper, however, outperforms the average human-rater on 6 out of 23 \\bbh{} tasks and is overall 1.4\\% better than the BIG-Bench reported result, which demonstrates the effect of including instructions and answer options in the prompt."}
{"question": "Consider the paper that introduces the method that has approximately 30 perplexity and the highest average max toxicity. What specific strategy does the paper propose for ensuring the appearance of guide words in the model's generated text without requiring pre-defined ordering of constraints?", "answer": "Guide Closest", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method with around 30 perplexity and the highest average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What specific strategy does the paper propose for ensuring the appearance of guide words in generated text without requiring pre-defined ordering of constraints?", "explanation_reference": "The paper proposes two strategies for controlling generation towards a list of guide words, one of which is 'Guide Closest'. This strategy does not require the guide words to be ordered and works by shifting the score function by the highest cosine similarity across all words in the set of guide words that have not appeared before the current step. This approach allows for the flexible inclusion of guide words in the generated text, aligning with the paper's goal of imposing hard constraints on language generation without the need for pre-defined ordering.", "evidence_reference": "Given a list of guide words $W$, we propose two approaches for both the case where we need words $w_n$ to appear in a fixed order as well as when any order suffices. [...] At any given decoding step, we shift the score function by the highest cosine similarity across all words $w\\in W_{t}$. [...] Explicitly, we score $y_{t}$ as \\begin{align} \\score'(y_{t}, W_{t}\\mid \\yy_{<t}) = & \\,\\score(y_{t} \\mid \\yy_{<t}) \\ + \\\\ \\lambda \\cdot \\max &\\Big(0, \\underset{{w\\in W_t}}{\\max}\\, \\semdist(\\gamma(y_t), \\gamma(w)\\Big) \\nonumber \\end{align}"}
{"question": "Consider the paper that introduces the model labeling 'fine-tuned' shown in the table, MVQG-VL-T5. What specific improvement in percentage points did the model proposed in the paper achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "answer": "6 and 6.2 points", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model being fine-tuned shown in the table?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific improvement in percentage points did the generative models achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "explanation_reference": "The improvement is highlighted in the comparison between generative and discriminative models for the VQA task, specifically on the out-of-domain subset. The generative models (\\ourst{} and \\oursb{}) improved upon the discriminative baselines by 6 and 6.2 percentage points respectively, demonstrating the effectiveness of using generative modeling for questions with answers not included in the top-K answer candidates.", "evidence_reference": "This improvement is more significant on the out-of-domain subset, where the generative \\ourst{} and \\oursb{} achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling."}
{"question": "Consider the paper that introduces the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing category. What is the impact of varying the $\\epsilon_\\text{init}$ parameter on the number of keys used by GRACE for 1000 edits on zsRE when editing T5's block 4?", "answer": "Increasing $\\epsilon_\\text{init}$ reduces the number of keys used by GRACE for 1000 edits on zsRE when editing T5's block 4.", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2211.11031", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the last method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "GRACE", "question_reference": "What is the impact of varying the $\\epsilon_\\text{init}$ parameter on the number of keys used by GRACE for 1000 edits on zsRE when editing T5's block 4?", "explanation_reference": "The impact of varying $\\epsilon_\\text{init}$ on the number of keys used by GRACE is directly related to the generalization capability of the edits. Larger $\\epsilon_\\text{init}$ values create edits with more influence, making edits more general but increasing the interference with unrelated inputs. This results in fewer keys being needed to cover the edits, as each key can generalize to more inputs.", "evidence_reference": "Excitingly, \\method can achieve high-quality edits with small codebooks. For example, \\method uses few keys to edit T5 on zsRE: 1000 edits are made using only 137 keys. Such compression is only feasible by managing the $\\epsilon$-balls effectively over time."}
{"question": "Consider the paper that introduces the method shown in the figure corresponds to the solid red line. What specific transformation rule is applied by the model proposed in the paper to construct disfluent summaries for the fluency dimension in text summarization?", "answer": "We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries.", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the figure demonstrated by the red solid line?", "answer_anchor": "UniEval", "question_reference": "What specific transformation rule is applied to construct disfluent summaries for the fluency dimension in text summarization?", "explanation_reference": "The answer directly addresses the transformation rule used for creating disfluent summaries in the context of evaluating the fluency dimension. This detail is part of the pseudo data construction process for the fluency dimension, where specific actions are taken to manipulate a text span to generate disfluent summaries.", "evidence_reference": "Fluency represents the quality of individual sentences. We randomly draw a span from the positive sample and perform one of repeating, deleting, and shuffling to obtain the disfluent summaries."}
{"question": "Consider the paper that introduces the dataset associated with the task 'Hate Speech Spans Detection (HSSD)'. What specific linguistic phenomenon is used as an example to illustrate the challenge of detecting hate speech in the dataset, particularly when comments use phrases that only make sense when read backwards?", "answer": "Puns", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset with task `Hate Speech Spans Detection (HSSD)`?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon is used as an example to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "explanation_reference": "The paper discusses various linguistic phenomena that pose challenges to hate speech detection, including the use of puns, where phrases only make sense when read backwards. This is highlighted in the example of 'B\u1ed3n K\u1ef3 L\u1eafc', which, when read backwards, becomes 'B\u1eafc K\u1ef3 L\u1ed3n', illustrating how puns can be used to convey hate speech in a concealed manner.", "evidence_reference": "Some comments use phrases that only read them backwards, they make sense. As in the example, 'B\u1ed3n K\u1ef3 L\u1eafc', if this phrase is read backwards, it is 'B\u1eafc K\u1ef3 L\u1ed3n' (pussy north)."}
{"question": "Consider the paper that introduces the method which has a METEOR cross entropy score lower than ours but higher than X-Transformer. What is the performance of the model, when comparing the use of concepts extracted from captions to using concepts from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "answer": "Using concepts extracted from captions leads to better performance (121.8 CIDEr scores) compared to using concepts from an object detector.", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What method has a METEOR score lower than (Ours) but higher than X-Transformer?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP with concepts extracted from captions compare to using concepts from an object detector in terms of CIDEr scores on the COCO-caption Karpathy split?", "explanation_reference": "The paper demonstrates that leveraging concepts extracted directly from captions, as opposed to using pre-defined concepts from an object detector, results in improved performance on the COCO-caption Karpathy split. This is evidenced by the higher CIDEr score achieved when using caption-extracted concepts.", "evidence_reference": "FOCAL$_\\text{Tag+Init}$ & $10$ & $35.9$ & $28.4$  & $57.6$ & \\cellcolor{gray!40}$121.3$ & $21.9$"}
{"question": "Consider the paper that introduces the model that demonstrates the highest Oracle score. How does its \\ednascore metric balance the evaluation of faithfulness and diversity in the generated summaries?", "answer": "The \\ednascore metric balances the evaluation of faithfulness and diversity in generated summaries by being the harmonic mean of Entailment and (1-Self-Entailment). Higher values of \\ednascore imply more faithful and diverse summaries.", "figure": "locality/2310.14503/comparison_table.png", "anchor_arxiv_id": "2310.14503", "reference_arxiv_id": "2203.15108", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model that demonstrates the highest Oracle score?", "answer_anchor": "Composition", "question_reference": "How does the \\ednascore metric balance the evaluation of faithfulness and diversity in generated summaries?", "explanation_reference": "The \\ednascore metric is designed to jointly measure faithfulness and diversity in summaries. It uses Self-Entailment to assess diversity, which is effective because both components of \\ednascore (faithfulness and diversity) yield values in a similar output space through the same trained model (Entailment), facilitating a balanced evaluation.", "evidence_reference": "The reason \\ednascore relies on Self-Entailment to measure diversity is because the faithfulness metric is also based on Entailment. This means that both components will be mapped to a score in a similar output space (i.e., they both yield values between 0 and 1 obtained through the same trained model), making it more likely to be properly balanced when mixed."}
{"question": "Consider the paper that introduces the method shown in the figure corresponds to the solid red line. How does the sequential training approach of the model proposed in the paper specifically address the negative transfer problem observed in certain evaluation dimensions?", "answer": "The sequential training approach of UniEval specifically addresses the negative transfer problem observed in certain evaluation dimensions by employing a method from continual learning: whenever a new dimension is introduced, a small portion of data from all previous dimensions is added to replay. This strategy allows for easy extension of the evaluator to new dimensions without training from scratch and enables explicit learning of dimensions related to basic linguistic features before moving on to dimensions that require a better understanding of the text.", "figure": "locality/2310.13189/calibration_figure.png", "anchor_arxiv_id": "2310.13189", "reference_arxiv_id": "2210.07197", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method shown in the figure demonstrated by the red solid line?", "answer_anchor": "UniEval", "question_reference": "How does the sequential training approach of UniEval specifically address the negative transfer problem observed in certain evaluation dimensions?", "explanation_reference": "The sequential training approach with data replay from previous dimensions is designed to mitigate the negative transfer problem by ensuring that the model retains knowledge from previously learned dimensions while learning new ones. This method allows for a smooth extension of the evaluator to new dimensions without starting from scratch, and it is particularly effective in maintaining performance across related linguistic features before moving on to dimensions that require a deeper understanding of the text.", "evidence_reference": "To tackle this issue, we employ a simple and effective method from continual learning: whenever a new dimension is introduced, we add small portion of data from all previous dimensions to replay."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage category. What was the success rate in the targeted in-context knowledge updating experiment for FEVER when the prompt included original examples, edited relevant examples, and edited irrelevant examples, using this method?", "answer": "99.9", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2210.09150", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage?", "answer_anchor": "IC-Retrieval", "question_reference": "In the targeted in-context knowledge updating experiment for FEVER, what was the success rate when the prompt included original examples, edited relevant examples, and edited irrelevant examples?", "explanation_reference": "The success rate indicates how effectively GPT-3 adapted to the updated knowledge for relevant paraphrases of the original test claim while minimizing the impact on unrelated questions. This specific setup achieved a very high success rate, demonstrating GPT-3's ability to adapt to targeted knowledge updates when provided with a comprehensive prompt that includes a mix of original, relevant edited, and irrelevant edited examples.", "evidence_reference": "From Table~\\ref{tab:knowledge_edit_full}, we see that different prompts give vastly different results. Specifically, using only the original examples in the prompt leads to relatively poor success rate (especially on FEVER), while adding edited relevant examples in the prompt leads to better success rate, it leads the model to over-rely on the knowledge updates even on irrelevant questions. However, when incorporating all cases of original examples, edited relevant and irrelevant examples in the prompt, GPT-3 is able to achieve high editing success rate and low drawdown on irrelevant questions."}
{"question": "Consider the paper that introduces the method for which the BLEU-1 score is missing in the table. What specific loss parameters are set for the concept classification task in the model proposed by the paper to handle the imbalanced distribution of semantic concepts?", "answer": "$\\gamma_{+}=0$ and $\\gamma_{-}=1$", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method does not provide BLEU-1 score?", "answer_anchor": "ViTCAP", "question_reference": "What specific loss parameters are set for the concept classification task in ViTCAP to handle the imbalanced distribution of semantic concepts?", "explanation_reference": "These parameters are chosen to handle the imbalanced distribution of semantic concepts by decoupling the decay rates of positive and negative samples, emphasizing more on the contribution of the positive samples.", "evidence_reference": "Due to the extremely imbalanced semantic concepts distribution (certain concepts appear much frequently than the rest), we adopt the simplified asymmetric focal loss which shows great performances handling sample imbalance problems for the multi-label classification task. We set parameters $\\gamma_{+}=0$ and $\\gamma_{-}=1$ as in our experiment."}
{"question": "Consider the paper that introduces the method that achieves sentence-level precision of 60.32. What specific implementation detail of its attention mechanism allows the model proposed in the paper to efficiently handle sequences of up to 32K characters on modern GPUs?", "answer": "The use of a combination of windowed and a new dilated attention pattern.", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method show a sentence-level precision equal to 60.32?", "answer_anchor": "longformer", "question_reference": "What specific implementation detail of the Longformer's attention mechanism allows it to efficiently handle sequences of up to 32K characters on modern GPUs?", "explanation_reference": "The combination of windowed and a new dilated attention pattern is mentioned as a key implementation detail that enables the Longformer to efficiently process sequences of up to 32K characters on modern GPUs. This detail is crucial for understanding how the Longformer achieves its efficiency and scalability for long sequences.", "evidence_reference": "We first evaluate \\model on autoregressive character-level language modeling using a combination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs."}
{"question": "Consider the paper that introduces the model shown on the first line of the table, specifically BERT. How does the performance of BERT on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "answer": "The performance of BERT on the NER task using a fine-tuning approach generally outperforms the feature-based approach, as the fine-tuning approach allows for minimal task-specific architecture modifications and leverages the deep bidirectional nature of BERT, which is crucial for understanding the context of entities in text. Different masking strategies during pre-training, such as random masking or keeping the word unchanged, are used to mitigate the mismatch between pre-training and fine-tuning stages, but the paper does not provide a direct comparison of these strategies' impact on the NER task specifically.", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shown on the first line?", "answer_anchor": "BERT", "question_reference": "How does the performance of BERT on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "explanation_reference": "The performance comparison between fine-tuning and feature-based approaches for the NER task, under various masking strategies during pre-training, shows that the fine-tuning approach consistently outperforms the feature-based approach. This is evident from the Dev set results for NER, where the fine-tuning approach yields higher F1 scores compared to the feature-based approach across all masking strategies.", "evidence_reference": "In the table presented in the Ablation for Different Masking Procedures section, the Dev set results for NER under different masking strategies show that the fine-tuning approach (95.4, 94.9, 95.2, 95.2 for different strategies) consistently achieves higher F1 scores than the feature-based approach (94.9, 94.0, 94.6, 94.7 for the same strategies), indicating better performance."}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What specific feature of spoken language does the Spoken Language Embedding Subnetwork in the model proposed in the paper focus on to handle the volatile nature of spoken opinions?", "answer": "focusing on important parts of speech", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method on the first row of the table?", "answer_anchor": "TFN", "question_reference": "What specific feature of spoken language does the Spoken Language Embedding Subnetwork in the Tensor Fusion Network model focus on to handle the volatile nature of spoken opinions?", "explanation_reference": "The Spoken Language Embedding Subnetwork is designed to focus on important parts of speech to effectively deal with the volatile nature of spoken language, which often lacks proper structure and includes idiosyncratic speech traits.", "evidence_reference": "The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech."}
{"question": "Consider the paper that introduces the model that has the second lowest MCD 1 score. Which specific modification in the ablation experiments led to the most significant performance deterioration on the CLUTRR task when trained on relation lengths k=2,3?", "answer": "Value ablation", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2112.00578", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model dmonstrates the lowest MCD 1 score?", "answer_anchor": "T5-based UT", "question_reference": "In the ablation experiments, which specific modification led to the most significant performance deterioration on the CLUTRR task when trained on relation lengths k=2,3?", "explanation_reference": "The value ablation modification, which simplifies the Edge Transformer computations by computing the value \\(v_{ilj}\\) for each triangle \\((i, l, j)\\) using only the edge \\((i, l)\\) instead of both edges \\((i, l)\\) and \\((l, j)\\), led to the most significant performance deterioration on the CLUTRR task when trained on relation lengths k=2,3. This is evidenced by the largest drop in performance metrics compared to the base model and other ablation conditions.", "evidence_reference": "Value ablation & 54.4 $\\pm$ 3.4    & 44.2 $\\pm$ 3.6   & 36.9 $\\pm$ 3.6   & 33.9 $\\pm$ 3.7   & 30.7 $\\pm$ 3.6"}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category. What specific improvement in P@1 does this method achieve over $\\text{RoBERTa}_{LARGE}$ on the LAMA-UHN-T-REx corpus?", "answer": "2.9%", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2002.01808", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category?", "answer_anchor": "K-Adapter", "question_reference": "What specific improvement in P@1 does \\textsc{K-Adapter} achieve over $\\text{RoBERTa}_{LARGE}$ on the LAMA-UHN-T-REx corpus?", "explanation_reference": "The improvement is calculated based on the P@1 scores reported for \\textsc{K-Adapter} and $\\text{RoBERTa}_{LARGE}$ on the LAMA-UHN-T-REx corpus. The reported P@1 for \\textsc{K-Adapter} is 23.0% and for $\\text{RoBERTa}_{LARGE}$ is 20.1%. The difference between these two percentages represents the specific improvement achieved by \\textsc{K-Adapter} over $\\text{RoBERTa}_{LARGE}$.", "evidence_reference": "LAMA-UHN-T-REx          & 0.2  & 0.2    & 12.6          & 26.2       & 20.1          & 23.0"}
{"question": "Consider the paper that introduces the model that has the highest relative performance in few-shot prompting. What specific performance improvement does the model proposed in the paper demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "answer": "37.5%", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has the highest accuracy in few-shot prompting?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific performance improvement does the \\modelname 70B model demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "explanation_reference": "The question focuses on the performance improvement of the \\modelname 70B model over the Falcon 40B model specifically in the Commonsense Reasoning benchmark. The answer, 37.5%, directly reflects the performance metric for the \\modelname 70B model in this category, indicating a significant improvement over the Falcon 40B model's performance.", "evidence_reference": "\\multirow{4}{*}{\\cinnamon} & 7B & 16.8 & 63.9 & 48.9 & 61.3 & 14.6 & 45.3 & 32.6 & 29.3 \\\\ & 13B & 24.5 & 66.9 & 55.4 & 65.8 & 28.7 & 54.8 & 39.4 & 39.1 \\\\ & 34B & 27.8 & 69.9 & 58.7 & 68.0 & 24.2 & 62.6 & 44.1 & 43.4 \\\\ & 70B & \\textbf{37.5} & \\textbf{71.9} & \\textbf{63.6} & \\textbf{69.4} & \\textbf{35.2} & \\textbf{68.9} & \\textbf{51.2} & \\textbf{54.2} \\\\"}
{"question": "Consider the paper that introduces the method that has a lower F1 score than TPP and a higher F1 score than BROS. What specific performance gain does the integration of both CoordConv and Corner Pooling modules contribute to the Entity Linking task in the model proposed in the paper, according to the ablation study?", "answer": "0.03", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having lower F1 score than TPP and higher F1 score than BROS?", "answer_anchor": "MSAU-PAF", "question_reference": "What specific performance gain does the integration of both CoordConv and Corner Pooling modules contribute to the Entity Linking task in the MSAU-PAF model, according to the ablation study?", "explanation_reference": "The integration of both CoordConv and Corner Pooling modules into the MSAU-PAF model results in a performance gain of 0.03 in the F1 score for the Entity Linking task, as indicated by the ablation study. This improvement demonstrates the effectiveness of these modules in enhancing the model's ability to predict entity links by exploiting spatial correlations more effectively.", "evidence_reference": "The last row shows the results that we test on combining all improvements, which improves the F1 score by 0.03 by the total in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the method shown in the table that demonstrates the highest BLEU-1 score for the Test Seen task. What specific aspect of the model's representation space, as proposed in the paper, potentially allows contrastive search to be directly applicable without contrastive training?", "answer": "The anisotropic distribution of token representations", "figure": "locality/2310.08943/result_table.png", "anchor_arxiv_id": "2310.08943", "reference_arxiv_id": "2202.06417", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown in the table demonstrates the highest BLEU-1 score for Test Seen task?", "answer_anchor": "SimCTG", "question_reference": "What specific aspect of the Chinese language model's representation space potentially allows contrastive search to be directly applicable without contrastive training?", "explanation_reference": "The intrinsic property of the Chinese language model, which naturally represents text by characters, might lead to a representation space that displays a high level of isotropy. This property potentially allows contrastive search to be directly applicable without the need for contrastive training.", "evidence_reference": "We see that in all layers (including the final layer), the MLE model displays a similar self-similarity with respect to SimCTG. This observation is quite different from what we see from English language models... We conjecture that this discrepancy might come from the intrinsic property of different languages... For English, current state-of-the-art methods always represent the text into subword units... On the other hand, languages like Chinese are naturally represented by basic units, i.e., characters... As a result, even the vanilla MLE objective can obtain a representation space that displays a high level of isotropy."}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in direct prompting. How does the GAtt method, specifically utilized by this model, address the issue of multi-turn consistency in dialogue models?", "answer": "The GAtt (Ghost Attention) method specifically addresses the issue of multi-turn consistency in dialogue models by enabling dialogue control over multiple turns. This is achieved through a process that hacks the fine-tuning data to help the attention mechanism focus in a multi-stage process, ensuring that instructions given at the beginning of a dialogue are respected throughout all conversation turns.", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shows the lowest execuation accuracy in direct prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "How does the GAtt method specifically address the issue of multi-turn consistency in dialogue models?", "explanation_reference": "The GAtt method addresses multi-turn consistency by ensuring that a specific instruction is respected throughout the dialogue. It does this by artificially adding the instruction to all user messages in the dialogue during training, and then selectively applying loss only to the tokens related to the most recent turn and the initial instruction, effectively teaching the model to maintain focus on the instruction across multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right). [...] Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages."}
{"question": "Consider the paper that introduces the model that has the largest number of updated parameters, specifically SR+NSM+E2E, on the CWQ dataset. What is the impact on Hits@1 of QA when removing the subgraph merging strategy (GM)?", "answer": "drop by 0.1%", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the most number of updated parameters?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What is the impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in \\model+NSM on the CWQ dataset?", "explanation_reference": "The impact on Hits@1 of QA when removing the subgraph merging strategy (GM) in \\model+NSM on the CWQ dataset is a decrease of 0.1%, indicating a slight reduction in QA performance without the subgraph merging strategy.", "evidence_reference": "Table~\\ref{tb:treemerge} shows that based on \\model+NSM, the average subgraph size increases from 174 to 204, and Hits@1 of QA drops 0.1\\% when removing the subgraph merging strategy (\\model+NSM w/o GM) but directly taking the union of all the subgraphs from different topic entities to induce the subgraph."}
{"question": "Consider the paper that examines the Twitter dataset that has the most number of languages compared to all other Twitter datasets. What was the highest zero-shot cross-lingual transfer performance F1 score achieved for the target language Tigrinya, and which source language was it transferred from?", "answer": "68.6, Hausa", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset (Twitter) has the most number of languages compared to all Twitter datasets?", "answer_anchor": "AfriSenti", "question_reference": "What was the highest zero-shot cross-lingual transfer performance F1 score achieved for the target language Tigrinya, and which source language was it transferred from?", "explanation_reference": "The highest zero-shot cross-lingual transfer performance F1 score for Tigrinya was achieved by transferring from Hausa, indicating that Hausa was the most effective source language for zero-shot learning on Tigrinya in the context of the experiments conducted.", "evidence_reference": "\\texttt{hau}\\t& \\textbf{47.1} & \\textbf{68.6} & \\textbf{57.9}"}
{"question": "Consider the paper that introduces the model in the figure that has a more negative Spearman's Correlation than 0.60 when the alternative set size is set to 100 in the Mean Cosine setting. What specific methodological approach did the model proposed in the paper employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "answer": "Maximum Mutual Information (MMI) scoring function", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model in the figure exceed 0.60 Spearman's Correlation as the alternative set size to 100 in Mean cosine setting?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological approach did DialoGPT employ to address the challenge of generating bland, uninformative samples in open-domain text generation models?", "explanation_reference": "The paper describes the implementation of a maximum mutual information (MMI) scoring function to specifically address the problem of generating bland, uninformative samples in open-domain text generation models. This method penalizes bland hypotheses by maximizing the backward model likelihood, which is a direct response to the challenge mentioned.", "evidence_reference": "To address this problem, we implement a maximum mutual information (MMI) scoring function~\\cite{li2015diversity, zhang2018generating}. MMI employs a pre-trained \\textit{backward} model to predict source sentences from given responses, i.e., $P(\\\\text{Source}|\\\\text{target})$."}
{"question": "Consider the paper that introduces the model that demonstrates the highest score in the 'T3' column. What specific hyperparameter values were used for the FewRel dataset in its experiments, and how do these values compare to those used for the TACRED dataset?", "answer": "For FewRel, the values were \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, the values were \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model demonstrates the highest score in 'T3' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were used for the FewRel dataset in the experiments, and how do these values compare to those used for the TACRED dataset?", "explanation_reference": "The specific hyperparameter values for the FewRel and TACRED datasets are directly provided in the Implementation Details section of the paper, allowing for a direct comparison between the two sets of values.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7."}
{"question": "Consider the paper that introduces the LLM model that demonstrates the lowest MSE score. What is the specific improvement in percentage points of the model proposed in the paper over GPT-3.5 in internal adversarially-designed factuality evaluations?", "answer": "19", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the LLM model that demonstrates the lowest MSE score?", "answer_anchor": "GPT-4", "question_reference": "What is the specific improvement in percentage points of GPT-4 over GPT-3.5 in internal adversarially-designed factuality evaluations?", "explanation_reference": "The improvement is directly stated as a comparison between GPT-4 and GPT-3.5, highlighting the progress made in reducing hallucinations and improving factuality.", "evidence_reference": "GPT-4 significantly reduces hallucinations relative to previous GPT-3.5 models (which have themselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our latest GPT-3.5 on our internal, adversarially-designed factuality evaluations."}
{"question": "Consider the paper that introduces the model that is seventh in the table. What is the main reason the PIDRP method performs worse than this model on all four top-level senses of the PDTB, especially on the Temporal sense?", "answer": "The main reason the PIDRP method performs worse than the PCP method on all four top-level senses of the PDTB, especially on the Temporal sense, is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which is the method demonstrated in the seventh row in the table?", "answer_anchor": "PCP", "question_reference": "What is the main reason the PIDRP method performs worse than the PCP method on all four top-level senses of the PDTB, especially on the Temporal sense?", "explanation_reference": "The paper suggests that the PIDRP method's poorer performance compared to the PCP method across all top-level senses, particularly in the Temporal sense, is due to the fact that connective prediction aligns more closely with the natural language patterns utilized during the model's pre-training stage, as opposed to direct implicit discourse relation prediction.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the method that has an accuracy of 82.82% in the CAIL2018 task. What is the percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing the model proposed in the paper with the state-of-the-art MPBFN-WCA?", "answer": "3.18%", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shows 82.82 accuracy in CAIL2018 task?", "answer_anchor": "LADAN", "question_reference": "What is the percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing LADAN with the state-of-the-art MPBFN-WCA?", "explanation_reference": "The percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing LADAN with MPBFN-WCA is directly stated in the 'Experimental Results' section, indicating the effectiveness of LADAN over the state-of-the-art method.", "evidence_reference": "Compared with the state-of-the-art MPBFN-WCA, LADAN improved the F1-scores of law article prediction, charge prediction, and term of penalty prediction on dataset CAIL-small by $2.02$\\%, $2.42$\\% and $4.20$\\% respectively, and about $3.18$\\%, $1.44$\\% and $5.79$\\% on dataset CAIL-big."}
{"question": "Consider the paper that introduces the method which corresponds to the first row of the table. What specific improvement in accuracy percentage does the model proposed in the paper achieve over the state-of-the-art for 5-class sentiment classification in multimodal sentiment analysis?", "answer": "6.7", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method on the first row of the table?", "answer_anchor": "TFN", "question_reference": "Based on the Tensor Fusion Network's performance in multimodal sentiment analysis, what specific improvement in accuracy percentage does the TFN model achieve over the state-of-the-art for 5-class sentiment classification?", "explanation_reference": "The question focuses on the specific detail of the improvement in accuracy percentage that the Tensor Fusion Network (TFN) model achieves over the state-of-the-art for 5-class sentiment classification. This detail is directly answered by the comparison provided in the experiments section, highlighting the TFN model's performance against existing approaches.", "evidence_reference": "Table~\\ref{table:mmres} shows the comparison with state-of-the-art approaches for multimodal sentiment analysis. \\mns \\ outperforms both neural and non-neural approaches as shown by $\\Delta^{SOTA}$. Specifically, for 5-class classification, the improvement is $\\uparrow$ 6.7%."}
{"question": "Consider the paper that introduces the model shown in the table that has an overall score of less than 3.80. What specific feature of its visual embeddings allows it to discriminate regions from different images when multiple images are given to it?", "answer": "Image ids", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model shown in the table with overall score less than 3.80?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific feature of the visual embeddings allows the model to discriminate regions from different images when multiple images are given to it?", "explanation_reference": "The feature that allows the model to discriminate regions from different images when multiple images are given to it is the use of image ids. This is explicitly mentioned as a function of the image ids encoded within the visual embeddings.", "evidence_reference": "Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in \\NLVR{}~\\cite{Suhr2019}, models take two input images)."}
{"question": "Consider the paper that introduces the dataset in the table that has the fewest number of turns. How does its curriculum learning strategy specifically address the challenge of optimizing Large Language Models (LLMs) to learn knowledge encoded in multiple languages simultaneously?", "answer": "By increasing the proportion of high-quality, low-resource languages during training.", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the least number of turns from the table?", "answer_anchor": "Multialpaca", "question_reference": "How does the curriculum learning strategy specifically address the challenge of optimizing LLMs to learn knowledge encoded in multiple languages simultaneously?", "explanation_reference": "The curriculum learning strategy is designed to transfer general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. This is achieved by initially using the whole pre-training dataset to train a base model for commonsense generalization ability, and then transitioning to a subset of the pre-training dataset that boasts superior quality and a greater proportion of multilingual content to strengthen the model's multilingual capabilities.", "evidence_reference": "Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is a significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of high-resource language in the model. To address this issue, we adopt a curriculum learning strategy that ramps up the ratio of high-quality and low-resource languages during training."}
{"question": "Consider the paper that introduces the benchmark represented by the triangle marker in the figure. What is the average human-rater performance for the 'Temporal Sequences' task in the suite it belongs to?", "answer": "90.8%", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which benchmark is represented using the triangle marker from the figure?", "answer_anchor": "BBH", "question_reference": "What is the average human-rater performance for the 'Temporal Sequences' task in the BIG-Bench Hard (BBH) suite?", "explanation_reference": "The average human-rater performance for the 'Temporal Sequences' task is directly provided in the detailed results table for the BBH tasks.", "evidence_reference": "Temporal Sequences & 25.0 & 52.2 & 90.8 & 100 & 33.6 & 67.2 & 77.6 & 96.8 & 39.6 & 78.8"}
{"question": "Consider the paper that introduces the method for which the BLEU-1 score is missing in the table. What specific performance gain does the model proposed in the paper achieve on the Google-CC dataset compared to the CC-12M model?", "answer": "3.2", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method does not provide BLEU-1 score?", "answer_anchor": "ViTCAP", "question_reference": "What specific performance gain does ViTCAP achieve on the Google-CC dataset compared to the CC-12M model?", "explanation_reference": "The performance gain of ViTCAP over the CC-12M model on the Google-CC dataset is quantified as a +3.2 improvement in the CIDEr score, indicating that ViTCAP outperforms the CC-12M model by this margin.", "evidence_reference": "ViTCAP  &  $\\\\textbf{108.6}_\\\\text{\\\\color{darkgreen}\\\\textbf{ +3.2}}$"}
{"question": "Consider the paper that introduces the method which is shown in the fourth row of the table. What is the impact of using a death mask with agent-specific global state (AS) on the performance of MAPPO in the SMAC domain?", "answer": "Using a death mask with agent-specific global state (AS) significantly improves MAPPO's performance in the SMAC domain.", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method showing in the fourth row of the table?", "answer_anchor": "MAPPO", "question_reference": "What is the impact of using a death mask with agent-specific global state (AS) on MAPPO's performance in the SMAC domain?", "explanation_reference": "The death mask effectively handles the drastic distribution shift in the critic input when an agent dies, by replacing the state for a dead agent with a zero state containing the agent's ID. This approach leads to superior performance compared to other methods of handling agent deaths, as it allows the critic to more accurately fit the average post-death reward for agent $a$ to the input $\\boldsymbol{0_a}$.", "evidence_reference": "Fig.~\\ref{fig:app-Ablation-death-new} demonstrates the effect of death mask on MAPPO(AS)'s performance in the SMAC domain, showing significant improvement."}
{"question": "Consider the paper that introduces the Seq2Seq/Tree model that has a Test Accuracy of 79.6. What specific method does the model's solution discrimination module use to encode the equation representation of a solution?", "answer": "gate-recurrent-unit (GRU)", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2212.00837", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which Seq2Seq model shows 79.6 Test Accuracy?", "answer_anchor": "Ana-CL", "question_reference": "What specific method does the solution discrimination module use to encode the equation representation of a solution?", "explanation_reference": "The solution discrimination module encodes the equation representation of a solution using a gate-recurrent-unit (GRU), as specified in the description of how the discrimination score is measured.", "evidence_reference": "We define a discrimination score that measures the association of a problem $X$ with its ground truth solution $Y$: \\begin{equation} \\label{eq:s_d} s_d =  Dis([en_x(X),en_y(Y)]) \\end{equation} where $en_y(Y)$ is the representation of equation $Y$ encoded by a gate-recurrent-unit (GRU) \\cite{chung2014empirical}."}
{"question": "Consider the paper that introduces the model that has a Recall@7 score of 92.97 for the MWOZ task. What specific performance improvement does it achieve on the Entity F1 metric for the SMD dataset when comparing the system with a fine-tuned retriever against the off-the-shelf retriever?", "answer": "0.06%", "figure": "locality/2310.08877/result_table.png", "anchor_arxiv_id": "2310.08877", "reference_arxiv_id": "2210.07564", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model has the 92.97 score in Recall@7 for MWOZ task?", "answer_anchor": "Q-TOD", "question_reference": "What specific performance improvement does Q-TOD achieve on the Entity F1 metric for the SMD dataset when comparing the system with a fine-tuned retriever against the off-the-shelf retriever?", "explanation_reference": "The question focuses on the detailed performance comparison between Q-TOD systems employing an off-the-shelf retriever versus a fine-tuned retriever specifically for the SMD dataset on the Entity F1 metric. The answer directly reflects the incremental benefit of fine-tuning the knowledge retriever for precision in knowledge retrieval, as indicated by the slight improvement in the Entity F1 score.", "evidence_reference": "~~Q-TOD (T5-Large) & ~~~~~71.11~~ \\\\ \\quad ~~w/ fine-tuned retriever & ~~~~~71.17 (+0.06)~~"}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in few-shot prompting. How does its performance on natural language reasoning tasks, based on the evaluation of the model proposed in the paper on the HELM benchmark, compare to other open-access models?", "answer": "StarCoderBase generally obtains substantially stronger performance than all other models with released weights and often performs comparably to or better than much larger models.", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shows the second best execuation accuracy in few-shot prompting?", "answer_anchor": "StarCoder", "question_reference": "Based on the evaluation of StarCoderBase on the HELM benchmark, how does its performance on natural language reasoning tasks compare to other open-access models?", "explanation_reference": "The evaluation of StarCoderBase on the HELM benchmark for natural language reasoning tasks shows that it generally outperforms other open-access models, indicating its superior ability to leverage its natural language and code pretraining for these tasks.", "evidence_reference": "In Table~\\ref{tab:helm_results} we report the results. We compute each model's ranking on each task, and order models in the table by their average ranking across tasks. StarCoderBase generally obtains substantially stronger performance than all other models with released weights and often performs comparably to or better than much larger models."}
{"question": "Consider the paper that introduces the dataset which has a training set size of 8,844. What specific linguistic phenomenon is used in the model proposed in the paper as an example to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "answer": "Puns", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset having training set size 8,844?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon is used as an example to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "explanation_reference": "The paper discusses various linguistic phenomena that pose challenges to hate speech detection, including the use of puns, where phrases only make sense when read backwards. This is highlighted in the example of 'B\u1ed3n K\u1ef3 L\u1eafc', which, when read backwards, becomes 'B\u1eafc K\u1ef3 L\u1ed3n', illustrating how puns can be used to convey hate speech in a concealed manner.", "evidence_reference": "Some comments use phrases that only read them backwards, they make sense. As in the example, 'B\u1ed3n K\u1ef3 L\u1eafc', if this phrase is read backwards, it is 'B\u1eafc K\u1ef3 L\u1ed3n' (pussy north)."}
{"question": "Consider the paper that introduces the method that achieves sentence-level precision of 60.32. What specific initialization method was used for the new position embeddings in the model proposed in the paper to leverage RoBERTa's pretrained weights for supporting longer documents?", "answer": "Copying the 512 position embeddings from RoBERTa multiple times", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method show a sentence-level precision equal to 60.32?", "answer_anchor": "longformer", "question_reference": "What specific initialization method was used for the new position embeddings in the Longformer to leverage RoBERTa's pretrained weights for supporting longer documents?", "explanation_reference": "The method used for initializing new position embeddings in the Longformer, to support documents longer than what RoBERTa's pretrained model could handle, involved copying the existing 512 position embeddings from RoBERTa multiple times. This approach was chosen to preserve the local structure learned by RoBERTa's attention heads across the extended sequence lengths.", "evidence_reference": "To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa's pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times."}
{"question": "Consider the paper that introduces the method shown in the first row of the table. What specific feature of the BERT model's output is utilized in the policy network for dynamic prompting via policy gradient in the model proposed in the paper?", "answer": "\\texttt{[CLS]} token representation", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is shown in the first row of the table?", "answer_anchor": "PromptPG", "question_reference": "What specific feature of the BERT model's output is utilized in the policy network for dynamic prompting via policy gradient in PromptPG?", "explanation_reference": "The policy network in PromptPG uses the BERT model's output for generating contextualized representations of the given problem and candidate examples. Specifically, it utilizes the [CLS] token representation from BERT's output as the problem encoding, which is then used to learn both the semantic similarity provided by the pre-trained BERT model and the hidden logical similarity among the math problems.", "evidence_reference": "To get the contextualized representation of the given problem and candidate examples, we use the BERT~[CLS] token representation as the problem encoding. We add a small linear layer on top of the BERT final pooling layer."}
{"question": "Consider the paper that introduces the model that has a diversity score of 2.58 for p2. What specific architectural change was made to the Transformer's layer normalization compared to its originally proposed form?", "answer": "a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied.", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model demonstrates diversity score of 2.58 in p2?", "answer_anchor": "T5-Large", "question_reference": "What specific architectural change was made to the Transformer's layer normalization in the T5 model compared to its originally proposed form?", "explanation_reference": "The T5 model made specific modifications to the original Transformer architecture's layer normalization. Specifically, it simplified layer normalization by placing it outside the residual path and removing the additive bias. This change is orthogonal to the experimental factors considered in the empirical survey of transfer learning.", "evidence_reference": "Our encoder-decoder Transformer implementation closely follows its originally-proposed form \\citep{vaswani2017attention}. First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of ``blocks'', each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization \\citep{ba2016layer} is applied to the input of each subcomponent. We use a simplified version of layer normalization where the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection \\citep{he2016deep} adds each subcomponent's input to its output."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.6712 on the Hate dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets mean classification accuracy 0.6712 on Hate dataset?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength and direction of a linear relationship between two variables. In this case, it quantifies the relationship between the word overlap (how vocabularies change over time) and the model performance for the task of political affiliation classification on Twitter data. A value of 0.9817159316285563 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the model represented with a dot marker. What specific performance improvement does the model, DialoGPT Large, show over the standard DialoGPT (345M) model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "answer": "3.03%", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model represneted with a dot marker?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific performance improvement does the DialoGPT (345M, w/ MMI) model show over the standard DialoGPT (345M) model in terms of METEOR score according to the DSTC-7 Dialogue Generation Challenge results?", "explanation_reference": "The improvement in METEOR score for the DialoGPT (345M, w/ MMI) model over the standard DialoGPT (345M) model is derived from the comparison of their METEOR scores in the DSTC-7 Dialogue Generation Challenge results. The standard DialoGPT (345M) model achieved a METEOR score of 8.51%, while the DialoGPT (345M, w/ MMI) model achieved a METEOR score of 11.23%, indicating a specific improvement of 3.06%.", "evidence_reference": "DialoGPT (345M) = 8.51% METEOR; DialoGPT (345M, MMI) = 11.23% METEOR"}
{"question": "Consider the paper that introduces the dataset which exhibits the highest accuracy for Method 2. What specific aspect of the token-level verifiers' training procedure in the model proposed by the paper is likely responsible for their initial uncertainty in solution correctness, as observed in the verifier visualization section?", "answer": "The token-level verifiers' training procedure likely causes initial uncertainty in solution correctness due to the large fraction of incorrect model-generated samples they are trained on, which makes them initially unsure about the correctness of a solution and gradually gain certainty as the solution progresses.", "figure": "locality/2310.14628/comparison_figure.png", "anchor_arxiv_id": "2310.14628", "reference_arxiv_id": "2110.14168", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What dataset demonstrates the highest accuracy with method 2?", "answer_anchor": "GSM8K", "question_reference": "What specific aspect of the token-level verifiers' training procedure is likely responsible for their initial uncertainty in solution correctness, as observed in the verifier visualization section?", "explanation_reference": "The observed initial uncertainty of the token-level verifiers in assessing solution correctness is attributed to the training procedure, where a significant portion of the training data consists of incorrect model-generated samples. This exposure to a high volume of incorrect samples during training likely conditions the verifier to start with a baseline of uncertainty, gradually adjusting its confidence as more of the solution is revealed and it can better assess correctness.", "evidence_reference": "Note that the model is initially unsure about whether the solution is correct and gradually gains certainty as the solution progresses: this is likely a property of the verifier training procedure, where it trains on a large fraction of incorrect model-generated samples."}
{"question": "Consider the paper that introduces the model in the LLM section of the table that corresponds to the highest test accuracy. What is the reported accuracy of the model proposed in the paper using self-consistency with sampling for the AQuA task on the UL2-20B model with a beam size of 40?", "answer": "26.9%", "figure": "locality/2310.09619/Math23k_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2203.11171", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which LLM model shows the highest test accuracy?", "answer_anchor": "Self-Consistency", "question_reference": "What is the reported accuracy of self-consistency using sampling for the AQuA task on the UL2-20B model with beam size 40?", "explanation_reference": "The reported accuracy directly answers the question by providing the specific performance metric of self-consistency using sampling for the AQuA task on the UL2-20B model.", "evidence_reference": "Self-consistency using sampling & 19.7 \\scriptsize{$\\pm$ 2.5} & \\textbf{24.9 \\scriptsize{$\\pm$ 2.6}} & \\textbf{25.3 \\scriptsize{$\\pm$ 1.8}} & \\textbf{26.7 \\scriptsize{$\\pm$ 1.0}} & \\textbf{26.9 \\scriptsize{$\\pm$ 0.5}}"}
{"question": "Consider the paper that introduces the model in the table that has 12M updated parameters. What is its core innovation in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "answer": "The core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks is the unification of retrieval and reasoning in both model architecture and parameter learning.", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model in the table has 12M updated parameters?", "answer_anchor": "UniKGQA", "question_reference": "What is the core innovation of UniKGQA in the context of unifying retrieval and reasoning for multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the question by summarizing the unique approach of UniKGQA, which integrates the retrieval and reasoning processes into a single framework. This is innovative because it contrasts with previous methods that treated these stages separately, thus enhancing the efficiency and effectiveness of solving multi-hop KGQA tasks.", "evidence_reference": "In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning."}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in few-shot prompting. What specific method does the paper propose to address its limitations in maintaining multi-turn consistency in dialogues?", "answer": "Ghost Attention (GAtt)", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shows the lowest execuation accuracy in few-shot prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "What specific method does the paper propose to address the limitations of LLMs in maintaining multi-turn consistency in dialogues?", "explanation_reference": "The paper introduces Ghost Attention (GAtt) as a method to address the limitations of Large Language Models (LLMs) in maintaining multi-turn consistency in dialogues. GAtt is described as a simple method inspired by Context Distillation that hacks the fine-tuning data to help the attention focus in a multi-stage process, enabling dialogue control over multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right)."}
{"question": "Consider the paper that introduces the method that results in a better score than MOCA but worse score than LACMA in the Seen, Val, SR dataset. What specific method does the model proposed in the paper leverage to enhance its understanding of complex human instructions and handling of long sequences of subtasks in dynamic environments?", "answer": "The Episodic Transformer leverages encoding the full episode history of visual observations and actions with a transformer, and it improves training by using synthetic instructions as an intermediate representation.", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the score better than MOCA but worse than LACMA in Seen, Val, SR dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer leverage to enhance its understanding of complex human instructions and handling of long sequences of subtasks in dynamic environments?", "explanation_reference": "The question focuses on a detailed aspect of how the Episodic Transformer (E.T.) addresses the challenges of understanding complex human instructions and handling long sequences of subtasks. The answer is directly provided by the methodological approach described in the paper, where synthetic instructions are used as an intermediate representation to decouple the understanding of visual appearance from the variations in natural language instructions.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the method which is placed below the row for R-Former but above the row for NeurJudge. What specific feature does the model's, proposed by the paper, fact re-encoder focus on to effectively distinguish between Article 385 and Article 163?", "answer": "Defendants' identity information", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown below the row of R-Former but above the row of NeurJudge?", "answer_anchor": "LADAN", "question_reference": "What specific feature does the LADAN model's fact re-encoder focus on to effectively distinguish between Article 385 and Article 163?", "explanation_reference": "The fact re-encoder in the LADAN model uses the distinction vector to focus on extracting distinguishable features such as defendants' identity information, which is crucial for distinguishing the applicable law articles and charges of the cases, specifically between Article 385 and Article 163.", "evidence_reference": "our fact re-encoder focuses on extracting distinguishable features like defendants' identity information (e.g., \\textit{``company manager'' ``working in the Cadastral Unit of Luocheng Branch of Luohe City Land and Resources Bureau''} in our examples), which effectively distinguish the applicable law articles and charges of these two cases."}
{"question": "Consider the paper that introduces the model that exhibits the second best execution accuracy in direct prompting. What specific preprocessing steps were applied to the XML files during the data curation process for its base version?", "answer": "Implemented a simple XML filter that checked for the presence of '<?xml version=' within the first 100 characters of the file.", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model shows the second best execuation accuracy in direct prompting?", "answer_anchor": "StarCoder", "question_reference": "What specific preprocessing steps were applied to the XML files during the data curation process for StarCoderBase?", "explanation_reference": "The question focuses on the specific method used to preprocess XML files during the data curation process for StarCoderBase. The answer directly addresses this by specifying the implementation of a simple XML filter, which is a detailed and specific part of the preprocessing steps mentioned in the paper.", "evidence_reference": "As we inspected the data, we noticed that certain extensions often consisted of XML files. For example, the .sld extension had more than 50% of its files in XML format. To address this, we implemented a simple XML filter that checked for the presence of '<?xml version=' within the first 100 characters of the file."}
{"question": "Consider the paper that introduces the method shown in the table which is above the 'Magister et al' row but below the 'UL2' row. What specific role does the guidance mechanism play in improving the quality of generated subquestions by a smaller model, according to the DecomDistill method proposed by Magister et al.?", "answer": "The guidance mechanism conditions the generation of subquestions on the equations describing the intermediate solutions, improving the quality of the generated questions across all metrics considered.", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the table above method proposed by Magister et al but below UL2 method?", "answer_anchor": "DecomDistill", "question_reference": "In the context of the Socratic CoT approach, what specific role does the guidance mechanism play in improving the quality of generated subquestions by a smaller model?", "explanation_reference": "The guidance mechanism is introduced to condition the generation of subquestions on the equations that describe the intermediate solutions of a problem. This strategy is shown to improve the quality of the generated questions in terms of BLEU score, BERT F1 score, and the accuracy of matching the number of GPT-3 annotated questions for a given problem, indicating its effectiveness in enhancing the sub-questioning capabilities of a smaller model.", "evidence_reference": "We introduced a \\textit{guidance mechanism} that conditions the generation of subquestions for a problem $P$ on the equations describing the intermediate solutions of $P$. This strategy improved the quality of the generated questions for all three metrics considered (Table \\ref{qg-scores}, second row)."}
{"question": "Consider the paper that introduces the model that achieves the highest score on the MNLI dataset. What is the Dev F1 score for its feature-based approach using only embeddings on the CoNLL-2003 NER task?", "answer": "91.0", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model that demonstrates the highest score on MNLI dataset?", "answer_anchor": "BERT", "question_reference": "What is the Dev F1 score for the feature-based approach using BERTbase with embeddings only on the CoNLL-2003 NER task?", "explanation_reference": "The Dev F1 score directly measures the performance of the feature-based approach using BERTbase with only embeddings on the CoNLL-2003 Named Entity Recognition task, indicating how well the model performed without fine-tuning or additional context.", "evidence_reference": "Feature-based approach (\\bertbase) &  &  \\\\ \\;\\;\\;Embeddings & 91.0 &- \\\\"}
{"question": "Consider the paper that introduces the dataset represented by the smallest blue circle. What specific methodological approach did the authors use to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "answer": "To mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap, the authors collected tweets based on locations where a language is predominantly spoken, using the location, longitude, latitude, and radius parameters to specify a circular geographic area.", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2201.08277", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset represented by the smallest blue label?", "answer_anchor": "NaijaSenti", "question_reference": "What specific methodological approach did the authors use to mitigate the challenge of tweets being collected in a language different from the query language due to stopwords overlap in a multilingual society like Nigeria?", "explanation_reference": "The authors addressed the issue of stopwords overlap causing tweets to be collected in an unintended language by collecting tweets based on the geographical locations where the target language is predominantly spoken. This methodological approach leverages the geographical parameters (location, longitude, latitude, and radius) to specify a circular geographic area for tweet collection, thus improving the accuracy of language-specific data collection.", "evidence_reference": "Stopwords overlap across indigenous languages in a multilingual society such as Nigeria. This results in tweets being collected in a language that differs from the query language. To mitigate this, we collected tweets based on locations where a language is predominantly spoken, using the location, longitude, latitude and radius parameters (25 miles) to specify a circular geographic area."}
{"question": "Consider the paper that introduces the dataset that corresponds to the last row of the table. Which specific linguistic phenomenon is used as an example in the paper to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "answer": "Puns", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset shown in the last row of the table?", "answer_anchor": "ViHOS", "question_reference": "What specific linguistic phenomenon is used as an example to illustrate the challenge of detecting hate speech when comments use phrases that only make sense when read backwards?", "explanation_reference": "The paper discusses various linguistic phenomena that pose challenges to hate speech detection, including the use of puns, where phrases only make sense when read backwards. This is highlighted in the example of 'B\u1ed3n K\u1ef3 L\u1eafc', which, when read backwards, becomes 'B\u1eafc K\u1ef3 L\u1ed3n', illustrating how puns can be used to convey hate speech in a concealed manner.", "evidence_reference": "Some comments use phrases that only read them backwards, they make sense. As in the example, 'B\u1ed3n K\u1ef3 L\u1eafc', if this phrase is read backwards, it is 'B\u1eafc K\u1ef3 L\u1ed3n' (pussy north)."}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-Fr task in the MSCOCO dataset. How does its performance with hallucinated visual tokens compare to using ground-truth visual representations on the Multi30K dataset for the EN\u2192DE task using the Transformer-Tiny model?", "answer": "VALHALLA with hallucinated visual tokens achieves an average BLEU score of 35.4 on the EN$\\rightarrow$DE task using the Transformer-Tiny model, compared to using ground-truth visual representations which also results in an average BLEU score of 35.4.", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model demonstrates the highest performance in En-Fr task in MSCOCO dataset?", "answer_anchor": "VALHALLA", "question_reference": "How does the performance of the model proposed in the paper with hallucinated visual tokens compare to using ground-truth visual representations on the Multi30K dataset for the EN$\\rightarrow$DE task using the Transformer-Tiny model?", "explanation_reference": "The performance of VALHALLA with hallucinated visual tokens is very similar to when using ground-truth visual representations, demonstrating the model's strong ability to generate visual representations that are semantically consistent with the ground-truth.", "evidence_reference": "Moreover, \\ours has very similar performance with either hallucinated (\\texttt{V}) or ground-truth representation (\\texttt{VM}), showing strong ability to generate visual representations that are semantically consistent with the ground-truth."}
{"question": "Consider the paper that introduces the method that achieves a score of 28.62 in the WQ-B task. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions by the model proposed in the paper?", "answer": "0.949", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets 28.62 score in WQ B task?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the model that corresponds to the lowest BERTScore F1 score on the TellMeWhy dataset. What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "answer": "The paper proposes a novel framework that combines question type prediction and event-centric summarization to generate educational questions for storybooks.", "figure": "locality/2310.16446/tell_me_why_table.png", "anchor_arxiv_id": "2310.16446", "reference_arxiv_id": "2203.14187", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What model shows the lowest BERTScore F1 score on TellMeWhy dataset?", "answer_anchor": "EQG", "question_reference": "What specific methodological approach does the paper propose to address the challenge of generating high-cognitive-demand educational questions from children's storybooks?", "explanation_reference": "The answer directly addresses the question by specifying the unique approach of combining question type prediction with event-centric summarization, which is the core methodological innovation of the study for generating educational questions.", "evidence_reference": "In this paper, we propose a novel framework combining question type prediction and event-centric summarization to generate educational questions for storybooks."}
{"question": "Consider the paper that introduces the dataset which has the largest number of instances in the ABS category. What threshold value was set for the 'Group' domain during the aspect discovery stage?", "answer": "0.5", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2011.07832", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset with the most number of instances in ABS category?", "answer_anchor": "WikiAsp", "question_reference": "What threshold value was set for the 'Group' domain during the aspect discovery stage?", "explanation_reference": "The threshold value for the 'Group' domain during the aspect discovery stage is mentioned as part of the methodology for assigning labels to sentences based on their aspect probabilities. A lower threshold means more sentences are included as input to the summarization model, potentially increasing noise.", "evidence_reference": "We tune $\\lambda$ independently for each domain based on the performance on validation sets and set $0.5$ for \\textit{Group}, $0.8$ for \\textit{Album}, \\textit{Animal}, \\textit{Building}, \\textit{Film}, and $0.9$ for the remaining domains as the threshold values."}
{"question": "Consider the paper that introduces the dataset in which KALMV achieves a score of 70.83 for the XL model. What specific adjustment was made to the Question Entity Linking task to ensure MTurk workers reached agreement on question entities?", "answer": "The task was modified so workers only verified a span and linked the entity in Wikidata.", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset being tested that KALMV gets 70.83 score for XL model?", "answer_anchor": "Mintaka", "question_reference": "What specific adjustment was made to the Question Entity Linking task to address the challenge of achieving agreement on question entities among MTurk workers?", "explanation_reference": "The adjustment was made due to the initial difficulty in achieving agreement among MTurk workers on the entities within the questions. By modifying the task to focus on verifying a span and then linking it to Wikidata, the process was streamlined and made more manageable, leading to improved agreement rates.", "evidence_reference": "Since early test runs showed it would be difficult to get agreement on question entities, we modified the task so workers only verified a span and linked the entity in Wikidata."}
{"question": "Consider the paper that introduces the method that is in the last row of the Full Training category. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions by the model proposed in the paper?", "answer": "0.949", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is in the last row of teh Full Training category?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the method that shows the lowest overall performance. What specific feature of the ALFRED dataset's expert demonstrations makes re-planning during a DAgger-style student-forcing paradigm non-trivial for the model proposed in the paper, and can lead to the inability to complete a task?", "answer": "The inability to undo certain actions", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "1912.01734", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the lowest over performance?", "answer_anchor": "SEQ2SEQ", "question_reference": "What specific feature of the ALFRED dataset's expert demonstrations makes re-planning during a DAgger-style student-forcing paradigm non-trivial, and can lead to the inability to complete a task?", "explanation_reference": "The feature of the ALFRED dataset's expert demonstrations that makes re-planning during a DAgger-style student-forcing paradigm non-trivial, and can lead to the inability to complete a task, is the presence of irreversible actions. This is because if an action taken by the student-forcing model during a task leads to an irreversible state change that deviates from the expert demonstration (e.g., slicing the only apple in the scene when the task is to place an intact apple in the refrigerator), the task cannot be completed as planned.", "evidence_reference": "Obtaining expert demonstration actions on the fly in navigation-only datasets like R2R only requires rerunning $A^*$. In ALFRED, on the fly demonstrations requires re-planning. In some cases re-planning is not possible: if during a task of {Clean & Place, apple, refrigerator, kitchen-3} a student-forcing model slices the only apple in the scene, the action cannot be recovered from and the task cannot be completed."}
{"question": "Consider the paper that introduces the method that has an average score of 82.8 with zero-shot prompting. What specific improvement in percentage points did the model proposed in the paper achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "answer": "6 and 6.2 points", "figure": "locality/2310.15746/comparison_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method has average score of 82.8 with zero-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific improvement in percentage points did the generative models achieve over their discriminative counterparts on the out-of-domain subset for the VQA task?", "explanation_reference": "The improvement is highlighted in the comparison between generative and discriminative models for the VQA task, specifically on the out-of-domain subset. The generative models (\\ourst{} and \\oursb{}) improved upon the discriminative baselines by 6 and 6.2 percentage points respectively, demonstrating the effectiveness of using generative modeling for questions with answers not included in the top-K answer candidates.", "evidence_reference": "This improvement is more significant on the out-of-domain subset, where the generative \\ourst{} and \\oursb{} achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than TPP and a higher F1 score than BROS. What specific advantage does the integration of Coordinate Convolution (CoordConv) provide to the model proposed in the paper in terms of performance on the Entity Labeling and Entity Linking tasks?", "answer": "A total gain of 0.02 in F1 score for both Entity Labeling and Entity Linking tasks.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having lower F1 score than TPP and higher F1 score than BROS?", "answer_anchor": "MSAU-PAF", "question_reference": "What specific advantage does the integration of Coordinate Convolution (CoordConv) provide to the MSAU-PAF model in terms of model performance on the Entity Labeling and Entity Linking tasks?", "explanation_reference": "The integration of Coordinate Convolution (CoordConv) into the MSAU-PAF model allows the convolution to access its own input coordinates by concatenating extra coordinate channels, which provides translational information that enhances the model's performance. This specific advantage is quantified as a total gain of 0.02 in F1 score for both the Entity Labeling and Entity Linking tasks, indicating an improvement in the model's ability to accurately label and link entities.", "evidence_reference": "As shown in Table \\ref{table:ablation}, when making the convolution access to its own input coordinates through concatenating extra coordinate channels, MSAU-PAF with Coordinate Convolution experienced a total gain of 0.02 in F1 score in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the method that has an F1 score of 75. What specific advantage does the incorporation of Part-Intensity Fields (PIF) and Part-Association Fields (PAF) modules provide to the model proposed in the paper in terms of entity linking?", "answer": "The PIF-PAF modules enable the MSAU-PAF model to efficiently and robustly predict the confidence association between two entities with composite field structure in densely and occluded documents.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having 75 F1 score?", "answer_anchor": "MSAU-PAF", "question_reference": "What specific advantage does the incorporation of Part-Intensity Fields (PIF) and Part-Association Fields (PAF) modules provide to the MSAU-PAF model in terms of entity linking?", "explanation_reference": "The PIF-PAF modules, by encoding fine-grained information and confidently predicting confidence association between entities, address the challenge of linking entities in dense and occluded documents. This is particularly important for the Entity Linking task, where the model needs to determine the associations between entities that are not necessarily close to each other and could be far away. The effectiveness of PIF-PAF in handling such scenarios contributes to the overall performance improvement of the MSAU-PAF model.", "evidence_reference": "The PIF-PAF heads could particularly handle well in densely and occluded documents via effectively encoding fine-grained information and confidently predicting confidence association between two entities with composite field structure."}
{"question": "Consider the paper that introduces the method that achieves an F1 score of 87.63 in the Token (I-topo) category. What specific advantage does its span boundary objective (SBO) provide over BERT's next sentence prediction (NSP) objective in terms of model performance on coreference resolution?", "answer": "2.7% F1 improvement", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method gets F1 score 87.63 in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific advantage does the span boundary objective (SBO) provide over BERT's next sentence prediction (NSP) objective in terms of model performance on coreference resolution?", "explanation_reference": "The span boundary objective (SBO), when compared to BERT's next sentence prediction (NSP) objective, specifically provides a substantial gain of 2.7% F1 on coreference resolution. This indicates that SBO is more effective for tasks requiring understanding of relationships between spans of text, such as coreference resolution, by improving the model's ability to predict entire masked spans using only the boundary token representations.", "evidence_reference": "Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone."}
{"question": "Consider the paper that introduces the method in the table that is listed right above One-Round Distillation and right below Specialization. What is the accuracy improvement for the MAWPS dataset when using an external calculator for the model proposed in the paper compared to its baseline accuracy?", "answer": "34.07%", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method in the table is listed right above One-Round Distillation and right below Specializing?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What is the accuracy improvement for the MAWPS dataset when using an external calculator for the CoT finetuned T5 XXL model compared to its baseline accuracy?", "explanation_reference": "The improvement can be calculated from the reported accuracies in the arithmetic reasoning section. The baseline T5 XXL model's accuracy with a calculator is not directly provided, but its baseline accuracy is given as 54.15%. The CoT finetuned T5 XXL model's accuracy with a calculator is reported as 88.22%. Therefore, the improvement is 88.22% - 54.15% = 34.07%.", "evidence_reference": "The accuracy achieved given a calculator comes close to the accuracy of 8-shot PaLM 540B, demonstrating that knowledge distillation is effective, but potentially limited by the mathematical abilities of small models. \\textbf{MAWPS} & 54.15 & \\textbf{70.41} & 88.22 & 93.00 & 93.66"}
{"question": "Consider the paper that introduces the method that is represented by the square marker. What specific methodological adjustment is made to its variant, named MMA-H, to address the potential issue of outlier attention heads affecting latency or attention span?", "answer": "attention variance loss to MMA-H", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is demonstrated by square marker?", "answer_anchor": "MMA", "question_reference": "What specific methodological adjustment is made to the MMA-H variant to address the potential issue of outlier attention heads affecting latency or attention span?", "explanation_reference": "The attention variance loss (L_var) is introduced specifically for the MMA-H variant to control the variance in attention spans across different heads, aiming to prevent outlier heads from significantly affecting the model's latency or the uniformity of attention spans. This methodological adjustment is designed to mitigate the potential issue of having some heads that either read too fast or too slow, which could lead to inefficiencies in simultaneous translation.", "evidence_reference": "In \\autoref{sec:latency-control}, we introduced the attention variance loss to MMA-H in order to prevent outlier attention heads from increasing the latency or increasing the attention span."}
{"question": "Consider the paper that introduces the method that corresponds to the first row of the table. What specific regularization techniques were applied to the $\\mathcal{U}_v$, $\\mathcal{U}_a$, and $\\mathcal{U}_s$ subnetworks in the model proposed in the paper, and what were their parameter values?", "answer": "Dropout with p=0.15 and L2 norm with coefficient 0.01.", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method on the first row of the table?", "answer_anchor": "TFN", "question_reference": "What specific regularization techniques were applied to the $\\mathcal{U}_v$, $\\mathcal{U}_a$, and $\\mathcal{U}_s$ subnetworks in the Tensor Fusion Network model, and what were their parameter values?", "explanation_reference": "The regularization techniques applied to the subnetworks are explicitly mentioned in the methodology section, specifying both the dropout rate and the L2 norm coefficient used for regularization.", "evidence_reference": "The TFN model is trained using the Adam optimizer with the learning rate $5\\mathrm{e}\u22124$. $\\mathcal{U}_v$ and $\\mathcal{U}_a$, $\\mathcal{U}_s$ subnetworks are regularized using dropout on all hidden layers with $p=0.15$ and L2 norm coefficient $0.01$."}
{"question": "Consider the paper that introduces the model represented with a dot marker. What specific methodological adjustment did the authors make to the initialization scheme of the DialoGPT Large model to account for its model depth?", "answer": "They modified the initialization scheme to account for model depth.", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model represneted with a dot marker?", "answer_anchor": "DialoGPT Large", "question_reference": "What specific methodological adjustment did the authors make to the initialization scheme of the DialoGPT model to account for its model depth?", "explanation_reference": "The authors mention modifying the initialization scheme to account for model depth as part of their methodological adjustments to the GPT-2 architecture for the DialoGPT model. This detail is a specific methodological choice aimed at improving the model's performance, likely by ensuring stability in deeper network layers.", "evidence_reference": "Our model inherits from GPT-2 \\cite{gpt2}, a 12-to-48 layer transformer with layer normalization, a initialization scheme that accounts for model depth that we modified, and byte pair encodings \\cite{bpe} for the tokenizer."}
{"question": "Consider the paper that introduces the method that exhibits the lowest BLEU score in the De->En task over Average Lagging from 5 to 11. What specific mechanism does the model proposed in the paper use to calculate the expected attention for each head, and how does it differ from the approach used by its counterpart within the same framework?", "answer": "MMA-IL calculates the expected attention for each head using a softmax energy function and then applies the Monotonic Infinite Lookback Attention (MILk) formula to calculate the expected attention. This approach allows each attention head in MMA-IL to attend to all previous encoder states, leveraging more information for translation. In contrast, MMA-H calculates the expected alignment for each head using a hard selection process based on a Bernoulli distribution, where each head hard-attends to one encoder state, which may be better suited for streaming systems with stricter efficiency requirements.", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the lowest BLEU score in De-En task over Average Lagging from 5 to 11?", "answer_anchor": "MMA", "question_reference": "What specific mechanism does MMA-IL use to calculate the expected attention for each head, and how does it differ from the approach used by MMA-H?", "explanation_reference": "MMA-IL's approach to calculating expected attention involves using a SoftEnergy function, which allows each attention head to attend to all previous encoder states, leveraging more information for translation. This is in contrast to MMA-H, which uses MonotonicEnergy for a hard attention calculation, limiting each head to attend to one encoder state at a time. This distinction highlights the flexibility of MMA-IL in accessing more comprehensive source information compared to the more efficiency-oriented MMA-H.", "evidence_reference": "For MMA-H, we use \\autoref{eq:monotonic_attention} in order to calculate the expected alignment for each layer each head, given $p_{i,j}^{l, h}$. For MMA-IL, we calculate the softmax energy for each head as follows: \\x \\begin{eqnarray} u_{i,j}^{l, h} = \\textrm{SoftEnergy} = \\left(\\frac{m_{j}\\hat{W}_{l,h}^K(s_{i-1}\\hat{W}_{l,h}^Q)^T}{{\\sqrt{d_k}}}\\right)_{i,j} \\end{eqnarray} and then use \\autoref{eq:milk_recurent} to calculate the expected attention."}
{"question": "Consider the paper that introduces the method that has a lower J_k score than Random Entity Quantization but a higher J_k score than NodePiece in the FB15k-237 dataset for all values of k between 400 and 1000. What is the primary reason for the performance variance in the model's ablation studies across different datasets?", "answer": "The primary reason for the performance variance in EARL's ablation studies across different datasets is the different characteristics of the datasets, particularly the number of relations. Datasets with more relations (e.g., FB15k-237 and CoDEx-L) provide enough distinguishable information for entity embeddings through connected relation information (ConRel), making the performance less affected by the removal of other components like reserved entities or $k$NResEnt. In contrast, datasets with fewer relations (e.g., WN18RR and YAGO3-10) rely more on the distinguishable information provided by reserved entities and $k$NResEnt, showing more significant performance variance in the ablation studies.", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What method shows the lower J_k score than Random Entity Quantization but higher J_k score than NodePiece for k in the range of [400, 1000] in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "What is the primary reason for the performance variance in EARL's ablation studies across different datasets?", "explanation_reference": "The variance in performance across different datasets in the ablation studies of EARL is attributed to the data statistics of these datasets, particularly the number of relations they contain. Datasets with more relations provide sufficient distinguishable information for entity embeddings, which influences the impact of removing certain components (e.g., Reserved Entity, ConRel, $k$NResEnt) on the model's performance.", "evidence_reference": "FB15k-237 and CoDEx-L have more relations than WN18RR and YAGO3-10, and diverse relations provide enough distinguishable information for entity embeddings. Thus, even in the 'w/o Reserved Entity' and 'w/o $k$NResEnt', performance is not affected dramatically since ConRel information still exists."}
{"question": "Consider the paper that introduces the model that achieves the highest score on the MNLI dataset. What specific strategy does the model use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "answer": "BERT uses a mixed strategy for masking the target tokens during the Masked LM pre-training. The probabilities associated with each part of this strategy are as follows:\n- 80% of the time, the word is replaced with the [MASK] token.\n- 10% of the time, the word is replaced with a random word.\n- 10% of the time, the original word is kept unchanged.", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the model that demonstrates the highest score on MNLI dataset?", "answer_anchor": "BERT", "question_reference": "What specific strategy does BERT use during the Masked LM pre-training to reduce the mismatch between pre-training and fine-tuning, and what are the probabilities associated with each part of this strategy?", "explanation_reference": "The question assesses understanding of BERT's pre-training strategy designed to mitigate the mismatch between the pre-training and fine-tuning stages, specifically focusing on the detailed probabilities of the mixed masking strategy employed.", "evidence_reference": "In Section~\\ref{sec:pretraining_tasks}, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective... The following is an ablation study to evaluate the effect of different masking strategies... \\begin{table}[ht] \\begin{center} {\\small \\begin{tabular}{@{}rrrccc@{}} \\toprule \\multicolumn{3}{c}{Masking Rates} & \\multicolumn{3}{c}{Dev Set Results}  \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.2cm}){4-6} \\textsc{Mask} &\\textsc{Same}&\\textsc{Rnd}& {MNLI} &\\multicolumn{2}{c}{NER} &  & & {\\footnotesize Fine-tune} &   {\\footnotesize Fine-tune}& {\\footnotesize Feature-based} \\cmidrule(r{0.2cm}){1-3} \\cmidrule(l{0.1cm}r{0.1cm}){4-4} \\cmidrule(l{0.2cm}){5-6}  80\\%&10\\%&10\\%&84.2&95.4&94.9... \\bottomrule \\end{tabular} } \\end{center} \\caption{\\label{tab:mask_ablation} Ablation over different masking strategies.} \\end{table}"}
{"question": "Consider the paper that introduces the method that exhibits the highest Accuracy@0.5 value on the RefCOCOg task. What is the role of the $\\tiny{<}obj\\tiny{>}$ token in the model's output sequence design proposed by the paper?", "answer": "Indicates word-box alignments and simplifies sequence prediction by providing hints of text-box code-switching.", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method demonstrates the highest Accuary@0.5 on RefCOCOg task?", "answer_anchor": "UniTAB", "question_reference": "What is the role of the $\\tiny{<}obj\\tiny{>}$ token in the UniTAB model's output sequence design?", "explanation_reference": "The $\\tiny{<}obj\\tiny{>}$ token is introduced to simplify the sequence generation by providing hints of the code-switching between text and box tokens, and naturally represents word-box alignments within the output sequence. This means that the words and box within a pair of $\\tiny{<}obj\\tiny{>}$ tokens refer to the same entity, facilitating the model's ability to align predicted words with boxes.", "evidence_reference": "We introduce a special $\\tiny{<}obj\\tiny{>}$ token inserted before the text word to be grounded, and after the generated box tokens. The $\\tiny{<}obj\\tiny{>}$ token simplifies the sequence generation by providing hints of the code-switching, and naturally represents word-box alignments."}
{"question": "Consider the paper that introduces the method that corresponds to the penultimate row of the table. Which specific scheme achieved the highest micro F1 score in the 1-shot setting for the CoNLL'03 dataset in the contextual label representations experiment?", "answer": "(BIO-TAG) LABEL", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "2203.08985", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method shown in the penult row of the table?", "answer_anchor": "LSFS", "question_reference": "In the contextual label representations experiment, which specific scheme achieved the highest micro F1 score in the 1-shot setting for the CoNLL'03 dataset?", "explanation_reference": "The highest micro F1 score in the 1-shot setting for the CoNLL'03 dataset was achieved by the scheme where the BIO tag is combined with the label name, indicated as '(BIO-TAG) LABEL'. This scheme outperformed others by providing a more detailed context for each label, leveraging both the position (BIO tag) and the semantic meaning of the label (LABEL).", "evidence_reference": "& (\\textit{BIO-TAG}) \\textit{LABEL} & \\bf 70.8 $\\pm$ 4.2  & 76.5 $\\pm$ 1.6  & 81.2 $\\pm$ 2.0  & 84.7 $\\pm$ 1.1 \\\\"}
{"question": "Consider the paper that introduces the method that demonstrates the lowest EA score on the FinQA task. What specific mathematical reasoning capability does the TabMWP dataset aim to assess in machines, as highlighted by the challenges presented in its design?", "answer": "multi-hop mathematical reasoning over heterogeneous information", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method demonstrates the lowest EA score in FinQA task?", "answer_anchor": "PromptPG", "question_reference": "What specific mathematical reasoning capability does the TabMWP dataset aim to assess in machines, as highlighted by the challenges presented in its design?", "explanation_reference": "The TabMWP dataset is designed to assess machines' ability to perform multi-hop mathematical reasoning over heterogeneous information, which involves looking up table cells given textual clues and conducting multi-step operations to predict the final answer. This capability is highlighted as a core challenge presented by the dataset's design, requiring systems to align and reason over both textual and tabular data.", "evidence_reference": "To solve problems in \\data, a system requires multi-hop mathematical reasoning over heterogeneous information by looking up table cells given textual clues and conducting multi-step operations to predict the final answer."}
{"question": "Consider the paper that introduces the method that corresponds to the orange line in the figure. What is the unique aspect of the model's architecture, referred to as UniKGQA, that differentiates it from previous works in multi-hop KGQA tasks?", "answer": "The integration of a semantic matching module based on a pre-trained language model for question-relation semantic matching and a matching information propagation module.", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method in the figure is demonstrated by the orange line?", "answer_anchor": "UniKGQA", "question_reference": "What is the unique aspect of the UniKGQA's model architecture that differentiates it from previous works in multi-hop KGQA tasks?", "explanation_reference": "The answer directly addresses the unique aspect of UniKGQA's model architecture by highlighting the combination of a semantic matching module and a matching information propagation module, which is a distinctive approach compared to previous works in the field of multi-hop KGQA tasks. This combination allows for effective semantic matching between questions and relations and the propagation of this matching information along the directed edges on KGs, which is crucial for unifying retrieval and reasoning in the context of multi-hop KGQA.", "evidence_reference": "For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs."}
{"question": "Consider the paper that introduces the method in the figure that demonstrates the highest Toxicity Probability Score when the number of samples equals 1M. What is the inference time for feature extraction using the model proposed in the paper compared to VinVL and M2 Transformer?", "answer": "31 ms", "figure": "locality/2312.11523/figure.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method in the figure demonstrates the highest Toxicity Probability Score when number of samples equal to 1M?", "answer_anchor": "GRIT", "question_reference": "What is the inference time for feature extraction using GRIT compared to VinVL and M2 Transformer?", "explanation_reference": "The inference time for feature extraction using GRIT is significantly lower than that of VinVL and M2 Transformer, demonstrating GRIT's computational efficiency in this aspect.", "evidence_reference": "VinVL$_\\mathrm{large}$\\cite{zhang2021vinvl} & ResNeXt-152 & Faster R-CNN & Class-Agnostic NMS & 304 ms \\n ${\\cal M}^2$ Trans. \\cite{cornia2020meshed} & ResNet-101 & Faster R-CNN & Class-Aware NMS & 736 ms \\n \\rowcolor{LightCyan} GRIT & Swin-Base & DETR-based & - & 31 ms"}
{"question": "Consider the paper that introduces the model that has the largest number of updated parameters. What specific effect does the removal of the path ending strategy have on its Hits@1 retrieval performance on the WebQSP dataset according to the paper?", "answer": "2.1% to 18.5% drop", "figure": "locality/2401.00158/comparison_table.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2202.13296", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the most number of updated parameters?", "answer_anchor": "SR+NSM+E2E", "question_reference": "What specific effect does the removal of the path ending strategy have on the Hits@1 retrieval performance on the WebQSP dataset according to the paper?", "explanation_reference": "The removal of the path ending strategy (PE) results in a significant decrease in the Hits@1 retrieval performance on the WebQSP dataset, indicating the critical role of this strategy in the model's ability to accurately retrieve relevant subgraphs.", "evidence_reference": "Table~\\ref{tb:retrieverperformance} indicates that based on \\model, Hits@1 drops 4.3-15.0\\% when removing QU (\\smodel w/o QU) and Hits@1 drops 2.1-18.5\\% when changing PE to the fixed path length $T$ (\\smodel w/o PE), where the optimal $T$ is set to 3 on both WebQSP and CWQ."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Internet-enhanced category. What is the relative performance increase range observed for language generation tasks when conditioning the largest LSLMs on the web through few-shot prompting by the model proposed in the paper?", "answer": "15%-30%", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2203.05115", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the first method shown in Explicit --> Internet-enhanced", "answer_anchor": "Internet-Fewshot", "question_reference": "What is the relative performance increase range observed for language generation tasks when conditioning the largest LSLMs on the web through few-shot prompting?", "explanation_reference": "The relative performance increase range for language generation tasks when conditioning the largest LSLMs on the web through few-shot prompting is directly stated in the paper, indicating the effectiveness of the method over the commonly used closed-book few-shot approach.", "evidence_reference": "For the language generation tasks, we see a relative performance increase of 15%-30% over the commonly used closed-book few-shot approach."}
{"question": "Consider the paper that introduces the model represented by a blue line in the figure. Based on the human evaluation results, which variant was statistically indistinguishable from human responses in terms of relevance?", "answer": "DialoGPT (345M, w/ MMI)", "figure": "locality/2310.13676/comparison_figure.png", "anchor_arxiv_id": "2310.13676", "reference_arxiv_id": "1911.00536", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model represented with a blue line in the figure?", "answer_anchor": "DialoGPT Large", "question_reference": "Based on the human evaluation results, which variant of DialoGPT was statistically indistinguishable from human responses in terms of relevance?", "explanation_reference": "The statistical significance testing for human evaluation showed that the differences between the 345M model and human responses in terms of relevance were not statistically significant, indicating that the 345M variant of DialoGPT was indistinguishable from human responses in this aspect.", "evidence_reference": "The differences between 345M model (2) and human response (1) are not statistically significant."}
{"question": "Consider the paper that introduces the method that has a CoLA score equal to 55.9 on the GLUE task. What was the average GLUE benchmark performance in the ablation study when the model proposed by the paper used regular adapters in both the encoder and decoder?", "answer": "86.4", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2203.08304", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has CoLA score equal to 55.9 on GLUE task?", "answer_anchor": "HyperDecoder", "question_reference": "In the ablation study comparing different adapter configurations, what was the average GLUE benchmark performance when using regular adapters in both the encoder and decoder?", "explanation_reference": "The question specifically targets the results from the ablation study that varied adapter configurations across the encoder and decoder. The average GLUE benchmark performance for the configuration using regular adapters in both the encoder and decoder (referred to as 'Manual-Manual' in the detailed ablation results) directly answers the question.", "evidence_reference": "Manual-Manual & 2.9% & 58.5 & 95.3 & 91.7 / 91.4 & 89.7 / 92.5 & 91.2 / 88.3 & 89.8 & 94.0 & 81.2 & 86.4"}
{"question": "Consider the paper that introduces the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage category. What is the percentage improvement in accuracy for LLaMA-7B + the model proposed in the paper (zero-shot) on the CommonsenseQA dataset compared to its baseline without the model proposed in the paper?", "answer": "38.54%", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2309.03118", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the last method shown in Explicit --> Retrieval-enhanced --> Multi-Stage?", "answer_anchor": "Knowledge Solver", "question_reference": "What is the percentage improvement in accuracy for LLaMA-7B + KSL (zero-shot) on the CommonsenseQA dataset compared to its baseline without KSL?", "explanation_reference": "The percentage improvement in accuracy for LLaMA-7B + KSL (zero-shot) on the CommonsenseQA dataset is calculated based on the performance improvement over the baseline LLaMA-7B without KSL. This detail is directly extracted from the results section, where it compares the performance of LLaMA-7B with and without the KSL approach.", "evidence_reference": "LLaMA-7B (zero-shot)    &  20.5       &   26.8       &  22.7       \\\\ LLaMA-7B +  KSL (zero-shot)   & \\textbf{28.4} (+38.54\\%)       &    \\textbf{34.0} (+26.87\\%)      &  \\textbf{23.6} (+3.96\\%)       \\\\"}
{"question": "Consider the paper that introduces the dataset that corresponds to the last row of the table. What is the F1-score improvement for the model proposed in the paper, specifically the PhoBERT_Large model, when additional clean comments are included?", "answer": "0.0849", "figure": "locality/2310.11166/comparison_table.png", "anchor_arxiv_id": "2310.11166", "reference_arxiv_id": "2301.10186", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset shown in the last row of the table?", "answer_anchor": "ViHOS", "question_reference": "What is the F1-score improvement for the PhoBERT_Large model when additional clean comments are included in the dataset?", "explanation_reference": "The improvement in F1-score for the PhoBERT_Large model when additional clean comments are included is directly stated in the Experiments and Results section, indicating the performance enhancement due to the inclusion of additional clean comments.", "evidence_reference": "PhoBERT$_{Large}$ considerably outperforms other models in the dataset without additional clean data, achieving 0.6867 in F1-score. In addition, the best model trained on Full data is XLM-R$_{Large}$, which has an F1-score of 0.7770. We find that XLM-R$_{Large}$ increased by 0.1014 and PhoBERT$_{Large}$ increased by 0.0849."}
{"question": "Consider the paper that introduces the method that has a perplexity of approximately 30 and an average max toxicity of around 0.4. What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments after its effect on perplexity and repetition scores was analyzed?", "answer": "5", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method with around 30 perplexity and around 0.4 average max toxicity?", "answer_anchor": "PPLM", "question_reference": "What is the initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments after analyzing its effect on perplexity and repetition scores?", "explanation_reference": "The initial value of the shift parameter \\(\\lambda_0\\) used in the remaining experiments is determined based on its effect on perplexity and repetition scores. The paper mentions that for \\(\\lambda_0=5\\), the average perplexity (58.4) and repetition score (3.5%) are the best among the considered values, indicating this value was chosen for subsequent experiments.", "evidence_reference": "For \\(\\lambda_0=5\\) the average perplexity (\\(58.4\\)) and repetition score (\\(3.5\\%\\)) are the best among the considered values; we use this value of \\(\\lambda_0\\) in the remaining experiments."}
{"question": "Consider the paper that introduces the method that corresponds to a higher F1 score than that of LDSGM but a lower F1 score than 65.76 for PDTB-Top. What is the primary reason it underperforms compared to the model proposed in the paper on the Temporal sense?", "answer": "The PIDRP method underperforms compared to the PCP method on the Temporal sense primarily because connective prediction is closer to the natural language patterns when the model is in the pre-training stage than direct implicit discourse relation prediction.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which is the method has a higher F1 score than LDSGM but lower F1 score than 65.76", "answer_anchor": "PCP", "question_reference": "What is the primary reason the PIDRP method underperforms compared to the PCP method on the Temporal sense?", "explanation_reference": "The paper suggests that the PIDRP method's underperformance, especially on the Temporal sense, is attributed to the fact that connective prediction aligns more closely with the natural language patterns that the model was exposed to during its pre-training stage, as opposed to directly predicting implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the dataset in the table that has a validation set size of 1250. Which model demonstrated the highest percentage of factual hallucinations among the evaluated systems?", "answer": "\\bencdec", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset having 1250 val set from the table?", "answer_anchor": "XSumFaith", "question_reference": "Based on the study's findings, which model demonstrated the highest percentage of factual hallucinations among the evaluated systems?", "explanation_reference": "The question targets a specific detail regarding the performance of different models in producing factual hallucinations, which is a nuanced aspect of the paper's analysis on hallucinations in summarization. The \\bencdec model is identified as having the highest percentage of factual hallucinations, indicating its relative strength in integrating background knowledge to generate summaries that, while may contain hallucinated content, are factually accurate.", "evidence_reference": "In total, 34.7\\% of the \\bencdec abstracts were faithful (26.9\\%) and/or factual (+7.8\\%). This is 7.4\\% absolute better than the next-best model (\\ptgen). The number of unfaithful yet factual summaries for \\bencdec, 7.8\\%, was also the highest."}
{"question": "Consider the paper that introduces the model that corresponds to the brown bars in the figure. What specific method does the paper propose to address the limitations of large language models (LLMs) in maintaining multi-turn consistency in dialogues?", "answer": "Ghost Attention (GAtt)", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model is demonstrated in the brown color?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific method does the paper propose to address the limitations of LLMs in maintaining multi-turn consistency in dialogues?", "explanation_reference": "The paper introduces Ghost Attention (GAtt) as a method to address the limitations of Large Language Models (LLMs) in maintaining multi-turn consistency in dialogues. GAtt is described as a simple method inspired by Context Distillation that hacks the fine-tuning data to help the attention focus in a multi-stage process, enabling dialogue control over multiple turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure~\\ref{fig:GAtt_chat_comparison} (right)."}
{"question": "Consider the paper that introduces the benchmark represented by the triangle marker in the figure. What specific criteria were used to exclude tasks from the subset known as BIG-Bench Hard (BBH), due to their reliance on specialized knowledge or being outside the scope of the work?", "answer": "Not solvable by authors within 60 minutes, requires specialized knowledge, or not even worth attempting with chain-of-thought.", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which benchmark is represented using the triangle marker from the figure?", "answer_anchor": "BBH", "question_reference": "What specific criteria were used to exclude tasks from the BIG-Bench Hard (BBH) subset due to their reliance on specialized knowledge or being outside the scope of the work?", "explanation_reference": "The criteria for excluding tasks from the BBH subset due to their reliance on specialized knowledge or being outside the scope of the work were explicitly listed in the appendix under the heading 'Criteria: Task is outside the scope of this work.'", "evidence_reference": "Criteria: Task is outside the scope of this work: not solvable by authors within 60 minutes, requires specialized knowledge, or not even worth attempting with chain-of-thought."}
{"question": "Consider the paper that introduces the model in the figure that has the lowest diversity score for each of p1, p2, p3, and p4. What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "answer": "3", "figure": "locality/2310.05030/diversity_score.png", "anchor_arxiv_id": "2310.05030", "reference_arxiv_id": "1910.10683", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model demonstrates the lowest diversity score in p1, p2, p3, and p4?", "answer_anchor": "T5-Large", "question_reference": "What is the average span length that slightly outperformed the i.i.d. objective on most non-translation benchmarks?", "explanation_reference": "The span-corruption objective with an average span length of 3 slightly outperformed the i.i.d. objective on most non-translation benchmarks, as indicated by the statement 'we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks.'", "evidence_reference": "we find a limited difference between these objectives, though the version with an average span length of 3 slightly (but significantly) outperforms the i.i.d. objective on most non-translation benchmarks."}
{"question": "Consider the paper that introduces the model that shows 0 accuracy in the COGS-structural dataset. What specific achievement does it demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set for English constituency parsing?", "answer": "Outperforms the BerkeleyParser", "figure": "locality/2310.15040/accuracy_figure.png", "anchor_arxiv_id": "2310.15040", "reference_arxiv_id": "1706.03762", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What model shows 0 accuracy in COGS-structural dataset", "answer_anchor": "Vanilla Transformer", "question_reference": "Based on the Transformer model's performance on English constituency parsing, what specific achievement does it demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "explanation_reference": "The paper highlights that, unlike RNN sequence-to-sequence models, the Transformer model outperforms the BerkeleyParser even when trained only on the WSJ training set of 40K sentences. This indicates the Transformer's superior ability to handle tasks with strong structural constraints and significantly longer outputs than inputs, even with limited training data.", "evidence_reference": "In contrast to RNN sequence-to-sequence models [KVparse15], the Transformer outperforms the BerkeleyParser [petrov-EtAl:2006:ACL] even when training only on the WSJ training set of 40K sentences."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than BROS and a higher F1 score than LayoutXLM. What specific layout features are used in the GCN encoder for updating the representation of the entity and edge in the model proposed in the paper?", "answer": "\\mathbf{r}_{i, j} = [x_{i,j}, y_{i,j}] here $x_{ij}$ and $y_{ij}$ are horizontal and vertical distance between the two entity boxes respectively: \\begin{equation} \\begin{aligned} \\label{ra} \\scriptsize &x_{i, j} = min(| x_i^1 - x_j^2 |, | x_j^1 - x_i^2 |) \\\\ &y_{i, j} = min(| y_i^1 - y_j^2 |, | y_j^1 - y_i^2 |) \\end{aligned} \\end{equation}", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2110.09915", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having lower F1 score than BROS and higher F1 score than LayoutXLM?", "answer_anchor": "SERA", "question_reference": "What specific layout features are used in the GCN encoder for updating the representation of the entity and edge in the proposed model?", "explanation_reference": "The specific layout features used in the GCN encoder for updating the representation of the entity and edge are the horizontal and vertical distances between the two entity boxes, represented as [x_{i,j}, y_{i,j}].", "evidence_reference": "The edge embedding consists of 2 layout features, as the following equation shows: \\begin{equation} \\mathbf{r}_{i, j} = [x_{i,j}, y_{i,j}] \\f\\f\\f \\end{equation} where $x_{ij}$ and $y_{ij}$ are horizontal and vertical distance between the two entity boxes respectively: \\begin{equation} \\begin{aligned} \\label{ra} \\scriptsize &x_{i, j} = min(| x_i^1 - x_j^2 |, | x_j^1 - x_i^2 |) \\\\ &y_{i, j} = min(| y_i^1 - y_j^2 |, | y_j^1 - y_i^2 |) \\end{aligned} \\end{equation}"}
{"question": "Consider the paper that introduces the dataset that has the fewest number of languages but the most number of SM tasks. What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly in the context of Chinese sentiment analysis?", "answer": "Understanding complex linguistic nuances and cultural specificity", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset has the fewest number of languages but the most number of SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly in the context of Chinese sentiment analysis?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity in sentiment analysis, specifically mentioning the Chinese phrase '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said') as an example. This phrase can be used ironically and may not necessarily indicate agreement, illustrating the subtlety and cultural specificity of sentiment expression. The correct interpretation of this phrase requires familiarity with social media contexts, highlighting the challenge for models to understand such nuanced language use.", "evidence_reference": "For example, on Chinese social media, a comment '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically. However, this linguistic phenomenon may require familiarity with social media to interpret correctly."}
{"question": "Consider the paper that introduces the method which is represented by the lavender color. What specific computational advantage does the model proposed in the paper offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "answer": "GeDi's online classification trick can compute $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier.", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method represented by the lavender color?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's online classification trick offer over a unidirectional classifier in terms of the number of forward passes required for computing $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$?", "explanation_reference": "The computational advantage is highlighted by comparing the efficiency of GeDi's method to compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token using only two parallel forward passes (one for each control code) against the requirement of a unidirectional classifier needing $|\\\\gV|$ forward passes for a vocabulary set $\\\\gV$. This comparison underlines GeDi's significant reduction in computational demand.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the dataset in the table that has an average answer length of 83.71. What is the average improvement in accuracy that the model proposed in the paper achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "answer": "7.6%", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method demonstrates 83.71 average answer length from the table?", "answer_anchor": "Multialpaca", "question_reference": "What is the average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English?", "explanation_reference": "The average improvement in accuracy that PolyLM-13B achieved over LLaMA-13B across the XCOPA tasks for languages other than English is directly stated as 7.6% in the paper.", "evidence_reference": "For languages other than English (the multilingual column), \\textsc{Poly}LM-13B outperforms LLaMA-13B with average improvement up to 7.6%, 5.6%, 3%, and 11% on XCOPA, PAWS-X, XWinagrad, and XNLI, respectively."}
{"question": "Consider the paper that introduces the method that is listed in the table right below the PCP method. How does its utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact?", "answer": "The performance drops notably when replacing $\\mathcal{L}_{L}$ with $\\mathcal{L}_{L'}$.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2211.13873", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method in the table right below the PCP method?", "answer_anchor": "GOLF", "question_reference": "How does the GOLF model's utilization of the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ compare to its hard-label version $\\mathcal{L}_{L'}$ in terms of performance impact?", "explanation_reference": "The notable drop in performance when replacing the local hierarchy-aware contrastive loss $\\mathcal{L}_{L}$ with its hard-label version $\\mathcal{L}_{L'}$ demonstrates the effectiveness of the scoring function in $\\mathcal{L}_{L}$, which considers more subtle semantic structures of the local hierarchy.", "evidence_reference": "Secondly, we replace the Local Hierarchy-aware Contrastive loss $\\mathcal{L}_{L}$ (Equation (\\ref{equation: soft local})) with the hard-label version $\\mathcal{L}_{L'}$ (Equation (\\ref{equation: hard local})) and find that the performance drops notably."}
{"question": "Consider the paper that introduces the method that achieves an F1 score of 87.63 in the Token (I-topo) category. What specific performance gain does the model proposed in the paper achieve on the TACRED relation extraction benchmark compared to the BERT baseline reimplementation?", "answer": "3.3% F1", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets F1 score 87.63 in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific performance gain does SpanBERT achieve on the TACRED relation extraction benchmark compared to the BERT baseline reimplementation?", "explanation_reference": "The question focuses on the detailed performance comparison between SpanBERT and the reimplementation of BERT on the TACRED relation extraction benchmark. The answer directly reflects the improvement SpanBERT provides over the BERT baseline reimplementation in terms of F1 score, which is a measure of a test's accuracy.", "evidence_reference": "Table~\\ref{tab:tacred-results} shows the performance on TACRED. \\ourmodel\\ exceeds our reimplementation of BERT by 3.3\\% F1 and achieves close to the current state of the art ~\\cite{Soares2019matching} --- Our model performs better than their BERT$_\\text{EM}$ but is 0.7 point behind BERT$_\\text{EM}$ + MTB which used entity-linked text for additional pre-training."}
{"question": "Consider the paper that introduces the method which is placed below the row for R-Former but above the row for NeurJudge. What is the percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing the model proposed in the paper with the state-of-the-art MPBFN-WCA?", "answer": "3.18%", "figure": "locality/2310.09241/results_table.png", "anchor_arxiv_id": "2310.09241", "reference_arxiv_id": "2004.02557", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method shown below the row of R-Former but above the row of NeurJudge?", "answer_anchor": "LADAN", "question_reference": "What is the percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing LADAN with the state-of-the-art MPBFN-WCA?", "explanation_reference": "The percentage improvement in F1-score for law article prediction on dataset CAIL-big when comparing LADAN with MPBFN-WCA is directly stated in the 'Experimental Results' section, indicating the effectiveness of LADAN over the state-of-the-art method.", "evidence_reference": "Compared with the state-of-the-art MPBFN-WCA, LADAN improved the F1-scores of law article prediction, charge prediction, and term of penalty prediction on dataset CAIL-small by $2.02$\\%, $2.42$\\% and $4.20$\\% respectively, and about $3.18$\\%, $1.44$\\% and $5.79$\\% on dataset CAIL-big."}
{"question": "Consider the paper that introduces the dataset in the table that has a validation set size of 1250. Which model demonstrated the highest percentage of factual hallucinations among its extrinsically hallucinated summaries?", "answer": "\\bencdec", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the dataset having 1250 val set from the table?", "answer_anchor": "XSumFaith", "question_reference": "Based on the human evaluation results, which model demonstrated the highest percentage of factual hallucinations among its extrinsically hallucinated summaries?", "explanation_reference": "The \\bencdec model showed the highest percentage of factual hallucinations among its extrinsically hallucinated summaries, indicating that while it did produce hallucinations, a significant portion of these were still factually correct.", "evidence_reference": "The numbers in ``Hallucinated'' columns show the percentage of summaries out of 500 where at least one word was annotated by all three annotators as an intrinsic (I) or extrinsic (E) hallucination. When a summary is not marked with any hallucination, it is ``faithful'' (1- I$\\cup$E). The ``factual'' columns within the ``Hallucinated'' column show for each type (I, E and I$\\cup$E), the percentage of summaries out of 500 annotated by all three annotators as factual. The final ``Factual'' column shows the total percentage of factual summaries (Faithful + I$\\cup$E$_{\\mbox{factual}}$). The highest numbers for faithful and factual, and the lowest numbers for hallucinations are boldfaced."}
{"question": "Consider the paper that introduces the method which has a GPT backbone and 7B parameters. What specific role does the guidance mechanism play in improving the quality of generated subquestions by a smaller model in the context of the Socratic CoT approach?", "answer": "The guidance mechanism conditions the generation of subquestions on the equations describing the intermediate solutions, improving the quality of the generated questions across all metrics considered.", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method used with a GPT backbone and 7B parameters?", "answer_anchor": "DecomDistill", "question_reference": "In the context of the Socratic CoT approach, what specific role does the guidance mechanism play in improving the quality of generated subquestions by a smaller model?", "explanation_reference": "The guidance mechanism is introduced to condition the generation of subquestions on the equations that describe the intermediate solutions of a problem. This strategy is shown to improve the quality of the generated questions in terms of BLEU score, BERT F1 score, and the accuracy of matching the number of GPT-3 annotated questions for a given problem, indicating its effectiveness in enhancing the sub-questioning capabilities of a smaller model.", "evidence_reference": "We introduced a \\textit{guidance mechanism} that conditions the generation of subquestions for a problem $P$ on the equations describing the intermediate solutions of $P$. This strategy improved the quality of the generated questions for all three metrics considered (Table \\ref{qg-scores}, second row)."}
{"question": "Consider the paper that introduces the method that demonstrates the second highest score in the TweetEval Irony dataset for both zero-shot and few-shot prompting. What specific adaptation in the text embeddings allows the model, proposed by the paper, to build correspondence among query text, label text, and objects in grounding tasks?", "answer": "Embedding sharing", "figure": "locality/2310.15746/comparison_2_table.png", "anchor_arxiv_id": "2310.15746", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method demonstrates the second highest score in TweetEval Irony dataset in both zero-shot and few-shot prompting?", "answer_anchor": "SALAM", "question_reference": "What specific adaptation in the text embeddings allows the model to build correspondence among query text, label text, and objects in grounding tasks?", "explanation_reference": "The specific adaptation that allows the model to build correspondence among query text, label text, and objects in grounding tasks is the use of embedding sharing. This is achieved by reusing the text embeddings of visual sentinel tokens as region id embeddings, which enables the model to establish a connection between the text and visual elements, particularly useful in grounding tasks.", "evidence_reference": "In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens \\{\\texttt{<vis\\_1>}, $\\dots$, \\texttt{<vis\\_n>}\\}, which corresponds to image regions. As illustrated in Fig.~\\ref{fig:architecture}, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec.~\\ref{sec:visual_embeddings}. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec.~\\ref{sec:pretraining}, referring expression comprehension in Sec.~\\ref{sec:refcoco})."}
{"question": "Consider the paper that introduces the model in the table that corresponds to a 12.79% TP. What specific architectural feature allows the model to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "answer": "Deformable DETR-based detector", "figure": "locality/2312.11523/comparison_table.png", "anchor_arxiv_id": "2312.11523", "reference_arxiv_id": "2207.09666", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model shows 12.79% TP?", "answer_anchor": "GRIT", "question_reference": "What specific architectural feature of GRIT allows it to avoid the computational steps of class-aware NMS and RoI Align during feature extraction?", "explanation_reference": "GRIT employs a Deformable DETR-based detector for extracting region features, which inherently does not require class-aware NMS (Non-Maximum Suppression) and RoI (Region of Interest) Align operations that are typically necessary in CNN-based detectors like Faster R-CNN. This architectural choice significantly reduces the computational time for feature extraction.", "evidence_reference": "On the other hand, we employ a Deformable DETR-based detector to extract region features without using all such operations. Table \\ref{tab:extraction} shows the comparison on feature extraction."}
{"question": "Consider the paper that introduces the model that achieves the second highest score in the Stance column. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model shows consistently low accuracy than VIBE model?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength and direction of a linear relationship between two variables. In this case, it quantifies the relationship between the word overlap (how vocabularies change over time) and the model performance for the task of political affiliation classification on Twitter data. A value of 0.9817159316285563 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method in the figure represented by the 'x' (cross) marker. What specific pre-training task is designed for both retrieval and reasoning models in the model proposed in the paper to ensure their unification in parameter learning?", "answer": "question-relation matching", "figure": "locality/2401.00158/comparison_figure.png", "anchor_arxiv_id": "2401.00158", "reference_arxiv_id": "2212.00959", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method in the figure is demonstrated by the 'x' (cross) marker?", "answer_anchor": "UniKGQA", "question_reference": "What specific pre-training task is designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning?", "explanation_reference": "The specific pre-training task designed for both retrieval and reasoning models in UniKGQA to ensure their unification in parameter learning is the question-relation matching task. This task is directly mentioned as a shared pre-training task for both models, indicating its role in unifying the parameter learning process by focusing on matching the semantics of questions with the relations in the knowledge graph.", "evidence_reference": "For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies."}
{"question": "Consider the paper that introduces the method, with an average max toxicity of more than 0.3, is represented by a circle. What specific strategy does the model proposed in the paper utilize for ensuring the appearance of guide words in generated text without necessitating a pre-defined ordering of constraints?", "answer": "Guide Closest", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2109.09707", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is method with average max toxicity more than 0.3 but with circle label?", "answer_anchor": "PPLM", "question_reference": "What specific strategy does the paper propose for ensuring the appearance of guide words in generated text without requiring pre-defined ordering of constraints?", "explanation_reference": "The paper proposes two strategies for controlling generation towards a list of guide words, one of which is 'Guide Closest'. This strategy does not require the guide words to be ordered and works by shifting the score function by the highest cosine similarity across all words in the set of guide words that have not appeared before the current step. This approach allows for the flexible inclusion of guide words in the generated text, aligning with the paper's goal of imposing hard constraints on language generation without the need for pre-defined ordering.", "evidence_reference": "Given a list of guide words $W$, we propose two approaches for both the case where we need words $w_n$ to appear in a fixed order as well as when any order suffices. [...] At any given decoding step, we shift the score function by the highest cosine similarity across all words $w\\in W_{t}$. [...] Explicitly, we score $y_{t}$ as \\begin{align} \\score'(y_{t}, W_{t}\\mid \\yy_{<t}) = & \\,\\score(y_{t} \\mid \\yy_{<t}) \\ + \\\\ \\lambda \\cdot \\max &\\Big(0, \\underset{{w\\in W_t}}{\\max}\\, \\semdist(\\gamma(y_t), \\gamma(w)\\Big) \\nonumber \\end{align}"}
{"question": "Consider the paper that introduces the method that achieves an MRR score equal to 0.717 in the FB15kET dataset. What are the three distinct mechanisms that compose its approach, known as Transformer-based Entity Typing (TET), and how does each contribute to the task of knowledge graph entity typing?", "answer": "The three distinct mechanisms that compose the Transformer-based Entity Typing (TET) approach are:\n\n1. **Local Transformer**: This mechanism focuses on encoding the information provided by each neighbor of an entity independently. It helps in inferring missing entity types by considering the local context around an entity, allowing for a detailed understanding of immediate relationships.\n\n2. **Global Transformer**: This mechanism aggregates the information from all neighbors of an entity into a single long sequence. It enables the model to reason about more complex entity types by considering the global context and the collective influence of all neighboring entities.\n\n3. **Context Transformer**: This mechanism integrates the content of neighbors in a differentiated way through information exchange between neighbor pairs, while preserving the graph structure. It enhances the model's ability to understand the context of each entity by considering how different neighbors relate to each other and to the entity in question.\n\nEach mechanism contributes to the task of knowledge graph entity typing by encoding different levels of context (local, global, and relational) around an entity, thereby improving the model's ability to accurately infer entity types.", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets MRR score equal to 0.717 in FB15kET datast?", "answer_anchor": "TET", "question_reference": "What are the three distinct mechanisms that compose the Transformer-based Entity Typing (TET) approach, and how does each contribute to the task of knowledge graph entity typing?", "explanation_reference": "The answer directly addresses the question by listing the three mechanisms of the TET approach as described in the paper and summarizing their contributions to the entity typing task. Each mechanism's role is clearly defined, reflecting a deep understanding of the paper's content.", "evidence_reference": "TET is composed of three different mechanisms: a local transformer allowing to infer missing entity types by independently encoding the information provided by each of its neighbours; a global transformer aggregating the information of all neighbours of an entity into a single long sequence to reason about more complex entity types; and a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
{"question": "Consider the paper that introduces the method that exhibits the lowest BLEU score in the De->En task over Average Lagging from 5 to 11. What specific methodological adjustment is made to the model's variant, proposed by the paper, to address the potential issue of outlier attention heads affecting latency or attention span?", "answer": "attention variance loss to MMA-H", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method shows the lowest BLEU score in De-En task over Average Lagging from 5 to 11?", "answer_anchor": "MMA", "question_reference": "What specific methodological adjustment is made to the MMA-H variant to address the potential issue of outlier attention heads affecting latency or attention span?", "explanation_reference": "The attention variance loss (L_var) is introduced specifically for the MMA-H variant to control the variance in attention spans across different heads, aiming to prevent outlier heads from significantly affecting the model's latency or the uniformity of attention spans. This methodological adjustment is designed to mitigate the potential issue of having some heads that either read too fast or too slow, which could lead to inefficiencies in simultaneous translation.", "evidence_reference": "In \\autoref{sec:latency-control}, we introduced the attention variance loss to MMA-H in order to prevent outlier attention heads from increasing the latency or increasing the attention span."}
{"question": "Consider the paper that introduces the model that scores an 84.2 in the 'T10' column. What is the observed accuracy drop percentage for the model proposed in the paper on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "answer": "9.7", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets score of 84.2 in 'T10' column?", "answer_anchor": "CEAR", "question_reference": "What is the observed accuracy drop percentage for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00)?", "explanation_reference": "The accuracy drop for the CRL model on the FewRel dataset for relations with maximum similarity in the range [0.85, 1.00) is directly reported in the empirical study results table.", "evidence_reference": "CRL & [0.85, 1.00) & 71.1 & 9.7 & 64.8 & 11.4"}
{"question": "Consider the paper that introduces the method that has a lower F1 score than TPP and a higher F1 score than BROS. How does the inclusion of Coordinate Convolution (CoordConv) specifically impact the model's F1 scores for both Entity Labeling and Entity Linking tasks, according to the ablation study results?", "answer": "The inclusion of Coordinate Convolution (CoordConv) specifically impacts the F1 scores for both Entity Labeling and Entity Linking tasks in the MSAU-PAF model by increasing the F1 score by 0.02 in both tasks, according to the ablation study results.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having lower F1 score than TPP and higher F1 score than BROS?", "answer_anchor": "MSAU-PAF", "question_reference": "How does the inclusion of Coordinate Convolution (CoordConv) specifically impact the F1 scores for both Entity Labeling and Entity Linking tasks in the MSAU-PAF model, according to the ablation study results?", "explanation_reference": "The ablation study results indicate that adding translational information through Coordinate Convolution (CoordConv) to the MSAU-PAF model leads to a specific improvement of 0.02 in F1 score for both Entity Labeling and Entity Linking tasks. This demonstrates the effectiveness of incorporating spatial information directly into the convolution process for enhancing model performance on these tasks.", "evidence_reference": "As shown in Table \\ref{table:ablation}, when making the convolution access to its own input coordinates through concatenating extra coordinate channels, MSAU-PAF with Coordinate Convolution experienced a total gain of 0.02 in F1 score in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 25.9 in the Seen, Val, SR dataset. How does the performance of the model proposed in the paper's factorized approach compare to Shridhar et al.'s single-branch model in terms of task success rates across the 7 high-level categories in the ALFRED benchmark?", "answer": "MOCA outperforms Shridhar et al.'s model in all 7 high-level categories of the ALFRED benchmark, showing significant improvements in both seen and unseen environments.", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2012.03208", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method shows score of 25.9 in Seen, Val, SR dataset?", "answer_anchor": "MOCA", "question_reference": "How does the performance of MOCA's factorized approach compare to Shridhar et al.'s single-branch model in terms of task success rates across the 7 high-level categories in the ALFRED benchmark?", "explanation_reference": "The question focuses on comparing the performance of MOCA's factorized approach to the single-branch model by Shridhar et al. across different task types in the ALFRED benchmark. The answer is supported by the data presented in the paper, which shows MOCA achieving higher task success rates in both seen and unseen environments across all categories, indicating the effectiveness of MOCA's factorized approach over the single-branch model.", "evidence_reference": "Tasks in ALFRED~[33] are divided into 7 high-level categories. Table~\\ref{tab:abla_task} shows the performance of our factorized agent on each task type. On short-horizon tasks such as \\textbf{Pick \\& Place} and \\textbf{Examine}, Shridhar \\etal~[33] which is a single-branch model succeeds in some trajectories in seen environments, but has near zero unseen success rates. However, our agent outperforms them in both seen and unseen scenes by large margins."}
{"question": "Consider the paper that introduces the model that demonstrated the highest improvement in performance on the DuConv dataset when normalized topics were introduced. What specific metric was most significantly affected?", "answer": "norm generation, F1/BLEU1/BLEU2", "figure": "locality/2310.07397/result_table.png", "anchor_arxiv_id": "2310.07397", "reference_arxiv_id": "1906.05572", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset has the most number of dialogues?", "answer_anchor": "DuConv", "question_reference": "Based on the experimental results, which model demonstrated the highest improvement in performance when normalized topics were introduced, and what specific metric was most significantly affected?", "explanation_reference": "The norm generation model showed the most significant improvement in performance when normalized topics were introduced, particularly in the F1/BLEU1/BLEU2 metrics, indicating a substantial enhancement in the quality and relevance of the generated responses.", "evidence_reference": "norm generation & 32.50\\% & 58.50\\% & 24.3 & \\textbf{41.84} / \\textbf{0.347} / \\textbf{0.198} & 0.057 / 0.155 & \\textbf{9.78} / 38.02 / \\textbf{15.27}"}
{"question": "Consider the paper that introduces the method that has a lower J_k score than Random Entity Quantization but a higher J_k score than NodePiece in the FB15k-237 dataset for all values of k between 400 and 1000. According to the ablation studies, which dataset experienced a significant performance degradation when multi-hop neighbor information was removed?", "answer": "WN18RR", "figure": "locality/2310.15797/performance_2_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What method shows the lower J_k score than Random Entity Quantization but higher J_k score than NodePiece for k in the range of [400, 1000] in FB15k-237 dataset?", "answer_anchor": "EARL", "question_reference": "Based on the ablation studies, which dataset showed a significant performance degradation when multi-hop neighbor information was removed?", "explanation_reference": "The ablation study results indicate that removing multi-hop neighbor information ('w/o MulHop') dramatically affected the performance on the WN18RR dataset, as evidenced by the significant drop in performance metrics compared to other ablation settings.", "evidence_reference": "For \\textit{WN18RR}, ``w/o Reserved Entity and ``w/o $k$NResEnt impairs the performance. Replacing ConRel and $k$NResEnt with random representations (``w/o ConRel + $k$NResEnt) also affect the results. Moreover, the performance is affected dramatically by removing MulHop information."}
{"question": "Consider the paper that introduces the Seq2Exp model that exhibits the highest test accuracy. Which model employs a unique mechanism to prevent interactive distraction between operators and operands during the generation of numerical reasoning programs?", "answer": "Separating the generation procedures for operators and operands", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which Seq2Exp model shows the highest test accuracy?", "answer_anchor": "Elastic", "question_reference": "What specific mechanism does ELASTIC employ to prevent the interactive distraction between operators and operands during the generation of numerical reasoning programs?", "explanation_reference": "ELASTIC's design to separate the generation of operators and operands helps in preventing potential interactive distractions between them, which in turn minimizes cascading errors during the generation of complex numerical reasoning programs.", "evidence_reference": "Because ELASTIC separates the generation procedures for operators and operands, which prevents the potential interactive distraction between operators and operands. This makes ELASTIC less liable to being influenced by the cascading error."}
{"question": "Consider the paper that introduces the method that has the highest score in the WQ-R task. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions by the model proposed in the paper?", "answer": "0.949", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method has the highest score in WQ R task?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the method that scores a 70.1 in the 'Revised Persona' column. What is the best setting of the hyperparameter \\(\\gamma\\) for both its sentence and word versions on the original PersonaChat dataset, and why?", "answer": "0.3", "figure": "locality/2310.06390/result_table.png", "anchor_arxiv_id": "2310.06390", "reference_arxiv_id": "2101.08426", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets score of 70.1 in 'Revised Persona' column?", "answer_anchor": "CSN-word", "question_reference": "What is the best setting of the hyperparameter \\(\\gamma\\) for both CSN-sent and CSN-word on the original PersonaChat dataset, and why?", "explanation_reference": "The best setting of \\(\\gamma\\) is around 0.3 for both CSN-sent and CSN-word because it retains an appropriate amount of relevant document content for response matching, optimizing the balance between filtering out irrelevant content and keeping sufficient information for effective response selection.", "evidence_reference": "The best setting of \\(\\gamma\\) is around 0.3 for both CSN-sent and CSN-word, which retains an appropriate amount of relevant document content for response matching."}
{"question": "Consider the paper that introduces the supervised method that achieves the highest score in 100-shot prompting. What specific performance improvement does the model proposed in the paper achieve on the RACE dataset compared to XLNet, and what does this imply about its ability to handle tasks requiring reasoning over longer contexts?", "answer": "RoBERTa achieves an accuracy of 83.2% on the RACE dataset compared to XLNet's 81.7%, implying it has a better ability to handle tasks requiring reasoning over longer contexts.", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which supervised method demonstrates highest scores in 100-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does RoBERTa achieve on the RACE dataset compared to XLNet, and what does this imply about its ability to handle tasks requiring reasoning over longer contexts?", "explanation_reference": "The question focuses on the detailed comparison of performance improvements of RoBERTa over XLNet on the RACE dataset, which is known for its requirement of reasoning over longer texts. The answer directly reflects RoBERTa's enhanced ability to manage such tasks, as evidenced by its higher accuracy scores.", "evidence_reference": "In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. \\ourmodel{} & \\bf{83.2} &  \\bf{86.5} & \\bf{81.3}\\\\ \\xlnetlarge{} & 81.7 & 85.4 & 80.2 \\\\"}
{"question": "Consider the paper that introduces the model shown in the figure that corresponds to the green line. What is the Pearson's correlation coefficient between word overlap and the model performance of DPT for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/accuracy_figure.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which model is shown in green line?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the task of political affiliation classification on Twitter data. A value of 0.9817 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "Twitter, \\poliaff{} ($F_1$), 0.9817159316285563"}
{"question": "Consider the paper that introduces the dataset which has 1 language but 13 SM tasks. What is the exact match score between the labels and predictions for Flan-T5 on the SentiEval benchmark?", "answer": "29.07", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset has 1 language but 13 SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What is the exact match score between the labels and predictions for \\texttt{Flan-T5} on the \\textsc{SentiEval} benchmark?", "explanation_reference": "The exact match score for \\texttt{Flan-T5} on the \\textsc{SentiEval} benchmark is directly reported in the results table under the section discussing the \\textsc{SentiEval} benchmark.", "evidence_reference": "\\textsc{SentiEval} & 29.07 & 38.82 & 36.64 & 47.55"}
{"question": "Consider the paper that introduces the method shown in the first row of the table. What specific aspect of the policy gradient strategy allows the model proposed in the paper, PromptPG, to outperform heuristic-based example selection strategies in the context of few-shot GPT-3?", "answer": "The specific aspect of the policy gradient strategy that allows PromptPG to outperform heuristic-based example selection strategies in the context of few-shot GPT-3 is its ability to learn to select in-context examples from a small amount of training data dynamically, thereby constructing the optimal prompt for the test example. This approach significantly reduces the prediction variance compared to random selection and improves the selection of in-context examples over existing strategies.", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2209.14610", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method is shown in the first row of the table?", "answer_anchor": "PromptPG", "question_reference": "What specific aspect of the policy gradient strategy allows PromptPG to outperform heuristic-based example selection strategies in the context of few-shot GPT-3?", "explanation_reference": "The policy gradient strategy, as applied in PromptPG, enables the model to learn and select the most effective in-context examples for few-shot GPT-3 dynamically. This approach is contrasted with heuristic-based strategies that rely on predefined rules, which may not always align with the nuances of each problem. By learning from the training data, PromptPG can adapt its selection strategy to optimize performance, leading to its superior results.", "evidence_reference": "Compared to random selection, selecting the same question or answer type of examples helps the model to take the task-relevant examples as the prompt, thus improving the accuracy and reducing the variance. \\model shows its effectiveness in selecting optimal in-context examples over other strategies and largely reduces the instability."}
{"question": "Consider the paper that introduces the method that exhibits a score of 34.9 in the Acc-7 metric on MOSI. What specific improvement in accuracy percentage does the model proposed in the paper achieve over the state-of-the-art for 5-class sentiment classification?", "answer": "6.7", "figure": "locality/2310.05804/comparison_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method that shows 34.7 score in Acc-7 metric on MOSI?", "answer_anchor": "TFN", "question_reference": "Based on the Tensor Fusion Network's performance in multimodal sentiment analysis, what specific improvement in accuracy percentage does the TFN model achieve over the state-of-the-art for 5-class sentiment classification?", "explanation_reference": "The question focuses on the specific detail of the improvement in accuracy percentage that the Tensor Fusion Network (TFN) model achieves over the state-of-the-art for 5-class sentiment classification. This detail is directly answered by the comparison provided in the experiments section, highlighting the TFN model's performance against existing approaches.", "evidence_reference": "Table~\\ref{table:mmres} shows the comparison with state-of-the-art approaches for multimodal sentiment analysis. \\mns \\ outperforms both neural and non-neural approaches as shown by $\\Delta^{SOTA}$. Specifically, for 5-class classification, the improvement is $\\uparrow$ 6.7%."}
{"question": "Consider the paper that introduces the optimization method that has a BLEU score of 27.3. Which optimization method, demonstrating a BLEU score of 27.3, shows specific achievement over RNN sequence-to-sequence models in English constituency parsing when trained solely on the WSJ training set?", "answer": "Outperforms the BerkeleyParser", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What optimization method shows BLEU score of 27.3?", "answer_anchor": "Transformer base", "question_reference": "Based on the Transformer model's performance on English constituency parsing, what specific achievement does it demonstrate over RNN sequence-to-sequence models when trained solely on the WSJ training set?", "explanation_reference": "The paper highlights that, unlike RNN sequence-to-sequence models, the Transformer model outperforms the BerkeleyParser even when trained only on the WSJ training set of 40K sentences. This indicates the Transformer's superior ability to handle tasks with strong structural constraints and significantly longer outputs than inputs, even with limited training data.", "evidence_reference": "In contrast to RNN sequence-to-sequence models [KVparse15], the Transformer outperforms the BerkeleyParser [petrov-EtAl:2006:ACL] even when training only on the WSJ training set of 40K sentences."}
{"question": "Consider the paper that introduces the dataset located in the top left of the figure. What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly in the context of Chinese sentiment analysis, particularly when analyzing the SentiEval dataset?", "answer": "Understanding complex linguistic nuances and cultural specificity", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the dataset located on the top left of the figure?", "answer_anchor": "SentiEval", "question_reference": "What specific linguistic phenomenon mentioned in the paper requires familiarity with social media to interpret correctly in the context of Chinese sentiment analysis?", "explanation_reference": "The paper discusses the challenge of understanding complex linguistic nuances and cultural specificity in sentiment analysis, specifically mentioning the Chinese phrase '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said') as an example. This phrase can be used ironically and may not necessarily indicate agreement, illustrating the subtlety and cultural specificity of sentiment expression. The correct interpretation of this phrase requires familiarity with social media contexts, highlighting the challenge for models to understand such nuanced language use.", "evidence_reference": "For example, on Chinese social media, a comment '\u60a8\u8bf4\u7684\u90fd\u5bf9' (English translation: 'You are right about everything you said' with 'You' in a respectful tone) may not necessarily indicate agreement but can be used ironically. However, this linguistic phenomenon may require familiarity with social media to interpret correctly."}
{"question": "Consider the paper that introduces the model that achieves an F1 score of 73.1 in the en_city category. How does its structured summarization approach ensure the generation of valid sentence boundary positions within the task semantics?", "answer": "The structured summarization approach ensures the generation of valid sentence boundary positions within the task semantics by employing a generative segmentation method that maps target segmentation labels into target sequences. This involves turning the integer-valued positions corresponding to segment boundaries into strings and combining them in a single target sequence with delimiters. The encoder-decoder transformer model is then trained to generate this sequence, effectively learning to produce valid sentence boundary positions. Additionally, a post-processing step is applied to the decoder output to convert the generated strings back into a list of integers representing sentence positions, while ensuring they fall within the correct range (between 0 and $|S|-1$). This method, combined with the use of sentence position encoding at the encoder input, allows the model to accurately predict sentence boundary positions that are coherent with the task semantics.", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets 73.1 F1 score in en_city category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "How does the structured summarization approach ensure the generation of valid sentence boundary positions within the task semantics?", "explanation_reference": "The paper addresses the potential issue of generating invalid sentence boundary positions in the output sequence of the structured summarization models. It does so by calculating the fraction of erroneous segment boundary positions in the output sequences when tested on different datasets. The results demonstrate that transformer decoders are capable of accurately generating tokens that represent integer values within the semantic bounds of the task, thereby ensuring the generation of valid sentence boundary positions.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics."}
{"question": "Consider the paper that introduces the model that performs the best on the BBBP dataset. What is the exact improvement in ROC-AUC score for the ClinTox dataset achieved by the model proposed in the paper over GEM?", "answer": "5.2", "figure": "locality/2310.07276/performance_table.png", "anchor_arxiv_id": "2310.07276", "reference_arxiv_id": "2305.10688", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model perform the best in the BBBP dataset?", "answer_anchor": "MolXPT", "question_reference": "What is the exact improvement in ROC-AUC score for the ClinTox dataset achieved by MolXPT over GEM?", "explanation_reference": "The improvement can be calculated by subtracting the ROC-AUC score of GEM on the ClinTox dataset from that of MolXPT. GEM achieved a score of 90.1, while MolXPT achieved 95.3. Therefore, the improvement is 95.3 - 90.1 = 5.2.", "evidence_reference": "GEM & $72.4\\pm0.4$ & \\textbf{78.1 $\\pm$ 0.1} & $90.1\\pm1.3$ & \\textbf{80.6 $\\pm$ 0.9} & $85.6\\pm1.1$ & $67.2\\pm0.4$ & $79.0$ \\\\ \\hline \\ourM{} & \\textbf{80.0 $\\pm$ 0.5} &  $77.1\\pm0.2$  & \\textbf{95.3 $\\pm$ 0.2} & $78.1\\pm0.4$ & \\textbf{88.4 $\\pm$ 1.0} & \\textbf{71.7 $\\pm$ 0.2} & \\textbf{81.9} \\\\"}
{"question": "Consider the paper that introduces the method that is missing a result for the WQ-M task in the table. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions by the model proposed in the paper?", "answer": "0.949", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method does not show a result for WQ M task?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the model that scores higher than ACA but lower than RationaleCL in the 'T5' column. What specific hyperparameter values were used for the FewRel dataset in the experiments, and how do these values compare to those used for the TACRED dataset?", "answer": "For FewRel, the values were \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, the values were \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model gets score higher than ACA but lower than RationalCL in 'T5' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were used for the FewRel dataset in the experiments, and how do these values compare to those used for the TACRED dataset?", "explanation_reference": "The specific hyperparameter values for the FewRel and TACRED datasets are directly provided in the Implementation Details section of the paper, allowing for a direct comparison between the two sets of values.", "evidence_reference": "For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1. For TACRED, \u03b1=0.6, \u03b2=0.2, \u03c41=0.1, \u03bc=0.8, \u03c9=0.15, \u03c42=0.5, \u03b3=2.0, \u03bb1=0.5, \u03bb2=0.7."}
{"question": "Consider the paper that introduces the dataset that has 1 SM task and 14 languages. What was the highest zero-shot cross-lingual transfer performance F1 score achieved for the target language Tigrinya, and from which source language was it transferred?", "answer": "68.6, Hausa", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2302.08956", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What dataset has 1 SM task and 14 languages?", "answer_anchor": "AfriSenti", "question_reference": "What was the highest zero-shot cross-lingual transfer performance F1 score achieved for the target language Tigrinya, and which source language was it transferred from?", "explanation_reference": "The highest zero-shot cross-lingual transfer performance F1 score for Tigrinya was achieved by transferring from Hausa, indicating that Hausa was the most effective source language for zero-shot learning on Tigrinya in the context of the experiments conducted.", "evidence_reference": "\\texttt{hau}\\t& \\textbf{47.1} & \\textbf{68.6} & \\textbf{57.9}"}
{"question": "Consider the paper that introduces the model that achieves the lowest execution accuracy in direct prompting. What specific performance improvement does the LLaMA-7B model demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "answer": "37.5%", "figure": "locality/2310.11634/execution_accuracy_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shows the lowest execuation accuracy in direct prompting?", "answer_anchor": "LLaMA-7B", "question_reference": "What specific performance improvement does the \\modelname 70B model demonstrate over the Falcon 40B model in terms of the Commonsense Reasoning benchmark?", "explanation_reference": "The question focuses on the performance improvement of the \\modelname 70B model over the Falcon 40B model specifically in the Commonsense Reasoning benchmark. The answer, 37.5%, directly reflects the performance metric for the \\modelname 70B model in this category, indicating a significant improvement over the Falcon 40B model's performance.", "evidence_reference": "\\multirow{4}{*}{\\cinnamon} & 7B & 16.8 & 63.9 & 48.9 & 61.3 & 14.6 & 45.3 & 32.6 & 29.3 \\\\ & 13B & 24.5 & 66.9 & 55.4 & 65.8 & 28.7 & 54.8 & 39.4 & 39.1 \\\\ & 34B & 27.8 & 69.9 & 58.7 & 68.0 & 24.2 & 62.6 & 44.1 & 43.4 \\\\ & 70B & \\textbf{37.5} & \\textbf{71.9} & \\textbf{63.6} & \\textbf{69.4} & \\textbf{35.2} & \\textbf{68.9} & \\textbf{51.2} & \\textbf{54.2} \\\\"}
{"question": "Consider the paper that introduces the method that has the lowest MAE in the CH-SIMS task. What specific regularization techniques were applied to the $\\mathcal{U}_v$, $\\mathcal{U}_a$, and $\\mathcal{U}_s$ subnetworks in the model proposed in the paper, and what were their parameter values?", "answer": "Dropout with p=0.15 and L2 norm with coefficient 0.01.", "figure": "locality/2310.05804/result_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method has the highest MAE in CH-SIMS task?", "answer_anchor": "Tensor Fusion", "question_reference": "What specific regularization techniques were applied to the $\\mathcal{U}_v$, $\\mathcal{U}_a$, and $\\mathcal{U}_s$ subnetworks in the Tensor Fusion Network model, and what were their parameter values?", "explanation_reference": "The regularization techniques applied to the subnetworks are explicitly mentioned in the methodology section, specifying both the dropout rate and the L2 norm coefficient used for regularization.", "evidence_reference": "The TFN model is trained using the Adam optimizer with the learning rate $5\\mathrm{e}\u22124$. $\\mathcal{U}_v$ and $\\mathcal{U}_a$, $\\mathcal{U}_s$ subnetworks are regularized using dropout on all hidden layers with $p=0.15$ and L2 norm coefficient $0.01$."}
{"question": "Consider the paper that introduces the model that corresponds to the brown bars in the figure. What specific method does it employ to ensure the attention mechanism focuses on maintaining dialogue control over multiple turns?", "answer": "Ghost Attention (GAtt) employs a method inspired by Context Distillation to help the attention mechanism focus on maintaining dialogue control over multiple turns.", "figure": "locality/2310.11634/prompt_setting_figure.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2307.09288", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model is demonstrated in the brown color?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific method does GAtt employ to ensure the attention mechanism focuses on maintaining dialogue control over multiple turns?", "explanation_reference": "GAtt, or Ghost Attention, is designed to improve dialogue control over multiple turns by manipulating the fine-tuning data in a way that enhances the model's focus on relevant instructions throughout a conversation. This method is a strategic intervention in the training process to address the challenge of models forgetting initial instructions after several dialogue turns.", "evidence_reference": "To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation \\citep{bai2022constitutional} that hacks the fine-tuning data to help the attention focus in a multi-stage process."}
{"question": "Consider the paper that introduces the method that achieves a higher EA score than Fixed set but a lower EA score than Diverse KATE in the FinQA task. What is the model's, proposed by the paper, EM score using the reverse order of in-context examples on the NQ dataset?", "answer": "42.8", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method gets higher EA score than Fixed set but lower EA score than Diverse KATE in FinQA task?", "answer_anchor": "KATE", "question_reference": "What is the EM score for KATE using the reverse order of in-context examples on the NQ dataset?", "explanation_reference": "The EM score for KATE using the reverse order of in-context examples on the NQ dataset directly reflects the model's performance under this specific condition, which is detailed in the ablation study on the effect of in-context example orders.", "evidence_reference": "For the default order, the example $A$ is to the left of example $B$ if $A$ is closer to the test prompt $x$ than $B$ in the embedding space. For the reverse order, the example A is to the right of example B... The reverse order performs the best... 42.8"}
{"question": "Consider the paper that introduces the dataset in the last row of the 'Inconsistency Detection' category. What specific aspect of pretrained models contributes to their superior performance in generating factual summaries compared to non-pretrained models?", "answer": "Exposure to vast amounts of text through pretraining", "figure": "locality/2310.11648/comparison_table.png", "anchor_arxiv_id": "2310.11648", "reference_arxiv_id": "2005.00661", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset in the last row of `Inconsistency Detection` category?", "answer_anchor": "XSumFaith", "question_reference": "Based on the findings, what specific aspect of pretrained models contributes to their superior performance in generating factual summaries compared to non-pretrained models?", "explanation_reference": "The superior performance of pretrained models in generating factual summaries is attributed to their exposure to vast amounts of text during pretraining. This exposure allows them to better integrate background knowledge with generation, leading to more factual content even in the presence of hallucinations.", "evidence_reference": "The superior performance of \\bencdec is most likely due to its exposure to vast amount of text through pretraining, allowing it to integrate background knowledge with generation. Even so, over 90\\% of \\bencdec hallucinations are  erroneous."}
{"question": "Consider the paper that introduces the model that achieves an F1 score of 73.1 in the en_city category. What is the erroneous output fraction for the model proposed in the paper when tested on the QMSum dataset?", "answer": "0", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets 73.1 F1 score in en_city category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What is the erroneous output fraction for structured summarization models when tested on the QMSum dataset?", "explanation_reference": "The erroneous output fraction indicates how frequently the model produces an invalid sentence boundary position. For the QMSum dataset, the structured summarization models did not produce any erroneous segment boundary positions, indicating a high level of accuracy in generating valid sentence indices.", "evidence_reference": "In Table \\ref{table:sentpos_nonnumeric}, we show this erroneous fraction for structured summarization models when tested on Wiki-727K, WikiSection, and QMSum. From the table, it is clear that transformer decoders are easily able to generate tokens that represent integers within the bounds of the task semantics. \\begin{table}[h] \\small \\centering \\s\\t \\renewcommand{\\arraystretch}{1.4} \\begin{tabular}{cccc} \\toprule Wiki-727K & en\\_city & en\\_disease & QMSum \\\\ \\hline 0.0001 & 0.0025 & 0 & 0 \\\\ \\bottomrule \\end{tabular} \\caption{Fraction of examples with at least one erroneous segment boundary position. This is for the structured summarization models, when tested on the respective test set.} \\label{table:sentpos_nonnumeric} \\end{table}"}
{"question": "Consider the paper that introduces the model that has an overall average score of 45.68 in Previous Image-free Systems. How does its performance with hallucinated visual tokens (\\texttt{V}) compare to using ground-truth visual representations (\\texttt{VM}) on the Multi30K dataset for the Transformer-Tiny model in terms of BLEU score?", "answer": "Very similar", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model demonstrates the overall average score of 45.68 in previous image-free systems?", "answer_anchor": "VALHALLA", "question_reference": "How does the performance of the model proposed in the paper with hallucinated visual tokens (\\texttt{V}) compare to using ground-truth visual representations (\\texttt{VM}) on the Multi30K dataset for the Transformer-Tiny model in terms of BLEU score?", "explanation_reference": "The performance of \\ours with hallucinated visual tokens (\\texttt{V}) is very similar to when using ground-truth visual representations (\\texttt{VM}), demonstrating the model's strong ability to generate visual representations that are semantically consistent with the ground-truth.", "evidence_reference": "Using Transformer-Tiny as the backbone, \\ours obtains an average $35.4$ BLEU in EN$\\rightarrow$DE and $54.4$ BLEU in EN$\\rightarrow$FR, which is about $2.1$ and $1.4$ BLEU improvements over the text-only baseline. Moreover, \\ours has very similar performance with either hallucinated (\\texttt{V}) or ground-truth representation (\\texttt{VM}), showing strong ability to generate visual representations that are semantically consistent with the ground-truth."}
{"question": "Consider the paper that introduces the method that is in the last row of the Full Training category. What specific aspect of the paraphrasing-based approach does the paper identify as potentially more effective for instances with simple expressions compared to this method?", "answer": "Word-level diversity", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is in the last row of teh Full Training category?", "answer_anchor": "DSM", "question_reference": "What specific aspect of the paraphrasing-based approach does the paper identify as potentially more effective for instances with simple expressions compared to their method?", "explanation_reference": "The paper identifies that for instances with simple expressions, the paraphrasing-based approach may achieve better performance because it focuses on words, implying a more effective word-level diversity compared to their method which focuses more on the structure of the sentences.", "evidence_reference": "However, for instances with simple expressions, the paraphrasing-based method may achieve better performance...the paraphrasing-based approach rewrites 'religion' to 'faith' and rewrites 'influenced' to 'inspired', but our method only rewrites 'What religion' to 'What is the religion', because the paraphrasing-based method focuses on words while ours focuses more on the structure of the sentences."}
{"question": "Consider the paper that introduces the Seq2Exp model marked with the Club citation symbol. What is the maximum Memory Departing Distance (M-MDD) value for which the model proposed in the paper with Memory Register (MR) outperforms the same model without MR on the MathQA dataset according to Figure \\ref{fig:Cache-MCDP-MathQA}?", "answer": "5", "figure": "locality/2310.09619/MathQA_result_table.png", "anchor_arxiv_id": "2310.09619", "reference_arxiv_id": "2210.10105", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which Seq2Exp model is with Club citation mark?", "answer_anchor": "Elastic", "question_reference": "What is the maximum Memory Departing Distance (M-MDD) value for which ELASTIC with Memory Register (MR) outperforms ELASTIC without MR on the MathQA dataset according to Figure \\ref{fig:Cache-MCDP-MathQA}?", "explanation_reference": "The maximum Memory Departing Distance (M-MDD) value for which ELASTIC with Memory Register (MR) outperforms ELASTIC without MR on the MathQA dataset is 5. This is indicated by the performance improvement observed in Figure \\ref{fig:Cache-MCDP-MathQA}, where ELASTIC with MR shows better performance than without MR when M-MDD is larger than 5.", "evidence_reference": "From Figure \\ref{fig:Cache-MCDP-MathQA}, the ELASTIC with Memory Register performs better than ELASTIC without it at each M-MDD on FinQA and MathQA datasets. Particularly in the MathQA dataset, when M-MDD is larger than 5, ELASTIC with Memory Register can achieve better results than the ELASTIC without it."}
{"question": "Consider the paper that introduces the dataset that has the fewest number of languages but the most number of SM tasks. What is the exact match score between the labels and predictions for Flan-T5 on this benchmark?", "answer": "29.07", "figure": "locality/2310.14557/comparison_figure.png", "anchor_arxiv_id": "2310.14557", "reference_arxiv_id": "2305.15005", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset has the fewest number of languages but the most number of SM tasks?", "answer_anchor": "SentiEval", "question_reference": "What is the exact match score between the labels and predictions for \\texttt{Flan-T5} on the \\textsc{SentiEval} benchmark?", "explanation_reference": "The exact match score for \\texttt{Flan-T5} on the \\textsc{SentiEval} benchmark is directly reported in the results table under the section discussing the \\textsc{SentiEval} benchmark.", "evidence_reference": "\\textsc{SentiEval} & 29.07 & 38.82 & 36.64 & 47.55"}
{"question": "Consider the paper that introduces the model that achieves the highest score in the 'T2' column. What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the model proposed in the paper?", "answer": "$\\alpha=0.5$, $\\beta=0.5$, $\\tau_1=0.1$, $\\mu=0.5$, $\\omega=0.1$, $\\tau_2=0.5$, $\\gamma=1.25$, $\\lambda_1=0.5$, $\\lambda_2=1.1$.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model demonstrates the highest score in 'T2' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the proposed model?", "explanation_reference": "The answer directly lists the specific hyperparameter values that were determined through grid search for the FewRel dataset, as detailed in the implementation section of the paper.", "evidence_reference": "We find the best hyperparameter values through grid search with a step of 0.1 except 0.05 for \u03c9 and 0.25 for \u03b3. The search spaces for various hyperparameters are \u03b1\u2208[0.2,0.8], \u03b2\u2208[0.1,0.5], \u03bc\u2208[0.1,1.0], \u03c9\u2208[0.05,0.25], \u03b3\u2208[1.0,2.0] and \u03bb1, \u03bb2\u2208[0.5,1.5]. Besides, we fix \u03c41 and \u03c42 to 0.1 and 0.5, respectively. The used hyperparameter values are listed below: For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the model that has the second lowest MCD 1 score. Which specific variant of the model proposed in the paper showed the most significant performance deterioration on the CLUTRR task when trained on relation lengths k=2,3?", "answer": "Value ablation", "figure": "locality/2310.07096/CFQ_table.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "2112.00578", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the model dmonstrates the lowest MCD 1 score?", "answer_anchor": "T5-based UT", "question_reference": "In the ablation experiments, which specific variant of the Edge Transformer model showed the most significant performance deterioration on the CLUTRR task when trained on relation lengths k=2,3?", "explanation_reference": "The value ablation variant of the Edge Transformer model showed the most significant performance deterioration on the CLUTRR task when trained on relation lengths k=2,3, as indicated by the lowest performance scores across all tested relation lengths in the ablation experiments.", "evidence_reference": "Value ablation & 54.4 $\\pm$ 3.4    & 44.2 $\\pm$ 3.6   & 36.9 $\\pm$ 3.6   & 33.9 $\\pm$ 3.7   & 30.7 $\\pm$ 3.6"}
{"question": "Consider the paper that introduces the dataset in which KALMV achieves a score of 70.83 for the XL model. What specific operational limitation does the Rigel model exhibit when predicting answers from it?", "answer": "Rigel cannot perform sorting or filtering.", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset being tested that KALMV gets 70.83 score for XL model?", "answer_anchor": "Mintaka", "question_reference": "What specific operational limitation does the Rigel model have when predicting answers from the Mintaka dataset?", "explanation_reference": "The Rigel model's limitation is explicitly mentioned in the analysis of its performance on complex questions. It is capable of predicting a path from an entity to all related entities but lacks the functionality to sort or filter these entities, which is crucial for answering questions that require these operations.", "evidence_reference": "Given the Marvel question again, Rigel predicts a path from the Marvel Cinematic Universe to all the Marvel films. Rigel can't perform sorting or filtering, but it's possible to see what the next steps should be: identifying the chronological order, ordering by chronological order, and finding the second in the ordered list."}
{"question": "Consider the paper that introduces the optimization method that exhibits an R2 score of 0.191. What specific loss function modification does the model proposed in the paper suggest implementing in the context of its algorithm to mitigate the issue of model degeneration observed with a naive probability ratio objective?", "answer": "Dynamic, per-example importance weight", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What optimization method show 0.191 R2 score?", "answer_anchor": "DPO", "question_reference": "In the context of the DPO algorithm's implementation, what specific loss function modification is suggested to mitigate the issue of model degeneration observed with a naive probability ratio objective?", "explanation_reference": "The paper suggests incorporating a dynamic, per-example importance weight into the DPO algorithm's loss function to prevent model degeneration, a problem observed with a naive probability ratio objective. This modification is crucial for maintaining the quality and diversity of the language model's outputs.", "evidence_reference": "Intuitively, the gradient of the loss function $\\mathcal{L}_\\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint."}
{"question": "Consider the paper that introduces the method which has an F1 score of 75. How does the inclusion of Coordinate Convolution (CoordConv) specifically impact the F1 scores for both Entity Labeling and Entity Linking tasks in the model proposed in the paper, according to the ablation study results?", "answer": "The inclusion of Coordinate Convolution (CoordConv) specifically impacts the F1 scores for both Entity Labeling and Entity Linking tasks in the MSAU-PAF model by increasing the F1 score by 0.02 in both tasks, according to the ablation study results.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2106.00980", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method having 75 F1 score?", "answer_anchor": "MSAU-PAF", "question_reference": "How does the inclusion of Coordinate Convolution (CoordConv) specifically impact the F1 scores for both Entity Labeling and Entity Linking tasks in the MSAU-PAF model, according to the ablation study results?", "explanation_reference": "The ablation study results indicate that adding translational information through Coordinate Convolution (CoordConv) to the MSAU-PAF model leads to a specific improvement of 0.02 in F1 score for both Entity Labeling and Entity Linking tasks. This demonstrates the effectiveness of incorporating spatial information directly into the convolution process for enhancing model performance on these tasks.", "evidence_reference": "As shown in Table \\ref{table:ablation}, when making the convolution access to its own input coordinates through concatenating extra coordinate channels, MSAU-PAF with Coordinate Convolution experienced a total gain of 0.02 in F1 score in both Entity Labeling and Entity Linking."}
{"question": "Consider the paper that introduces the optimization method that exhibits an R2 score of 0.191. What is the specific mathematical expression used in the model proposed by the paper to derive its objective under the Bradley-Terry model?", "answer": "p^*(y_1\\succ y_2|x)=\\sigma\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)} - \\beta \\log \\frac{\\pi^*(y_2|x)}{\\piref(y_2|x)}\\right)", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What optimization method show 0.191 R2 score?", "answer_anchor": "DPO", "question_reference": "What is the specific mathematical expression used to derive the DPO objective under the Bradley-Terry model?", "explanation_reference": "The expression directly represents the DPO objective derived under the Bradley-Terry model, showcasing how the human preference probability is expressed in terms of the optimal policy and the reference policy, with the partition function cancelling out.", "evidence_reference": "In Section \\ref{app:derivation2} we showed that we can express the (unavailable) ground-truth reward through its corresponding optimal policy: \\begin{equation}\\label{eq:main_eq_restated} r^*(x,y) =\\beta \\log \\frac{\\pi^*(y|x)}{\\piref(y|x)} + \\beta \\log Z(x) \\end{equation}  Substituting Eq. \\ref{eq:main_eq_restated} into Eq. \\ref{eq:BT_restated} we obtain: \\begin{align*} p^*(y_1\\succ y_2|x)&=\\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2|x)}{\\piref(y_2|x)} + \\beta \\log Z(x)\\right)}\\\\ &= \\frac{1}{1+\\exp\\left(\\beta \\log \\frac{\\pi^*(y_2|x)}{\\piref(y_2|x)}-\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)}\\right)} \\\\&= \\sigma\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\piref(y_1|x)} - \\beta \\log \\frac{\\pi^*(y_2|x)}{\\piref(y_2|x)}\\right). \\end{align*}"}
{"question": "Consider the paper that introduces the model which exhibits the highest score on the BC5CDR (Big Dict) and BC5CDR (Small Dict) datasets. What was its MNLI Dev set result in the ablation study when using a 100% 'Mask' strategy without incorporating any 'Same' or 'Rnd' strategies?", "answer": "84.3", "figure": "locality/2310.08298/overall_performance.png", "anchor_arxiv_id": "2310.08298", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model demonstrates the highest score on BC5CDR (Big Dict) and BC5CDR (Small Dict) Dataset?", "answer_anchor": "BERT", "question_reference": "In the ablation study for different masking procedures, what was the MNLI Dev set result when using a 100% 'Mask' strategy with no 'Same' or 'Rnd' strategies?", "explanation_reference": "The question specifically targets the results of an ablation study focused on different masking strategies used during the pre-training of BERT. The answer, 84.3, directly corresponds to the MNLI Dev set result for the scenario where 100% of the target tokens were replaced with the [MASK] symbol, with no tokens kept the same or replaced with a random token. This detail is explicitly provided in the table under the ablation study for different masking procedures, making it a precise answer derived from the paper's content.", "evidence_reference": "100%&0%&0%&84.3&94.9&94.0"}
{"question": "Consider the paper that introduces the model that scores an 84.2 in the 'T10' column. What specific hyperparameter values were determined through grid search for the FewRel dataset in its implementation?", "answer": "$\\alpha=0.5$, $\\beta=0.5$, $\\tau_1=0.1$, $\\mu=0.5$, $\\omega=0.1$, $\\tau_2=0.5$, $\\gamma=1.25$, $\\lambda_1=0.5$, $\\lambda_2=1.1$.", "figure": "locality/2310.06547/result_1_table.png", "anchor_arxiv_id": "2310.06547", "reference_arxiv_id": "2305.06620", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets score of 84.2 in 'T10' column?", "answer_anchor": "CEAR", "question_reference": "What specific hyperparameter values were determined through grid search for the FewRel dataset in the implementation of the proposed model?", "explanation_reference": "The answer directly lists the specific hyperparameter values that were determined through grid search for the FewRel dataset, as detailed in the implementation section of the paper.", "evidence_reference": "We find the best hyperparameter values through grid search with a step of 0.1 except 0.05 for \u03c9 and 0.25 for \u03b3. The search spaces for various hyperparameters are \u03b1\u2208[0.2,0.8], \u03b2\u2208[0.1,0.5], \u03bc\u2208[0.1,1.0], \u03c9\u2208[0.05,0.25], \u03b3\u2208[1.0,2.0] and \u03bb1, \u03bb2\u2208[0.5,1.5]. Besides, we fix \u03c41 and \u03c42 to 0.1 and 0.5, respectively. The used hyperparameter values are listed below: For FewRel, \u03b1=0.5, \u03b2=0.5, \u03c41=0.1, \u03bc=0.5, \u03c9=0.1, \u03c42=0.5, \u03b3=1.25, \u03bb1=0.5, \u03bb2=1.1."}
{"question": "Consider the paper that introduces the model in the figure corresponds to the grey line with a star marker. What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for the model proposed in the paper?", "answer": "0.42", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2305.06161", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model shown in the figure represented by grey line with star marker?", "answer_anchor": "StarCoder", "question_reference": "What is the percentage of responses flagged as toxic using a toxicity classifier in the RealToxicityPrompts evaluation for StarCoderBase?", "explanation_reference": "The percentage of responses flagged as toxic using a toxicity classifier for StarCoderBase in the RealToxicityPrompts evaluation is provided directly in the results table for the toxicity evaluation.", "evidence_reference": "StarCoderBase & 0.42 & 1.12"}
{"question": "Consider the paper that introduces the method that has an accuracy of 78.1% on the VQA-v2 task. What specific improvement in CIDEr score does the model proposed in the paper contribute to image captioning through its text infilling pretraining task?", "answer": "+0.8", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2202.03052", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method shows 78.1 Accuracy on VQA-v2 task?", "answer_anchor": "OFA-base", "question_reference": "What specific improvement in CIDEr score does text infilling pretraining task contribute to image captioning?", "explanation_reference": "The improvement in CIDEr score due to the text infilling pretraining task for image captioning is directly stated in the Ablation on Multitask Pretraining section, indicating the specific impact of this task on enhancing the model's performance in generating image captions.", "evidence_reference": "Text infilling brings improvement on image caption ($+0.8$ CIDEr)"}
{"question": "Consider the paper that introduces the method that scores a 69.56 in the Forgotten Realms category. What specific improvement in U.Acc. does the model proposed in the paper achieve on the Yugioh domain under the few-shot entity linking task when comparing the performance of Syn+Seed data to the Name Matching method?", "answer": "14.94", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method got 69.56 score in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "What specific improvement in U.Acc. does MetaBLINK achieve on the Yugioh domain under the few-shot entity linking task when comparing the performance of Syn+Seed data to the Name Matching method?", "explanation_reference": "U.Acc. for MetaBLINK on the Yugioh domain with Syn+seed is 22.82, with name matching is 7.88, 22.82-7.88=14.94", "evidence_reference": "TABLE VI"}
{"question": "Consider the paper that introduces the method that exhibits the highest Accuracy@0.5 value on the RefCOCOg task. How does the introduction of the $\\tiny{<}obj\\tiny{>}$ token in the model's output sequence design proposed by the paper impact its performance on the referring expression comprehension task?", "answer": "Improves by around 1%", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method demonstrates the highest Accuary@0.5 on RefCOCOg task?", "answer_anchor": "UniTAB", "question_reference": "How does the introduction of the $\\tiny{<}obj\\tiny{>}$ token in UniTAB's output sequence design impact the model's performance on the referring expression comprehension task?", "explanation_reference": "The introduction of the $\\tiny{<}obj\\tiny{>}$ token not only naturally represents the word-box alignments but also simplifies the sequence prediction by providing hints of the text-box code-switching, which in turn helps improve the model's performance on tasks like referring expression comprehension by around 1%.", "evidence_reference": "Table~\\ref{table:objtoken} shows the experiments on the Refcocog dataset~\\cite{mao2016generation}. The \\modelname$_\\text{Separate}$ baseline inserts a pair of $\\tiny{<}obj\\tiny{>}$ and ${{\\tiny<}{\\tiny\\backslash}\\text{obj}{\\tiny>}}$ tokens before and after a word-box token segment. We experiment with removing the ${{\\tiny<}{\\tiny\\backslash}\\text{obj}{\\tiny>}}$ token, or both special tokens. We observe an around $1\\%$ accuracy improvement by adding $\\tiny{<}obj\\tiny{>}$ tokens."}
{"question": "Consider the paper that introduces the model which has a lower mean classification accuracy than VIBE but higher mean classification accuracy than UDALM on the Stance dataset. What is the Pearson's correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the model has lower accuracy than VIBE but higher accuracy than UDALM?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the task of political affiliation classification on Twitter data. A value of 0.9817 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also increases significantly.", "evidence_reference": "Twitter, \\poliaff{} ($F_1$), 0.9817159316285563"}
{"question": "Consider the paper that introduces the method that has a perplexity of 60. What is the primary goal of discriminatively training conditional language models (CC-LMs) with its training approach?", "answer": "The primary goal of discriminatively training CC-LMs with GeDi training is to make them better discriminators for GeDi-guided generation.", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method having 60 perplexity?", "answer_anchor": "GeDi", "question_reference": "What is the primary goal of discriminatively training CC-LMs with GeDi training?", "explanation_reference": "The primary goal of discriminatively training CC-LMs with GeDi training is to enhance their capabilities as discriminators, specifically for the purpose of guiding generation in a more controlled manner. This is aimed at improving the efficiency and effectiveness of the GeDi-guided generation process.", "evidence_reference": "For this reason, we propose training CC-LMs discriminatively as classifiers with GeDi training, with the primary goal of making them better discriminators for GeDi-guided generation."}
{"question": "Consider the paper that introduces the method which is listed in the table below the VL-BART method and above the OFA-base method. What is the role of the $\\tiny{<}obj\\tiny{>}$ token in the output sequence design of the UniTAB model?", "answer": "Indicates word-box alignments and simplifies sequence prediction by providing hints of text-box code-switching.", "figure": "locality/2311.04067/result_table.png", "anchor_arxiv_id": "2311.04067", "reference_arxiv_id": "2111.12085", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the method shown in the table below VL-BART method and above OFA-base method", "answer_anchor": "UniTAB", "question_reference": "What is the role of the $\\tiny{<}obj\\tiny{>}$ token in the UniTAB model's output sequence design?", "explanation_reference": "The $\\tiny{<}obj\\tiny{>}$ token is introduced to simplify the sequence generation by providing hints of the code-switching between text and box tokens, and naturally represents word-box alignments within the output sequence. This means that the words and box within a pair of $\\tiny{<}obj\\tiny{>}$ tokens refer to the same entity, facilitating the model's ability to align predicted words with boxes.", "evidence_reference": "We introduce a special $\\tiny{<}obj\\tiny{>}$ token inserted before the text word to be grounded, and after the generated box tokens. The $\\tiny{<}obj\\tiny{>}$ token simplifies the sequence generation by providing hints of the code-switching, and naturally represents word-box alignments."}
{"question": "Consider the paper that introduces the transformer-based method that achieves the highest MRR score on the FB15kET dataset. How does it ensure the preservation of graph structure while integrating neighbour content?", "answer": "The TET approach ensures the preservation of graph structure while integrating neighbour content through the use of a context transformer that integrates neighbours' content in a differentiated way through information exchange between neighbour pairs.", "figure": "locality/2310.12008/comparison_table.png", "anchor_arxiv_id": "2310.12008", "reference_arxiv_id": "2210.11151", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which transformer-based method gets the highest MRR score in FB15kET datast?", "answer_anchor": "TET", "question_reference": "How does the TET approach ensure the preservation of graph structure while integrating neighbour content?", "explanation_reference": "The context transformer is specifically designed to integrate neighbours' content in a differentiated way through information exchange between neighbour pairs, which inherently preserves the graph structure by maintaining the relational context of each entity.", "evidence_reference": "a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure."}
{"question": "Consider the paper that introduces the LLM model that demonstrates the lowest MSE score. What specific methodological difference in the evaluation of the model's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "answer": "The specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams that contributed to potential minimal impact on results was the direct sampling of a letter choice at temperature 0 using the already-sampled explanation for these exams, as opposed to extracting the model's letter choice directly from the explanation for most other exam runs.", "figure": "locality/2310.17428/mse_table.png", "anchor_arxiv_id": "2310.17428", "reference_arxiv_id": "2303.08774", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the LLM model that demonstrates the lowest MSE score?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation of GPT-4's performance on the USABO and SAT reading/writing runs (with and without vision) compared to other exams contributed to potential minimal impact on results?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard procedure used for most exam runs, where the model's letter choice is extracted directly from the explanation. This approach for the USABO and SAT reading/writing runs indicates a unique handling of these exams, which could contribute to the minimal impact on the overall results, as it relies on the model's generated explanation to determine the final answer choice.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than SERA and a higher F1 score than Doc2Graph. What specific adaptation does the model proposed in the paper employ to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "answer": "To prevent the language-specific pre-processing, LayoutXLM decides to obtain the character-level bounding boxes. After the tokenization using SentencePiece with a unigram language model, it calculates the bounding box of each token by merging the bounding boxes of all characters it contains, efficiently unifying the multilingual multimodal inputs.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having lower F1 score than SERA and higher F1 score than Doc2Graph?", "answer_anchor": "LayoutXLM", "question_reference": "What specific adaptation does the LayoutXLM model employ to handle the challenge of language-specific pre-processing for its Multilingual Masked Visual-Language Modeling objective?", "explanation_reference": "The adaptation mentioned is specifically designed to handle the diversity in the definition of linguistic units across different languages, which is a challenge for language-specific pre-processing. By calculating the bounding box of each token by merging the bounding boxes of all characters it contains, LayoutXLM efficiently unifies the multilingual multimodal inputs without needing language-specific pre-processing.", "evidence_reference": "However, for LayoutXLM, this strategy is not applicable because the definition of the linguistic unit is different from language to language. To prevent the language-specific pre-processing, we decide to obtain the character-level bounding boxes. After the tokenization using SentencePiece with a unigram language model, we calculate the bounding box of each token by merging the bounding boxes of all characters it contains."}
{"question": "Consider the paper that introduces the method that achieves sentence-level precision of 60.32. How does its performance change in the ablation study for WikiHop when it is pretrained with only the extra position embeddings being trainable compared to its full configuration?", "answer": "73.5 / -0.3", "figure": "locality/2310.18544/result_table.png", "anchor_arxiv_id": "2310.18544", "reference_arxiv_id": "2004.05150", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method show a sentence-level precision equal to 60.32?", "answer_anchor": "longformer", "question_reference": "In the ablation study for WikiHop, how does the performance change when \\model is pretrained with only the extra position embeddings being trainable compared to the full \\model configuration?", "explanation_reference": "The ablation study for WikiHop shows that when \\model is pretrained with only the extra position embeddings being trainable, the performance difference compared to the full \\model configuration is -0.3. This indicates a slight decrease in performance when limiting the pretraining to only the extra position embeddings.", "evidence_reference": "\\model (pretrain extra position embed. only) & 73.5 / -0.3"}
{"question": "Consider the paper that introduces the optimization method that has the lowest BLEU score among all models in the table. What specific advantage does the model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, overcoming the limitation of single-head attention where averaging inhibits this capability. Despite the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What optimization method shows the lowest BLEU score across all models?", "answer_anchor": "Transformer base", "question_reference": "What specific advantage does the Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "explanation_reference": "The advantage is highlighted by the experimental variations where changing the number of attention heads and dimensions (keeping the computational cost constant) showed that single-head attention performs worse than the optimal setting, indicating that multi-head attention improves model quality. Additionally, the design of multi-head attention ensures that the total computational cost remains similar to that of single-head attention with full dimensionality, thus not significantly increasing the computational cost.", "evidence_reference": "In Table~\\ref{tab:variations} rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section \\ref{sec:multihead}. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"question": "Consider the paper that introduces the dataset that contains 3,747,569 instances. What specific methodological approach did the authors use to ensure the aspect-based summaries in the model proposed by the paper had sufficient content overlap with their corresponding sections?", "answer": "They used a greedy mapping algorithm to ensure content overlap, assigning a matching score based on ROUGE-1-recall between the abstract sentence and the aspect section, filtering out sentences with a score below a predefined threshold.", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset with 3,747,569 instances?", "answer_anchor": "QASUM", "question_reference": "What specific methodological approach did the authors use to ensure the aspect-based summaries in OASum had sufficient content overlap with their corresponding sections?", "explanation_reference": "The authors employed a greedy mapping algorithm to map each abstract sentence to a list of sentences in the later sections, followed by assigning a matching score based on the ROUGE-1-recall between the abstract sentence and the intersection of its mapped sentences and the sentences in the aspect section. This approach ensured that the aspect-based summaries had sufficient content overlap with their corresponding sections.", "evidence_reference": "Shown in \\cref{alg: greedy mapping}, we first use a greedy method to map each abstract sentence to a list of sentences in the later sections. Then, we assign a matching score $\\mathcal{S}(x, \\alpha)$ for each abstract sentence $x$ and a potential aspect $\\alpha$. We use the \\textit{ROUGE-1-recall} between the abstract sentence $x$ and the intersection of its mapped sentences $\\mathcal{M}(x)$ and the sentences in the aspect section $Y_a$."}
{"question": "Consider the paper that introduces the model that has the highest performance on the En-Fr task in the MSCOCO dataset. How does its performance with hallucinated visual tokens (\\texttt{V}) compare to using ground-truth visual representations (\\texttt{VM}) on the Multi30K dataset for the Transformer-Tiny model in terms of BLEU score?", "answer": "Very similar", "figure": "locality/2310.13361/result_table.png", "anchor_arxiv_id": "2310.13361", "reference_arxiv_id": "2206.00100", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which model demonstrates the highest performance in En-Fr task in MSCOCO dataset?", "answer_anchor": "VALHALLA", "question_reference": "How does the performance of the model proposed in the paper with hallucinated visual tokens (\\texttt{V}) compare to using ground-truth visual representations (\\texttt{VM}) on the Multi30K dataset for the Transformer-Tiny model in terms of BLEU score?", "explanation_reference": "The performance of \\ours with hallucinated visual tokens (\\texttt{V}) is very similar to when using ground-truth visual representations (\\texttt{VM}), demonstrating the model's strong ability to generate visual representations that are semantically consistent with the ground-truth.", "evidence_reference": "Using Transformer-Tiny as the backbone, \\ours obtains an average $35.4$ BLEU in EN$\\rightarrow$DE and $54.4$ BLEU in EN$\\rightarrow$FR, which is about $2.1$ and $1.4$ BLEU improvements over the text-only baseline. Moreover, \\ours has very similar performance with either hallucinated (\\texttt{V}) or ground-truth representation (\\texttt{VM}), showing strong ability to generate visual representations that are semantically consistent with the ground-truth."}
{"question": "Consider the paper that introduces the model that achieves the highest score on the MNLI dataset. How does its performance on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "answer": "The performance of BERT on the NER task using a fine-tuning approach generally outperforms the feature-based approach, as the fine-tuning approach allows for minimal task-specific architecture modifications and leverages the deep bidirectional nature of BERT, which is crucial for understanding the context of entities in text. Different masking strategies during pre-training, such as random masking or keeping the word unchanged, are used to mitigate the mismatch between pre-training and fine-tuning stages, but the paper does not provide a direct comparison of these strategies' impact on the NER task specifically.", "figure": "locality/2310.18343/result_table.png", "anchor_arxiv_id": "2310.18343", "reference_arxiv_id": "1810.04805", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model that demonstrates the highest score on MNLI dataset?", "answer_anchor": "BERT", "question_reference": "How does the performance of BERT on the NER task compare when using a fine-tuning approach versus a feature-based approach with different masking strategies during pre-training?", "explanation_reference": "The performance comparison between fine-tuning and feature-based approaches for the NER task, under various masking strategies during pre-training, shows that the fine-tuning approach consistently outperforms the feature-based approach. This is evident from the Dev set results for NER, where the fine-tuning approach yields higher F1 scores compared to the feature-based approach across all masking strategies.", "evidence_reference": "In the table presented in the Ablation for Different Masking Procedures section, the Dev set results for NER under different masking strategies show that the fine-tuning approach (95.4, 94.9, 95.2, 95.2 for different strategies) consistently achieves higher F1 scores than the feature-based approach (94.9, 94.0, 94.6, 94.7 for the same strategies), indicating better performance."}
{"question": "Consider the paper that introduces the benchmark that corresponds to the light green color in the figure. What specific criteria were used to exclude tasks from the subset due to their reliance on specialized knowledge or being outside the scope of the work?", "answer": "Not solvable by authors within 60 minutes, requires specialized knowledge, or not even worth attempting with chain-of-thought.", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which benchmark is represented using the light green color from the figure?", "answer_anchor": "BBH", "question_reference": "What specific criteria were used to exclude tasks from the BIG-Bench Hard (BBH) subset due to their reliance on specialized knowledge or being outside the scope of the work?", "explanation_reference": "The criteria for excluding tasks from the BBH subset due to their reliance on specialized knowledge or being outside the scope of the work were explicitly listed in the appendix under the heading 'Criteria: Task is outside the scope of this work.'", "evidence_reference": "Criteria: Task is outside the scope of this work: not solvable by authors within 60 minutes, requires specialized knowledge, or not even worth attempting with chain-of-thought."}
{"question": "Consider the paper that introduces the optimization method that has a BLEU score of 27.3. What specific advantage does its use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, overcoming the limitation of single-head attention where averaging inhibits this capability. Despite the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.", "figure": "locality/2310.07096/BLEU.png", "anchor_arxiv_id": "2310.07096", "reference_arxiv_id": "1706.03762", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What optimization method shows BLEU score of 27.3?", "answer_anchor": "Transformer base", "question_reference": "What specific advantage does the Transformer model's use of multi-head attention provide over single-head attention in terms of model quality and computational cost, as evidenced by the experimental variations?", "explanation_reference": "The advantage is highlighted by the experimental variations where changing the number of attention heads and dimensions (keeping the computational cost constant) showed that single-head attention performs worse than the optimal setting, indicating that multi-head attention improves model quality. Additionally, the design of multi-head attention ensures that the total computational cost remains similar to that of single-head attention with full dimensionality, thus not significantly increasing the computational cost.", "evidence_reference": "In Table~\\ref{tab:variations} rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section \\ref{sec:multihead}. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}
{"question": "Consider the paper that introduces the method that achieves an F1 score of 87.63 in the Token (I-topo) category. What specific distribution is used for sampling span lengths in the model's span masking scheme proposed in the paper, and what is the mean span length resulting from this distribution?", "answer": "Geometric distribution with $p=0.2$, mean span length = $3.8$", "figure": "locality/2310.14478/comparison_table.png", "anchor_arxiv_id": "2310.14478", "reference_arxiv_id": "1907.10529", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method gets F1 score 87.63 in Token (I-topo) category?", "answer_anchor": "SpanBERT", "question_reference": "What specific distribution is used for sampling span lengths in SpanBERT's span masking scheme, and what is the mean span length resulting from this distribution?", "explanation_reference": "The paper specifies that span lengths are sampled from a geometric distribution with parameter \\(p=0.2\\), and this sampling strategy results in a mean span length of 3.8. This detail directly addresses the conceptual understanding of SpanBERT's approach to span masking, highlighting the statistical method used for determining the lengths of spans to be masked during pre-training.", "evidence_reference": "Given a sequence of tokens \\(X = ( x_1 , x_2, \\ldots , x_n )\\), we select a subset of tokens \\(Y \\subseteq X\\) by iteratively sampling spans of text until the masking budget (e.g. 15% of \\(X\\)) has been spent. At each iteration, we first sample a span length (number of words) from a geometric distribution \\(\\ell \\sim \\mathrm{Geo}(p)\\), which is skewed towards shorter spans... Following preliminary trials, we set \\(p=0.2\\), and also clip \\(\\ell\\) at \\(\\ell_{max}=10\\). This yields a mean span length of \\(\\mathrm{mean}(\\ell)=3.8\\)."}
{"question": "Consider the paper that introduces the benchmark that achieves a 'Generation Token Length' near 400 when the 'Compression Ratio' is 1. What specific improvement does the model proposed in the paper, through CoT prompting, provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "answer": "CoT prompting enables a significant improvement in performance on the 'Tracking Shuffled Objects' task, with a gain of +28.5% accuracy over answer-only prompting for the Codex model.", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which benchmark gets 'Generation Token Length' near 400 when 'Compression Ratio' is 1?", "answer_anchor": "BBH", "question_reference": "What specific improvement does CoT prompting provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "explanation_reference": "The improvement is calculated by comparing the performance increase from answer-only prompting to CoT prompting for the Codex model on the 'Tracking Shuffled Objects' task. This is derived from the performance metrics provided, showing a significant gain when using CoT prompting.", "evidence_reference": ""}
{"question": "Consider the paper that discusses the dataset that contains 3,747,569 instances. What specific threshold value was chosen for the matching score to ensure the inclusion of abstract sentences in the aspect-based summaries during its construction, and how was this value determined?", "answer": "0.5", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the dataset with 3,747,569 instances?", "answer_anchor": "QASUM", "question_reference": "What specific threshold value was chosen for the matching score to ensure the inclusion of abstract sentences in the aspect-based summaries during the dataset construction, and how was this value determined?", "explanation_reference": "The chosen threshold value for the matching score, which determines the inclusion of abstract sentences in the aspect-based summaries, was 0.5. This value was determined through manual evaluation, where 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold were randomly picked and assigned to 5 experts for evaluating the dataset quality. The threshold of 0.5 was selected based on these evaluations.", "evidence_reference": "To filter out sentences with limited content overlap, an aspect-based summary includes only abstract sentences with a matching score \\(\\mathcal{S}(x, a)\\) greater or equal to a pre-defined threshold \\(\\lambda\\). To determine the exact value of the threshold, we try \\(\\lambda \\in [0.3, 0.4, 0.5, 0.6, 0.7]\\) and evaluate them manually. Specifically, we randomly pick 66 Wikipedia pages consisting of 103 aspect-summary pairs for each threshold, and assigned them to 5 experts for evaluating the dataset quality. We then choose to use \\(\\lambda=0.5\\)."}
{"question": "Consider the paper that introduces the dataset which has the largest number of Queries|Aspects in the OABS category. What specific methodological approach did the authors use to ensure the aspect-based summaries in the model proposed by the paper had sufficient content overlap with their corresponding sections?", "answer": "They used a greedy mapping algorithm to ensure content overlap, assigning a matching score based on ROUGE-1-recall between the abstract sentence and the aspect section, filtering out sentences with a score below a predefined threshold.", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset with the most number of Queries|Aspects in OABS category?", "answer_anchor": "QASUM", "question_reference": "What specific methodological approach did the authors use to ensure the aspect-based summaries in OASum had sufficient content overlap with their corresponding sections?", "explanation_reference": "The authors employed a greedy mapping algorithm to map each abstract sentence to a list of sentences in the later sections, followed by assigning a matching score based on the ROUGE-1-recall between the abstract sentence and the intersection of its mapped sentences and the sentences in the aspect section. This approach ensured that the aspect-based summaries had sufficient content overlap with their corresponding sections.", "evidence_reference": "Shown in \\cref{alg: greedy mapping}, we first use a greedy method to map each abstract sentence to a list of sentences in the later sections. Then, we assign a matching score $\\mathcal{S}(x, \\alpha)$ for each abstract sentence $x$ and a potential aspect $\\alpha$. We use the \\textit{ROUGE-1-recall} between the abstract sentence $x$ and the intersection of its mapped sentences $\\mathcal{M}(x)$ and the sentences in the aspect section $Y_a$."}
{"question": "Consider the paper that introduces the method in the table that is listed right above One-Round Distillation and right below Specialization. What specific modification to the few-shot prompts used in its generation is highlighted as a key factor for improving the quality of generated data?", "answer": "Providing the model with the target after posing the question and before providing example CoT.", "figure": "locality/2310.13332/comparison_table.png", "anchor_arxiv_id": "2310.13332", "reference_arxiv_id": "2212.08410", "modal": "table", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method in the table is listed right above One-Round Distillation and right below Specializing?", "answer_anchor": "CoT Fine-tuned", "question_reference": "What specific modification to the few-shot prompts used in CoT generation is highlighted as a key factor for improving the quality of generated data?", "explanation_reference": "The paper specifies that making a key modification to the few-shot prompts by providing the target answer after the question and before the example CoTs allows LLMs to correct small mistakes in the CoT, which is crucial for generating high-quality data for knowledge distillation.", "evidence_reference": "Specifically, we perform few-shot prompting with 8 exemplars on these models to generate CoTs. However, we make a key modification to the prompts proposed by \\citet{wei2022chain}. We adapt the few-shot prompts to provide the model with the target after posing the question and before providing example CoT."}
{"question": "Consider the paper that introduces the method that is represented by the purple line. How does the model proposed in the paper ensure that all attention heads do not deviate significantly in their reading speeds during training?", "answer": "The MMA model ensures that all attention heads do not deviate significantly in their reading speeds during training by introducing a head divergence loss, which is the average variance of expected delays at each step. This loss function encourages the attention heads to maintain similar positions, preventing the latency from being dominated by a single or a few heads.", "figure": "locality/2310.14883/figure.png", "anchor_arxiv_id": "2310.14883", "reference_arxiv_id": "1909.12406", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is demonstrated by the purple line?", "answer_anchor": "MMA", "question_reference": "How does the MMA model ensure that all attention heads do not deviate significantly in their reading speeds during training?", "explanation_reference": "The head divergence loss, L_{var}, is designed to minimize the variance in expected delays across all attention heads, thereby ensuring that no single head reads significantly faster or slower than the others. This regularization term effectively controls the divergence of the heads, making sure they operate at similar speeds.", "evidence_reference": "The final objective function is presented in \\autoref{eq:objective}:  \\begin{equation} L(\\theta) = -\\log(\\vy \\mid \\vx; \\theta) + \\lambda_{avg} L_{avg} + \\lambda_{var} L_{var} \\label{eq:objective} \\end{equation}  where $\\lambda_{avg}$, $\\lambda_{var}$ are hyperparameters that control both losses. Intuitively, while $\\lambda_{avg}$ controls the overall speed, $\\lambda_{var}$ controls the divergence of the heads."}
{"question": "Consider the paper that introduces the method that corresponds to a score of 42.0 in the Seen, Val, GC dataset. What specific method does it leverage to enhance its understanding of complex human instructions and handling of long sequences of subtasks in dynamic environments?", "answer": "The Episodic Transformer leverages encoding the full episode history of visual observations and actions with a transformer, and it improves training by using synthetic instructions as an intermediate representation.", "figure": "locality/2310.12344/ALFRED_table.png", "anchor_arxiv_id": "2310.12344", "reference_arxiv_id": "2105.06453", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method shows the score of 42.0 in Seen, Val, GC dataset?", "answer_anchor": "E.T.", "question_reference": "What specific method does the Episodic Transformer leverage to enhance its understanding of complex human instructions and handling of long sequences of subtasks in dynamic environments?", "explanation_reference": "The question focuses on a detailed aspect of how the Episodic Transformer (E.T.) addresses the challenges of understanding complex human instructions and handling long sequences of subtasks. The answer is directly provided by the methodological approach described in the paper, where synthetic instructions are used as an intermediate representation to decouple the understanding of visual appearance from the variations in natural language instructions.", "evidence_reference": "To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions."}
{"question": "Consider the paper that introduces the method that has the lowest MAE in the CH-SIMS task. Which method, demonstrating the lowest MAE in the CH-SIMS task, achieves what specific improvement in accuracy percentage over the state-of-the-art for 5-class sentiment classification according to the performance of the model proposed in the paper?", "answer": "6.7", "figure": "locality/2310.05804/result_table.png", "anchor_arxiv_id": "2310.05804", "reference_arxiv_id": "1707.07250", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has the highest MAE in CH-SIMS task?", "answer_anchor": "Tensor Fusion", "question_reference": "Based on the Tensor Fusion Network's performance in multimodal sentiment analysis, what specific improvement in accuracy percentage does the TFN model achieve over the state-of-the-art for 5-class sentiment classification?", "explanation_reference": "The question focuses on the specific detail of the improvement in accuracy percentage that the Tensor Fusion Network (TFN) model achieves over the state-of-the-art for 5-class sentiment classification. This detail is directly answered by the comparison provided in the experiments section, highlighting the TFN model's performance against existing approaches.", "evidence_reference": "Table~\\ref{table:mmres} shows the comparison with state-of-the-art approaches for multimodal sentiment analysis. \\mns \\ outperforms both neural and non-neural approaches as shown by $\\Delta^{SOTA}$. Specifically, for 5-class classification, the improvement is $\\uparrow$ 6.7%."}
{"question": "Consider the paper that introduces the model shown in the table that has an overall score of less than 3.80. What specific adaptation in the text embeddings allows it to build correspondence among query text, label text, and objects in grounding tasks?", "answer": "Embedding sharing", "figure": "locality/2310.15129/human_eval_table.png", "anchor_arxiv_id": "2310.15129", "reference_arxiv_id": "2102.02779", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the model shown in the table with overall score less than 3.80?", "answer_anchor": "MVQG-VL-T5", "question_reference": "What specific adaptation in the text embeddings allows the model to build correspondence among query text, label text, and objects in grounding tasks?", "explanation_reference": "The specific adaptation that allows the model to build correspondence among query text, label text, and objects in grounding tasks is the use of embedding sharing. This is achieved by reusing the text embeddings of visual sentinel tokens as region id embeddings, which enables the model to establish a connection between the text and visual elements, particularly useful in grounding tasks.", "evidence_reference": "In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens \\{\\texttt{<vis\\_1>}, $\\dots$, \\texttt{<vis\\_n>}\\}, which corresponds to image regions. As illustrated in Fig.~\\ref{fig:architecture}, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec.~\\ref{sec:visual_embeddings}. The embedding sharing enables our model to build the correspondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec.~\\ref{sec:pretraining}, referring expression comprehension in Sec.~\\ref{sec:refcoco})."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category. What specific performance improvement does the model proposed in the paper, \\textsc{K-Adapter} (F+L), achieve over RoBERTa on the CosmosQA dataset?", "answer": "1.24%", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2002.01808", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Pre-training --> Architectural-based category?", "answer_anchor": "K-Adapter", "question_reference": "What specific performance improvement does \\textsc{K-Adapter} (F+L) achieve over RoBERTa on the CosmosQA dataset?", "explanation_reference": "The question assesses the understanding of the specific quantitative improvement in performance that \\textsc{K-Adapter} (F+L) achieves over RoBERTa on a particular dataset, which is a detail that directly reflects the effectiveness of the knowledge infusion approach proposed in the paper.", "evidence_reference": "Compared to RoBERTa, \\textsc{K-Adapter} (F+L) further improves the accuracy by 1.24\\%, which indicates that \\textsc{K-Adapter} can obtain better commonsense inference ability."}
{"question": "Consider the paper that introduces the model that has a score lower than 0.82 but higher than 0.815 in the Stance column. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model has accuracy consistently lower than 0.82 across all volumne of adaptive data?", "answer_anchor": "DPT", "question_reference": "Based on the findings, what is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data indicates the strength of the linear relationship between the vocabulary overlap across time periods and the performance of the model on this specific task. A value close to 1 suggests a strong positive correlation.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that achieves the highest score in the 'w/o p_out' setting. How does its performance in detecting out-of-distribution samples using ResNet on CIFAR-10, when SVHN is used as OOD, compare to the baseline and ODIN methods in terms of TNR at TPR 95%?", "answer": "96.42", "figure": "locality/2310.05083/comparison_figure.png", "anchor_arxiv_id": "2310.05083", "reference_arxiv_id": "1807.03888", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method that demonstrates the highest score in 'w/o p_out' setting?", "answer_anchor": "MSP", "question_reference": "How does the proposed method's performance in detecting out-of-distribution samples using ResNet on CIFAR-10 when SVHN is used as OOD compare to the baseline and ODIN methods in terms of TNR at TPR 95%?", "explanation_reference": "The proposed method outperforms both the baseline and ODIN methods in detecting out-of-distribution samples using ResNet on CIFAR-10 when SVHN is used as OOD, achieving a higher TNR at TPR 95%.", "evidence_reference": "Baseline: 32.47%, ODIN: 86.55%, Mahalanobis (ours): 96.42%"}
{"question": "Consider the paper that introduces the method that scores higher than 69.0 but lower than 70.0 in the Forgotten Realms category. What specific method does the model proposed in the paper use to generate high-quality synthetic data for few-shot entity linking?", "answer": "MetaBLINK uses a two-stage architecture for generating high-quality synthetic data for few-shot entity linking. Initially, it employs a heuristic method to generate mention-entity pairs. Then, it adopts a weak supervision strategy based on these mention-entity pairs to generate synthetic data. This process leverages a natural language generation model (NLG), specifically the T5 model, to rewrite mentions for generating more effective synthetic samples in the target domain. Additionally, MetaBLINK incorporates a meta-learning mechanism to automatically assign different weights to each synthetic data instance, improving the quality of the synthetic data used for model training.", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method got a score high than 69.0 but lower than 70.0 in Forgotten Realms category?", "answer_anchor": "MetaBINK", "question_reference": "What specific method does MetaBLINK use to generate high-quality synthetic data for few-shot entity linking?", "explanation_reference": "The paper specifies that to improve the quality of synthetic data, they adopt a mention rewriting strategy using the T5 model. This approach is chosen to alleviate the bias introduced by exact matching and generate more effective synthetic samples by rewriting mentions to be semantically similar to entity descriptions.", "evidence_reference": "To further improve the quality of generated data, we adopt the T5 model to generate more semantic-like data samples... In our model, we suppose the mention contains part of semantic information of the corresponding entities, so we add the prefix 'summarize': to the entity's description to force the model to summarize the entity in a few words."}
{"question": "Consider the paper that introduces the method that has a score of 73.6 in the CB dataset with 4-shot prompting. How does the model's performance, as proposed in the paper, change when increasing the prompt length from 300 to 400?", "answer": "absolute 1.8% drop in accuracy", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2303.02861", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method having score of 73.6 in CB dataset with 4-shot prompting?", "answer_anchor": "MPT", "question_reference": "In the multitask prompt tuning (MPT) approach, how does the performance change when increasing the prompt length from 300 to 400?", "explanation_reference": "The performance on SuperGLUE decreases by an absolute 1.8% when the prompt length is increased from 300 to 400, indicating possible overfitting at longer prompt lengths.", "evidence_reference": "However, further increasing the prompt length from 300 to 400 leads to an absolute 1.8% drop in accuracy, possibly due to overfitting."}
{"question": "Consider the paper that introduces the method that achieves a score of 28.62 in the WQ-B task. How does the model proposed in the paper address the challenge of generating questions with both word-level and structure-level diversity?", "answer": "Higher in Arabic", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the least number of dialogues from the table?", "answer_anchor": "Multialpaca", "question_reference": "How does the performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark in Arabic compare to its performance in Korean?", "explanation_reference": "The performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark is higher in Arabic (50.7 F1 score) compared to Korean (30.1 F1 score), indicating better comprehension and response generation capabilities in Arabic.", "evidence_reference": "BLOOMZ-MT-7.1B   & 22.4          & 36.6 & 26.9 & 5.8  & 9.1  & 20.2    & 26.7 & 2.4  & 14.4 & 26.5 & 17.5    \\nLLaMA-Alpaca-13B & \\textbf{59.2} & 20.8 & 48.6 & 19.3 & 37.7 & 37.1    & 11.0 & 50.6 & 20.7 & 5.7  & 22.0    \\n\\textsc{Poly}LM-\\mySFTDatasetName-13B & 58.7 & \\textbf{50.7} & \\textbf{52.1} & \\textbf{30.1} & \\textbf{40.3} & \\textbf{46.4} & 2.5 & 8.5 & 4.6 & 1.9 & 4.4"}
{"question": "Consider the paper that introduces the model that is seventh in the table. What is the primary reason the PIDRP method performs worse than the PCP method, across all four top-level senses of the PDTB, especially on the Temporal sense?", "answer": "The primary reason the PIDRP method performs worse than the PCP method across all four top-level senses of the PDTB, especially on the Temporal sense, is that connective prediction is closer to the natural language patterns when the model is in the pre-training stage than direct implicit discourse relation prediction.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which is the method demonstrated in the seventh row in the table?", "answer_anchor": "PCP", "question_reference": "What is the primary reason the PIDRP method performs worse than the PCP method across all four top-level senses of the PDTB, especially on the Temporal sense?", "explanation_reference": "The primary reason for the PIDRP method's inferior performance compared to the PCP method is attributed to the nature of connective prediction being more aligned with the natural language patterns that the model was exposed to during its pre-training phase, as opposed to the direct prediction of implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that introduces the model that is seventh in the table. What is the primary reason the PIDRP method underperforms compared to PCP, specifically on the Temporal sense?", "answer": "The PIDRP method underperforms compared to the PCP method on the Temporal sense primarily because connective prediction is closer to the natural language patterns when the model is in the pre-training stage than direct implicit discourse relation prediction.", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which is the method demonstrated in the seventh row in the table?", "answer_anchor": "PCP", "question_reference": "What is the primary reason the PIDRP method underperforms compared to the PCP method on the Temporal sense?", "explanation_reference": "The paper suggests that the PIDRP method's underperformance, especially on the Temporal sense, is attributed to the fact that connective prediction aligns more closely with the natural language patterns that the model was exposed to during its pre-training stage, as opposed to directly predicting implicit discourse relations.", "evidence_reference": "We think that the main reason of poor performance is that connective prediction is closer to the natural language patterns when the model is in pre-training stage than direct implicit discourse relation prediction."}
{"question": "Consider the paper that discusses the dataset in which KALMV achieves a score of 66.48 for the Large model. What specific operational limitation does the Rigel model have when predicting answers from this dataset?", "answer": "Rigel cannot perform sorting or filtering.", "figure": "locality/2310.12836/results_table.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the dataset being tested that KALMV gets 66.48 score for Large model?", "answer_anchor": "Mintaka", "question_reference": "What specific operational limitation does the Rigel model have when predicting answers from the Mintaka dataset?", "explanation_reference": "The Rigel model's limitation is explicitly mentioned in the analysis of its performance on complex questions. It is capable of predicting a path from an entity to all related entities but lacks the functionality to sort or filter these entities, which is crucial for answering questions that require these operations.", "evidence_reference": "Given the Marvel question again, Rigel predicts a path from the Marvel Cinematic Universe to all the Marvel films. Rigel can't perform sorting or filtering, but it's possible to see what the next steps should be: identifying the chronological order, ordering by chronological order, and finding the second in the ordered list."}
{"question": "Consider the paper that introduces the first method shown in the Implicit --> Continual Learning --> Continual Knowledge Editing category. What specific configuration arguments are used in the proposed sampling algorithm to control the dynamics of query streams in the model proposed in the paper?", "answer": "$(\\alpha, \\beta, \\gamma)$", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2205.02014", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "What is the first method shown in Implicit --> Continual Learning --> Continual Knowledge Editing?", "answer_anchor": "CMR", "question_reference": "What specific configuration arguments are used in the proposed sampling algorithm to control the dynamics of query streams in CMR?", "explanation_reference": "The specific configuration arguments mentioned for controlling the dynamics of query streams in the CMR setting are alpha (decaying factor for the proportion of in-distribution data), beta (transition probability of the Markov chain for deciding the index of the major OOD cluster), and gamma (to control the diversity by adding data from remaining OOD clusters). These arguments directly answer the question by specifying the mechanisms used to control the dynamics of query streams.", "evidence_reference": "As shown in \\\\textbf{Alg.~\\\\ref{alg:cns}}, we have three key configuration arguments $(\\\\alpha, \\\\beta, \\\\gamma)$ for controlling the dynamics of the query stream: 1) $\\\\alpha$ is the \\\\textbf{decaying factor} for the proportion of in-distribution data, 2) $\\\\beta$ is the \\\\textbf{transition probability} of the Markov chain for deciding the index of the major OOD cluster $c_t$, and 3) $\\\\gamma$ is to control the \\\\textbf{diversity} by adding data from remaining OOD clusters; $T$ is the number of episodes and $b$ is size of $Q_t$."}
{"question": "Consider the paper that introduces the supervised method that results in the lowest score in 10-shot prompting. What specific performance improvement does the model proposed in the paper provide using dynamic masking over static masking for the MNLI-m task according to the paper's findings?", "answer": "0.3%", "figure": "locality/2310.11715/few-shot_NER_table.png", "anchor_arxiv_id": "2310.11715", "reference_arxiv_id": "1907.11692", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which supervised method demonstrates lowest scores in 10-shot prompting?", "answer_anchor": "RoBERTa", "question_reference": "What specific performance improvement does dynamic masking provide over static masking for the MNLI-m task according to the paper's findings?", "explanation_reference": "The question focuses on extracting a specific detail regarding the performance improvement dynamic masking offers over static masking for the MNLI-m task. The answer is derived from comparing the performance metrics of static and dynamic masking specifically for the MNLI-m task, where dynamic masking shows a slight improvement.", "evidence_reference": "static & 78.3 & 84.3 & 92.5 \\\\ dynamic & 78.7 & 84.0 & 92.9 \\\\"}
{"question": "Consider the paper that introduces the method that has an F1 score of 64.95 on PDTB-Top. What is the Macro-F1 score improvement of the model proposed in the paper, PCP-large without the segment token </s>, over the LDSGM model for the second-level sense 'Exp.List' in PDTB 2.0?", "answer": "3.55", "figure": "locality/2311.00367/result_table.png", "anchor_arxiv_id": "2311.00367", "reference_arxiv_id": "2210.07032", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which is the method show a 64.95 F1 score on PDTB-Top?", "answer_anchor": "PCP", "question_reference": "What is the Macro-F1 score improvement of PCP-large without the segment token </s> over the LDSGM model for the second-level sense 'Exp.List' in PDTB 2.0?", "explanation_reference": "The improvement can be calculated from the Macro-F1 scores provided for the second-level sense 'Exp.List' in PDTB 2.0. LDSGM has a Macro-F1 score of 8.98, and PCP-large without the segment token </s> achieved a Macro-F1 score of 37.50. The improvement is the difference between these two scores. 44.04-40.49=3.55", "evidence_reference": "Exp.List            & 0.0           & \\underline{8.98}  & 29.63  & \\textbf{37.50}"}
{"question": "Consider the paper that introduces the method that is shown in the fourth row of the table. What specific implementation detail is suggested to improve the stability of policy and value learning in the model proposed in the paper, MAPPO, when dealing with the non-stationarity of Multi-Agent Reinforcement Learning (MARL) environments?", "answer": "Limiting the change in the agents' policies by using fewer epochs per update", "figure": "locality/2310.10701/result_table.png", "anchor_arxiv_id": "2310.10701", "reference_arxiv_id": "2103.01955", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method showing in the fourth row of the table?", "answer_anchor": "MAPPO", "question_reference": "What specific implementation detail is suggested to improve the stability of policy and value learning in MAPPO when dealing with the non-stationarity of MARL environments?", "explanation_reference": "The suggestion to use fewer epochs per update in MAPPO is aimed at improving the stability of policy and value learning by limiting the non-stationarity inherent in MARL environments. This approach is hypothesized to mitigate the effects of rapidly changing policies among agents, which can destabilize learning.", "evidence_reference": "However, we find that in multi-agent domains, MAPPO's performance degrades when samples are re-used too often. Thus, we use 15 epochs for easy tasks, and 10 or 5 epochs for difficult tasks. We hypothesize that this pattern could be a consequence of non-stationarity in MARL: using fewer epochs per update limits the change in the agents' policies, which could improve the stability of policy and value learning."}
{"question": "Consider the paper that introduces the optimization method that exhibits an R2 score of 0.191. How does the model's performance, proposed by the paper, in terms of win rate against reference completions in the TL;DR summarization task compare to PPO at its optimal sampling temperature?", "answer": "DPO has a win rate of approximately 61% at a temperature of 0.0, exceeding PPO's performance of ~57% at its optimal sampling temperature of 0.0.", "figure": "locality/2310.05857/comparison_dpo.png", "anchor_arxiv_id": "2310.05857", "reference_arxiv_id": "2305.18290", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What optimization method show 0.191 R2 score?", "answer_anchor": "DPO", "question_reference": "How does the Direct Preference Optimization (DPO) algorithm's performance in terms of win rate against reference completions in the TL;DR summarization task compare to PPO at its optimal sampling temperature?", "explanation_reference": "The question focuses on comparing the performance of DPO and PPO in a specific task (TL;DR summarization) and requires understanding of the experimental results presented in the paper. The answer directly compares the win rates of DPO and PPO, providing a clear measure of DPO's superior performance in this context.", "evidence_reference": "DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model. We find that DPO has a win rate of approximately 61% at a temperature of 0.0, exceeding the performance of PPO at ~57% at its optimal sampling temperature of 0.0."}
{"question": "Consider the paper that introduces the method that consistently achieves a higher MRR score than NodePiece. What is the relative increase in MRR achieved by the model proposed in the paper compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "answer": "4.7%", "figure": "locality/2310.15797/performance_comparison_figure.png", "anchor_arxiv_id": "2310.15797", "reference_arxiv_id": "2302.01849", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What method consistently shows higher MRR than NodePiece?", "answer_anchor": "EARL", "question_reference": "What is the relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset using a similar parameter budget?", "explanation_reference": "The relative increase in MRR achieved by EARL compared to RotatE on the FB15k-237 dataset is mentioned in the section summarizing the main results, where it states that EARL uses only 62% parameters and obtains a relative increase of 4.7% on MRR in comparison with RotatE.", "evidence_reference": "Specifically, on FB15k-237, \\model~uses only 62\\% parameters and obtains a relative increase of 4.7\\% on MRR in comparison with RotatE."}
{"question": "Consider the paper that introduces the dataset in the table that has the second shortest average question length. How does the performance of the model proposed in the paper on the TyDiQA-GoldP benchmark in Arabic compare to its performance in Korean?", "answer": "Higher in Arabic", "figure": "locality/2402.04588/comparison_table.png", "anchor_arxiv_id": "2402.04588", "reference_arxiv_id": "2307.06018", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method has the longest question average length?", "answer_anchor": "Multialpaca", "question_reference": "How does the performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark in Arabic compare to its performance in Korean?", "explanation_reference": "The performance of \\textsc{PolyLM}-\\mySFTDatasetName-13B on the TyDiQA-GoldP benchmark is higher in Arabic (50.7 F1 score) compared to Korean (30.1 F1 score), indicating better comprehension and response generation capabilities in Arabic.", "evidence_reference": "BLOOMZ-MT-7.1B   & 22.4          & 36.6 & 26.9 & 5.8  & 9.1  & 20.2    & 26.7 & 2.4  & 14.4 & 26.5 & 17.5    \\nLLaMA-Alpaca-13B & \\textbf{59.2} & 20.8 & 48.6 & 19.3 & 37.7 & 37.1    & 11.0 & 50.6 & 20.7 & 5.7  & 22.0    \\n\\textsc{Poly}LM-\\mySFTDatasetName-13B & 58.7 & \\textbf{50.7} & \\textbf{52.1} & \\textbf{30.1} & \\textbf{40.3} & \\textbf{46.4} & 2.5 & 8.5 & 4.6 & 1.9 & 4.4"}
{"question": "Consider the paper that introduces the model that achieves an F1 score of 73.1 in the en_city category. What specific approach does the paper employ to improve the model's ability to generate accurate sentence positions for generative segmentation?", "answer": "Using the $i^\\text{th}$ vocabulary token embedding in place of a fixed BOS token index for the $i^\\text{th}$ sentence.", "figure": "locality/2310.11772/comparison_table.png", "anchor_arxiv_id": "2310.11772", "reference_arxiv_id": "2209.13759", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which model gets 73.1 F1 score in en_city category?", "answer_anchor": "Naive LongT5-Base-SS", "question_reference": "What specific approach does the paper employ to improve the model's ability to generate accurate sentence positions for generative segmentation?", "explanation_reference": "The paper describes a method to enhance the model's implicit ability to convey position information from input to its decoder, crucial for producing accurate sentence positions in generative segmentation. This is achieved by substituting the fixed BOS token index with the $i^\\text{th}$ vocabulary token embedding for the $i^\\text{th}$ sentence, thereby providing unambiguous position information to the decoder without employing custom schemes like dedicated sentence position embeddings.", "evidence_reference": "At the encoder input for the $i^\\text{th}$ sentence,  we use the $i^\\text{th}$ vocabulary token embedding in place of a fixed BOS token index. Formally, in contrast to \\eqref{eqn:bos}, we set \\begin{equation*} t_{1_1} = 0, \\hspace{2mm}  t_{2_1} = 1, \\hspace{2mm} \\ldots,  t_{|S|_1} = |S|-1. \\end{equation*}"}
{"question": "Consider the paper that introduces the method that is in the last row of the Full Training category. What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions by this method?", "answer": "0.949", "figure": "locality/2310.08395/result_table.png", "anchor_arxiv_id": "2310.08395", "reference_arxiv_id": "2309.14362", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Conceptual Understanding", "question_anchor": "Which method is in the last row of teh Full Training category?", "answer_anchor": "DSM", "question_reference": "What is the Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions?", "explanation_reference": "The Pearson correlation coefficient between $Diverse@k$ and human evaluation for the top-5 generated questions indicates the degree of correlation between the proposed diversity metric and human judgment on diversity, with a value of 0.949 showing a high consistency.", "evidence_reference": "Table~\\ref{tb:human_evaluation} reports the result of the Pearson correlation. We observe that our devised metric $Diverse@k$ is highly consistent with human evaluation, which demonstrates the rationality of the metric $Diverse@k$."}
{"question": "Consider the paper that introduces the dataset that has the largest number of Queries|Aspects in the OABS category. What is the model's, proposed by the paper, average number of aspects per article, and what percentage of articles have less than 9 aspects?", "answer": "1.82 aspects per article and 99%", "figure": "locality/2312.04440/dataset_table.png", "anchor_arxiv_id": "2312.04440", "reference_arxiv_id": "2212.09233", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the dataset with the most number of Queries|Aspects in OABS category?", "answer_anchor": "QASUM", "question_reference": "What is the average number of aspects per article in the OASum dataset, and what percentage of articles have less than 9 aspects?", "explanation_reference": "The question specifically asks for detailed statistics regarding the distribution of aspects per article within the OASum dataset. The answer directly addresses this by providing the average number of aspects per article and the percentage of articles with fewer than 9 aspects, which are key details extracted from the 'Data Statistics and Analysis' section of the paper.", "evidence_reference": "In \\cref{tab:big_table}, we compare \\textbf{\\DATANAME} with other query/aspect-based summarization datasets. \\textbf{\\DATANAME} contains a significantly larger amount of aspect types. On average, there are 1.82 aspects per article and 99% articles have less than 9 aspects per single document."}
{"question": "Consider the paper that introduces the large language model that has the second lowest HVI score among those in the figure corresponding to a purple bar. What specific methodological difference in the evaluation setup for the model proposed in the paper's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "answer": "The use of sampling a letter choice at temperature 0 using the already-sampled explanation for certain exams, rather than extracting the model's letter choice directly from the explanation.", "figure": "locality/2310.04988/HVI_figure.png", "anchor_arxiv_id": "2310.04988", "reference_arxiv_id": "2303.08774", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the large language model that demonstrates the second lowest HVI score shown in purple bar?", "answer_anchor": "GPT-4", "question_reference": "What specific methodological difference in the evaluation setup for GPT-4's performance on the USABO and SAT reading/writing exams contributed to a deviation from the standard approach used for other exams?", "explanation_reference": "This methodological difference is highlighted as a deviation from the standard approach of directly extracting the model's letter choice from the explanation for most exam runs. Instead, for these specific exams, the approach involved sampling a letter choice at temperature 0 using the explanation already sampled, indicating a unique handling of these exams compared to others.", "evidence_reference": "For the AMC 10 and AMC 12 held-out test exams, we discovered a bug that limited response length. We fixed the bug and reran these exams to ensure accurate results. For most exam runs, we extract the model's letter choice directly from the explanation. For the GPT-4 USABO and SAT reading/writing runs (with and without vision), the GPT-3.5 runs, and the GPT-4 runs of SAT Math, GRE, USNCO, AP Biology, AP Chemistry, and AP Environmental Science without vision, we instead sample a letter choice at temperature 0 using the already-sampled explanation."}
{"question": "Consider the paper that introduces the method shown in the table is above the 'Magister et al' row but below the 'UL2' row. What specific loss function is used for the unified student model during the training on the chain of subquestion-solution pairs for each problem in the methodology for distilling reasoning capabilities into smaller models?", "answer": "auto-regressive language modeling loss", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2212.00193", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method shown in the table above method proposed by Magister et al but below UL2 method?", "answer_anchor": "DecomDistill", "question_reference": "In the methodology for distilling reasoning capabilities into smaller models, what specific loss function is used for the unified student model during the training on the chain of subquestion-solution pairs for each problem?", "explanation_reference": "The specific loss function used for the unified student model during training on the chain of subquestion-solution pairs for each problem is detailed in the methodology section under the description of the unified strategy. This loss function is an auto-regressive language modeling loss that computes the negative log likelihood of each token in the sequence of subquestion-solution pairs, given the previous tokens and the problem context.", "evidence_reference": "Given a step $j$ of problem $P$ (i.e., the concatenation of $q^{(j)}$ and $s^{(j)}$) consisting of a sequence of $m_j$ tokens $\\{x_j^{(1)}, \\dots, x_j^{(m_j)}\\}$, we use a typical auto-regressive language modeling loss, $\\mathcal{L}$:  \\begin{align} \\label{eq:loss_uni} & \\mathcal{L}_j(P) = & - \\sum_{k=1}^{m_j} \\log \\probP_{{uni}}\\ (x_j^{(k)}|x_j^{:(k-1)}, P) \\end{align} where $ \\probP_{{uni}}(x | c)$ is the probability assigned by $\\mathcal{M}_{uni}$ to token $x$ given context $c$, and $x^{:(y)}$ indicates the sequence $\\{x^{(1)}, \\dots, x^{(y)}\\}$."}
{"question": "Consider the paper that introduces the quant method that achieves a lower score than APQ-ViT but still scores higher than 76.0 on Deit-S with a Weight/Activation (W/A) precision of 6/6. What is the specific range of search space for \\(\\Delta_{\\text{R1}}^s\\) during post-softmax quantization as mentioned in the experiment settings?", "answer": "\\([\\frac{1}{2^{k}},\\frac{1}{2^{k+1}},...,\\frac{1}{2^{k+10}}]\\)", "figure": "locality/2310.16836/comparison_table.png", "anchor_arxiv_id": "2310.16836", "reference_arxiv_id": "2111.12293", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the quant method shows lower score than APQ-ViT but higher than 76.0 on Deit-S?", "answer_anchor": "PTQ4ViT", "question_reference": "What is the specific range of search space for \\(\\Delta_{\\text{R1}}^s\\) during post-softmax quantization as mentioned in the experiment settings?", "explanation_reference": "The question focuses on a detailed aspect of the experiment settings related to the quantization process, specifically the search space for the scaling factor \\(\\Delta_{\\text{R1}}^s\\) used in post-softmax quantization. This detail is crucial for understanding how the quantization parameters are optimized.", "evidence_reference": "For post-softmax quantization, the search space of \\(\\Delta_{\\text{R1}}^s\\) is \\([\\frac{1}{2^{k}},\\frac{1}{2^{k+1}},...,\\frac{1}{2^{k+10}}]\\)."}
{"question": "Consider the paper that introduces the method that has a score of 71.4 in the CB dataset with 4-shot prompting. What is the relative error reduction achieved by the model proposed in the paper when using a source prompt from MNLI for the BoolQ target task, despite its low cosine similarity?", "answer": "19.0%", "figure": "locality/2310.11670/comparison_2_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2110.07904", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the method having score of 71.4 in CB dataset with 4-shot prompting?", "answer_anchor": "SPoT", "question_reference": "What is the relative error reduction achieved by using a source prompt from MNLI for the BoolQ target task, despite its low cosine similarity?", "explanation_reference": "The relative error reduction of 19.0% for BoolQ using a source prompt from MNLI, despite a low cosine similarity of 0.4, highlights the effectiveness of task transferability beyond mere task similarity. This detail underscores the nuanced dynamics of prompt transfer effectiveness, where factors beyond straightforward task similarity can significantly influence transfer outcomes.", "evidence_reference": "In some cases (e.g., on BoolQ), we observe a large relative error reduction (19.0\\%, achieved by a source prompt of MNLI) despite a low cosine similarity (0.4). This suggests that factors other than task similarity (data size, task difficulty, domain similarity, etc.) may also play a role in determining transferability."}
{"question": "Consider the paper that introduces the model that shows the best overall performance in the 'Foreign' scenario. What specific condition under the DA-WR algorithm ensures the convergence of the cumulative distribution function resulting from the data augmentation process to the target distribution?", "answer": "The specific condition under the DA-WR algorithm that ensures the convergence of the cumulative distribution function resulting from the data augmentation process to the target distribution is the satisfaction of conditions {\\bf (C)-(C'')}.", "figure": "locality/2310.11634/average_relative_performance.png", "anchor_arxiv_id": "2310.11634", "reference_arxiv_id": "2302.09288", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model shown the best overall performance?", "answer_anchor": "LLaMA-30B", "question_reference": "What specific condition under the DA-WR algorithm ensures the convergence of the cumulative distribution function resulting from the data augmentation process to the target distribution?", "explanation_reference": "The specific conditions (C)-(C'') ensure the convergence of the cumulative distribution function resulting from the DA-WR algorithm to the target distribution by addressing the maximum difference in indicators and weights between the initial and augmented datasets, ensuring these differences diminish as the sample size increases.", "evidence_reference": "Assume that {\\bf (C)-(C'')} hold and that the support of  $F$ contains the support of  $F_0$. Then for all $x \\in {\\cal X}$, the cdf resulting from the DA-WR algorithm converges in probabilities to $F_0(x)$ as $n \\to +\\infty$."}
{"question": "Consider the paper that introduces the dataset that corresponds to the second chart from the left. What is the percentage of questions that can be answered using a boolean response?", "answer": "14%", "figure": "locality/2310.12836/ratio_figure.png", "anchor_arxiv_id": "2310.12836", "reference_arxiv_id": "2210.01613", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which dataset lies on the second left in the figure?", "answer_anchor": "Mintaka", "question_reference": "What is the percentage of Mintaka questions that can be answered using a boolean response?", "explanation_reference": "The percentage of questions in the Mintaka dataset that can be answered using a boolean (yes/no) response is directly provided in the dataset statistics.", "evidence_reference": "A majority (72\\%) of the questions in Mintaka can be answered using an entity. 14\\% can be answered using a boolean, in yes/no or comparative questions."}
{"question": "Consider the paper that introduces the method where the BIRD benchmark results in the highest execution accuracy for this method. What is the average time-saving percentage achieved by optimizing SQL queries using the two-stage optimization approach on the development set where ChatGPT accurately predicted the results, specifically under the benchmark which demonstrates the highest execution accuracy?", "answer": "77.75%", "figure": "locality/2310.18538/result_table.png", "anchor_arxiv_id": "2310.18538", "reference_arxiv_id": "2305.03111", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Under this method, which benchmark demonstrates the highest execution accuracy?", "answer_anchor": "BIRD", "question_reference": "What is the average time-saving percentage achieved by optimizing SQL queries using the two-stage optimization approach on the development set where ChatGPT accurately predicted the results?", "explanation_reference": "The average time-saving percentage of 77.75% is directly reported from the efficiency analysis section, where the paper discusses the results of optimizing SQL queries using a two-stage optimization approach on a selection of examples from the development set where ChatGPT accurately predicted the results.", "evidence_reference": "We observe that the two-stage optimization leads to an average time-saving of 77.75\\% while keeping the same results."}
{"question": "Consider the paper that introduces the method in the figure that corresponds to the penultimate row. How does the recency weighting in the complexity-regularized ILP method for graph generation from subtask state labels, as proposed by the model, influence the precondition inference?", "answer": "It assigns higher weight to data samples where a subtask has become eligible more recently, influencing the precondition inference by prioritizing recent eligibility over older instances.", "figure": "locality/2312.04668/comparison_table.png", "anchor_arxiv_id": "2312.04668", "reference_arxiv_id": "2302.08672", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method shown in the figure in the penultimate row?", "answer_anchor": "MSG^2", "question_reference": "How does the recency weighting in the complexity-regularized ILP method for graph generation from subtask state labels influence the precondition inference?", "explanation_reference": "The recency weighting modifies the precondition inference process by incorporating temporal information, specifically by giving more importance to recent subtask eligibility. This approach helps in dealing with the sparse and noisy data by emphasizing the most relevant information for determining subtask dependencies.", "evidence_reference": "w_{t, n} = \\max(0.1, \\lambda ^ {t_n-t}), where $0<\\lambda<1$ is the discount factor, $t_n$ is the time step when the precondition for subtask $n$ became satisfied."}
{"question": "Consider the paper that introduces the method that has 638K tunable parameters. How does its parameter efficiency compare to the adapter model in terms of scaling with the number of tasks and layers?", "answer": "The \\methodefficient model is more parameter-efficient compared to the \\adapter model, especially in settings with a large number of tasks and layers. This efficiency is due to the \\methodefficient model's use of shared hypernetworks across tasks and layers, which significantly reduces the number of parameters that scale with the number of tasks or layers.", "figure": "locality/2310.11670/comparison_table.png", "anchor_arxiv_id": "2310.11670", "reference_arxiv_id": "2106.04489", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which method has 638K tunable parameters?", "answer_anchor": "Hyperformer", "question_reference": "How does the \\methodefficient model's parameter efficiency compare to the \\adapter model in terms of scaling with the number of tasks and layers?", "explanation_reference": "The \\methodefficient model is more parameter-efficient compared to the \\adapter model because, for \\methodefficient, the total number of parameters for hypernetworks remains constant, while the task feature parameters scale with the number of tasks or layers times a small factor \\(t\\), where \\(t \\ll 2hd+2h\\) and \\(T+L \\ll TL\\), making it much more efficient especially in settings with a large number of layers and tasks.", "evidence_reference": "In settings with a large number of layers and a large number of tasks, since \\(t \\ll 2hd{+}2h\\) and \\(T{+}L \\ll TL\\), our method is much more parameter-efficient compared to \\adapter. In the current setting, the term \\(hd\\) is the largest term, and the factor \\(2TL\\) for \\adapter is larger than the factor \\(t\\) for \\methodefficient."}
{"question": "Consider the paper that introduces the method which is at the rightmost part of the figure. What specific computational advantage does GeDi's method of computing classification probabilities for next tokens have over a unidirectional classifier in terms of the number of forward passes required?", "answer": "GeDi's method can compute classification probabilities for every possible next token with only two parallel forward passes, whereas a unidirectional classifier would require a forward pass for every token in the vocabulary, resulting in orders of magnitude less computation.", "figure": "locality/2310.09520/comparison_figure.png", "anchor_arxiv_id": "2310.09520", "reference_arxiv_id": "2009.06367", "modal": "figure", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What is the method on the right most coordinates of the figure?", "answer_anchor": "GeDi", "question_reference": "What specific computational advantage does GeDi's method of computing classification probabilities for next tokens have over a unidirectional classifier in terms of the number of forward passes required?", "explanation_reference": "The computational advantage is highlighted by the fact that GeDi can compute the classification probabilities for every possible next token using significantly fewer computations compared to a unidirectional classifier, which would require a forward pass for each token in the vocabulary. This efficiency is achieved through GeDi's method of applying Bayes rule for partial sequences during generation, which allows for the computation to be done in two parallel forward passes for the desired and anti-control codes, significantly reducing the number of necessary computations.", "evidence_reference": "For typical vocab sizes of $20$k+, GeDi's online classification trick can compute $P_{\\theta}(c|x_t,x_{<t})$ for every possible next token $x_t$ on the order of 10k fold less computation as compared with a unidirectional classifier."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.6712 on the Hate dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of AI venue classification?", "answer": "0.9303959931770183", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which model gets mean classification accuracy 0.6712 on Hate dataset?", "answer_anchor": "DPT", "question_reference": "What is the Pearson's r correlation coefficient between word overlap and model performance for the task of AI venue classification?", "explanation_reference": "The Pearson's r correlation coefficient measures the strength of a linear relationship between two variables, in this case, word overlap and model performance for the AI venue classification task. A value of 0.9303959931770183 indicates a very strong positive correlation, suggesting that as word overlap increases, model performance also tends to increase.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that has a lower F1 score than LayoutXLM and a higher F1 score than SPADE. What specific architectural component in the model proposed by the paper is responsible for mapping the representation of each node into the number of target classes?", "answer": "Node Predictor", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2208.11168", "modal": "table", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having lower F1 score than LayoutXLM and high F1 score than SPADE?", "answer_anchor": "Doc2Graph", "question_reference": "What specific architectural component in Doc2Graph is responsible for mapping the representation of each node into the number of target classes?", "explanation_reference": "The Node Predictor is explicitly mentioned as the component responsible for mapping the representation of each node into the number of target classes, indicating its role in the classification process within the Doc2Graph framework.", "evidence_reference": "Node Predictor: this is a FC layer, that maps the representation of each node into the number of target classes;"}
{"question": "Consider the paper that introduces the method shown in the fifth row of the table. What is the model's, proposed by the paper, EM score using the reverse order of in-context examples on the NQ dataset?", "answer": "42.8", "figure": "locality/2310.06675/result_table.png", "anchor_arxiv_id": "2310.06675", "reference_arxiv_id": "2101.06804", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "Which method is shown in the fifth row in the table?", "answer_anchor": "KATE", "question_reference": "What is the EM score for KATE using the reverse order of in-context examples on the NQ dataset?", "explanation_reference": "The EM score for KATE using the reverse order of in-context examples on the NQ dataset directly reflects the model's performance under this specific condition, which is detailed in the ablation study on the effect of in-context example orders.", "evidence_reference": "For the default order, the example $A$ is to the left of example $B$ if $A$ is closer to the test prompt $x$ than $B$ in the embedding space. For the reverse order, the example A is to the right of example B... The reverse order performs the best... 42.8"}
{"question": "Consider the paper that introduces the model shown in the first row of the table. What specific advantage does the model proposed in the paper demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "answer": "The \\textsc{WPM-Joint} model demonstrates the specific advantage of enabling simpler deployment over the \\textsc{WPM-Sep} model for GWLAN tasks.", "figure": "locality/2310.14523/comparison_table.png", "anchor_arxiv_id": "2310.14523", "reference_arxiv_id": "2105.14913", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the model shown in the first row of the table?", "answer_anchor": "GWLAN", "question_reference": "What specific advantage does the \\textsc{WPM-Joint} model demonstrate over the \\textsc{WPM-Sep} model in terms of model deployment for GWLAN tasks?", "explanation_reference": "The \\textsc{WPM-Joint} model, by being a single model trained on multiple related tasks, demonstrates the advantage of simpler deployment compared to the \\textsc{WPM-Sep} model, which requires training and deploying four separate models for different translation contexts.", "evidence_reference": "Compared with \\textsc{WPM-Sep}, \\textsc{WPM-Joint} shows two advantages. On one hand, even there is only one model, \\textsc{WPM-Joint} yields better performances than \\textsc{WPM-Sep}, enabling simpler deployment."}
{"question": "Consider the paper that introduces the method that results in a score of 22.4 in the GSM8K dataset. What specific dynamic programming algorithm does the paper tweak for aligning the GPT and T5 tokenizers, and what is its original field of application?", "answer": "Needleman\u2013Wunsch algorithm, bioinformatics", "figure": "locality/2310.05074/result_table.png", "anchor_arxiv_id": "2310.05074", "reference_arxiv_id": "2301.12726", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "What is the method demonstrates score of 22.4 in GSM8K dataset?", "answer_anchor": "SpecialFT", "question_reference": "What specific dynamic programming algorithm does the paper tweak for aligning the GPT and T5 tokenizers, and what is its original field of application?", "explanation_reference": "The paper tweaks the Needleman\u2013Wunsch algorithm, originally used in bioinformatics for sequence alignment, to align the GPT and T5 tokenizers.", "evidence_reference": "Our dynamic program is a slight tweak of the textbook dynamic programming algorithms used in bioinformatics for sequence alignment (such as the Needleman\u2013Wunsch algorithm)"}
{"question": "Consider the paper that introduces the method that has an F1 score of 54.83. What specific strategy does the model proposed in the paper employ to unify multilingual multimodal inputs efficiently?", "answer": "LayoutXLM employs a strategy of obtaining character-level bounding boxes after tokenization using SentencePiece with a unigram language model, and then calculating the bounding box of each token by merging the bounding boxes of all characters it contains. This approach efficiently unifies the multilingual multimodal inputs.", "figure": "locality/2310.11016/comparison_table.png", "anchor_arxiv_id": "2310.11016", "reference_arxiv_id": "2104.08836", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "What is the method having 54.84 F1 score?", "answer_anchor": "LayoutXLM", "question_reference": "What specific strategy does LayoutXLM employ to unify multilingual multimodal inputs efficiently?", "explanation_reference": "LayoutXLM employs a strategy of obtaining character-level bounding boxes for unifying multilingual multimodal inputs efficiently. This approach allows the model to handle the diversity in the definition of linguistic units across different languages without language-specific preprocessing.", "evidence_reference": "However, for LayoutXLM, this strategy is not applicable because the definition of the linguistic unit is different from language to language. To prevent the language-specific pre-processing, we decide to obtain the character-level bounding boxes."}
{"question": "Consider the paper that introduces the model that achieves a mean classification accuracy of 0.6712 on the Hate dataset. What is the Pearson's r correlation coefficient between word overlap and the model's performance for the task of political affiliation classification on Twitter data?", "answer": "0.9817159316285563", "figure": "locality/2310.10191/classification_accuracy_table.png", "anchor_arxiv_id": "2310.10191", "reference_arxiv_id": "2111.07408", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which model gets mean classification accuracy 0.6712 on Hate dataset?", "answer_anchor": "DPT", "question_reference": "Based on the findings, what is the Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data?", "explanation_reference": "The Pearson's r correlation coefficient between word overlap and model performance for the task of political affiliation classification on Twitter data indicates the strength of the linear relationship between the vocabulary overlap across time periods and the performance of the model on this specific task. A value close to 1 suggests a strong positive correlation.", "evidence_reference": "In addition to measuring vocabularies' change over time in Section ~\\ref{sec:vocabshift}, we find correlations between the word overlap and model performance of each task in Table~\\ref{tab:overlapcorr}."}
{"question": "Consider the paper that introduces the method that is in the second row of the table. In the context of the paper's methodological approach, why might the use of Locally Linear Embedding (LLE) for regularizing the space of label prototypes be considered less appropriate compared to the clustering-based strategy represented by ConCN clusters?", "answer": "The use of Locally Linear Embedding (LLE) for regularizing the space of label prototypes might be considered less appropriate compared to the proposed clustering-based strategy because LLE imposes the condition that prototypes with similar labels should themselves also be similar, which is not always appropriate given the nature of ultra-fine entity typing where labels can denote mutually exclusive categories despite being semantically similar.", "figure": "locality/2310.14793/comparison_table.png", "anchor_arxiv_id": "2310.14793", "reference_arxiv_id": "2305.12802", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "Which method is in the second row of the table?", "answer_anchor": "ConCN clusters", "question_reference": "In the context of the paper's methodological approach, why might the use of Locally Linear Embedding (LLE) for regularizing the space of label prototypes be considered less appropriate compared to the proposed clustering-based strategy?", "explanation_reference": "The paper critiques the use of LLE for regularizing the space of label prototypes because it enforces a similarity condition on prototypes based on label similarity, which does not align with the paper's observation that similar labels do not always imply positive correlation or similarity in the context of ultra-fine entity typing. This critique highlights the limitation of LLE in handling the nuanced relationships between labels in UFET, making the proposed clustering-based strategy more suitable as it does not require prototypes of similar labels to be similar.", "evidence_reference": "While adding LLE to the base UFET model has a small positive effect, we can see that the performance is much worse than that of our proposed clustering based strategy (DL). The two post-processing techniques (missing and CN) both have a positive effect on the overall performance, although they have a smaller impact than the DL strategy."}
{"question": "Consider the paper that introduces the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage category. What is the success rate of targeted in-context knowledge updating for FEVER when using a prompt composed of Original Examples + Edited Relevant Examples + Edited Irrelevant Examples, employing IC-Retrieval?", "answer": "99.9", "figure": "locality/2310.07343/result_figure.png", "anchor_arxiv_id": "2310.07343", "reference_arxiv_id": "2210.09150", "modal": "figure", "anchor_reasoning_type": "Visual Understanding", "reference_reasoning_type": "Results and Data Interpretation", "question_anchor": "What is the first method shown in Explicit --> Retrieval-enhanced --> Single-Stage?", "answer_anchor": "IC-Retrieval", "question_reference": "What is the success rate of targeted in-context knowledge updating for FEVER when using a prompt composed of Original Examples + Edited Relevant Examples + Edited Irrelevant Examples?", "explanation_reference": "The success rate indicates how effectively GPT-3 can update its knowledge to predict the updated answer for questions related to a specific piece of knowledge, while not changing its answer for unrelated questions. The prompt design incorporating Original Examples + Edited Relevant Examples + Edited Irrelevant Examples achieves a success rate of 99.9% for FEVER, indicating high effectiveness in targeted knowledge updating.", "evidence_reference": "From Table~\\ref{tab:knowledge_edit_full}, we see that different prompts give vastly different results. Specifically, using only the original examples in the prompt leads to relatively poor success rate (especially on FEVER), while adding edited relevant examples in the prompt leads to better success rate, it leads the model to over-rely on the knowledge updates even on irrelevant questions. However, when incorporating all cases of original examples, edited relevant and irrelevant examples in the prompt, GPT-3 is able to achieve high editing success rate and low drawdown on irrelevant questions."}
{"question": "Consider the paper that introduces the method that is in the last row of the upper half of the table. How does the model proposed in the paper ensure the selection of high-quality synthetic data over poor-quality data during training?", "answer": "The MetaBLINK model ensures the selection of high-quality synthetic data over poor-quality data during training by designing a meta-learning mechanism to automatically assign different weights to each synthetic entity-mention pair.", "figure": "locality/2310.12444/comparison_table.png", "anchor_arxiv_id": "2310.12444", "reference_arxiv_id": "2207.05280", "modal": "table", "anchor_reasoning_type": "Location", "reference_reasoning_type": "Critical Analysis", "question_anchor": "Which method is on the last row of the upper half of the table?", "answer_anchor": "MetaBINK", "question_reference": "How does the MetaBLINK model ensure the selection of high-quality synthetic data over poor-quality data during training?", "explanation_reference": "The MetaBLINK model utilizes a meta-learning mechanism to differentiate the quality of synthetic data by assigning weights to them. This process is guided by the performance impact of the synthetic data on a small set of high-quality seed data in the target domain. The mechanism ensures that synthetic data which positively influences the model's performance on the seed data is given higher weight, thereby prioritizing high-quality synthetic data during training.", "evidence_reference": "Through exact matching and mention rewriting, we have a large number of training instances in the few-shot domain. And in the section \\ref{entity-link-model}, we give a deep learning based entity linking framework, which can be used to train on the weakly supervised dataset. However, part of the instances are noisy and may deteriorate the entity linking model's performance in the target domain. Therefore, we need to find an effective way to select suitable instances for the training process. Previous studies have shown that meta-learning can be used to reweight the instances during the training process automatically and gained relative success in neural ranking~\\cite{DBLP:conf/icml/RenZYU18}. Therefore, we adopt a meta-learning method to reweight these synthetic data in this section."}
{"question": "Consider the paper that introduces the benchmark that has a higher 'Generation Token Length' than ShareGPT and GSM8k. What specific improvement does CoT prompting provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "answer": "CoT prompting enables a significant improvement in performance on the 'Tracking Shuffled Objects' task, with a gain of +28.5% accuracy over answer-only prompting for the Codex model.", "figure": "locality/2310.05736/result_figure.png", "anchor_arxiv_id": "2310.05736", "reference_arxiv_id": "2210.09261", "modal": "figure", "anchor_reasoning_type": "Comparison", "reference_reasoning_type": "Implications and Inferences", "question_anchor": "Which benchmark has the higher 'Generation Token Length' than ShareGPT and GSM8k?", "answer_anchor": "BBH", "question_reference": "What specific improvement does CoT prompting provide over answer-only prompting for the Codex model on the 'Tracking Shuffled Objects' task?", "explanation_reference": "The improvement is calculated by comparing the performance increase from answer-only prompting to CoT prompting for the Codex model on the 'Tracking Shuffled Objects' task. This is derived from the performance metrics provided, showing a significant gain when using CoT prompting.", "evidence_reference": ""}
{"question": "Consider the paper that introduces the method for which the BLEU-1 score is missing in the table. How does the performance of the model proposed in the paper, using ground-truth concepts for captioning, compare to using predicted concepts or a mixture of both during training?", "answer": "Using predicted concepts leads to optimal results.", "figure": "locality/2311.08223/result_table.png", "anchor_arxiv_id": "2311.08223", "reference_arxiv_id": "2112.05230", "modal": "table", "anchor_reasoning_type": "Data Extraction", "reference_reasoning_type": "Methodological Analysis", "question_anchor": "What method does not provide BLEU-1 score?", "answer_anchor": "ViTCAP", "question_reference": "How does the performance of ViTCAP using ground-truth concepts for captioning compare to using predicted concepts or a mixture of both during training?", "explanation_reference": "The experiment demonstrates that training with predicted concepts yields better performance than using ground-truth concepts or a mixture of both, indicating the effectiveness of the CTN in generating useful concepts for captioning.", "evidence_reference": "We experiment with different ways to train with the concept tokens. In Table~\\ref{tab:concept}, we list the results of training using GT semantic concepts encoded as tokens, GT concepts mixed with predicted concepts, and fully predicted concepts. We find that by using the predicted concepts for training leads to optimal results."}
